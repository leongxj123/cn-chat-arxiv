<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SIM-FSVGD&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#25104;&#21151;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#65292;&#33021;&#22815;&#22312;&#20302;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#24615;&#33021;&#36187;&#36710;&#31995;&#32479;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16644</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Sim-to-Real Gap with Bayesian Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16644
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SIM-FSVGD&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#25104;&#21151;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#65292;&#33021;&#22815;&#22312;&#20302;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#24615;&#33021;&#36187;&#36710;&#31995;&#32479;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SIM-FSVGD&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;SIM-FSVGD&#21033;&#29992;&#20302;&#20445;&#30495;&#24230;&#30340;&#29289;&#29702;&#20808;&#39564;&#65292;&#22914;&#27169;&#25311;&#22120;&#30340;&#24418;&#24335;&#65292;&#26469;&#35268;&#33539;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#24050;&#32463;&#23398;&#20064;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#65292;SIM-FSVGD&#22312;&#26356;&#22810;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#33021;&#22815;&#25193;&#23637;&#21644;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23398;&#20064;&#38544;&#24335;&#29289;&#29702;&#20808;&#39564;&#23548;&#33268;&#20934;&#30830;&#30340;&#24179;&#22343;&#27169;&#22411;&#20272;&#35745;&#20197;&#21450;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SIM-FSVGD&#22312;&#39640;&#24615;&#33021;RC&#36187;&#36710;&#31995;&#32479;&#19978;&#32553;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#24046;&#36317;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39640;&#24230;&#21160;&#24577;&#30340;&#20572;&#36710;&#36716;&#21521;&#21160;&#20316;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#29616;&#26377;&#25216;&#26415;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16644v1 Announce Type: cross  Abstract: We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;Agent Actor Critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23545;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#29616;&#20102;&#25913;&#21892;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11914</link><description>&lt;p&gt;
&#21333;Agent Actor Critic&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Single-Agent Actor Critic for Decentralized Cooperative Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;Agent Actor Critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23545;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#29616;&#20102;&#25913;&#21892;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#20132;&#36890;&#31649;&#29702;&#32467;&#21512;&#33258;&#20027;&#36710;&#36742;&#65288;AVs&#65289;&#25215;&#35834;&#26410;&#26469;&#25317;&#26377;&#20943;&#23569;&#25317;&#22581;&#21644;&#22686;&#24378;&#20132;&#36890;&#27969;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#24320;&#21457;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#36830;&#32493;&#20132;&#36890;&#27969;&#37327;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25512;&#21160;&#20027;&#21160;&#20132;&#36890;&#31649;&#29702;&#39046;&#22495;&#26397;&#30528;&#26356;&#22823;&#31243;&#24230;&#30340;&#21435;&#20013;&#24515;&#21270;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;actor-critic&#27169;&#22411;&#65292;&#26088;&#22312;&#21033;&#29992;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#33258;&#20027;&#36710;&#36742;&#30340;&#21435;&#20013;&#24515;&#21270;&#21512;&#20316;&#39550;&#39542;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20855;&#26377;&#25513;&#30721;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#23454;&#38469;&#20132;&#36890;&#27969;&#37327;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#38024;&#23545;&#22522;&#32447;&#25511;&#21046;&#22120;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#22312;&#36947;&#36335;&#31995;&#32479;&#20869;&#19981;&#21516;&#29942;&#39048;&#20301;&#32622;&#25913;&#21892;&#20132;&#36890;&#27969;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11914v1 Announce Type: new  Abstract: Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow. However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability. To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability. Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system. Additionally, we explore t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#65292;&#21487;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21644;&#20854;&#23545;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04118</link><description>&lt;p&gt;
&#20840;&#23616;&#31283;&#23450;&#30340;&#31070;&#32463;&#20223;&#30495;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Globally Stable Neural Imitation Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#65292;&#21487;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21644;&#20854;&#23545;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20174;&#22836;&#24320;&#22987;&#22312;&#35299;&#20915;&#31354;&#38388;&#20013;&#23398;&#20064;&#25919;&#31574;&#30340;&#36164;&#28304;&#23494;&#38598;&#21644;&#32791;&#26102;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#32467;&#26524;&#25919;&#31574;&#21487;&#20197;&#21487;&#38752;&#22320;&#27169;&#20223;&#19987;&#23478;&#28436;&#31034;&#65292;&#20294;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#26410;&#25506;&#32034;&#21306;&#22495;&#20013;&#24120;&#24120;&#32570;&#20047;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#32473;&#22312;&#38754;&#23545;&#25200;&#21160;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#25919;&#31574;&#26550;&#26500;&#65292;&#20419;&#36827;&#22522;&#20110;&#26446;&#20122;&#26222;&#35834;&#22827;&#23450;&#29702;&#30340;&#31283;&#23450;&#24615;&#34920;&#31034;&#65292;&#24182;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21450;&#20854;&#30456;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#65292;&#20197;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;&#25919;&#31574;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#26800;&#25163;&#33218;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SNDS&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04118v1 Announce Type: cross  Abstract: Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space. Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations. To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees. We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability. We validate our approach by conducting extensive experiments in simulation and successfully deploying the trained policies on a real-world manipulator arm. The experimental resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15173</link><description>&lt;p&gt;
&#26080;&#30171;&#20154;&#24037;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#38454;&#24494;&#35843;&#65306;&#19968;&#31181;&#22522;&#20110;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32972;&#21521;&#20256;&#25773;&#36807;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;GPU&#20869;&#23384;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36716;&#21521;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21463;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#21442;&#25968;&#26354;&#29575;&#30340;&#24322;&#36136;&#24615;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#21033;&#29992;&#23545;&#35282;Hessian&#22686;&#24378;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;LLMs&#24494;&#35843;&#30340;&#24037;&#20316;&#12290;HiZOO&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#19988;&#27599;&#27493;&#21482;&#22686;&#21152;&#20102;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#23545;&#21508;&#31181;&#27169;&#22411;&#65288;350M&#12316;66B&#21442;&#25968;&#65289;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;HiZOO&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;HiZOO&#22312;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.13108</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Gradient Descent for Large Learning Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#29702;&#35299;&#8220;&#31283;&#23450;&#24615;&#36793;&#32536;&#65288;EoS&#65289;&#8221;&#29616;&#35937;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#65292;&#36825;&#19968;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#20854;&#29305;&#28857;&#26159;&#25439;&#22833;&#20989;&#25968;&#22312;&#19981;&#21516;&#32426;&#20803;&#38388;&#30340;&#38750;&#21333;&#35843;&#19979;&#38477;&#65292;&#32780;&#25439;&#22833;&#30340;&#38497;&#23789;&#24230;&#65288;Hessian&#30340;&#35889;&#33539;&#25968;&#65289;&#36880;&#28176;&#25509;&#36817;&#24182;&#31283;&#23450;&#22312;2/(&#23398;&#20064;&#29575;)&#38468;&#36817;&#12290;&#26368;&#36817;&#26377;&#20154;&#25552;&#20986;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#20986;&#29616;EoS&#30340;&#21407;&#22240;&#8212;&#8212;&#27839;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#38468;&#36817;&#32570;&#20047;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#21516;&#26102;&#23384;&#22312;&#32039;&#33268;&#30340;&#27491;&#21521;&#19981;&#21464;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#20248;&#21270;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#31532;&#19968;&#20010;&#20551;&#35774;&#20197;&#21450;&#31532;&#20108;&#20010;&#20551;&#35774;&#30340;&#19968;&#20010;&#24517;&#35201;&#26465;&#20214;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#26500;&#25104;&#19968;&#20010;&#20809;&#28369;&#27969;&#24418;&#65292;&#24182;&#19988;&#31283;&#23450;&#30340;&#26497;&#23567;&#20540;&#26500;&#25104;&#26377;&#30028;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i
&lt;/p&gt;</description></item><item><title>&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2402.12189</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#25104;&#21592;&#36164;&#26684;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12189
&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(LMs)&#30001;&#20110;&#25968;&#25454;&#35760;&#24518;&#32780;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#12290;&#35813;&#31574;&#30053;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20854;&#30446;&#30340;&#26159;&#21152;&#24378;LM&#23545;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#30041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#25910;&#38598;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#30446;&#26631;LM&#30340;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#25152;&#34920;&#31034;&#30340;&#25104;&#21592;&#36817;&#20284;&#20540;&#20026;&#36825;&#20123;&#29983;&#25104;&#25991;&#26412;&#20351;&#29992;&#20266;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;LM&#20197;&#25903;&#25345;&#37027;&#20123;&#26356;&#26377;&#21487;&#33021;&#28304;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26681;&#25454;&#20854;&#25104;&#21592;&#36164;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11228</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#20248;&#21270;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Balancing for Optimal Random Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11228
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#26862;&#26519;&#36890;&#24120;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24212;&#24615;&#65292;&#25110;&#22312;&#31616;&#21333;&#12289;&#24179;&#28369;&#24773;&#26223;&#19979;&#22833;&#21435;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26641;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;Lipschitz&#31867;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#24179;&#28369;&#24615;&#27700;&#24179;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35813;&#29256;&#26412;&#22312;&#20219;&#24847;$q \in \mathbb{N}$&#21644;$\beta \in (0,1]$&#30340;H&#246;lder&#31867;$\mathcal{H}^{q,\beta}$&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24179;&#34913;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36807;&#24230;&#20381;&#36182;&#36741;&#21161;&#38543;&#26426;&#24615;&#21487;&#33021;&#20250;&#25439;&#23475;&#26641;&#27169;&#22411;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#26356;&#24179;&#34913;&#12289;&#26356;&#23569;&#38543;&#26426;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10802</link><description>&lt;p&gt;
TimeSeriesBench&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10802
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#21644;&#35268;&#27169;&#30340;&#34067;&#24310;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#30340;&#38656;&#27714;&#30456;&#27604;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#24403;&#21069;&#31639;&#27861;&#36890;&#24120;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#65292;&#28982;&#32780;&#22312;&#20855;&#26377;&#25968;&#20197;&#19975;&#35745;&#26354;&#32447;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#31995;&#32479;&#20013;&#65292;&#32500;&#25252;&#36825;&#20040;&#22810;&#27169;&#22411;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24615;&#33021;&#23578;&#19981;&#26126;&#30830;&#12290;&#22823;&#22810;&#25968;TSAD&#27169;&#22411;&#37117;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21382;&#21490;&#37096;&#20998;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20854;&#26410;&#26469;&#37096;&#20998;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#65292;&#32463;&#24120;&#37096;&#32626;&#21644;&#21319;&#32423;&#31995;&#32479;&#65292;&#27599;&#22825;&#37117;&#20250;&#20986;&#29616;&#26032;&#30340;&#12289;&#20197;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25152;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#26102;&#38388;&#24207;&#21015;&#30340;&#24615;&#33021;&#20063;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10802v1 Announce Type: new  Abstract: Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09146</link><description>&lt;p&gt;
ResQuNNs: &#23454;&#29616;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QuNNs&#65289;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#24182;&#35299;&#20915;&#19982;&#20854;&#30456;&#20851;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;quanvolutional&#23618;&#34429;&#28982;&#26377;&#21161;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20294;&#24448;&#24448;&#26159;&#38745;&#24577;&#30340;&#65292;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#36825;&#20123;&#23618;&#20869;&#37096;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;QuNNs&#30340;&#28789;&#27963;&#24615;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#30340;&#24341;&#20837;&#32473;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#35775;&#38382;&#26799;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Residual Quanvolutional Neural Networks (ResQuNNs)&#65292;&#21033;&#29992;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#28155;&#21152;&#36339;&#36807;&#36830;&#25509;&#20197;&#20419;&#36827;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04845</link><description>&lt;p&gt;
AlphaFold&#36935;&#21040;Flow Matching&#29983;&#25104;&#34507;&#30333;&#36136;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
AlphaFold Meets Flow Matching for Generating Protein Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#29983;&#29289;&#21151;&#33021;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#24577;&#32467;&#26500;&#38598;&#21512;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#39640;&#31934;&#24230;&#30340;&#21333;&#24577;&#39044;&#27979;&#22120;&#65292;&#22914;AlphaFold&#21644;ESMFold&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#27969;&#21305;&#37197;&#26694;&#26550;&#19979;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#24207;&#21015;&#26465;&#20214;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#12290;&#22312;PDB&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;AlphaFold&#21644;MSA&#23376;&#37319;&#26679;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#21512;&#12290;&#24403;&#36827;&#19968;&#27493;&#35757;&#32451;&#25152;&#26377;&#21407;&#23376;MD&#30340;&#32452;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#26410;&#35265;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#12289;&#20301;&#32622;&#20998;&#24067;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26356;&#24555;&#30340;&#26102;&#38047;&#25910;&#25947;&#36895;&#24230;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#27604;&#22797;&#21046;&#30340;MD&#36712;&#36857;&#26356;&#20855;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25104;&#21151;&#29983;&#25104;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;EuroPED&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#39564;&#35777;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00760</link><description>&lt;p&gt;
EuroPED-NN: &#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EuroPED-NN: Uncertainty aware surrogate model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25104;&#21151;&#29983;&#25104;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;EuroPED&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29289;&#29702;&#39564;&#35777;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20808;&#39564;&#65288;BNN-NCP&#65289;&#25216;&#26415;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#23545;EuroPED&#31561;&#31163;&#23376;&#20307;&#24213;&#24231;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;JET-ILW&#24213;&#24231;&#25968;&#25454;&#24211;&#21644;&#21518;&#32493;&#27169;&#22411;&#35780;&#20272;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#31216;&#20026;EuroPED-NN&#12290;BNN-NCP&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#22909;&#36873;&#25321;&#65292;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#20379;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#31361;&#20986;&#26174;&#31034;&#20986;&#20998;&#24067;&#33539;&#22260;&#22806;&#65288;OOD&#65289;&#21306;&#22495;&#12290;&#36825;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;EuroPED-NN&#24050;&#32463;&#24471;&#21040;&#20102;&#29289;&#29702;&#39564;&#35777;&#65292;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#30005;&#23376;&#23494;&#24230;$n_e\!\left(\psi_{\text{pol}}=0.94\right)$&#38543;&#31561;&#31163;&#23376;&#20307;&#30005;&#27969;$I_p$&#30340;&#22686;&#21152;&#32780;&#21464;&#21270;&#65292;&#24182;&#39564;&#35777;&#20102;&#19982;EuroPED&#27169;&#22411;&#30456;&#20851;&#30340;$\Delta-\beta_{p,ped}$&#20851;&#31995;&#12290;&#36825;&#35777;&#23454;&#20102;&#20195;&#29702;&#27169;&#22411;&#25152;&#23398;&#21040;&#30340;&#24213;&#23618;&#29289;&#29702;&#23398;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties. This provides critical insights into model robustness and reliability. EuroPED-NN has been physically validated, first, analyzing electron density $n_e\!\left(\psi_{\text{pol}}=0.94\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\Delta-\beta_{p,ped}$ relation associated with the EuroPED model. Affirming the robustness of the underlying physics learned by the surrogate mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.12843</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#35745;&#31639;&#30340;&#26102;&#38388;&#22270;
&lt;/p&gt;
&lt;p&gt;
An embedding-based distance for temporal graphs. (arXiv:2401.12843v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#20351;&#29992;&#26102;&#38388;&#23562;&#37325;&#30340;&#38543;&#26426;&#28216;&#36208;&#26500;&#24314;&#30340;&#22270;&#23884;&#20837;&#26469;&#23450;&#20041;&#20102;&#19968;&#31181;&#26102;&#38388;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21305;&#37197;&#22270;&#21644;&#19981;&#21305;&#37197;&#22270;&#30340;&#24773;&#20917;&#65292;&#24403;&#23384;&#22312;&#24050;&#30693;&#30340;&#33410;&#28857;&#20851;&#31995;&#26102;&#65292;&#20197;&#21450;&#24403;&#19981;&#23384;&#22312;&#35813;&#20851;&#31995;&#24182;&#19988;&#22270;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#26102;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#26102;&#38388;&#32593;&#32476;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#36317;&#31163;&#23450;&#20041;&#30340;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#30340;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#31561;&#36890;&#20449;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.12801</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#22522;&#20110;&#30446;&#26631;&#21040;&#29992;&#25143;&#20851;&#32852;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems. (arXiv:2401.12801v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#31561;&#36890;&#20449;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#20013;&#65292;&#23558;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#29992;&#25143;&#35774;&#22791;&#65288;UEs&#65289;&#36827;&#34892;&#21305;&#37197;&#23545;&#20110;&#20960;&#31181;&#36890;&#20449;&#20219;&#21153;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#22914;&#20027;&#21160;&#20999;&#25442;&#21644;&#27874;&#26463;&#39044;&#27979;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#38647;&#36798;&#36741;&#21161;&#36890;&#20449;&#31995;&#32479;&#65292;&#19968;&#20010;&#22522;&#31449;&#65288;BS&#65289;&#37197;&#22791;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#38647;&#36798;&#65292;&#38647;&#36798;&#20855;&#26377;&#21452;&#37325;&#30446;&#26631;&#65306;&#65288;i&#65289;&#23558;&#36710;&#36742;&#38647;&#36798;&#30446;&#26631;&#19982;&#36890;&#20449;&#27874;&#26463;&#31354;&#38388;&#20013;&#30340;&#36710;&#36742;&#35774;&#22791;&#65288;VEs&#65289;&#20851;&#32852;&#36215;&#26469;&#65292;&#65288;ii&#65289;&#26681;&#25454;&#38647;&#36798;&#25968;&#25454;&#39044;&#27979;&#27599;&#20010;VE&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#12290;&#25552;&#20986;&#30340;&#30446;&#26631;&#21040;&#29992;&#25143;&#65288;T2U&#65289;&#20851;&#32852;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#36317;&#31163;-&#35282;&#24230;&#22270;&#20687;&#26816;&#27979;&#36710;&#36742;&#38647;&#36798;&#30446;&#26631;&#65292;&#24182;&#20026;&#27599;&#20010;&#30446;&#26631;&#20272;&#35745;&#19968;&#20010;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#12290;&#28982;&#21518;&#65292;&#23558;&#25512;&#26029;&#24471;&#21040;&#30340;&#27599;&#20010;&#30446;&#26631;&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#19982;BS&#29992;&#20110;&#36890;&#20449;&#30340;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#25191;&#34892;&#30446;&#26631;&#21040;&#29992;&#25143;&#65288;T2U&#65289;&#20851;&#32852;&#12290;&#36890;&#36807;&#20462;&#25913;&#20320;&#21482;&#30475;&#33080;&#37096;&#32593;&#32476;&#65288;YOLO&#65289;&#31639;&#27861;&#23454;&#29616;&#20102;&#32852;&#21512;&#22810;&#30446;&#26631;&#26816;&#27979;&#21644;&#27874;&#26463;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Integrated Sensing and Communication (ISAC) systems, matching the radar targets with communication user equipments (UEs) is functional to several communication tasks, such as proactive handover and beam prediction. In this paper, we consider a radar-assisted communication system where a base station (BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a double aim: (i) associate vehicular radar targets to vehicular equipments (VEs) in the communication beamspace and (ii) predict the beamforming vector for each VE from radar data. The proposed target-to-user (T2U) association consists of two stages. First, vehicular radar targets are detected from range-angle images, and, for each, a beamforming vector is estimated. Then, the inferred per-target beamforming vectors are matched with the ones utilized at the BS for communication to perform target-to-user (T2U) association. Joint multi-target detection and beam inference is obtained by modifying the you only look
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05737</link><description>&lt;p&gt;
HVAC&#25511;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#26159;&#21830;&#19994;&#21644;&#23621;&#20303;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#21453;&#24212;&#24335;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#27604;&#24615;&#30340;&#26631;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;Sinergym&#26694;&#26550;&#65292;&#20197;&#33298;&#36866;&#24230;&#21644;&#33021;&#28304;&#28040;&#32791;&#20026;&#35780;&#21028;&#26631;&#20934;&#65292;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HVAC&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#21644;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#26816;&#26597;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;SAC&#21644;TD3&#31561;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02698</link><description>&lt;p&gt;
&#25506;&#32034;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#30340;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20943;&#23569;&#33258;&#36866;&#24212;&#26080;&#20559;&#23458;&#25143;&#37319;&#26679;&#26041;&#24046;&#65292;&#25506;&#32034;&#20102;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#23545;&#19968;&#37096;&#20998;&#23458;&#25143;&#36827;&#34892;&#37319;&#26679;&#26469;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;&#26469;&#33258;&#37319;&#26679;&#23458;&#25143;&#30340;&#20449;&#24687;&#24314;&#31435;&#20840;&#23616;&#27169;&#22411;&#30340;&#20840;&#23616;&#20272;&#35745;&#26041;&#24046;&#19982;&#32852;&#37030;&#20248;&#21270;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#8220;&#20813;&#36153;&#8221;&#30340;&#33258;&#36866;&#24212;&#23458;&#25143;&#37319;&#26679;&#25216;&#26415;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#26500;&#24314;&#20102;&#26377;&#21069;&#36884;&#30340;&#37319;&#26679;&#27010;&#29575;&#21644;&#21487;&#38752;&#30340;&#20840;&#23616;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#25429;&#25417;&#20102;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#23567;&#21464;&#20307;&#65292;&#24182;&#30456;&#24212;&#25913;&#36827;&#20102;&#20840;&#23616;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Vib&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#20248;&#21270;&#20013;&#36981;&#24490;&#23458;&#25143;&#37319;&#26679;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#22312;&#36890;&#20449;&#39044;&#31639;K&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32447;&#24615;&#36895;&#29575;&#19978;&#21319;&#65292;&#20855;&#26377;&#36951;&#25022;&#36793;&#30028;$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{}3}\big)$&#12290;&#32467;&#26524;&#26159;&#65292;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of "free" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04565</link><description>&lt;p&gt;
&#35299;&#25918;&#22270;&#23398;&#20064;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#21644;&#24212;&#29992;&#65292;&#20294;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21644;&#22312;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#20219;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#38754;&#23545;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#23478;&#20204;&#22312;&#36817;&#24180;&#26469;&#35774;&#35745;&#20102;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#20182;&#20204;&#36824;&#23454;&#26045;&#20102;&#22270;&#20013;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;AutoGraph&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20182;&#20204;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#31649;&#29702;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#65292;&#65288;2&#65289;&#22788;&#29702;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#30340;&#27969;&#31243;&#65288;&#36229;&#36807;&#26550;&#26500;&#35774;&#35745;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;AutoGraph&#26102;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#26469;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29992;&#25143;&#35831;&#27714;&#65288;&#35813;&#35831;&#27714;&#21487;&#33021;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#32536;&#25110;&#22270;&#32423;&#21035;&#30340;&#19981;&#21516;&#25968;&#25454;&#21644;&#23398;&#20064;&#30446;&#26631;&#65289;&#65292;&#22797;&#26434;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#23558;&#30001;LLM&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03919</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery. (arXiv:2309.03919v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#28508;&#22312;&#33647;&#29289;&#20998;&#23376;&#19982;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#34507;&#30333;&#30452;&#25509;&#24433;&#21709;&#30142;&#30149;&#30340;&#36827;&#23637;&#26102;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#32467;&#21512;&#20146;&#21644;&#21147;&#38656;&#35201;&#26174;&#33879;&#30340;&#36130;&#21153;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26032;&#20852;&#30340;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22266;&#26377;&#30340;&#24182;&#34892;&#24615;&#21644;&#31649;&#29702;&#25968;&#25454;&#32500;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25910;&#25947;&#31283;&#23450;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. S
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16848</link><description>&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#20013;&#28608;&#21457;&#24577;&#30340;&#33258;&#28982;&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#30340;&#26368;&#20302;&#28608;&#21457;&#24577;&#65292;&#36825;&#26159;&#23545;&#23547;&#25214;&#22522;&#24577;&#30340;&#20272;&#35745;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#35813;&#26041;&#27861;&#27809;&#26377;&#33258;&#30001;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26174;&#24335;&#27491;&#20132;&#21270;&#19981;&#21516;&#30340;&#24577;&#65292;&#32780;&#26159;&#23558;&#23547;&#25214;&#32473;&#23450;&#31995;&#32479;&#30340;&#28608;&#21457;&#24577;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#19981;&#21516;&#24577;&#20043;&#38388;&#30340;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#65292;&#22914;&#36291;&#36801;&#20598;&#26497;&#30697;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#23436;&#20840;&#36890;&#29992;&#65292;&#20294;&#19982;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#30005;&#23376;&#31995;&#32479;&#21464;&#20998;&#21442;&#25968;&#30340;&#24037;&#20316;&#32467;&#21512;&#20351;&#29992;&#25928;&#26524;&#29305;&#21035;&#22909;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#19982;FermiNet&#21644;Psiformer&#21464;&#20998;&#21442;&#25968;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#33519;&#31561;&#22823;&#20998;&#23376;&#30340;&#22402;&#30452;&#28608;&#21457;&#33021;&#21644;&#25391;&#23376;&#24378;&#24230;&#12290;&#38500;&#20102;&#22312;&#20998;&#23376;&#19978;&#30340;&#31034;&#20363;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;...
&lt;/p&gt;
&lt;p&gt;
We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.06590</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#37327;&#21270;&#25919;&#31574;&#38271;&#26399;&#32489;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#30001;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21442;&#25968;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#20540;&#20989;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#20998;&#26512;&#38480;&#21046;&#22312;&#23569;&#25968;&#20998;&#24067;&#20540;&#19978;&#65292;&#25110;&#32773;&#32422;&#26463;&#20998;&#24067;&#24418;&#29366;&#65292;&#20363;&#22914;&#65292;&#39640;&#26031;&#20998;&#24067;&#12290;&#21463;&#21040;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;Bellman&#31639;&#23376;&#65292;&#20854;&#22266;&#23450;&#28857;&#26159;&#20540;&#20998;&#24067;&#20989;&#25968;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Epistemic Quantile-Regression&#65288;EQR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#20540;&#20998;&#24067;&#20989;&#25968;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;EQR&#20855;&#26377;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.03887</link><description>&lt;p&gt;
&#29992;&#19968;&#31181;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35270;&#39057;&#26174;&#24494;&#38236;&#35760;&#24405;&#20934;&#30830;&#36319;&#36394;&#27963;&#32454;&#32990;&#20173;&#28982;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26041;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#20960;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#24212;&#29992;&#23581;&#35797;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#25972;&#21512;&#21040;&#35813;&#20219;&#21153;&#20013;&#65292;&#20294;&#22823;&#37096;&#20998;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#23884;&#20837;&#20854;&#26550;&#26500;&#25110;&#20854;&#20182;&#21069;&#25552;&#26465;&#20214;&#20013;&#30340;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24191;&#20041;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#32454;&#32990;&#21487;&#20197;&#26681;&#25454;&#20854;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#30340;&#20551;&#35774;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#36830;&#32493;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39069;&#22806;&#20248;&#28857;&#26159;&#32454;&#32990;&#30340;&#36816;&#21160;&#27169;&#24335;&#21487;&#20197;&#23436;&#20840;&#30001;&#39044;&#27979;&#22120;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#35270;&#39057;&#24103;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2307.15438</link><description>&lt;p&gt;
&#33258;&#20027;&#36733;&#33655;&#28909;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#22411;&#21355;&#26143;&#20013;&#65292;&#28909;&#25511;&#21046;&#35774;&#22791;&#12289;&#31185;&#23398;&#20202;&#22120;&#21644;&#30005;&#23376;&#37096;&#20214;&#30340;&#31354;&#38388;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#30005;&#23376;&#35774;&#22791;&#30340;&#36817;&#36317;&#31163;&#20351;&#24471;&#21151;&#32791;&#25955;&#28909;&#22256;&#38590;&#65292;&#23384;&#22312;&#26080;&#27861;&#36866;&#24403;&#25511;&#21046;&#28201;&#24230;&#12289;&#38477;&#20302;&#37096;&#20214;&#23551;&#21629;&#21644;&#20219;&#21153;&#24615;&#33021;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#21033;&#29992;&#21355;&#26143;&#19978;&#36880;&#28176;&#22686;&#21152;&#30340;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#23398;&#20064;&#26426;&#36733;&#28909;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#26410;&#26469;&#23558;&#36816;&#24448;ISS&#24182;&#22312;IMAGIN-e&#20219;&#21153;&#20013;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#30340;&#30495;&#23454;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#25511;&#21046;&#36733;&#33655;&#22788;&#29702;&#21151;&#29575;&#65292;&#20197;&#20445;&#25345;&#28201;&#24230;&#22312;&#25805;&#20316;&#33539;&#22260;&#20869;&#65292;&#34917;&#20805;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15398</link><description>&lt;p&gt;
&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#36825;&#26159;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#19968;&#20010;&#31867;&#20284;&#20154;&#31867;&#30340;&#31579;&#36873;&#32773;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#20505;&#36873;&#20154;&#27744;&#20013;&#25214;&#21040;&#21069;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#65292;&#32780;&#19981;&#26159;&#26368;&#22909;&#30340;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#34920;&#31034;&#31867;&#20154;&#31579;&#36873;&#32773;&#22312;&#31579;&#36873;&#20043;&#21069;&#22914;&#20309;&#23433;&#25490;&#20505;&#36873;&#20154;&#27744;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#36873;&#25321;&#23545;&#25152;&#36873;&#30340;k&#20010;&#20505;&#36873;&#20154;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#20505;&#36873;&#20154;&#22810;&#20110;&#22899;&#24615;&#20505;&#36873;&#20154;&#65289;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#21463;&#20445;&#25252;&#30340;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#24179;&#31561;&#30340;&#21162;&#21147;&#12290;&#20854;&#20182;&#20844;&#24179;&#24615;&#32467;&#26524;&#20063;&#22312;&#31867;&#20154;&#31579;&#36873;&#32773;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#30340;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#28508;&#22312;&#33258;&#21160;&#21270;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.17708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#20013;&#30340;&#25925;&#38556;&#21644;&#39118;&#38505;&#20998;&#26512;&#65306;&#20197;ONNX&#29983;&#24577;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem. (arXiv:2303.17708v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#23545;ONNX&#30456;&#20851;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#25253;&#21578;&#20102;&#25925;&#38556;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#24320;&#21457;&#65292;&#20248;&#21270;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20182;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#26694;&#26550;&#20013;&#20351;&#29992;&#21644;&#37325;&#26032;&#20351;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#36816;&#34892;&#26102;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#22312;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#24037;&#31243;&#24072;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#23558;&#27169;&#22411;&#20174;&#26694;&#26550;&#31227;&#21160;&#21040;&#36816;&#34892;&#26102;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36716;&#25442;&#22120;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#24182;&#30772;&#22351;&#37096;&#32626;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#22120;&#30340;&#25925;&#38556;&#39057;&#29575;&#21644;&#25925;&#38556;&#27169;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;ONNX (Open Neural Network eXchange)&#30456;&#20851;&#30340;&#27169;&#22411;&#36716;&#25442;&#22120;&#36827;&#34892;&#20102;&#39318;&#27425;&#25925;&#38556;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ONNX&#36716;&#25442;&#22120;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;DL&#26694;&#26550;PyTorch&#21644;TensorFlow&#20013;&#30340;&#36807;&#21435;&#25925;&#38556;&#12290;&#36824;&#25253;&#21578;&#20102;&#25925;&#38556;&#65288;N=200&#20010;&#38382;&#39064;&#65289;&#30340;&#30151;&#29366;&#65292;&#21407;&#22240;&#21644;&#20301;&#32622;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36716;&#25442;8,797&#20010;&#27169;&#22411;&#65288;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#23454;&#20363;&#65289;&#26469;&#35780;&#20272;&#24403;&#20170;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers develop, fine-tune, and deploy deep learning (DL) models. They use and re-use models in a variety of development frameworks and deploy them on a range of runtime environments. In this diverse ecosystem, engineers use DL model converters to move models from frameworks to runtime environments. However, errors in converters can compromise model quality and disrupt deployment. The failure frequency and failure modes of DL model converters are unknown.  In this paper, we conduct the first failure analysis on DL model converters. Specifically, we characterize failures in model converters associated with ONNX (Open Neural Network eXchange). We analyze past failures in the ONNX converters in two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and locations of failures (for N=200 issues), and trends over time are also reported. We also evaluate present-day failures by converting 8,797 models, both real-world and synthetically generated instances. The consis
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.14942</link><description>&lt;p&gt;
&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Optimality of Misspecified Spectral Algorithms. (arXiv:2303.14942v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#38382;&#39064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20551;&#35774;&#22320;&#19979;&#30495;&#23454;&#20989;&#25968;$f_{\rho}^{*} \in [\mathcal{H}]^{s}$&#65292;&#20854;&#20013;$\mathcal{H}$&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#36739;&#24179;&#28369;&#25554;&#20540;&#31354;&#38388;&#65292;$s\in (0,1)$&#12290;&#29616;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#32467;&#26524;&#35201;&#27714;$\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$&#65292;&#36825;&#38544;&#21547;&#22320;&#35201;&#27714;$s &gt; \alpha_{0}$&#65292;&#20854;&#20013;$\alpha_{0}\in (0,1)$&#26159;&#23884;&#20837;&#25351;&#25968;&#65292;&#19968;&#20010;&#20381;&#36182;&#20110;$\mathcal{H}$&#30340;&#24120;&#25968;&#12290;&#20851;&#20110;&#35889;&#31639;&#27861;&#26159;&#21542;&#23545;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;&#38382;&#39064;&#24050;&#32463;&#23384;&#22312;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35889;&#31639;&#27861;&#26159;&#23545;&#20110;&#20219;&#24847;&#30340;$\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#20854;&#20013;$\beta$&#26159;$\mathcal{H}$&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#20960;&#31867;&#28385;&#36275;$ \alpha_0 = \frac{1}{\beta} $&#30340;RKHSs&#65292;&#22240;&#27492;&#65292;&#35889;&#31639;&#27861;&#22312;&#36825;&#20123;RKHSs&#19978;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the misspecified spectral algorithms problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$ which implicitly requires $s &gt; \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the spectral algorithms are optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that spectral algorithms are minimax optimal for any $\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$, where $\beta$ is the eigenvalue decay rate of $\mathcal{H}$. We also give several classes of RKHSs whose embedding index satisfies $ \alpha_0 = \frac{1}{\beta} $. Thus, the spectral algorithms are minimax optimal for all $s\in (0,1)$ on these RKHSs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#22312;&#36870;&#38382;&#39064;&#20013;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#35299;&#23494;&#21464;&#25442;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;</title><link>http://arxiv.org/abs/2301.07820</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#30340;&#20986;&#29616;&#22312;&#36870;&#38382;&#39064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Emergence of the SVD as an interpretable factorization in deep learning for inverse problems. (arXiv:2301.07820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07820
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#22312;&#36870;&#38382;&#39064;&#20013;&#25104;&#20026;&#21487;&#35299;&#37322;&#30340;&#22240;&#23376;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#35299;&#23494;&#21464;&#25442;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35299;&#37322;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#24403;&#19982;&#35299;&#23494;&#21464;&#25442;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#38024;&#23545;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#36890;&#36807;&#32771;&#34385;&#20256;&#36882;&#32473;&#35299;&#23494;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#24179;&#22343;&#25928;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#26497;&#38480;&#19979;&#65292;&#35299;&#23494;&#21464;&#25442;&#21487;&#20197;&#29992;NN&#26435;&#37325;&#30340;SVD&#21644;&#36755;&#20837;&#33258;&#30456;&#20851;&#30697;&#38453;&#26469;&#34920;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#31867;&#20013;&#65292;SVD&#21487;&#20197;&#26159;&#35757;&#32451;&#32593;&#32476;&#32534;&#30721;&#20449;&#21495;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#29992;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20449;&#21495;&#27169;&#22411;&#30340;&#23454;&#35777;&#35777;&#25454;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#25968;&#23398;&#29702;&#35770;&#21644;&#35821;&#20041;&#21457;&#23637;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Within the framework of deep learning we demonstrate the emergence of the singular value decomposition (SVD) of the weight matrix as a tool for interpretation of neural networks (NN) when combined with the descrambling transformation--a recently-developed technique for addressing interpretability in noisy parameter estimation neural networks \cite{amey2021neural}. By considering the averaging effect of the data passed to the descrambling minimization problem, we show that descrambling transformations--in the large data limit--can be expressed in terms of the SVD of the NN weights and the input autocorrelation matrix. Using this fact, we show that within the class of noisy parameter estimation problems the SVD may be the structure through which trained networks encode a signal model. We substantiate our theoretical findings with empirical evidence from both linear and non-linear signal models. Our results also illuminate the connections between a mathematical theory of semantic developm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2210.12583</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control. (arXiv:2210.12583v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#31934;&#30830;&#19988;&#23433;&#20840;&#22320;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#22312;&#25805;&#20316;&#26465;&#20214;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#19981;&#26029;&#35843;&#25972;&#20197;&#24357;&#34917;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#20027;&#21160;&#24314;&#27169;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#31163;&#32447;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#21644;&#22312;&#32447;&#23398;&#20064;&#24403;&#21069;&#26426;&#22120;&#20154;&#19982;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#22823;&#22823;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#25805;&#20316;&#33539;&#22260;&#20869;&#20063;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#30340;aleatoric&#65288;&#25968;&#25454;&#65289;&#19981;&#30830;&#23450;&#24615;&#21551;&#21457;&#24335;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#26368;&#20248;&#30340;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in the presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control act
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.10230</link><description>&lt;p&gt;
&#20174;&#38745;&#24577;&#21040;&#21160;&#24577;&#30340;&#32467;&#26500;&#65306;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#32467;&#21512;&#20146;&#21644;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Static to Dynamic Structures: Improving Binding Affinity Prediction with a Graph-Based Deep Learning Model. (arXiv:2208.10230v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#26159;&#32467;&#26500;&#22522;&#30784;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#20013;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#20173;&#28982;&#21463;&#38480;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#21482;&#21033;&#29992;&#38745;&#24577;&#26230;&#20307;&#32467;&#26500;&#65292;&#32780;&#23454;&#38469;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#36890;&#24120;&#30001;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#25551;&#36848;&#12290;&#36924;&#36817;&#36825;&#26679;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#12290;&#26412;&#25991;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;3,218&#20010;&#19981;&#21516;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;MD&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Dynaformer&#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290; Dynaformer&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#20174;MD&#36712;&#36857;&#20013;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#20307;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CASF-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of the protein-ligand binding affinities is an essential challenge in the structure-based drug design. Despite recent advance in data-driven methods in affinity prediction, their accuracy is still limited, partially because they only take advantage of static crystal structures while the actual binding affinities are generally depicted by the thermodynamic ensembles between proteins and ligands. One effective way to approximate such a thermodynamic ensemble is to use molecular dynamics (MD) simulation. Here, we curated an MD dataset containing 3,218 different protein-ligand complexes, and further developed Dynaformer, which is a graph-based deep learning model. Dynaformer was able to accurately predict the binding affinities by learning the geometric characteristics of the protein-ligand interactions from the MD trajectories. In silico experiments demonstrated that our model exhibits state-of-the-art scoring and ranking power on the CASF-2016 benchmark dataset, outpe
&lt;/p&gt;</description></item></channel></rss>