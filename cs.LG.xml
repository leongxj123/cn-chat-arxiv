<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20840;&#38754;&#27604;&#36739;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#65292;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01360</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#23398;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Harnessing Data and Physics for Deep Learning Phase Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#27604;&#36739;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#65292;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#26159;&#20174;&#20809;&#24378;&#24230;&#27979;&#37327;&#20013;&#35745;&#31639;&#20809;&#27874;&#30340;&#30456;&#20301;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#30456;&#24178;&#34893;&#23556;&#25104;&#20687;&#12289;&#33258;&#36866;&#24212;&#20809;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#26041;&#38754;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#20998;&#21035;&#20026;&#25968;&#25454;&#39537;&#21160;&#65288;DD&#65289;&#19982;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#20197;&#21450;&#29289;&#29702;&#39537;&#21160;&#65288;PD&#65289;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#12290;DD&#21644;PD&#20197;&#19981;&#21516;&#26041;&#24335;&#23454;&#29616;&#30456;&#21516;&#30446;&#26631;&#65292;&#24182;&#32570;&#20047;&#24517;&#35201;&#30340;&#30740;&#31350;&#26469;&#25581;&#31034;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01360v1 Announce Type: cross  Abstract: Phase recovery, calculating the phase of a light wave from its intensity measurements, is essential for various applications, such as coherent diffraction imaging, adaptive optics, and biomedical imaging. It enables the reconstruction of an object's refractive index distribution or topography as well as the correction of imaging system aberrations. In recent years, deep learning has been proven to be highly effective in addressing phase recovery problems. Two main deep learning phase recovery strategies are data-driven (DD) with supervised learning mode and physics-driven (PD) with self-supervised learning mode. DD and PD achieve the same goal in different ways and lack the necessary study to reveal similarities and differences. Therefore, in this paper, we comprehensively compare these two deep learning phase recovery strategies in terms of time consumption, accuracy, generalization ability, ill-posedness adaptability, and prior capac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.20324</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#20174;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#20013;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306;
&lt;/p&gt;
&lt;p&gt;
Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#35768;&#22810;&#24739;&#32773;&#22312;&#33647;&#29289;&#26080;&#27861;&#25511;&#21046;&#30315;&#30187;&#21457;&#20316;&#26102;&#38656;&#35201;&#25163;&#26415;&#24178;&#39044;&#12290;&#20026;&#20102;&#21462;&#24471;&#26377;&#25928;&#30340;&#25163;&#26415;&#32467;&#26524;&#65292;&#20934;&#30830;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; - &#36890;&#24120;&#36817;&#20284;&#20026;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; (SOZ) - &#33267;&#20851;&#37325;&#35201;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#30005;&#21050;&#28608;&#36827;&#34892;&#20027;&#21160;&#25506;&#27979;&#24050;&#32463;&#25104;&#20026;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#21306;&#22495;&#30340;&#26631;&#20934;&#20020;&#24202;&#23454;&#36341;&#12290;&#26412;&#25991;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20351;&#29992;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608; (SPES) &#21709;&#24212;&#36827;&#34892; SOZ &#23450;&#20301;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20445;&#30041;&#30340;&#24739;&#32773;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20324v1 Announce Type: new  Abstract: Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namel
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.18957</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35843;&#33410;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#22312;&#32447;&#22270;&#29255;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#65288;UGCGs&#65289;&#22312;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#31038;&#20132;&#20114;&#21160;&#21644;&#26356;&#26377;&#21019;&#24847;&#30340;&#22312;&#32447;&#23089;&#20048;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30528;&#26356;&#39640;&#30340;&#26292;&#38706;&#19981;&#33391;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#22312;&#32447;&#23433;&#20840;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#30740;&#31350;&#23545;&#19981;&#23433;&#20840;UGCGs&#30340;&#36829;&#27861;&#25512;&#24191;&#36827;&#34892;&#23041;&#32961;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2,924&#24352;&#23637;&#31034;&#19981;&#21516;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#20869;&#23481;&#34987;&#28216;&#25103;&#21019;&#24314;&#32773;&#29992;&#20110;&#25512;&#24191;UGCGs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18957v1 Announce Type: cross  Abstract: Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18330</link><description>&lt;p&gt;
&#20351;&#29992;&#36319;&#36394;&#36741;&#21161;&#30340;&#20107;&#20214;&#30456;&#26426;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tracking-Assisted Object Detection with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26080;&#21160;&#24577;&#27169;&#31946;&#31561;&#29305;&#27530;&#23646;&#24615;&#65292;&#20107;&#20214;&#39537;&#21160;&#30446;&#26631;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#30340;&#19981;&#21516;&#27493;&#24615;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#20102;&#30001;&#20110;&#30456;&#26426;&#19982;&#20043;&#27809;&#26377;&#30456;&#23545;&#36816;&#21160;&#32780;&#23548;&#33268;&#30340;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#65292;&#36825;&#23545;&#20219;&#21153;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#35270;&#20026;&#20266;&#36974;&#25377;&#23545;&#35937;&#65292;&#24182;&#26088;&#22312;&#25581;&#31034;&#20854;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29616;&#26377;&#30340;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#19978;&#38468;&#21152;&#39069;&#22806;&#30340;&#21487;&#35265;&#24615;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAREMed&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;</title><link>https://arxiv.org/abs/2403.17745</link><description>&lt;p&gt;
&#19981;&#35753;&#20219;&#20309;&#24739;&#32773;&#25481;&#38431;&#65306;&#22686;&#24378;&#32597;&#35265;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17745
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAREMed&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#20379;&#23450;&#21046;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#30456;&#36739;&#20110;&#24739;&#26377;&#24120;&#35265;&#30142;&#30149;&#30340;&#24739;&#32773;&#65292;&#23545;&#20110;&#24739;&#26377;&#32597;&#35265;&#30149;&#30151;&#30340;&#24739;&#32773;&#65292;&#25512;&#33616;&#24448;&#24448;&#26356;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robust and Accurate REcommendations for Medication&#65288;RAREMed&#65289;&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#26469;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;RAREMed&#37319;&#29992;&#20855;&#26377;&#32479;&#19968;&#36755;&#20837;&#24207;&#21015;&#26041;&#27861;&#30340;Transformer&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#30142;&#30149;&#21644;&#31243;&#24207;&#20195;&#30721;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21363;Sequence Matching Prediction&#65288;SMP&#65289;&#21644;Self Reconstruction&#65288;SR&#65289;&#65292;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17745v1 Announce Type: new  Abstract: Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2403.15740</link><description>&lt;p&gt;
Ghost Sentence&#65306;&#19968;&#31181;&#20379;&#26222;&#36890;&#29992;&#25143;&#20351;&#29992;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#36827;&#34892;&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15740
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26723;&#20013;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#24182;&#35782;&#21035;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#30830;&#35748;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28389;&#29992;&#20854;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#29992;&#25143;&#25968;&#25454;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#24494;&#35843;&#21464;&#31181;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24314;&#35758;&#29992;&#25143;&#22312;&#20854;&#25991;&#26723;&#20013;&#21453;&#22797;&#25554;&#20837;&#20010;&#20154;&#23494;&#30721;&#65292;&#20351;LLMs&#33021;&#22815;&#35760;&#24518;&#36825;&#20123;&#23494;&#30721;&#12290;&#36825;&#20123;&#29992;&#25143;&#25991;&#26723;&#20013;&#38544;&#34255;&#30340;&#23494;&#30721;&#65292;&#34987;&#31216;&#20026;&#8220;&#24189;&#28789;&#21477;&#23376;&#8221;&#65292;&#19968;&#26086;&#23427;&#20204;&#20986;&#29616;&#22312;LLMs&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#65292;&#29992;&#25143;&#23601;&#21487;&#20197;&#30830;&#20449;&#20182;&#20204;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#29256;&#26435;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24189;&#28789;&#21477;&#23376;&#23450;&#20041;&#20102;&#8220;&#29992;&#25143;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#8221;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#12289;&#19981;&#21516;&#35268;&#27169;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#36827;&#34892;&#27979;&#35797;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#21518;$k$&#20010;&#21333;&#35789;&#39564;&#35777;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15740v1 Announce Type: new  Abstract: Web user data plays a central role in the ecosystem of pre-trained large language models (LLMs) and their fine-tuned variants. Billions of data are crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web users}} confirm if LLMs misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling LLMs to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of LLMs, users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with LLMs of different sizes. For evaluation, we introduce a last $k$ words verification manner along 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12968</link><description>&lt;p&gt;
LLMLingua-2: &#39640;&#25928;&#19988;&#24544;&#23454;&#30340;&#26080;&#20219;&#21153;Prompt&#21387;&#32553;&#30340;&#25968;&#25454;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#23454;&#29616;Prompt&#30340;&#21387;&#32553;&#65292;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;&#25552;&#31034;&#20445;&#25345;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#26080;&#20219;&#21153;&#30340;Prompt&#21387;&#32553;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#20174;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMa-7B&#65289;&#33719;&#24471;&#30340;&#20449;&#24687;&#29109;&#26469;&#21024;&#38500;token&#25110;&#35789;&#27719;&#21333;&#20301;&#26469;&#21387;&#32553;prompt&#12290;&#25361;&#25112;&#22312;&#20110;&#20449;&#24687;&#29109;&#21487;&#33021;&#26159;&#19968;&#20010;&#27425;&#20248;&#30340;&#21387;&#32553;&#24230;&#37327;&#65306;(i)&#23427;&#20165;&#21033;&#29992;&#21333;&#21521;&#19978;&#19979;&#25991;&#65292;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#25152;&#26377;&#29992;&#20110;prompt&#21387;&#32553;&#30340;&#20851;&#38190;&#20449;&#24687;&#65307;(ii)&#23427;&#19982;prompt&#21387;&#32553;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#31934;&#28860;&#36807;&#31243;&#65292;&#20174;LLM&#20013;&#33719;&#24471;&#30693;&#35782;&#20197;&#21387;&#32553;prompt&#32780;&#19981;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#25277;&#21462;&#24335;&#25991;&#26412;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;prompt&#21387;&#32553;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;token&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#21387;&#32553;&#21518;&#30340;prompt&#19982;&#21407;&#22987;prompt&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12968v1 Announce Type: new  Abstract: This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.12559</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence Self-Calibration for Multi-Label Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MLCIL&#65289;&#20013;&#30340;&#37096;&#20998;&#26631;&#31614;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#21482;&#26377;&#26032;&#31867;&#21035;&#34987;&#26631;&#35760;&#65292;&#32780;&#36807;&#21435;&#21644;&#26410;&#26469;&#26631;&#31614;&#20173;&#28982;&#19981;&#21487;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#30001;&#20110;&#38169;&#35823;&#22320;&#39640;&#32622;&#20449;&#24230;&#22810;&#26631;&#31614;&#39044;&#27979;&#32780;&#20986;&#29616;&#22823;&#37327;&#35823;&#25253;&#38169;&#35823;&#65292;&#21152;&#21095;&#20102;&#22312;&#19981;&#21516;&#26631;&#31614;&#31354;&#38388;&#20869;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;MLCIL&#20013;&#25913;&#36827;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24515;&#33258;&#26657;&#20934;&#65288;CSC&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#26631;&#31614;&#20851;&#31995;&#26657;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31867;&#22686;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23398;&#20064;&#30340;&#12289;&#21160;&#24577;&#25193;&#23637;&#30340;&#26631;&#31614;&#20851;&#31995;&#22270;&#26469;&#36830;&#25509;&#23396;&#31435;&#30340;&#26631;&#31614;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20449;&#24515;&#26657;&#20934;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#22810;&#26631;&#31614;&#22686;&#37327;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#36807;&#20110;&#33258;&#20449;&#30340;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#24809;&#32602;&#65292;&#20419;&#36827;&#20102;&#20449;&#24515;&#30340;&#33258;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12559v1 Announce Type: cross  Abstract: The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph. Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.08042</link><description>&lt;p&gt;
CT&#35780;&#20272;2D&#21644;3D&#25972;&#20307;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27668;&#36947;&#30149;&#21464;&#30340;&#20307;&#31215;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;2D&#21644;3D&#26684;&#24335;&#20013;&#30340;&#25972;&#20307;&#20998;&#21106;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;&#22218;&#24615;&#32420;&#32500;&#21270;&#65288;CF&#65289;&#30149;&#21464;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#26469;&#33258;&#20004;&#20010;CF&#21442;&#32771;&#20013;&#24515;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#20027;&#35201;&#30340;CF&#32467;&#26500;&#21464;&#21270;&#12290;&#39318;&#20808;&#27604;&#36739;&#20102;2D&#21644;3D&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#31896;&#28082;&#26643;&#21644;&#23454;&#21464;&#31561;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#36234;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;2D&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#30340;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#23613;&#31649;&#27809;&#26377;&#36229;&#36234;3D&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#32954;&#21151;&#33021;&#27979;&#35797;&#65288;PFTs&#65289;&#30340;&#22806;&#37096;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#65292;&#30830;&#35748;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#38480;&#20110;&#27604;&#36739;&#25351;&#26631;&#65307;&#36824;&#21253;&#25324;&#23545;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08042v1 Announce Type: cross  Abstract: This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#21457;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.00013</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Informative Features and Examples for Deep Learning from Noisy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00013
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#21457;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#24320;&#21457;&#36807;&#31243;&#30340;&#27599;&#20010;&#38454;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#65292;&#24182;&#25552;&#39640;&#29305;&#24449;&#23398;&#20064;&#12289;&#25968;&#25454;&#26631;&#35760;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#30340;&#20998;&#24067;&#25968;&#25454;&#25552;&#21462;&#21482;&#19982;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#20998;&#24067;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#29305;&#24449;&#26469;&#21435;&#38500;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#22122;&#22768;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#26080;&#26631;&#31614;&#30340;&#22024;&#26434;&#25968;&#25454;&#20013;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#20197;&#38477;&#20302;&#20027;&#21160;&#23398;&#20064;&#30340;&#26631;&#35760;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#32431;&#24230;-&#20449;&#24687;&#22256;&#22659;&#65292;&#21363;&#23581;&#35797;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#20250;&#23548;&#33268;&#36873;&#25321;&#35768;&#22810;&#22122;&#22768;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#25214;&#21040;&#26368;&#20339;&#32431;&#24230;&#21644;&#20449;&#24687;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00013v1 Announce Type: new  Abstract: In this dissertation, we propose a systemic framework that prioritizes informative features and examples to enhance each stage of the development process. Specifically, we prioritize informative features and examples and improve the performance of feature learning, data labeling, and data selection. We first propose an approach to extract only informative features that are inherent to solving a target task by using auxiliary out-of-distribution data. We deactivate the noise features in the target distribution by using that in the out-of-distribution data. Next, we introduce an approach that prioritizes informative examples from unlabeled noisy data in order to reduce the labeling cost of active learning. In order to solve the purity-information dilemma, where an attempt to select informative examples induces the selection of many noisy examples, we propose a meta-model that finds the best balance between purity and informativeness. Lastl
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17003</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30417;&#27979;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20026;&#20010;&#24615;&#21270;&#20020;&#24202;&#35797;&#39564;&#20013;&#21442;&#19982;&#32773;&#30340;&#27835;&#30103;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#39046;&#22495;&#37096;&#32626;&#22312;&#32447;&#33258;&#20027;&#31639;&#27861;&#20351;&#24471;&#36136;&#37327;&#25511;&#21046;&#21644;&#25968;&#25454;&#36136;&#37327;&#29305;&#21035;&#38590;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#30340;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;&#23427;&#24378;&#35843;&#20102;&#31639;&#27861;&#23545;&#65288;1&#65289;&#20445;&#25252;&#21442;&#19982;&#32773;&#21644;&#65288;2&#65289;&#20445;&#30041;&#25968;&#25454;&#22312;&#35797;&#39564;&#21518;&#20998;&#26512;&#20013;&#30340;&#31185;&#23398;&#25928;&#29992;&#30340;&#36131;&#20219;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#32626;&#21069;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#32773;&#21644;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#30830;&#20445;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;Oralytics&#20020;&#24202;&#35797;&#39564;&#30340;&#30495;&#23454;&#26696;&#20363;&#12290;&#33258;2023&#24180;&#26149;&#23395;&#20197;&#26469;&#65292;&#36825;&#39033;&#35797;&#39564;&#25104;&#21151;&#22320;&#37096;&#32626;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#26469;&#36827;&#34892;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17003v1 Announce Type: cross  Abstract: Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to persona
&lt;/p&gt;</description></item><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.12954</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional Logical Message Passing Transformer for Complex Query Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;KGs&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#27169;&#22411;&#26469;&#36890;&#36807;&#25191;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;CQA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#19981;&#33021;&#21516;&#26102;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#34429;&#28982;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#37117;&#26377;&#25928;&#65292;&#20294;&#23427;&#24573;&#30053;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26356;&#26032;&#38454;&#27573;&#65292;&#35813;&#26426;&#21046;&#19981;&#33021;&#21160;&#24577;&#34913;&#37327;&#19981;&#21516;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#23427;&#33021;&#21542;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#65288;CLMPT&#65289;&#65292;&#32771;&#34385;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#19988;&#20855;&#26377;&#21160;&#24577;&#27979;&#37327;&#19981;&#21516;&#28040;&#24687;&#37325;&#35201;&#24615;&#20197;&#21450;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12954v1 Announce Type: cross  Abstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;</title><link>https://arxiv.org/abs/2402.07867</link><description>&lt;p&gt;
PoisonedRAG: &#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20063;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#32570;&#20047;&#26368;&#26032;&#30340;&#30693;&#35782;&#21644;&#34394;&#26500;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;RAG&#20174;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#36755;&#20837;&#12290;&#20363;&#22914;&#65292;&#24403;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;&#20174;&#32500;&#22522;&#30334;&#31185;&#25910;&#38598;&#30340;&#25968;&#30334;&#19975;&#20010;&#25991;&#26412;&#26102;&#65292;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#21487;&#20197;&#26159;&#19982;&#32473;&#23450;&#38382;&#39064;&#22312;&#35821;&#20041;&#19978;&#26368;&#30456;&#20284;&#30340;&#21069;K&#20010;&#25991;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;LLM&#21487;&#20197;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#20316;&#20026;&#19978;&#19979;&#25991;&#20026;&#32473;&#23450;&#38382;&#39064;&#29983;&#25104;&#31572;&#26696;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;RAG&#30340;&#20934;&#30830;&#24615;&#25110;&#25928;&#29575;&#65292;&#32780;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge pois
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05052</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#36827;&#34892;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#20010;&#36890;&#29992;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning from Multiple Distributions: A General Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#27979;&#37327;&#21464;&#37327;&#65288;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65289;&#21482;&#26159;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#30340;&#27010;&#24565;&#25110;&#23545;&#35937;&#65289;&#30340;&#25968;&#23398;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#25110;&#23545;&#31995;&#32479;&#36827;&#34892;&#36866;&#24403;&#30340;&#26356;&#25913;&#65292;&#24674;&#22797;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;$Z_i$&#20197;&#21450;&#30001;&#22270;$\mathcal{G}_Z$&#34920;&#31034;&#30340;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#26368;&#36817;&#34987;&#31216;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#65288;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#25110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#35774;&#32622;&#65292;&#19981;&#38656;&#35201;&#20551;&#35774;&#20998;&#24067;&#25913;&#21464;&#32972;&#21518;&#23384;&#22312;&#30828;&#24178;&#39044;&#12290;&#25105;&#20204;&#26088;&#22312;&#22312;&#36825;&#20010;&#22522;&#26412;&#24773;&#20917;&#19979;&#24320;&#21457;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65307;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#26377;&#21161;&#20110;&#30475;&#21040;&#20854;&#20182;&#20551;&#35774;&#65288;&#22914;&#21442;&#25968;&#22240;&#26524;&#27169;&#22411;&#25110;&#30828;&#24178;&#39044;&#65289;&#25552;&#20379;&#30340;&#29420;&#29305;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#24674;&#22797;&#36807;&#31243;&#20013;&#23545;&#22270;&#30340;&#31232;&#30095;&#24615;&#32422;&#26463;&#19979;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#23398;&#20064;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#23398;&#31616;&#21270;&#21644;&#25512;&#23548;&#65292;&#20351;&#24471;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#20934;&#30830;&#26131;&#20248;&#21270;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#12290;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#32479;&#19968;&#20102;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;</title><link>https://arxiv.org/abs/2402.03701</link><description>&lt;p&gt;
&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Improving and Unifying Discrete&amp;Continuous-time Discrete Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#23398;&#31616;&#21270;&#21644;&#25512;&#23548;&#65292;&#20351;&#24471;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#20934;&#30830;&#26131;&#20248;&#21270;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#12290;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#32479;&#19968;&#20102;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#31163;&#25955;&#25968;&#25454;&#22914;&#35821;&#35328;&#21644;&#22270;&#24418;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#31163;&#25955;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#27573;&#26102;&#38388;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;Campbell&#31561;&#20154;&#65288;2022&#65289;&#25165;&#24341;&#20837;&#20102;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#30340;&#31532;&#19968;&#20010;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#19982;&#31163;&#25955;&#26102;&#38388;&#29256;&#26412;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#36817;&#20284;&#25165;&#33021;&#36827;&#34892;&#21487;&#34892;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23545;&#21464;&#20998;&#19979;&#30028;&#30340;&#25968;&#23398;&#31616;&#21270;&#65292;&#36825;&#20123;&#31616;&#21270;&#20351;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#21152;&#20934;&#30830;&#21644;&#26131;&#20110;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21453;&#21521;&#21435;&#22122;&#20844;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#33021;&#22815;&#20248;&#38597;&#22320;&#32479;&#19968;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21069;&#21521;&#21644;&#29616;&#22312;&#20063;&#21253;&#25324;&#20102;&#21518;&#21521;&#27010;&#29575;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02097</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#39062;&#24615;&#20849;&#20139;&#35299;&#20915;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#21327;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#20840;&#23616;&#29366;&#24577;&#30340;&#26032;&#39062;&#24615;&#19981;&#21487;&#29992;&#65292;&#32780;&#23616;&#37096;&#35266;&#23519;&#30340;&#26032;&#39062;&#24615;&#23384;&#22312;&#20559;&#24046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26234;&#33021;&#20307;&#22914;&#20309;&#21327;&#35843;&#22320;&#36827;&#34892;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;MACE&#12290;&#36890;&#36807;&#20165;&#20256;&#25773;&#23616;&#37096;&#26032;&#39062;&#24615;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#32771;&#34385;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#32047;&#35745;&#26032;&#39062;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#20869;&#22312;&#22238;&#25253;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25506;&#32034;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#19977;&#31181;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2312.10794</link><description>&lt;p&gt;
Transformers&#30340;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A mathematical perspective on Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#23558;Transformers&#35299;&#37322;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;Transformers&#65292;&#25581;&#31034;&#20102;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#65292;&#24182;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.17693</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#30524;&#31185;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17693
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#31995;&#32479;&#22312;&#25552;&#39640;&#25163;&#26415;&#31934;&#30830;&#24230;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#29420;&#29305;&#20559;&#22909;&#21644;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#36890;&#25163;&#26415;&#65288;&#22914;&#33145;&#33108;&#38236;&#25163;&#26415;&#65289;&#65292;&#19981;&#36866;&#29992;&#20110;&#38750;&#24120;&#31934;&#23494;&#30340;&#24494;&#21019;&#25163;&#26415;&#65292;&#22914;&#30524;&#31185;&#25163;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#21487;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#36807;&#31243;&#20013;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#39318;&#36873;&#22806;&#31185;&#25163;&#26415;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#26469;&#35757;&#32451;&#20197;&#22270;&#20687;&#25968;&#25454;&#20026;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#25191;&#34892;&#30333;&#20869;&#38556;&#25163;&#26415;&#30340;&#20999;&#21475;&#38454;&#27573;&#25152;&#26377;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22806;&#31185;&#21307;&#29983;&#30340;&#21160;&#20316;&#21644;&#20559;&#22909;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35753;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17693v2 Announce Type: replace-cross  Abstract: Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our ap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16356</link><description>&lt;p&gt;
cDVGAN: &#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25925;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16356
&lt;/p&gt;
&lt;p&gt;
cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30495;&#23454;&#30340;&#26102;&#38388;&#22495;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#35266;&#27979;&#21644;GW&#25506;&#27979;&#22120;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#25512;&#36827;GW&#25968;&#25454;&#20998;&#26512;&#12290;&#27169;&#25311;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#29992;&#20110;&#20449;&#21495;&#25628;&#32034;&#30340;&#25968;&#25454;&#38598;&#65292;&#24179;&#34913;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39564;&#35777;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cDVGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#30340;&#26032;&#22411;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#34920;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#30340;&#22810;&#31181;&#31867;&#21035;&#30340;&#26102;&#38388;&#22495;&#35266;&#27979;&#12290;cDVGAN&#36824;&#21487;&#20197;&#36890;&#36807;&#22312;&#26465;&#20214;&#31867;&#21035;&#21521;&#37327;&#20013;&#36827;&#34892;&#25554;&#20540;&#29983;&#25104;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24191;&#20041;&#28151;&#21512;&#26679;&#26412;&#12290;cDVGAN&#22312;&#20856;&#22411;&#30340;GANs&#30340;&#20108;&#20154;&#23545;&#25239;&#21338;&#24328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#21442;&#19982;&#32773;&#65292;&#20854;&#20013;&#19968;&#20010;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#25552;&#20379;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
&lt;/p&gt;</description></item><item><title>MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13660</link><description>&lt;p&gt;
MambaByte: &#26080;&#26631;&#35760;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13660
&lt;/p&gt;
&lt;p&gt;
MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#23383;&#33410;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#23383;&#33410;&#20250;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;MambaByte&#65292;&#23427;&#26159;&#22522;&#20110;&#23383;&#33410;&#24207;&#21015;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#26080;&#26631;&#35760;&#36866;&#24212;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#23383;&#33410;&#32423;&#27169;&#22411;&#30456;&#27604;&#65292;MambaByte&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;MambaByte&#22312;&#24615;&#33021;&#19978;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#24230;&#30340;&#32447;&#24615;&#25193;&#23637;&#65292;MambaByte&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#24471;&#20102;&#24555;&#36895;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;Transformer&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;MambaByte&#22312;&#23454;&#29616;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.09493</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#26377;&#20851;&#30340;&#19977;&#32500;&#36752;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36752;&#23556;&#21453;&#39304;&#24433;&#21709;&#20102;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#30340;&#24378;&#21270;&#65292;&#20294;&#29616;&#26377;&#35786;&#26029;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20351;&#20854;&#26080;&#27861;&#29992;&#26469;&#30740;&#31350;&#19981;&#23545;&#31216;&#25110;&#30636;&#24577;&#30340;&#36752;&#23556;&#21152;&#28909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#26469;&#23398;&#20064;&#36752;&#23556;&#19982;&#23454;&#38469;&#27169;&#25311;&#30340;&#27668;&#26059;&#34920;&#38754;&#24378;&#21270;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#38480;&#21046;VED&#27169;&#22411;&#30340;&#36755;&#20837;&#21487;&#20197;&#21033;&#29992;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#36752;&#23556;&#23545;&#24378;&#21270;&#26356;&#37325;&#35201;&#30340;&#26102;&#26399;&#12290;&#23545;&#25552;&#21462;&#30340;&#19977;&#32500;&#36752;&#23556;&#32467;&#26500;&#30340;&#32454;&#33268;&#26816;&#26597;&#34920;&#26126;&#65292;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#22312;&#25972;&#20307;&#19978;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27973;&#20113;&#30340;&#19979;&#39118;&#22788;&#30340;&#28145;&#23545;&#27969;&#23545;&#28023;&#29141;&#30340;&#24378;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21457;&#29616;&#28909;&#21147;-&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36724;&#23545;&#31216;&#25110;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.06035</link><description>&lt;p&gt;
RAVEN&#65306;&#29992;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#24615;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21463;&#21040;&#19977;&#32500;&#24863;&#30693;&#29983;&#25104;&#26694;&#26550;&#21551;&#21457;&#30340;&#28151;&#21512;&#26174;&#24335;-&#38544;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#24182;&#20837;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21807;&#19968;&#30340;&#28508;&#22312;&#32534;&#30721;&#26469;&#24314;&#27169;&#25972;&#20010;&#35270;&#39057;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#20174;&#20013;&#38388;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#20013;&#21512;&#25104;&#21333;&#20010;&#35270;&#39057;&#24103;&#65292;&#35813;&#34920;&#31034;&#27861;&#26412;&#36523;&#26159;&#20174;&#20027;&#35201;&#28508;&#22312;&#32534;&#30721;&#20013;&#27966;&#29983;&#30340;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#20102;2&#20493;&#65292;&#20197;FLOPs&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20415;&#20110;&#39640;&#25928;&#21644;&#26102;&#38388;&#36830;&#36143;&#22320;&#29983;&#25104;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35270;&#35273;&#20266;&#24433;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#38598;&#25104;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04343</link><description>&lt;p&gt;
&#31169;&#26377;&#38646;&#38454;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04343
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23384;&#22312;&#36829;&#21453;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21046;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;DP-SGD&#21487;&#20197;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20294;&#20250;&#24102;&#26469;&#24615;&#33021;&#25439;&#22833;&#21644;&#37325;&#22823;&#24037;&#31243;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38646;&#38454;&#31639;&#27861;SPSA&#20013;&#30340;&#26799;&#24230;&#26041;&#21521;&#22987;&#32456;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#31169;&#26377;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#27493;&#38271;&#65292;&#21363;&#19968;&#20010;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26631;&#37327;&#27493;&#38271;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#36825;&#26159;&#23384;&#20648;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#12290;DP-ZO&#21487;&#20197;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#25110;&#39640;&#26031;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24378;&#22823;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
&lt;/p&gt;</description></item><item><title>&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38081;&#30913;&#30456;&#23398;&#20064;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#38190;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#23398;&#29983;&#27604;&#25945;&#24072;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04191</link><description>&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dense Hopfield Networks in the Teacher-Student Setting. (arXiv:2401.04191v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04191
&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38081;&#30913;&#30456;&#23398;&#20064;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#38190;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#23398;&#29983;&#27604;&#25945;&#24072;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#20197;&#20854;&#20174;&#29305;&#24449;&#21040;&#21407;&#22411;&#30340;&#36716;&#21464;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20854;&#23384;&#20648;&#23481;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;p-&#20307;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#30340;&#30456;&#22270;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#21407;&#22411;&#21644;&#29305;&#24449;&#23398;&#20064;&#33539;&#22260;&#30340;&#38081;&#30913;&#30456;&#12290;&#22312;Nishimori&#32447;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#39640;&#25928;&#27169;&#24335;&#26816;&#32034;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#30340;&#20020;&#30028;&#22823;&#23567;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24072;&#29983;&#27169;&#24335;&#30340;&#39034;&#30913;&#21040;&#38081;&#30913;&#36716;&#21464;&#19982;&#30452;&#25509;&#27169;&#22411;&#65288;&#21363;&#38543;&#26426;&#27169;&#24335;&#65289;&#30340;&#39034;&#30913;&#21040;&#33258;&#26059;&#29627;&#29827;&#36716;&#21464;&#19968;&#33268;&#12290;&#22312;Nishimori&#32447;&#20043;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#19982;&#25512;&#26029;&#28201;&#24230;&#21644;&#25968;&#25454;&#38598;&#22122;&#22768;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23398;&#29983;&#27604;&#25945;&#24072;&#20351;&#29992;&#36739;&#22823;&#30340;p&#20540;&#26102;&#65292;&#23398;&#29983;&#20855;&#26377;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense Hopfield networks are known for their feature to prototype transition and adversarial robustness. However, previous theoretical studies have been mostly concerned with their storage capacity. We bridge this gap by studying the phase diagram of p-body Hopfield networks in the teacher-student setting of an unsupervised learning problem, uncovering ferromagnetic phases reminiscent of the prototype and feature learning regimes. On the Nishimori line, we find the critical size of the training set necessary for efficient pattern retrieval. Interestingly, we find that that the paramagnetic to ferromagnetic transition of the teacher-student setting coincides with the paramagnetic to spin-glass transition of the direct model, i.e. with random patterns. Outside of the Nishimori line, we investigate the learning performance in relation to the inference temperature and dataset noise. Moreover, we show that using a larger p for the student than the teacher gives the student an extensive toler
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.02277</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21644;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks. (arXiv:2401.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#34920;&#26126;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#20219;&#24847;&#25152;&#38656;&#30340;&#31934;&#24230;&#36924;&#36817;&#32039;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#12290;&#35813;&#23450;&#29702;&#25903;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20123;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#22797;&#25968;&#12289;&#22235;&#20803;&#25968;&#12289;&#22235;&#20803;&#25968;&#30690;&#37327;&#21644;Clifford&#20540;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#35813;&#23450;&#29702;&#22343;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#20855;&#26377;&#38468;&#21152;&#20195;&#25968;&#25110;&#20960;&#20309;&#24615;&#36136;&#30340;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#20316;&#20026;&#29305;&#27530;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#24182;&#38416;&#36848;&#20102;&#22312;&#36825;&#31181;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01923</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;: &#35270;&#37326;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01923
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#24863;&#30693;&#12289;&#32593;&#32476;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#26234;&#33021;&#38899;&#31665;&#21644;&#23478;&#24237;&#26426;&#22120;&#20154;&#65292;&#24050;&#32463;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;GPT&#12289;LLaMA&#12289;DALL-E&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#65292;&#32473;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#25105;&#20204;&#23545;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#24102;&#26469;&#30340;&#22909;&#22788;&#30340;&#30475;&#27861;&#21644;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;Generative AI&#22312;&#29289;&#32852;&#32593;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#20123;&#37325;&#35201;&#24212;&#29992;&#12290;&#20805;&#20998;&#21033;&#29992;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;Generative AI&#27169;&#22411;&#30340;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#21368;&#36733;&#12289;&#35774;&#22791;&#31471;&#24494;&#35843;&#12289;&#32852;&#37030;&#23398;&#20064;&#12289;&#23433;&#20840;&#20197;&#21450;&#24320;&#21457;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#24046;&#36317;&#20197;&#21450;&#20351;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#30340;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#33021;&#22815;&#28608;&#21457;&#26032;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07820</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07820
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#32534;&#30721;&#20026;&#19968;&#31995;&#21015;&#25968;&#23383;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35270;&#20026;&#25991;&#26412;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#22312;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20363;&#22914;GPT-3&#21644;LLaMA-2&#21487;&#20197;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#26631;&#35760;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24182;&#23558;&#31163;&#25955;&#20998;&#24067;&#36716;&#25442;&#20026;&#39640;&#24230;&#28789;&#27963;&#30340;&#36830;&#32493;&#20540;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25104;&#21151;&#28304;&#20110;&#23427;&#20204;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#30340;&#20559;&#35265;&#65292;&#36825;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#31561;&#26174;&#33879;&#29305;&#24449;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#36890;&#36807;&#38750;&#25968;&#23383;&#25991;&#26412;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#20197;&#21450;&#22914;&#20309;&#36866;&#24212;&#25991;&#26412;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15214</link><description>&lt;p&gt;
&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15214
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21315;&#31859;&#23610;&#24230;&#22823;&#27668;&#38477;&#23610;&#24230;&#30340;&#29983;&#25104;&#27531;&#24046;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20174;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#36827;&#34892;&#29289;&#29702;&#28798;&#23475;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#21315;&#31859;&#23610;&#24230;&#25968;&#20540;&#27169;&#25311;&#65292;&#24182;&#39537;&#21160;&#36739;&#31895;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21315;&#31859;&#23610;&#24230;&#38477;&#23610;&#24230;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#21488;&#28286;&#30340;&#21306;&#22495;&#39640;&#20998;&#36776;&#29575;&#22825;&#27668;&#27169;&#22411;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#24182;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#22522;&#30784;&#19979;&#36827;&#34892;&#20102;&#26465;&#20214;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22823;&#20998;&#36776;&#29575;&#27604;&#29575;&#65288;25km&#33267;2km&#65289;&#65292;&#19981;&#21516;&#23610;&#24230;&#19978;&#28041;&#21450;&#30340;&#19981;&#21516;&#29289;&#29702;&#36807;&#31243;&#20197;&#21450;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39044;&#27979;&#36890;&#36947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#26041;&#27861;&#65288;ResDiff&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;UNet&#65289;&#22238;&#24402;&#22312;&#31532;&#19968;&#27493;&#39044;&#27979;&#24179;&#22343;&#20540;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#31532;&#20108;&#27493;&#39044;&#27979;&#27531;&#24046;&#12290;\textit{ResDiff}&#22312;&#22359;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;CRPS&#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25216;&#33021;&#12290;ResDiff&#39044;&#27979;&#30340;&#20809;&#35889;&#21644;&#20998;&#24067;&#24544;&#23454;&#22320;&#24674;&#22797;&#20102;&#35843;&#33410;&#26377;&#23475;&#39118;&#21644;&#38632;&#30340;&#37325;&#35201;&#24130;&#24459;&#20851;&#31995;&#12290;&#32479;&#19968;&#30340;&#22825;&#27668;&#29616;&#35937;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20998;&#21306;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15602</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20998;&#21306;&#31574;&#30053;&#30340;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training. (arXiv:2308.15602v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20998;&#21306;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;GNN&#35757;&#32451;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#21487;&#33021;&#36229;&#36807;&#21333;&#21488;&#26426;&#22120;&#25110;GPU&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#25104;&#20026;&#22823;&#35268;&#27169;GNN&#35757;&#32451;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#20808;&#20915;&#26465;&#20214;&#26159;&#23558;&#36755;&#20837;&#22270;&#20998;&#21106;&#25104;&#36739;&#23567;&#30340;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20998;&#24067;&#22312;&#35745;&#31639;&#38598;&#32676;&#30340;&#22810;&#21488;&#26426;&#22120;&#38388;&#12290;&#34429;&#28982;&#22270;&#20998;&#21306;&#22312;&#22270;&#20998;&#26512;&#21644;&#22270;&#25968;&#25454;&#24211;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#23545;GNN&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#21306;&#23545;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#19981;&#21516;&#22240;&#32032;&#65288;&#22914;GNN&#21442;&#25968;&#12289;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#22270;&#31867;&#22411;&#12289;&#29305;&#24449;&#22823;&#23567;&#21644;&#25193;&#23637;&#22240;&#23376;&#65289;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs can exceed the capabilities of single machines or GPUs, making distributed GNN training a promising direction for large-scale GNN training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been extensively studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored.  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09165</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#26159;&#20004;&#20010;&#27704;&#24658;&#30340;&#36861;&#27714;&#12290;&#23601;&#25928;&#29575;&#32780;&#35328;&#65292;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#38598;&#20013;&#20110;&#22312;&#23553;&#38381;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;OOD&#26816;&#27979;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#36182;&#24615;&#65292;&#22312;&#23436;&#25972;&#25968;&#25454;&#35774;&#32622;&#19979;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#39318;&#27425;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#12290;&#36890;&#36807;&#31934;&#28860;InD&#26679;&#26412;&#21644;&#24322;&#24120;&#20540;&#65292;&#36825;&#20123;&#34987;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#35757;&#32451;&#20986;&#26082;&#25797;&#38271;InD&#20998;&#31867;&#21448;&#33021;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#30495;&#23454;&#24322;&#24120;&#20540;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;OOD&#26816;&#27979;&#26356;&#21152;&#23454;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23545;InD&#26679;&#26412;&#25439;&#22351;&#20197;&#29983;&#25104;&#20266;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;(FEX)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26368;&#20248;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#31995;&#25968;&#20540;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12268</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Finite Expression Method for Solving High-Dimensional Committor Problems. (arXiv:2306.12268v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;Committor&#38382;&#39064;&#30340;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;(FEX)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26368;&#20248;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#31995;&#25968;&#20540;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#36335;&#24452;&#29702;&#35770;&#65288;TPT&#65289;&#26159;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#36873;&#23450;&#30340;&#20122;&#31283;&#24577;$A$&#21040;$B$&#20043;&#38388;&#30340;&#31232;&#26377;&#36716;&#31227;&#20107;&#20214;&#12290;TPT&#30340;&#26680;&#24515;&#26159;Committor&#20989;&#25968;&#65292;&#20854;&#25551;&#36848;&#20102;&#20174;&#30456;&#31354;&#38388;&#30340;&#20219;&#20309;&#36215;&#22987;&#28857;&#21040;&#36798;&#20122;&#31283;&#24577;$B$&#20043;&#21069;&#21040;&#36798;$A$&#30340;&#27010;&#29575;&#12290;&#35745;&#31639;&#20986;Committor&#20043;&#21518;&#65292;&#21487;&#20197;&#31435;&#21363;&#25214;&#21040;&#36716;&#25442;&#36890;&#36947;&#21644;&#36716;&#25442;&#36895;&#29575;&#12290;Committor&#26159;&#20855;&#26377;&#36866;&#24403;&#36793;&#30028;&#26465;&#20214;&#30340;&#21453;&#21521;Kolmogorov&#26041;&#31243;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#38656;&#35201;&#32593;&#26684;&#21270;&#25972;&#20010;&#29615;&#22659;&#31354;&#38388;&#65292;&#35299;&#20915;Committor&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26377;&#38480;&#34920;&#36798;&#24335;&#26041;&#27861;&#65288;FEX&#65292;Liang&#21644;Yang&#65288;2022&#65289;&#65289;&#20316;&#20026;&#35745;&#31639;Committor&#30340;&#24037;&#20855;&#12290;FEX&#36890;&#36807;&#28041;&#21450;&#19968;&#23450;&#25968;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#21644;&#20108;&#36827;&#21046;&#31639;&#26415;&#36816;&#31639;&#30340;&#22266;&#23450;&#26377;&#38480;&#20195;&#25968;&#34920;&#36798;&#24335;&#26469;&#36924;&#36817;Committor&#12290;&#26368;&#20339;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12289;&#20108;&#36827;&#21046;&#36816;&#31639;&#21644;&#25968;&#20540;&#31995;&#25968;&#20540;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#39640;&#32500;Committor&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36798;400&#20010;&#32500;&#24230;&#65292;&#23637;&#31034;&#20102;FEX&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;FEX&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#26041;&#27861;&#21644;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coeffi
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04489</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fair Column Subset Selection. (arXiv:2306.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04489
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20013;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#36873;&#21015;&#23376;&#38598;&#24517;&#39035;&#30456;&#23545;&#20110;&#23427;&#20204;&#21508;&#33258;&#30340;&#26368;&#20339;&#31209;-k&#36924;&#36817;&#25552;&#20379;&#33391;&#22909;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20844;&#24179;&#35774;&#32622;&#24341;&#20837;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#20026;&#20102;&#25193;&#23637;&#24050;&#30693;&#32467;&#26524;&#65292;&#20154;&#20204;&#19981;&#33021;&#20570;&#24471;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#21407;&#22987;&#26041;&#27861;&#30340;&#20004;&#20493;&#21015;&#26356;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#30340;&#24050;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#37319;&#26679;&#36866;&#24403;&#22823;&#23567;&#30340;&#23376;&#38598;&#23601;&#21464;&#24471;NP&#38590;&#12290;&#32780;&#25214;&#21040;&#20004;&#20493;&#20110;&#25152;&#38656;&#22823;&#23567;&#30340;&#23376;&#38598;&#21017;&#38750;&#24120;&#31616;&#21333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#26412;&#19978;1.5&#20493;&#30340;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10379</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31526;&#21495;&#22238;&#24402;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#26041;&#31243;&#25311;&#21512;&#21040;&#25968;&#25454;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#27905;&#26131;&#25026;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;SR&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#12290;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;SR&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#25552;&#20986;&#19979;&#19968;&#27493;&#23454;&#39564;&#12290;&#29289;&#29702;&#32422;&#26463;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25913;&#21892;&#25152;&#24314;&#35758;&#30340;&#26041;&#31243;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;SR&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.04353</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#37327;&#23376;&#22810;&#20307;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Exponentially Improved Efficient Machine Learning for Quantum Many-body States with Provable Guarantees. (arXiv:2304.04353v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04353
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32463;&#20856;&#31639;&#27861;&#32780;&#35328;&#65292;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#36890;&#24120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#23450;&#20041;&#22312;&#29289;&#29702;&#21442;&#25968; $m$ &#32500;&#31354;&#38388;&#19978;&#30340;&#21704;&#23494;&#39039;&#37327;&#26063;&#65292;&#21482;&#35201;&#21487;&#20197;&#39640;&#25928;&#22320;&#20934;&#22791;&#21644;&#27979;&#37327;&#19968;&#32452; $N$ &#20010;&#24577;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#20854;&#22522;&#24577;&#21450;&#20854;&#22312;&#20219;&#24847;&#21442;&#25968;&#37197;&#32622;&#19979;&#30340;&#24615;&#36136;&#65292;&#31934;&#24230;&#20026; $\varepsilon$&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350; [Huang &#31561;&#20154;&#65292;Science 377&#65292;eabk3333&#65288;2022&#65289;] &#23545;&#36825;&#31181;&#19968;&#33324;&#21270;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#20445;&#38556;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26222;&#36866;&#30340;&#25351;&#25968;&#32553;&#25918;&#20026; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#36739;&#22823;&#65292;&#32780;&#31934;&#24230;&#30340;&#32553;&#25918;&#21017;&#19981;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#22240;&#32032;&#65292;&#19981;&#33021;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the ground state and the ground-state properties of quantum many-body systems is generically a hard task for classical algorithms. For a family of Hamiltonians defined on an $m$-dimensional space of physical parameters, the ground state and its properties at an arbitrary parameter configuration can be predicted via a machine learning protocol up to a prescribed prediction error $\varepsilon$, provided that a sample set (of size $N$) of the states can be efficiently prepared and measured. In a recent work [Huang et al., Science 377, eabk3333 (2022)], a rigorous guarantee for such an generalization was proved. Unfortunately, an exponential scaling, $N = m^{ {\cal{O}} \left(\frac{1}{\varepsilon} \right) }$, was found to be universal for generic gapped Hamiltonians. This result applies to the situation where the dimension of the parameter space is large while the scaling with the accuracy is not an urgent factor, not entering the realm of more precise learning and prediction. In th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.15244</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Manifold Learning by Mixture Models of VAEs for Inverse Problems. (arXiv:2303.15244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#34920;&#31034;&#39640;&#32500;&#25968;&#25454;&#30340;&#27969;&#24418;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#35201;&#27714;&#25968;&#25454;&#27969;&#24418;&#20855;&#26377;&#20840;&#23616;&#21442;&#25968;&#21270;&#12290;&#20026;&#20102;&#34920;&#31034;&#20219;&#24847;&#25299;&#25169;&#30340;&#27969;&#24418;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#36825;&#37324;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#34920;&#31034;&#27969;&#24418;&#30340;&#19968;&#20010;&#22270;&#34920;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#22823;&#21270;&#20284;&#28982;&#20272;&#35745;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#26550;&#26500;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#22270;&#34920;&#21450;&#20854;&#36870;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#19968;&#26086;&#23398;&#20064;&#20102;&#27969;&#24418;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#36890;&#36807;&#23558;&#25968;&#25454;&#25311;&#21512;&#39033;&#38480;&#21046;&#22312;&#23398;&#20064;&#30340;&#27969;&#24418;&#19978;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#20135;&#29983;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#30340;&#27969;&#24418;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#32500;&#29609;&#20855;&#20363;&#23376;&#20197;&#21450;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing a manifold of very high-dimensional data with generative models has been shown to be computationally efficient in practice. However, this requires that the data manifold admits a global parameterization. In order to represent manifolds of arbitrary topology, we propose to learn a mixture model of variational autoencoders. Here, every encoder-decoder pair represents one chart of a manifold. We propose a loss function for maximum likelihood estimation of the model weights and choose an architecture that provides us the analytical expression of the charts and of their inverses. Once the manifold is learned, we use it for solving inverse problems by minimizing a data fidelity term restricted to the learned manifold. To solve the arising minimization problem we propose a Riemannian gradient descent algorithm on the learned manifold. We demonstrate the performance of our method for low-dimensional toy examples as well as for deblurring and electrical impedance tomography on cert
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2201.08078</link><description>&lt;p&gt;
&#29992;&#20004;&#26679;&#26412;&#26816;&#39564;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing. (arXiv:2201.08078v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#20854;&#20182;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#26159;&#36825;&#20123;&#31639;&#27861;&#38754;&#20020;&#30340;&#24050;&#30693;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#29978;&#33267;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#23558;&#20559;&#24046;&#38382;&#39064;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#36827;&#34892;&#26694;&#26550;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#20272;&#35745;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#26368;&#22823;&#26399;&#26395;&#20540;&#65288;MEV&#65289;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;$T$-&#20272;&#35745;&#22120;&#65288;TE&#65289;&#65292;&#36890;&#36807;&#35843;&#25972;&#24213;&#23618;&#20551;&#35774;&#26816;&#39564;&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#65292;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#31181;&#21629;&#21517;&#20026;$K$-&#20272;&#35745;&#22120;&#65288;KE&#65289;&#30340;&#25512;&#24191;&#36981;&#23432;&#19982;TE&#30456;&#21516;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#20381;&#36182;&#20110;&#20960;&#20046;&#20219;&#24847;&#30340;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;TE&#21644;KE&#30340;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#65288;BDQN&#65289;&#30340;&#20462;&#25913;&#65292;&#24182;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#20307;&#30340;&#22522;&#20110;TE&#30340;BDQN&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-based reinforcement-learning algorithms have shown strong results in games, robotics, and other real-world applications. Overestimation bias is a known threat to those algorithms and can lead to dramatic performance decreases or even complete algorithmic failure. We frame the bias problem statistically and consider it an instance of estimating the maximum expected value (MEV) of a set of random variables. We propose the $T$-Estimator (TE) based on two-sample testing for the mean, that flexibly interpolates between over- and underestimation by adjusting the significance level of the underlying hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same bias and variance bounds as the TE while relying on a nearly arbitrary kernel function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and prove convergence in the tabular setting. Furthermore, we propose an adaptive variant of the TE-based BDQN that
&lt;/p&gt;</description></item></channel></rss>