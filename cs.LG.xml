<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01089</link><description>&lt;p&gt;
&#26080;&#20813;&#36153;&#20462;&#21098;&#65306;&#21021;&#22987;&#21270;&#26102;&#21098;&#26525;&#30340;&#20449;&#24687;&#35770;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
No Free Prune: Information-Theoretic Barriers to Pruning at Initialization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25277;&#22870;&#20013;&#22870;&#32773;&#8221;&#26159;&#21542;&#22312;&#21021;&#22987;&#21270;&#26102;&#23384;&#22312;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#20196;&#20154;&#30528;&#36855;&#30340;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#38656;&#35201;&#22823;&#22411;&#27169;&#22411;&#65292;&#25110;&#32773;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;&#21253;&#21547;&#23427;&#20204;&#30340;&#23494;&#38598;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#35782;&#21035;&#21644;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23581;&#35797;&#22312;&#21021;&#22987;&#21270;&#26102;&#25214;&#21040;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;&#8220;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#8221;&#65289;&#30340;&#21162;&#21147;&#22312;&#24191;&#27867;&#19978;&#37117;&#27809;&#26377;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;$p_\text{eff}$&#65292;&#30001;&#26368;&#32456;&#32593;&#32476;&#20013;&#38750;&#38646;&#26435;&#37325;&#30340;&#25968;&#37327;&#21644;&#31232;&#30095;&#25513;&#30721;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30340;&#24635;&#21644;&#32473;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#40065;&#26834;&#24615;&#23450;&#24459;&#8221;&#65288;arXiv:2105.12806&#65289;&#24310;&#20280;&#21040;&#31232;&#30095;&#32593;&#32476;&#65292;&#20854;&#20013;&#24120;&#35268;&#21442;&#25968;&#25968;&#37327;&#34987;$p_\text{eff}$&#25152;&#21462;&#20195;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#33021;&#22815;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of "lottery tickets" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ("pruning at initialization") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.01039</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#65306;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#39033;&#37325;&#35201;&#35758;&#31243;&#12290;&#30001;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#22312;&#25968;&#23398;&#19978;&#34987;&#34920;&#36798;&#20026;&#36229;&#22270;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#37492;&#20110;&#36825;&#19968;&#26032;&#36235;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20221;&#33268;&#21147;&#20110;HNNs&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;HNNs&#20998;&#35299;&#20026;&#22235;&#20010;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#36755;&#20837;&#29305;&#24449;&#65292;&#65288;ii&#65289;&#36755;&#20837;&#32467;&#26500;&#65292;&#65288;iii&#65289;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#21644;&#65288;iv&#65289;&#35757;&#32451;&#31574;&#30053;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32771;&#23519;HNNs&#22914;&#20309;&#36890;&#36807;&#21508;&#33258;&#30340;&#32452;&#25104;&#37096;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;HNNs&#22312;&#25512;&#33616;&#12289;&#29983;&#29289;&#21644;&#21307;&#23398;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01039v1 Announce Type: new  Abstract: Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, biological and med
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.17436</link><description>&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Particle identification with machine learning from incomplete data in the ALICE experiment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17436
&lt;/p&gt;
&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHC&#30340;ALICE&#23454;&#39564;&#27979;&#37327;&#22312;&#36229;&#30456;&#23545;&#35770;&#37325;&#31163;&#23376;&#23545;&#25758;&#20013;&#24418;&#25104;&#30340;&#24378;&#30456;&#20114;&#20316;&#29992;&#29289;&#36136;&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#20934;&#30830;&#30340;&#31890;&#23376;&#35782;&#21035;(PID)&#12290;ALICE&#36890;&#36807;&#20960;&#20010;&#25506;&#27979;&#22120;&#20026;&#21160;&#37327;&#20174;&#32422;100 MeV/c&#21040;20 GeV/c&#30340;&#31890;&#23376;&#25552;&#20379;PID&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#31890;&#23376;&#26159;&#36890;&#36807;&#30697;&#24418;&#20999;&#21106;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;(NN)&#20316;&#20026;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#20851;&#27880;&#25193;&#23637;&#20102;&#31890;&#23376;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#23545;&#21253;&#21547;&#19981;&#23436;&#25972;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#30340;&#38598;&#25104;&#65292;&#24182;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#65292;&#36825;&#26159;&#23558;&#30693;&#35782;&#20174;&#27169;&#25311;&#25968;&#25454;&#36716;&#31227;&#21040;&#23454;&#38469;&#23454;&#39564;&#25968;&#25454;&#25152;&#38656;&#30340;ML&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17436v1 Announce Type: cross  Abstract: The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12856</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#28145;&#24230;RL&#31574;&#30053;&#21644;&#20540;&#32593;&#32476;&#20998;&#21035;&#26159;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#20197;&#21033;&#29992;&#36825;&#20123;&#23545;&#31216;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#20851;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#26469;&#35774;&#35745;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#21482;&#33021;&#20351;&#29992;&#38750;&#24120;&#21463;&#38480;&#30340;&#32452;&#20214;&#24211;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31561;&#21464;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#26377;&#30410;&#20110;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#20998;&#31867;&#22120;DistClassiPy&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65292;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.12120</link><description>&lt;p&gt;
&#20351;&#29992;DistClassiPy&#36827;&#34892;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Light Curve Classification with DistClassiPy: a new distance-based classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#20998;&#31867;&#22120;DistClassiPy&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#20998;&#31867;&#65292;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#25991;&#23398;&#39046;&#22495;&#30340;&#24033;&#22825;&#35843;&#26597;&#30340;&#20852;&#36215;&#24341;&#39046;&#20102;&#26102;&#22495;&#22825;&#25991;&#23398;&#20013;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20351;&#24471;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#30740;&#31350;&#22825;&#20307;&#23545;&#35937;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#36317;&#31163;&#24230;&#37327;&#26469;&#36741;&#21161;&#23545;&#35937;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#26032;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;DistClassiPy&#12290;&#30452;&#25509;&#20351;&#29992;&#36317;&#31163;&#24230;&#37327;&#26159;&#19968;&#31181;&#22312;&#26102;&#22495;&#22825;&#25991;&#23398;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20294;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22686;&#21152;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23545;&#21487;&#21464;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;10&#20010;&#31867;&#21035;&#20013;&#30340;6000&#39063;&#21487;&#21464;&#26143;&#30340;&#30446;&#24405;&#24212;&#29992;18&#31181;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;&#21644;&#32500;&#24230;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12120v1 Announce Type: cross  Abstract: The rise of synoptic sky surveys has ushered in an era of big data in time-domain astronomy, making data science and machine learning essential tools for studying celestial objects. Tree-based (e.g. Random Forests) and deep learning models represent the current standard in the field. We explore the use of different distance metrics to aid in the classification of objects. For this, we developed a new distance metric based classifier called DistClassiPy. The direct use of distance metrics is an approach that has not been explored in time-domain astronomy, but distance-based methods can aid in increasing the interpretability of the classification result and decrease the computational costs. In particular, we classify light curves of variable stars by comparing the distances between objects of different classes. Using 18 distance metrics applied to a catalog of 6,000 variable stars in 10 classes, we demonstrate classification and dimensio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Combinatorial Optimization via Heat Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#28909;&#25193;&#25955;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#12290;&#38024;&#23545;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35775;&#38382;&#35299;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33324;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19968;&#31995;&#21015;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#24191;&#27867;&#36935;&#21040;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#65292;&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;</title><link>https://arxiv.org/abs/2403.02737</link><description>&lt;p&gt;
&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Fractional Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#65292;&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#65288;FDEs&#65289;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290; &#23427;&#20204;&#23558;&#20256;&#32479;&#30340;&#24494;&#20998;&#21644;&#31215;&#20998;&#27010;&#24565;&#25193;&#23637;&#21040;&#38750;&#25972;&#25968;&#38454;&#65292;&#20351;&#24471;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#34920;&#31034;&#20855;&#26377;&#38750;&#23616;&#37096;&#21644;&#35760;&#24518;&#20381;&#36182;&#34892;&#20026;&#29305;&#24449;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21463;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;FDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35843;&#25972;FDE&#20197;&#36866;&#24212;&#25968;&#25454;&#21160;&#24577;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36825;&#39033;&#24037;&#20316;&#20840;&#38754;&#27010;&#36848;&#20102;&#31070;&#32463;FDE&#20013;&#37319;&#29992;&#30340;&#25968;&#20540;&#26041;&#27861;&#21644;&#31070;&#32463;FDE&#26550;&#26500;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#35745;&#31639;&#35201;&#27714;&#26356;&#39640;&#65292;&#31070;&#32463;FDE&#21487;&#33021;&#20248;&#20110;&#31070;&#32463;OD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02737v1 Announce Type: new  Abstract: Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours.   This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions.   Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data.   This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural OD
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.18729</link><description>&lt;p&gt;
&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#28237;&#27969;&#23553;&#38381;&#27169;&#22411;&#36827;&#34892;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18729
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20026;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#20013;&#30340;&#23376;&#28388;&#27874;&#23610;&#24230;&#65288;SFS&#65289;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#29289;&#29702;&#30340;&#23553;&#38381;&#27169;&#22411;&#24418;&#24335;&#65292;&#20294;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#25552;&#20379;&#30340;&#22823;&#37327;&#25968;&#25454;&#20026;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#25216;&#26415;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#28789;&#27963;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20173;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#37319;&#29992;&#36825;&#31181;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#21487;&#38752;&#22320;&#20272;&#35745;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#25968;&#25454;&#30693;&#35782;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#26469;&#25429;&#25417;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#36923;&#36753;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28388;&#27874;&#36827;&#23637;&#21464;&#37327;&#26631;&#37327;&#32791;&#25955;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BNN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#39537;&#21160;&#23553;&#38381;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32467;&#26500;&#30340;&#29420;&#29305;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18729v1 Announce Type: cross  Abstract: While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the
&lt;/p&gt;</description></item><item><title>DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15534</link><description>&lt;p&gt;
DiCoM -- &#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#20197;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#30740;&#31350;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15534
&lt;/p&gt;
&lt;p&gt;
DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#20020;&#24202;&#25104;&#20687;&#27169;&#24577;&#65292;&#22312;&#21508;&#31181;&#32954;&#37096;&#21644;&#24515;&#33039;&#30456;&#20851;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#25918;&#23556;&#23398;&#35835;&#29255;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20020;&#24202;&#35786;&#26029;&#24037;&#20855;&#35774;&#35745;&#31574;&#30053;&#38656;&#35201;&#39640;&#36136;&#37327;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#32988;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20195;&#34920;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#19982;&#33258;&#28982;&#22270;&#20687;&#65288;&#20363;&#22914;ImageNet&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19981;&#21516;&#65292;&#22240;&#20026;&#20020;&#24202;&#22270;&#20687;&#20855;&#26377;&#29420;&#29305;&#23646;&#24615;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#65288;DiCoM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#23398;&#29983;&#25945;&#24072;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#26377;&#25928;&#34920;&#31034;CXR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15534v1 Announce Type: cross  Abstract: Chest X-Ray (CXR) is a widely used clinical imaging modality and has a pivotal role in the diagnosis and prognosis of various lung and heart related conditions. Conventional automated clinical diagnostic tool design strategies relying on radiology reads and supervised learning, entail the cumbersome requirement of high quality annotated training data. To address this challenge, self-supervised pre-training has proven to outperform supervised pre-training in numerous downstream vision tasks, representing a significant breakthrough in the field. However, medical imaging pre-training significantly differs from pre-training with natural images (e.g., ImageNet) due to unique attributes of clinical images. In this context, we introduce Diverse Concept Modeling (DiCoM), a novel self-supervised training paradigm that leverages a student teacher framework for learning diverse concepts and hence effective representation of the CXR data. Hence, e
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.11933</link><description>&lt;p&gt;
SLADE&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11933
&lt;/p&gt;
&lt;p&gt;
SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#27979;&#30495;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#24322;&#24120;&#65292;&#22914;&#31038;&#20132;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#37329;&#34701;&#32593;&#32476;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#22270;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#36793;&#32536;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65306;(a)&#22312;&#24322;&#24120;&#21457;&#29983;&#26102;&#21363;&#26102;&#26816;&#27979;&#24322;&#24120;&#65292;(b)&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#29366;&#24577;&#65292;(c)&#22788;&#29702;&#21160;&#24577;&#24322;&#24120;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLADE&#65288;&#36793;&#32536;&#27969;&#24322;&#24120;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#27969;&#20013;&#24555;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#12290;SLADE&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#22312;&#26102;&#38388;&#19978;&#30456;&#20114;&#20316;&#29992;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#36827;&#20837;&#24322;&#24120;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20026;&#27492;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#65306;(a)&#26368;&#23567;&#21270;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#28418;&#31227;&#65292;(b)&#20174;&#30701;&#26399;&#29983;&#25104;&#38271;&#26399;&#20132;&#20114;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11933v1 Announce Type: new  Abstract: To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05981</link><description>&lt;p&gt;
&#25506;&#32034;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of In-Browser Deep Learning Inference on Quality of User Experience and Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#36890;&#36807;&#8220;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#8221;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20854;&#20013;DL&#22788;&#29702;&#30452;&#25509;&#22312;Web&#27983;&#35272;&#22120;&#20013;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21450;&#20854;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#36825;&#31181;&#30693;&#35782;&#30340;&#31354;&#30333;&#38656;&#35201;&#26032;&#24418;&#24335;&#30340;QoE&#27979;&#37327;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#25351;&#26631;&#65292;&#22914;&#39029;&#38754;&#21152;&#36733;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#30340;&#39318;&#27425;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#30740;&#31350;&#21253;&#25324;9&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#22312;50&#20010;&#24120;&#29992;&#30340;PC Web&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65306;&#22312;CPU&#19978;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#65292;&#22312;GPU&#19978;&#24930;4.9&#20493;&#12290;&#36825;&#31181;&#24310;&#36831;&#26377;&#20960;&#20010;&#22240;&#32032;&#23548;&#33268;&#65292;&#21253;&#25324;&#26410;&#20805;&#20998;&#20351;&#29992;&#30340;&#30828;&#20214;&#25351;&#20196;&#38598;&#65292;&#22266;&#26377;&#30340;&#24310;&#36831;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is increasingly being integrated into Web applications through a method known as "in-browser inference", where the DL processes occur directly within Web browsers. However, the actual performance of this method and its effect on user experience quality (QoE) is not well-understood. This gap in knowledge necessitates new forms of QoE measurement, going beyond traditional metrics such as page load time. To address this, we conducted the first extensive performance evaluation of in-browser inference. We introduced new metrics for this purpose: responsiveness, smoothness, and inference accuracy.   Our thorough study included 9 widely-used DL models and tested them across 50 popular PC Web browsers. The findings show a significant latency issue with in-browser inference: it's on average 16.9 times slower on CPU and 4.9 times slower on GPU than native inference methods. Several factors contribute to this latency, including underused hardware instruction sets, inherent dela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02750</link><description>&lt;p&gt;
KIVI&#65306;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#26381;&#21153;&#38656;&#35201;&#23558;&#35768;&#22810;&#35831;&#27714;&#25209;&#37327;&#22788;&#29702;&#20197;&#20943;&#23569;&#27599;&#20010;&#35831;&#27714;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#20197;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#26174;&#33879;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25104;&#20026;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#29942;&#39048;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;GPU&#30340;SRAM&#24517;&#39035;&#20174;&#20027;GPU&#20869;&#23384;&#20013;&#21152;&#36733;&#25972;&#20010;KV&#32531;&#23384;&#20197;&#29983;&#25104;&#27599;&#20010;&#26631;&#35760;&#65292;&#23548;&#33268;&#35745;&#31639;&#26680;&#24515;&#22312;&#27492;&#36807;&#31243;&#20013;&#22788;&#20110;&#31354;&#38386;&#29366;&#24577;&#12290;&#20943;&#23567;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#19968;&#20010;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37327;&#21270;&#65292;&#36890;&#36807;&#20943;&#23569;KV&#32531;&#23384;&#25152;&#38656;&#30340;&#24635;&#23383;&#33410;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;KV&#32531;&#23384;&#20803;&#32032;&#20998;&#24067;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#20102;&#35299;KV&#32531;&#23384;&#37327;&#21270;&#30340;&#38590;&#24230;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20803;&#32032;&#20998;&#24067;&#30740;&#31350;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
&lt;/p&gt;</description></item><item><title>QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02290</link><description>&lt;p&gt;
&#29699;&#24418;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#32858;&#31867;&#65306;R&#21644;Python&#20013;&#30340;QuadratiK&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02290
&lt;/p&gt;
&lt;p&gt;
QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QuadratiK&#36719;&#20214;&#21253;&#65292;&#35813;&#36719;&#20214;&#21253;&#21253;&#21547;&#20102;&#21019;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#65292;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#20108;&#27425;&#36317;&#31163;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#23454;&#29616;&#20102;&#21333;&#26679;&#26412;&#12289;&#21452;&#26679;&#26412;&#21644;k&#26679;&#26412;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#25968;&#23398;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#24230;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#25193;&#23637;&#20102;&#21151;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;$d$&#32500;&#29699;&#19978;&#22343;&#21248;&#24615;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#27850;&#26494;&#26680;&#23494;&#24230;&#20013;&#29983;&#25104;&#38543;&#26426;&#26679;&#26412;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#19968;&#31181;&#38024;&#23545;&#29699;&#24418;&#25968;&#25454;&#32780;&#29305;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#29420;&#29305;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#29699;&#38754;&#19978;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#20854;&#20182;&#22270;&#24418;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms for generating random samples from Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson-kernel-based densities on the sphere. Alongside this, our software includes additional graphical func
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.03853</link><description>&lt;p&gt;
LLMs&#30340;&#20004;&#38754;&#24615;&#65306;Jekyll&#21338;&#22763;&#19982;Hyde&#20808;&#29983;
&lt;/p&gt;
&lt;p&gt;
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#19968;&#24180;&#21069;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#21512;&#20687;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#26102;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#21161;&#25163;&#20135;&#29983;&#19981;&#24403;&#22238;&#24212;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#65288;&#20197;&#21450;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;Bing chat&#65289;&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#36825;&#20123;&#25514;&#26045;&#65292;&#36825;&#20123;&#35282;&#33394;&#19982;&#23427;&#20204;&#26412;&#24212;&#25104;&#20026;&#30340;&#30495;&#23454;&#21161;&#25163;&#30340;&#29305;&#24449;&#30456;&#21453;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#36896;&#20986;&#36825;&#20123;&#20154;&#29289;&#35282;&#33394;&#30340;&#22797;&#26434;&#20256;&#35760;&#65292;&#28982;&#21518;&#22312;&#21516;&#19968;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26032;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23545;&#35805;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#39118;&#26684;&#65292;&#20197;&#33719;&#24471;&#21161;&#25163;&#19981;&#34987;&#20801;&#35768;&#25552;&#20379;&#30340;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#29289;&#35282;&#33394;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
&lt;/p&gt;</description></item><item><title>ServerlessLLM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#26412;&#22320;&#21270;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#26597;&#28857;&#21152;&#36733;&#12289;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#26381;&#21153;&#22120;&#20998;&#37197;&#26469;&#23454;&#29616;&#39640;&#25928;&#19988;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.14351</link><description>&lt;p&gt;
ServerlessLLM&#65306;&#22686;&#24378;&#26412;&#22320;&#21270;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models. (arXiv:2401.14351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14351
&lt;/p&gt;
&lt;p&gt;
ServerlessLLM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#26412;&#22320;&#21270;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#26597;&#28857;&#21152;&#36733;&#12289;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#26381;&#21153;&#22120;&#20998;&#37197;&#26469;&#23454;&#29616;&#39640;&#25928;&#19988;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ServerlessLLM&#65292;&#19968;&#31181;&#22686;&#24378;&#26412;&#22320;&#21270;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#12290;ServerlessLLM&#21033;&#29992;GPU&#26381;&#21153;&#22120;&#19978;&#21487;&#29992;&#30340;&#23384;&#20648;&#21644;&#20869;&#23384;&#35774;&#22791;&#30340;&#22823;&#23481;&#37327;&#21644;&#24102;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#36828;&#31243;&#26816;&#26597;&#28857;&#19979;&#36733;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#26816;&#26597;&#28857;&#21152;&#36733;&#12290;ServerlessLLM&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65306;(i)&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#36733;&#20248;&#21270;&#26816;&#26597;&#28857;&#26684;&#24335;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#22810;&#32423;&#26816;&#26597;&#28857;&#21152;&#36733;&#31995;&#32479;&#23454;&#29616;&#24555;&#36895;LLM&#26816;&#26597;&#28857;&#21152;&#36733;&#65307;(ii)&#21033;&#29992;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#23454;&#26102;&#36801;&#31227;&#65292;&#20351;ServerlessLLM&#33021;&#22815;&#22312;&#20445;&#25345;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#23454;&#29616;&#26412;&#22320;&#21270;&#39537;&#21160;&#30340;&#26381;&#21153;&#22120;&#20998;&#37197;&#65307;(iii)&#26412;&#22320;&#21270;&#24863;&#30693;&#30340;&#26381;&#21153;&#22120;&#20998;&#37197;&#65292;&#20351;ServerlessLLM&#33021;&#22815;&#35780;&#20272;&#38598;&#32676;&#20013;&#27599;&#20010;&#26381;&#21153;&#22120;&#30340;&#29366;&#24577;&#65292;&#24182;&#26377;&#25928;&#22320;&#23433;&#25490;&#27169;&#22411;&#21551;&#21160;&#26102;&#38388;&#20197;&#20805;&#20998;&#21033;&#29992;&#26412;&#22320;&#26816;&#26597;&#28857;&#20301;&#32622;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#24494;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;ServerlessLLM&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include micr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13429</link><description>&lt;p&gt;
&#30456;&#20851;&#38543;&#26426;&#21521;&#37327;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of Correlated Random Vectors. (arXiv:2401.13429v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21028;&#26029;&#20004;&#20010;&#26631;&#20934;&#27491;&#24577;&#38543;&#26426;&#21521;&#37327;$\mathsf{X}\in\mathbb{R}^{n}$&#21644;$\mathsf{Y}\in\mathbb{R}^{n}$&#26159;&#21542;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#32479;&#35745;&#29420;&#31435;&#30340;&#65292;&#32780;&#22312;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;$\mathsf{X}$&#21644;&#38543;&#26426;&#22343;&#21248;&#32622;&#25442;&#30340;$\mathsf{Y}$&#26159;&#20855;&#26377;&#30456;&#20851;&#31995;&#25968;$\rho$&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20449;&#24687;&#35770;&#19978;&#19981;&#21487;&#33021;&#21644;&#21487;&#33021;&#30340;&#26368;&#20248;&#27979;&#35797;&#38408;&#20540;&#65292;&#20316;&#20026;$n$&#21644;$\rho$&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#24471;&#20986;&#25105;&#20204;&#30340;&#20449;&#24687;&#35770;&#19979;&#30028;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#23637;&#24320;&#26469;&#35780;&#20272;&#20284;&#28982;&#27604;&#30340;&#20108;&#38454;&#30697;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25581;&#31034;&#20102;&#19982;&#25972;&#25968;&#20998;&#21106;&#20989;&#25968;&#20043;&#38388;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#36848;&#35774;&#32622;&#30340;&#22810;&#32500;&#27867;&#21270;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#25968;&#25454;&#24211;/&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#20004;&#20010;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of deciding whether two standard normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and $\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\mathsf{X}$ and a randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with correlation $\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04280</link><description>&lt;p&gt;
&#39044;&#27979;&#21160;&#24577;&#22270;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Predicting the structure of dynamic graphs. (arXiv:2401.04280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#12289;&#24402;&#32435;&#21644;&#22686;&#37327;&#23398;&#20064;&#26377;&#21161;&#20110;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20174;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#22270;&#32467;&#26500;&#65292;&#20801;&#35768;&#26377;&#26032;&#33410;&#28857;&#65292;&#24182;&#27809;&#26377;&#21463;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#23558;&#20854;&#19982;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#65288;&#19968;&#31181;&#22312;&#29983;&#29289;&#21270;&#23398;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#39044;&#27979;&#22270;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00773</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#23376;&#25277;&#26679;&#38598;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#32479;&#35745;&#21407;&#29702;&#19978;&#26377;&#30452;&#35266;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#20256;&#32479;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#24191;&#27867;&#37319;&#29992;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#21463;&#21040;&#19982;&#26500;&#24314;&#26816;&#27979;&#22120;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#65292;&#19981;&#20165;&#30830;&#20445;&#20102;&#39640;&#25928;&#35745;&#31639;&#65292;&#36824;&#22686;&#24378;&#20102;&#32467;&#26524;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#19968;&#33324;&#36866;&#29992;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.20498</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#36830;&#32493;&#25968;&#25454;&#30340;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Learning of Continuous Data by Tensor Networks. (arXiv:2310.20498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20498
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#19968;&#33324;&#36866;&#29992;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#38500;&#20102;&#29992;&#20110;&#24314;&#27169;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#22806;&#65292;&#36824;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#30417;&#30563;&#29983;&#25104;&#23398;&#20064;&#20013;&#12290;&#28982;&#32780;&#65292;&#20197;&#37327;&#23376;&#21551;&#21457;&#24335;&#20026;&#29305;&#28857;&#30340;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#20043;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38382;&#39064;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#21253;&#21547;&#36830;&#32493;&#38543;&#26426;&#21464;&#37327;&#30340;&#20998;&#24067;&#30340;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#23616;&#38480;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#30697;&#38453;&#31215;&#24577;&#30340;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#27169;&#22411;&#26063;&#33021;&#22815;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#20219;&#20309;&#30456;&#23545;&#24179;&#28369;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#19968;&#33324;&#34920;&#36798;&#24615;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.09149</link><description>&lt;p&gt;
&#24494;&#20998;&#27700;&#24179;&#31354;&#38388;&#20013;&#30340;&#26684;&#28857;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Lattice Approximations in Wasserstein Space. (arXiv:2310.09149v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;Wasserstein&#31354;&#38388;$W_p(\mathbb{R}^d)$&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#26469;&#23545;&#27979;&#24230;&#36827;&#34892;&#32467;&#26500;&#36924;&#36817;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#19968;&#20010;&#28385;&#31209;&#30340;&#26684;&#28857;$\Lambda$&#25353;&#29031;$h\in(0,1]$&#30340;&#27604;&#20363;&#36827;&#34892;&#32553;&#25918;&#65292;&#37027;&#20040;&#22522;&#20110;$h\Lambda$&#30340;Voronoi&#20998;&#21106;&#24471;&#21040;&#30340;&#27979;&#24230;&#36924;&#36817;&#26159;$O(h)$&#65292;&#19981;&#35770;$d$&#25110;$p$&#30340;&#21462;&#20540;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35206;&#30422;&#35770;&#35777;&#35777;&#26126;&#65292;&#23545;&#20110;&#32039;&#25903;&#25745;&#30340;&#27979;&#24230;&#30340;$N$&#39033;&#36924;&#36817;&#26159;$O(N^{-\frac1d})$&#65292;&#36825;&#19982;&#26368;&#20248;&#37327;&#21270;&#22120;&#21644;&#32463;&#39564;&#27979;&#24230;&#36924;&#36817;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24050;&#30693;&#30340;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#65292;&#35201;&#27714;&#20854;&#20855;&#26377;&#36275;&#22815;&#30340;&#34928;&#20943;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider structured approximation of measures in Wasserstein space $W_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ by discrete and piecewise constant measures based on a scaled Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice $\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Finally, we extend these results to noncompactly supported measures with sufficient decay.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#12290;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#26159;&#19968;&#31867;&#20855;&#26377;&#24179;&#28369;&#24615;&#36136;&#36807;&#28193;&#30340;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#21644;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.16571</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials. (arXiv:2309.16571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#12290;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#26159;&#19968;&#31867;&#20855;&#26377;&#24179;&#28369;&#24615;&#36136;&#36807;&#28193;&#30340;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#21644;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#36890;&#36807;&#23454;&#29616;&#30452;&#25509;&#26448;&#26009;&#36830;&#25509;&#65292;&#38761;&#26032;&#20102;&#22797;&#26434;&#38646;&#20214;&#30340;&#21046;&#36896;&#65292;&#24182;&#25552;&#20379;&#20102;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#22797;&#26434;&#38646;&#20214;&#21046;&#36896;&#12289;&#20943;&#23569;&#21046;&#36896;&#24223;&#26009;&#20197;&#21450;&#20026;&#21046;&#36896;&#33258;&#21160;&#21270;&#24320;&#21551;&#26032;&#30340;&#21487;&#33021;&#24615;&#31561;&#22810;&#20010;&#20248;&#21183;&#12290;&#20854;&#20013;&#65292;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#65288;FGMs&#65289;&#20316;&#20026;&#19968;&#31867;&#26448;&#26009;&#65292;&#22312;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;FGMs&#26159;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#20854;&#24615;&#36136;&#21576;&#24179;&#28369;&#36807;&#28193;&#65292;&#22240;&#27492;&#34987;&#33322;&#31354;&#12289;&#27773;&#36710;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#22269;&#38450;&#31561;&#34892;&#19994;&#24191;&#27867;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#22797;&#21512;&#26448;&#26009;&#19981;&#21516;&#65292;FGMs&#20013;&#30340;&#25104;&#20998;&#20250;&#36880;&#28176;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26448;&#26009;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#21046;&#36896;FGMs&#30340;&#26377;&#24076;&#26395;&#30340;&#25163;&#27573;&#65292;&#21487;&#20197;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#24182;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has revolutionized the manufacturing of complex parts by enabling direct material joining and offers several advantages such as cost-effective manufacturing of complex parts, reducing manufacturing waste, and opening new possibilities for manufacturing automation. One group of materials for which additive manufacturing holds great potential for enhancing component performance and properties is Functionally Graded Materials (FGMs). FGMs are advanced composite materials that exhibit smoothly varying properties making them desirable for applications in aerospace, automobile, biomedical, and defense industries. Such composition differs from traditional composite materials, since the location-dependent composition changes gradually in FGMs, leading to enhanced properties. Recently, machine learning techniques have emerged as a promising means for fabrication of FGMs through optimizing processing parameters, improving product quality, and detecting manufacturing defect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06679</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#23454;&#29616;&#23545;Spalart-Allmaras&#27169;&#22411;&#30340;&#21487;&#26222;&#36866;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data. (arXiv:2309.06679v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#27169;&#22411;&#21644;&#25968;&#25454;&#34701;&#21512;&#25913;&#36827;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;Spalart-Allmaras&#65288;SA&#65289;&#38381;&#21512;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#23558;&#31232;&#30095;&#30340;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#20197;&#25913;&#21892;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#33021;&#36890;&#36807;&#24674;&#22797;&#32463;&#20856;&#30340;SA&#34892;&#20026;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#21516;&#21270;&#65292;&#21363;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65288;EnKF&#65289;&#65292;&#36890;&#36807;&#23558;SA&#27169;&#22411;&#30340;&#31995;&#25968;&#26657;&#20934;&#21040;&#20998;&#31163;&#27969;&#20307;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#21442;&#25968;&#21270;&#20135;&#29983;&#12289;&#25193;&#25955;&#21644;&#30772;&#22351;&#39033;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;&#35813;&#26657;&#20934;&#20381;&#36182;&#20110;&#37319;&#38598;&#30340;&#20998;&#31163;&#27969;&#20307;&#36895;&#24230;&#21078;&#38754;&#12289;&#22721;&#25830;&#21147;&#21644;&#21387;&#21147;&#31995;&#25968;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#21516;&#21270;&#12290;&#23613;&#31649;&#20165;&#20351;&#29992;&#20102;&#26469;&#33258;&#21333;&#19968;&#27969;&#21160;&#26465;&#20214;&#65288;&#29615;&#32469;&#19968;&#20010;&#32972;&#38754;&#21488;&#38454;&#65289;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#26657;&#20934;&#30340;SA&#27169;&#22411;&#34920;&#29616;&#20986;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to o
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#25552;&#20379;&#20855;&#26377;&#20445;&#35777;&#30340;&#21518;&#39564;&#36817;&#20284;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#23545;&#20110;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#26080;&#38656;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;CANVI&#33021;&#22815;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14275</link><description>&lt;p&gt;
&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#30340;&#20998;&#25674;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Amortized Variational Inference with Coverage Guarantees. (arXiv:2305.14275v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14275
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#25552;&#20379;&#20855;&#26377;&#20445;&#35777;&#30340;&#21518;&#39564;&#36817;&#20284;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#23545;&#20110;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#26080;&#38656;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;CANVI&#33021;&#22815;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25674;&#21464;&#20998;&#25512;&#26029;&#20135;&#29983;&#20102;&#19968;&#20010;&#21518;&#39564;&#36817;&#20284;&#65292;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#32473;&#23450;&#20219;&#20309;&#26032;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#36817;&#20284;&#21518;&#39564;&#30340;&#36136;&#37327;&#65292;&#24456;&#23569;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CANVI&#30340;&#19968;&#33268;&#21270;&#20998;&#25674;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#36793;&#38469;&#35206;&#30422;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#20505;&#36873;&#30340;&#20998;&#25674;&#21518;&#39564;&#36817;&#20284;&#22120;&#65292;CANVI&#22522;&#20110;&#27599;&#20010;&#20505;&#36873;&#26500;&#24314;&#19968;&#33268;&#21270;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#27979;&#25928;&#29575;&#36825;&#20010;&#24230;&#37327;&#26631;&#20934;&#27604;&#36739;&#39044;&#27979;&#22120;&#65292;&#24182;&#36820;&#22238;&#26368;&#39640;&#25928;&#30340;&#39044;&#27979;&#22120;&#12290;CANVI&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#22120;&#26500;&#24314;&#30340;&#21306;&#22495;&#20197;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#27700;&#24179;&#21253;&#21547;&#30495;&#23454;&#20540;&#12290;CANVI&#23545;&#20505;&#36873;&#36817;&#20284;&#22120;&#30340;&#21046;&#23450;&#20915;&#31574;&#19981;&#20851;&#24515;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#21069;&#21521;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#22312;&#26080;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#27979;&#25928;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amortized variational inference produces a posterior approximation that can be rapidly computed given any new observation. Unfortunately, there are few guarantees about the quality of these approximate posteriors. We propose Conformalized Amortized Neural Variational Inference (CANVI), a procedure that is scalable, easily implemented, and provides guaranteed marginal coverage. Given a collection of candidate amortized posterior approximators, CANVI constructs conformalized predictors based on each candidate, compares the predictors using a metric known as predictive efficiency, and returns the most efficient predictor. CANVI ensures that the resulting predictor constructs regions that contain the truth with a user-specified level of probability. CANVI is agnostic to design decisions in formulating the candidate approximators and only requires access to samples from the forward model, permitting its use in likelihood-free settings. We prove lower bounds on the predictive efficiency of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#36827;&#34892;&#32858;&#31867;&#65306;&#33021;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Clustering with minimum spanning trees: How good can it be?. (arXiv:2303.05679v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#22312;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#20379;&#26041;&#20415;&#30340;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#24182;&#19988;&#35745;&#31639;&#30456;&#23545;&#36739;&#24555;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;MST&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#20013;&#30340;&#24847;&#20041;&#31243;&#24230;&#12290;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#65288;oracle&#65289;&#31639;&#27861;&#19982;&#22823;&#37327;&#22522;&#20934;&#25968;&#25454;&#30340;&#19987;&#23478;&#26631;&#31614;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#19981;&#26159;&#25552;&#20986;&#21478;&#19968;&#20010;&#21482;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#65292;&#32780;&#26159;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#26368;&#26032;MST-based&#21010;&#20998;&#26041;&#26696;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#38750;MST&#31639;&#27861;&#65292;&#22914;k-means&#65292;&#39640;&#26031;&#28151;&#21512;&#65292;&#35889;&#32858;&#31867;&#65292;Birch&#65292;&#22522;&#20110;&#23494;&#24230;&#21644;&#32463;&#20856;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#36824;&#26159;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum spanning trees (MSTs) provide a convenient representation of datasets in numerous pattern recognition activities. Moreover, they are relatively fast to compute. In this paper, we quantify the extent to which they can be meaningful in partitional data clustering tasks in low-dimensional spaces. By identifying the upper bounds for the agreement between the best (oracle) algorithm and the expert labels from a large battery of benchmark data, we discover that MST methods are overall very competitive. Next, instead of proposing yet another algorithm that performs well on a limited set of examples, we review, study, extend, and generalise existing, state-of-the-art MST-based partitioning schemes. This leads to a few new and noteworthy approaches. Overall, Genie and the information-theoretic methods often outperform the non-MST algorithms such as k-means, Gaussian mixtures, spectral clustering, Birch, density-based, and classical hierarchical agglomerative procedures. Nevertheless, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.00390</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;"Web of Science"&#20013;&#23545;&#30740;&#31350;&#39046;&#22495;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23618;&#27425;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#36890;&#36807;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#19981;&#21516;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25277;&#35937;&#23558;&#23398;&#26415;&#20986;&#29256;&#29289;&#33258;&#21160;&#20998;&#31867;&#21040;&#19977;&#32423;&#23618;&#27425;&#26631;&#31614;&#38598;&#65288;&#23398;&#31185;&#12289;&#39046;&#22495;&#12289;&#23376;&#39046;&#22495;&#65289;&#20013;&#65292;&#20197;&#22810;&#31867;&#21035;&#35774;&#32622;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25991;&#31456;&#30340;&#30693;&#35782;&#29983;&#20135;&#21644;&#24341;&#29992;&#30340;&#24433;&#21709;&#65292;&#23545;&#30740;&#31350;&#27963;&#21160;&#22312;&#25152;&#36848;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#31867;&#65292;&#24182;&#20801;&#35768;&#36825;&#20123;&#27963;&#21160;&#34987;&#24402;&#20026;&#22810;&#20010;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#31995;&#32479;&#22312;Microsoft Academic Graph (&#29256;&#26412;2018-05-17)&#30340;160 million&#20221;&#25688;&#35201;&#29255;&#27573;&#20013;&#21306;&#20998;&#20102;44&#20010;&#23398;&#31185;&#12289;718&#20010;&#39046;&#22495;&#21644;1,485&#20010;&#23376;&#39046;&#22495;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#21644;&#20998;&#24067;&#24335;&#26041;&#24335;&#36827;&#34892;&#25209;&#37327;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#21644;&#20801;&#35768;&#36328;&#23398;&#31185;&#21644;&#36328;&#39046;&#22495;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#24418;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;3,140&#27425;&#23454;&#39564;&#12290;&#20998;&#31867;&#20934;&#30830;&#29575;&gt; 90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set (discipline, field, subfield) in a multi-class setting. This system enables a holistic categorization of research activities in the mentioned hierarchy in terms of knowledge production through articles and impact through citations, permitting those activities to fall into multiple categories. The classification system distinguishes 44 disciplines, 718 fields and 1,485 subfields among 160 million abstract snippets in Microsoft Academic Graph (version 2018-05-17). We used batch training in a modularized and distributed fashion to address and allow for interdisciplinary and interfield classifications in single-label and multi-label settings. In total, we have conducted 3,140 experiments in all considered models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers). The classification accuracy is &gt; 90%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.08403</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#29983;&#25104;&#24207;&#21015;&#65306;&#29702;&#35770;&#21450;&#20854;&#22312;&#26080;&#20154;&#26426;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#21512;&#25104;&#24207;&#21015;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#29983;&#25104;&#26694;&#26550;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#65292;&#36890;&#36807;&#30456;&#20284;&#24615;&#29983;&#25104;&#23376;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#23545;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#27425;&#24615;&#29983;&#25104;&#27169;&#22411;&#26469;&#20174;&#21333;&#20010;&#24207;&#21015;&#30340;&#33539;&#22260;&#20869;&#21462;&#26679;&#65292;&#24182;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#22686;&#24378;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2209.02935</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#32858;&#31867;&#20934;&#30830;&#24230;&#65306;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#19968;&#20010;&#26368;&#22909;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#21306;&#20998;&#20986;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#20869;&#37096;&#25110;&#22806;&#37096;&#26377;&#25928;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#20869;&#37096;&#24230;&#37327;&#37327;&#21270;&#25152;&#24471;&#20998;&#21306;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#65292;&#31751;&#32039;&#23494;&#24230;&#30340;&#24179;&#22343;&#31243;&#24230;&#25110;&#28857;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#20351;&#30340;&#32858;&#31867;&#26377;&#26102;&#21487;&#33021;&#26159;&#26080;&#24847;&#20041;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22806;&#37096;&#24230;&#37327;&#23558;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24120;&#29992;&#30340;&#32463;&#20856;&#20998;&#21306;&#30456;&#20284;&#24615;&#35780;&#20998;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#20114;&#20449;&#24687;&#12289;Fowlkes-Mallows&#25110;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968;&#65292;&#32570;&#23569;&#19968;&#20123;&#21487;&#21462;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#19981;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#22351;&#24773;&#20917;&#65292;&#20063;&#19981;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
&lt;/p&gt;</description></item></channel></rss>