<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.07247</link><description>&lt;p&gt;
GuideGen&#65306;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;CT&#20307;&#31215;&#21644;&#35299;&#21078;&#32467;&#26500;&#29983;&#25104;&#30340;&#25991;&#26412;&#24341;&#23548;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#20026;&#20102;&#25910;&#38598;&#24102;&#26377;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#30340;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#32780;&#36827;&#34892;&#30340;&#27880;&#37322;&#36127;&#25285;&#21644;&#22823;&#37327;&#24037;&#20316;&#24456;&#23569;&#26159;&#21010;&#31639;&#19988;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#32570;&#20047;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21066;&#24369;&#20102;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21152;&#21095;&#20102;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#22270;&#20687;&#20998;&#26512;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#26435;&#23452;&#20043;&#35745;&#65292;&#37492;&#20110;&#29983;&#25104;&#24615;&#31070;&#32463;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#29616;&#22312;&#21487;&#20197;&#22312;&#22806;&#37096;&#32422;&#26463;&#30340;&#24341;&#23548;&#19979;&#20197;&#39640;&#20445;&#30495;&#24230;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;GuideGen&#65306;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#33145;&#37096;&#22120;&#23448;&#21644;&#32467;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#21644;&#32452;&#32455;&#25513;&#33180;&#30340;&#31649;&#32447;&#65292;&#20854;&#21463;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20307;&#31215;&#25513;&#33180;&#37319;&#26679;&#22120;&#65292;&#20197;&#36866;&#24212;&#25513;&#33180;&#26631;&#31614;&#30340;&#31163;&#25955;&#20998;&#24067;&#24182;&#29983;&#25104;&#20302;&#20998;&#36776;&#29575;3D&#32452;&#32455;&#25513;&#33180;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#22120;&#20250;&#22312;&#25910;&#21040;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#33258;&#22238;&#24402;&#29983;&#25104;CT&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 Announce Type: cross  Abstract: The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OCD-FL&#30340;&#26032;&#26041;&#26696;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;</title><link>https://arxiv.org/abs/2403.04037</link><description>&lt;p&gt;
OCD-FL: &#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#36873;&#25321;&#30340;&#36890;&#20449;&#39640;&#25928;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OCD-FL&#30340;&#26032;&#26041;&#26696;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#30340;&#32467;&#21512;&#24320;&#21019;&#20102;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#20316;&#20026;&#26368;&#31361;&#20986;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#36825;&#20123;&#23398;&#20064;&#26041;&#26696;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#35299;&#20915;&#23427;&#20204;&#26368;&#22522;&#26412;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#20107;&#23454;&#19978;&#65292;&#20855;&#26377;&#20013;&#24515;&#32858;&#21512;&#22120;&#30340;&#20256;&#32479;FL&#23384;&#22312;&#21333;&#28857;&#25925;&#38556;&#21644;&#32593;&#32476;&#29942;&#39048;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33410;&#28857;&#22312;&#28857;&#23545;&#28857;&#32593;&#32476;&#20013;&#21327;&#20316;&#30340;&#21435;&#20013;&#24515;&#21270;FL&#12290;&#23613;&#31649;&#21518;&#32773;&#25928;&#29575;&#39640;&#65292;&#20294;&#22312;&#21435;&#20013;&#24515;&#21270;FL&#20013;&#65292;&#36890;&#20449;&#25104;&#26412;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#20173;&#28982;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#20250;&#20027;&#20041;&#36890;&#20449;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;(OCD-FL)&#30340;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#21253;&#25324;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#20197;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;&#21516;&#26102;&#20943;&#23569;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04037v1 Announce Type: new  Abstract: The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy co
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04292</link><description>&lt;p&gt;
AdaFlow: &#21464;&#24322;&#33258;&#36866;&#24212;&#27969;&#31574;&#30053;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04292
&lt;/p&gt;
&lt;p&gt;
AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20915;&#31574;&#20013;&#25913;&#36827;&#20102;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#65292;&#20294;&#30001;&#20110;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#36882;&#24402;&#32780;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22810;&#26679;&#21270;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;AdaFlow&#20351;&#29992;&#29366;&#24577;&#26465;&#20214;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34920;&#31034;&#31574;&#30053;&#65292;&#36825;&#34987;&#31216;&#20026;&#27010;&#29575;&#27969;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#35757;&#32451;&#25439;&#22833;&#30340;&#26465;&#20214;&#26041;&#24046;&#19982;ODE&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#20351;AdaFlow&#25104;&#20026;&#19968;&#20010;&#33258;&#36866;&#24212;&#20915;&#31574;&#32773;&#65292;&#33021;&#22815;&#24555;&#36895;&#25512;&#29702;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21160;&#20316;&#20998;&#24067;&#34987;&#38477;&#20302;&#21040;&#19968;&#27493;&#29983;&#25104;&#22120;&#26102;&#65292;&#23427;&#33258;&#21160;&#36864;&#21270;&#21040;&#19968;&#20010;&#19968;&#27493;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05520</link><description>&lt;p&gt;
DL&#27169;&#22411;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#33021;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#24615;&#33021;&#19978;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20851;&#20110;&#35757;&#32451;DL&#27169;&#22411;&#24102;&#26469;&#24040;&#22823;&#30899;&#36275;&#36857;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#23545;&#23454;&#29616;&#26356;&#29615;&#20445;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#27491;&#30830;&#24615;&#22312;&#26368;&#20339;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#29615;&#22659;&#23545;&#29983;&#25104;&#26356;&#29615;&#20445;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#20851;&#31995;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#19982;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#22312;&#27979;&#37327;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35757;&#32451;&#29615;&#22659;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05857</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#34987;&#21098;&#26525;&#21040;&#22810;&#20040;&#31232;&#30095;&#65306;&#20960;&#20309;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Sparse Can We Prune A Deep Network: A Geometric Viewpoint. (arXiv:2306.05857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#12290;&#34429;&#28982;&#23427;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#21152;&#20102;&#37325;&#22823;&#30340;&#23384;&#20648;&#36127;&#25285;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#32593;&#32476;&#21098;&#26525;&#12290;&#19968;&#20010;&#33258;&#28982;&#32780;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21098;&#26525;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#21040;&#22810;&#20040;&#31232;&#30095;&#65288;&#20960;&#20046;&#19981;&#24433;&#21709;&#24615;&#33021;&#65289;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#31532;&#19968;&#21407;&#29702;&#26041;&#27861;&#65292;&#20855;&#20307;&#22320;&#65292;&#21482;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#25551;&#36848;&#21098;&#26525;&#27604;&#29575;&#30340;&#23574;&#38160;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#23545;&#24212;&#20110;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#65292;&#36825;&#20123;&#20984;&#20307;&#26159;&#30001;$l_1$-&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#24471;&#20986;&#30340;&#65292;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21098;&#26525;&#36807;&#31243;&#20013;&#21442;&#25968;&#30340;&#20998;&#24067;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterization constitutes one of the most significant hallmarks of deep neural networks. Though it can offer the advantage of outstanding generalization performance, it meanwhile imposes substantial storage burden, thus necessitating the study of network pruning. A natural and fundamental question is: How sparse can we prune a deep network (with almost no hurt on the performance)? To address this problem, in this work we take a first principles approach, specifically, by merely enforcing the sparsity constraint on the original loss function, we're able to characterize the sharp phase transition point of pruning ratio, which corresponds to the boundary between the feasible and the infeasible, from the perspective of high-dimensional geometry. It turns out that the phase transition point of pruning ratio equals the squared Gaussian width of some convex body resulting from the $l_1$-regularized loss function, normalized by the original dimension of parameters. As a byproduct, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.09145</link><description>&lt;p&gt;
&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#22810;&#38754;&#20307;&#24322;&#24120;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#32593;&#32476;&#26159;&#19968;&#31181;&#22810;&#38754;&#20307;&#19978;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#30740;&#31350;&#36825;&#31181;&#22810;&#38754;&#20307;&#30340;&#24615;&#36136;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#22810;&#38754;&#20307;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20165;&#20572;&#30041;&#22312;&#35745;&#31639;&#25968;&#37327;&#30340;&#27700;&#24179;&#65292;&#36825;&#36828;&#36828;&#19981;&#33021;&#23436;&#25972;&#22320;&#25551;&#36848;&#22810;&#38754;&#20307;&#12290;&#20026;&#20102;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19977;&#35282;&#21078;&#20998;&#22810;&#38754;&#20307;&#24471;&#20986;&#22810;&#38754;&#20307;&#30340;&#24418;&#29366;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;&#19981;&#21516;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;ReLU&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#20855;&#26377;&#30456;&#23545;&#31616;&#21333;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#65292;&#23613;&#31649;&#36825;&#20123;&#22810;&#38754;&#20307;&#20174;&#29702;&#35770;&#19978;&#26469;&#35828;&#21487;&#20197;&#38750;&#24120;&#20016;&#23500;&#21644;&#22797;&#26434;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#24179;&#20961;&#30340;&#32452;&#21512;&#25512;&#23548;&#26469;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#20160;&#20040;&#22686;&#21152;&#28145;&#24230;&#19981;&#20250;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#22810;&#38754;&#20307;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#32500;&#24230;&#30340;&#24179;&#22343;&#21333;&#32431;&#24418;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
&lt;/p&gt;</description></item><item><title>Spectrum Breathing&#26159;&#19968;&#31181;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2305.05933</link><description>&lt;p&gt;
Spectrum Breathing&#65306;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference. (arXiv:2305.05933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05933
&lt;/p&gt;
&lt;p&gt;
Spectrum Breathing&#26159;&#19968;&#31181;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#20998;&#24067;&#24335;&#31227;&#21160;&#25968;&#25454;&#20013;&#33976;&#39311;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#33539;&#20363;&#12290;&#20294;&#32852;&#21512;&#23398;&#20064;&#22312;&#31227;&#21160;&#32593;&#32476;&#20013;&#30340;&#37096;&#32626;&#21487;&#33021;&#20250;&#21463;&#21040;&#37051;&#36817;&#21333;&#20803;&#25110;&#24178;&#25200;&#28304;&#30340;&#24178;&#25200;&#32780;&#21463;&#25439;&#12290;&#29616;&#26377;&#30340;&#24178;&#25200;&#25233;&#21046;&#25216;&#26415;&#38656;&#35201;&#22810;&#21333;&#20803;&#21512;&#20316;&#25110;&#33267;&#23569;&#38656;&#35201;&#26114;&#36149;&#30340;&#24178;&#25200;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#24178;&#25200;&#35270;&#20026;&#22122;&#22768;&#36827;&#34892;&#21151;&#29575;&#25511;&#21046;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#65292;&#20063;&#30001;&#20110;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#35302;&#21457;&#24178;&#25200;&#28304;&#30340;&#21453;&#21046;&#25514;&#26045;&#12290;&#20316;&#20026;&#20445;&#25252;&#31354;&#20013;&#32852;&#21512;&#23398;&#20064;&#20813;&#21463;&#24178;&#25200;&#30340;&#23454;&#38469;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Spectrum Breathing&#65292;&#23427;&#23558;&#38543;&#26426;&#26799;&#24230;&#21098;&#26525;&#21644;&#25193;&#39057;&#32423;&#32852;&#36215;&#26469;&#65292;&#20197;&#21387;&#21046;&#24178;&#25200;&#32780;&#26080;&#38656;&#25193;&#23637;&#24102;&#23485;&#12290;&#20195;&#20215;&#26159;&#36890;&#36807;&#21033;&#29992;&#21098;&#26525;&#23548;&#33268;&#23398;&#20064;&#36895;&#24230;&#20248;&#38597;&#38477;&#20302;&#32780;&#22686;&#21152;&#30340;&#23398;&#20064;&#24310;&#36831;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#25805;&#20316;&#21516;&#27493;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#30340;&#32423;&#21035;&#26159;&#30456;&#20114;&#23545;&#24212;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely embraced paradigm for distilling artificial intelligence from distributed mobile data. However, the deployment of FL in mobile networks can be compromised by exposure to interference from neighboring cells or jammers. Existing interference mitigation techniques require multi-cell cooperation or at least interference channel state information, which is expensive in practice. On the other hand, power control that treats interference as noise may not be effective due to limited power budgets, and also that this mechanism can trigger countermeasures by interference sources. As a practical approach for protecting FL against interference, we propose Spectrum Breathing, which cascades stochastic-gradient pruning and spread spectrum to suppress interference without bandwidth expansion. The cost is higher learning latency by exploiting the graceful degradation of learning speed due to pruning. We synchronize the two operations such that their levels are contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03223</link><description>&lt;p&gt;
&#31038;&#20250;&#27491;&#20041;&#31639;&#27861;&#65306;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24179;&#26435;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Social Justice: Affirmative Action in Social Networks. (arXiv:2305.03223v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#23545;&#20110;&#19990;&#30028;&#21508;&#22320;&#25968;&#21313;&#20159;&#29992;&#25143;&#30340;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#36890;&#24120;&#24314;&#35758;&#36830;&#25509;&#30456;&#20114;&#30456;&#20284;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#34987;&#21457;&#29616;&#20250;&#20135;&#29983;&#20449;&#24687;&#23396;&#23707;&#65292;&#21152;&#21095;&#24369;&#21183;&#31361;&#20986;&#32676;&#20307;&#25152;&#36973;&#21463;&#30340;&#23396;&#31435;&#65292;&#24182;&#24310;&#32493;&#31038;&#20250;&#25104;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#23454;&#29616;&#20844;&#24179;&#30340;&#38142;&#25509;&#25512;&#33616;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#19981;&#36136;&#30097;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#30340;&#26368;&#32456;&#30446;&#26631;&#65292;&#21363;&#25968;&#25454;&#20132;&#26131;&#30340;&#22797;&#26434;&#21830;&#19994;&#27169;&#22411;&#20013;&#29992;&#25143;&#21442;&#19982;&#30340;&#36135;&#24065;&#21270;&#12290;&#26412;&#25991;&#20027;&#24352;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#29609;&#23478;&#21644;&#30446;&#30340;&#30340;&#22810;&#26679;&#21270;&#65292;&#20197;&#23454;&#29616;&#31038;&#20250;&#27491;&#20041;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#27010;&#24565;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ERA-Link&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#26032;&#22411;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#25269;&#28040;&#31995;&#32479;&#24615;&#30340;&#31038;&#20250;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link recommendation algorithms contribute to shaping human relations of billions of users worldwide in social networks. To maximize relevance, they typically propose connecting users that are similar to each other. This has been found to create information silos, exacerbating the isolation suffered by vulnerable salient groups and perpetuating societal stereotypes. To mitigate these limitations, a significant body of work has been devoted to the implementation of fair link recommendation methods. However, most approaches do not question the ultimate goal of link recommendation algorithms, namely the monetization of users' engagement in intricate business models of data trade. This paper advocates for a diversification of players and purposes of social network platforms, aligned with the pursue of social justice. To illustrate this conceptual goal, we present ERA-Link, a novel link recommendation algorithm based on spectral graph theory that counteracts the systemic societal discriminat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;GNN&#27169;&#22411;&#65292;&#25351;&#20986;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#21487;&#33021;&#26377;&#30456;&#20284;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.10851</link><description>&lt;p&gt;
GNNs&#21040;&#24213;&#22312;&#23398;&#20160;&#20040;&#65311;&#8212;&#8212;&#29702;&#35299;&#23427;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What Do GNNs Actually Learn? Towards Understanding their Representations. (arXiv:2304.10851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;GNN&#27169;&#22411;&#65292;&#25351;&#20986;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#21487;&#33021;&#26377;&#30456;&#20284;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#23884;&#20837;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65288;&#21363;&#23427;&#20204;&#26159;&#21542;&#33021;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#23545;&#65289;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#21738;&#20123;&#32467;&#26500;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#27969;&#34892;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22914;&#26524;&#20004;&#20010;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#22312;&#26576;&#19968;&#23618;$k&gt;1$ &#20013;&#30340;&#27493;&#38271;&#30456;&#21516;&#65292;&#21017;&#23427;&#20204;&#30340;&#34920;&#31034;&#21487;&#33021;&#30456;&#20284;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have achieved great success in the field of graph representation learning. Although prior work has shed light into the expressiveness of those models (\ie whether they can distinguish pairs of non-isomorphic graphs), it is still not clear what structural information is encoded into the node representations that are learned by those models. In this paper, we investigate which properties of graphs are captured purely by these models, when no node attributes are available. Specifically, we study four popular GNN models, and we show that two of them embed all nodes into the same feature vector, while the other two models generate representations that are related to the number of walks over the input graph. Strikingly, structurally dissimilar nodes can have similar representations at some layer $k&gt;1$, if they have the same number of walks of length $k$. We empirically verify our theoretical findings on real datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08162</link><description>&lt;p&gt;
Huber&#33021;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Huber-energy measure quantization. (arXiv:2212.08162v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#27979;&#37327;&#37327;&#21270;&#36807;&#31243;&#65292;&#21363;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;$Q$&#20010;&#29380;&#25289;&#20811;&#20989;&#25968;&#30340;&#24635;&#21644;&#65288;$Q$&#20026;&#37327;&#21270;&#21442;&#25968;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20026;&#26377;&#38480;&#21464;&#24046;&#27979;&#24230;&#65289;&#30340;&#26368;&#20339;&#36924;&#36817;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#23558;&#21407;&#27979;&#24230;&#19982;&#20854;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#65307;&#35813;&#36317;&#31163;&#22522;&#20110;&#36127;&#23450;&#26680;&#26500;&#24314;&#65292;&#24182;&#19988;&#22914;&#26524;&#24517;&#35201;&#65292;&#21487;&#20197;&#23454;&#26102;&#35745;&#31639;&#24182;&#36755;&#20837;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#65292;Adam&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#26368;&#20248;&#27979;&#37327;&#37327;&#21270;&#22120;&#30340;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#20445;&#35777;&#21512;&#36866;&#34892;&#20026;&#30340;&#26680;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#65288;BLUE&#65289;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#24179;&#26041;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#26080;&#20559;&#31243;&#24207;HEMQ&#20013;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#37327;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;HEMQ
&lt;/p&gt;
&lt;p&gt;
We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We propose two best linear unbiased (BLUE) estimators for the squared statistical distance and use them in an unbiased procedure, called HEMQ, to find the optimal quantization. We test HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space cub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;(CGNN)&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26465;&#20214;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#65292;&#20854;&#29983;&#25104;&#27969;&#24418;&#34987;&#29992;&#20110;&#27714;&#35299;&#21453;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14627</link><description>&lt;p&gt;
&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Generative Neural Networks. (arXiv:2205.14627v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;(CGNN)&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26465;&#20214;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#65292;&#20854;&#29983;&#25104;&#27969;&#24418;&#34987;&#29992;&#20110;&#27714;&#35299;&#21453;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;CGNN&#65289;&#65292;&#21363;&#36830;&#32493;&#24773;&#22659;&#19979;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;CGNN&#30340;&#36755;&#20986;&#23646;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#35813;&#26550;&#26500;&#21463;DCGAN&#30340;&#21551;&#21457;&#65292;&#37319;&#29992;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#22810;&#20010;&#21367;&#31215;&#23618;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#36830;&#32493;&#30340;$L^2$&#24773;&#22659;&#19979;&#65292;&#27599;&#23618;&#31354;&#38388;&#30340;&#32500;&#24230;&#34987;&#32039;&#25903;&#23567;&#27874;&#30340;&#22810;&#37325;&#20998;&#36776;&#29575;&#20998;&#26512;&#30340;&#23610;&#24230;&#25152;&#20195;&#26367;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#38750;&#32447;&#24615;&#30340;&#26465;&#20214;&#65292;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#12290;&#35813;&#29702;&#35770;&#24212;&#29992;&#20110;&#21453;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#23548;&#20986;&#19968;&#20010;CGNN&#29983;&#25104;&#27969;&#24418;&#30340;&#65288;&#21487;&#33021;&#38750;&#32447;&#24615;&#30340;&#65289;&#26080;&#38480;&#32500;&#21453;&#38382;&#39064;&#30340;Lipschitz&#31283;&#23450;&#24615;&#20272;&#35745;&#12290;&#21253;&#25324;&#20449;&#21495;&#21435;&#27169;&#31946;&#22312;&#20869;&#30340;&#22810;&#20010;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#24182;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present and study Continuous Generative Neural Networks (CGNNs), namely, generative models in the continuous setting: the output of a CGNN belongs to an infinite-dimensional function space. The architecture is inspired by DCGAN, with one fully connected layer, several convolutional layers and nonlinear activation functions. In the continuous $L^2$ setting, the dimensions of the spaces of each layer are replaced by the scales of a multiresolution analysis of a compactly supported wavelet. We present conditions on the convolutional filters and on the nonlinearity that guarantee that a CGNN is injective. This theory finds applications to inverse problems, and allows for deriving Lipschitz stability estimates for (possibly nonlinear) infinite-dimensional inverse problems with unknowns belonging to the manifold generated by a CGNN. Several numerical simulations, including signal deblurring, illustrate and validate this approach.
&lt;/p&gt;</description></item></channel></rss>