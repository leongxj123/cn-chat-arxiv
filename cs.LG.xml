<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.00673</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#30740;&#31350;&#32508;&#36848;&#65306;&#38544;&#31169;&#39118;&#38505;&#12289;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#37319;&#29992;&#19981;&#26029;&#25193;&#22823;&#65292;&#35299;&#20915;&#20854;&#38544;&#31169;&#24433;&#21709;&#30340;&#32039;&#36843;&#24615;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#38544;&#31169;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#20998;&#31867;&#27861;&#65292;&#20415;&#20110;&#26681;&#25454;&#30446;&#26631;&#35299;&#37322;&#23545;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#20998;&#26512;&#20013;&#21457;&#29616;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#24182;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#25163;&#25552;&#20379;&#26126;&#30830;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16576</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#21508;&#31181;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22914;&#27835;&#30103;&#21644;&#29983;&#29289;&#23398;&#65292;&#30001;&#20110;&#20854;&#38169;&#32508;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20316;&#20026;&#19968;&#20010;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#29702;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#21033;&#29992;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#25239;&#20307;&#20013;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24341;&#23548;&#29983;&#25104;&#26082;&#20855;&#26377;&#21512;&#29702;&#32467;&#26500;&#21448;&#20855;&#26377;&#26126;&#26174;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27531;&#22522;&#32423;&#20998;&#35299;&#33021;&#37327;&#20559;&#22909;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#26799;&#24230;&#25163;&#26415;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#33021;&#37327;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20363;&#22914;&#21560;&#24341;&#21644;&#26021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16576v1 Announce Type: cross  Abstract: Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12309</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#20174;&#24310;&#36831;&#35266;&#23519;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Delayed Observations via World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#36890;&#24120;&#20551;&#23450;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#31435;&#21363;&#33719;&#24471;&#20851;&#20110;&#34892;&#21160;&#25928;&#26524;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#29289;&#29702;&#38480;&#21046;&#65292;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#65292;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#22312;&#25972;&#21512;&#36807;&#21435;&#35266;&#23519;&#21644;&#23398;&#20064;&#21160;&#24577;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;&#24310;&#36831;POMDP&#38477;&#20302;&#20026;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#24310;&#36831;MDP&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20854;&#20013;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#35266;&#23519;&#24615;&#38477;&#20302;&#26102;&#23454;&#29616;&#27425;&#20248;&#24615;&#33021;&#29978;&#33267;&#36805;&#36895;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#19979;&#32988;&#36807;&#26420;&#32032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#36798;&#21040;30%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12309v1 Announce Type: cross  Abstract: In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;SDE&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#65292;&#31283;&#23450;&#31574;&#30053;&#26799;&#24230;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.04154</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#31283;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;SDE&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#65292;&#31283;&#23450;&#31574;&#30053;&#26799;&#24230;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20135;&#29983;&#39640;&#22238;&#25253;&#26679;&#26412;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20248;&#21270;&#21442;&#25968;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#65292;&#36825;&#26159;&#20855;&#26377;&#39640;&#21487;&#34920;&#36798;&#24615;&#30340;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#31639;&#27861;&#31574;&#30053;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;&#20110;SDE&#26102;&#65292;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#26159;&#22312;&#26377;&#38480;&#36712;&#36857;&#38598;&#19978;&#20272;&#35745;&#30340;&#65292;&#23427;&#21487;&#33021;&#26159;&#19981;&#26126;&#30830;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#31232;&#32570;&#21306;&#22495;&#30340;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#26080;&#27861;&#25511;&#21046;&#12290;&#36825;&#19968;&#25361;&#25112;&#24433;&#21709;&#20102;&#31574;&#30053;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23545;&#26679;&#26412;&#22797;&#26434;&#24230;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;SDE&#38480;&#21046;&#20026;&#19982;&#20854;&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#20445;&#25345;&#19968;&#33268;&#12290;&#30001;&#20110;&#25200;&#21160;&#36807;&#31243;&#35206;&#30422;&#25972;&#20010;&#31354;&#38388;&#19988;&#26131;&#20110;&#25277;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#32531;&#35299;&#21069;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20197;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04154v1 Announce Type: new  Abstract: Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effective
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21487;&#33021;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;TD&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02476
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;TD&#23398;&#20064;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#27492;&#35774;&#32622;&#19979;&#29616;&#26377;&#30340;&#35777;&#26126;&#35201;&#20040;&#20551;&#23450;&#31639;&#27861;&#20013;&#23384;&#22312;&#25237;&#24433;&#27493;&#39588;&#20197;&#31616;&#21270;&#20998;&#26512;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#30456;&#24403;&#22797;&#26434;&#30340;&#35770;&#35777;&#26469;&#30830;&#20445;&#36845;&#20195;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#65306;\textit{&#22312;&#19981;&#23454;&#38469;&#25191;&#34892;&#25237;&#24433;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#25237;&#24433;&#22522;&#30784;&#20998;&#26512;&#30340;&#31616;&#21333;&#24615;&#26159;&#21542;&#21487;&#33021;&#65311;}&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#27493;&#35770;&#35777;&#26469;&#23637;&#31034;&#36825;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#35777;&#26126;&#65292;&#22312;&#26631;&#20934;&#36873;&#25321;&#24120;&#37327;&#27493;&#38271;$\alpha$&#19979;&#65292;&#30001;TD&#23398;&#20064;&#29983;&#25104;&#30340;&#36845;&#20195;&#20445;&#25345;&#26399;&#26395;&#19978;&#30340;&#19968;&#33268;&#26377;&#30028;&#24615;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36882;&#24402;&#65292;&#27169;&#25311;&#20102;TD&#23398;&#20064;&#30340;&#31283;&#24577;&#21160;&#24577;&#65292;&#21463;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#25928;&#26524;&#30340;$O(\alpha^2)$&#25968;&#37327;&#32423;&#19978;&#30340;&#26377;&#30028;&#25668;&#21160;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02476v1 Announce Type: new  Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19455</link><description>&lt;p&gt;
&#21548;&#22122;&#22768;&#65306;&#20351;&#29992;Gibbs&#25193;&#25955;&#36827;&#34892;&#30450;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Listening to the Noise: Blind Denoising with Gibbs Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19455
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#38382;&#39064;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#23494;&#19981;&#21487;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#21435;&#22122;&#22120;&#65292;&#23427;&#20204;&#25152;&#24314;&#27169;&#30340;&#20998;&#24067;&#19982;&#36125;&#21494;&#26031;&#22270;&#20687;&#20013;&#30340;&#21435;&#22122;&#20808;&#39564;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#21518;&#39564;&#37319;&#26679;&#36827;&#34892;&#21435;&#22122;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#65292;&#36825;&#38459;&#30861;&#20102;&#30450;&#21435;&#22122;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#22788;&#29702;&#20449;&#21495;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#20551;&#35774;&#20219;&#24847;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;Gibbs&#31639;&#27861;&#65292;&#20132;&#26367;&#22320;&#20174;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#23558;&#20449;&#21495;&#20808;&#39564;&#26144;&#23556;&#21040;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#19968;&#20010;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#32570;&#38519;&#65292;&#25351;&#23548;&#20102;&#35786;&#26029;&#30340;&#20351;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;Gibbs s&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19455v1 Announce Type: cross  Abstract: In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#20219;&#21153;&#22806;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#26080;&#27861;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.12235</link><description>&lt;p&gt;
&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Fundamental Limits of Least-Privilege Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12235
&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#20219;&#21153;&#22806;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#26080;&#27861;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23569;&#26435;&#38480;&#23398;&#20064;&#30340;&#25215;&#35834;&#26159;&#25214;&#21040;&#23545;&#20110;&#23398;&#20064;&#20219;&#21153;&#26377;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20294;&#21516;&#26102;&#38450;&#27490;&#25512;&#26029;&#19982;&#35813;&#20219;&#21153;&#26080;&#20851;&#30340;&#20219;&#20309;&#25935;&#24863;&#20449;&#24687;&#65292;&#36825;&#19968;&#28857;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#21482;&#26159;&#20197;&#38750;&#27491;&#24335;&#30340;&#26041;&#24335;&#38472;&#36848;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#28165;&#26970;&#25105;&#20204;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#23567;&#26435;&#38480;&#21407;&#21017;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#25551;&#36848;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#39044;&#26399;&#20219;&#21153;&#20043;&#22806;&#30340;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#22522;&#26412;&#26435;&#34913;&#65306;&#19981;&#21487;&#33021;&#23398;&#20064;&#21040;&#23545;&#20110;&#39044;&#26399;&#20219;&#21153;&#20855;&#26377;&#39640;&#23454;&#29992;&#24615;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#21448;&#38450;&#27490;&#25512;&#26029;&#38500;&#20219;&#21153;&#26631;&#31614;&#26412;&#36523;&#20043;&#22806;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#36825;&#31181;&#26435;&#34913;&#26159;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#25216;&#26415;&#26469;&#23398;&#20064;&#20135;&#29983;&#36825;&#20123;&#34920;&#31034;&#30340;&#29305;&#24449;&#26144;&#23556;&#37117;&#26159;&#25104;&#31435;&#30340;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12235v1 Announce Type: new  Abstract: The promise of least-privilege learning -- to find feature representations that are useful for a learning task but prevent inference of any sensitive information unrelated to this task -- is highly appealing. However, so far this concept has only been stated informally. It thus remains an open question whether and how we can achieve this goal. In this work, we provide the first formalisation of the least-privilege principle for machine learning and characterise its feasibility. We prove that there is a fundamental trade-off between a representation's utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but, at the same time prevent inference of any attribute other than the task label itself. This trade-off holds regardless of the technique used to learn the feature mappings that produce these representations. We empirically validate thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.11039</link><description>&lt;p&gt;
&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#20122;&#32676;&#20307;&#36716;&#31227;&#40065;&#26834;&#24615;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#38024;&#23545;&#26368;&#20248;&#32452;&#20934;&#30830;&#24615;(WGA)&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#20110;&#20381;&#36182;&#20110;&#33391;&#22909;&#26631;&#27880;&#30340;&#32452;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#20013;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27880;&#37322;&#30340;&#25968;&#25454;&#22686;&#24378;&#20351;&#29992;&#19979;&#37319;&#26679;&#25110;&#19978;&#21152;&#26435;&#29992;&#20110;WGA&#26159;&#23481;&#26131;&#21463;&#21040;&#39046;&#22495;&#26631;&#27880;&#22122;&#22768;&#24178;&#25200;&#65292;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#25509;&#36817;&#20351;&#29992;&#21407;&#22987;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;WGA&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#26631;&#27880;&#27491;&#21017;&#21270;(RAD)&#26469;&#35757;&#32451;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#39046;&#22495;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RAD&#19982;&#20854;&#20182;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#39046;&#22495;&#26631;&#27880;&#25216;&#26415;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20165;&#26377;5%&#30340;&#22122;&#22768;&#65292;RAD&#20063;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#20381;&#36182;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08703</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;&#65306;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21382;&#26469;&#20195;&#20215;&#39640;&#26114;&#30340;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#65292;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#20351;&#29992;&#20013;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#20174;&#38646;&#24320;&#22987;&#21019;&#24314;&#26032;&#30340;&#29983;&#29289;&#21270;&#21512;&#29289;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21152;&#19978;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20026;&#26032;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#21019;&#36896;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#23616;&#38754;&#12290;&#22312;&#36825;&#20221;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#12290;&#22312;&#27599;&#20010;&#20027;&#39064;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#36827;&#34892;&#21508;&#31181;&#26041;&#27861;&#30340;&#24494;&#35266;&#27604;&#36739;&#21644;&#23439;&#35266;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08703v1 Announce Type: cross Abstract: Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06121</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#20174;&#29627;&#23572;&#20857;&#26364;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Iterated Denoising Energy Matching for Sampling from Boltzmann Densities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06121
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#27604;&#22914;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#34913;&#26679;&#26412;&#65292;&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#65288;iDEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#65292;&#20165;&#20351;&#29992;&#33021;&#37327;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230; - &#32780;&#19981;&#26159;&#25968;&#25454;&#26679;&#26412; - &#26469;&#35757;&#32451;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;iDEM&#22312;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65306;&#65288;I&#65289;&#20174;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#20013;&#37319;&#26679;&#39640;&#27169;&#22411;&#23494;&#24230;&#30340;&#21306;&#22495;&#65292;&#21644;&#65288;II&#65289;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22312;&#25105;&#20204;&#30340;&#38543;&#26426;&#21305;&#37197;&#30446;&#26631;&#20013;&#36827;&#19968;&#27493;&#25913;&#36827;&#37319;&#26679;&#22120;&#12290;iDEM&#22312;&#39640;&#32500;&#24230;&#19978;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#20869;&#37096;&#21305;&#37197;&#30446;&#26631;&#26159;&#26080;&#38656;&#27169;&#25311;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;MCMC&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;iDEM&#24179;&#28369;&#20102;&#33021;&#37327;&#32972;&#26223;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20998;&#25674;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#20219;&#21153;&#36827;&#34892;&#20102;iDEM&#30340;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks rang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03864</link><description>&lt;p&gt;
&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#38454;&#27573;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#35270;&#35282;&#26159;&#30740;&#31350;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35757;&#32451;&#21160;&#24577;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35270;&#35282;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;PINNs&#27714;&#35299;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#19981;&#21516;&#34892;&#20026;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#24378;&#35843;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#25361;&#25112;&#12290;&#25152;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#37117;&#24471;&#21040;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDEs&#30340;&#25968;&#20540;&#31034;&#20363;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33033;&#20914;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#21487;&#33021;&#22312;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02880</link><description>&lt;p&gt;
&#37322;&#25918;&#33033;&#20914;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33033;&#20914;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#21487;&#33021;&#22312;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#38656;&#35201;&#26368;&#20339;&#21033;&#29992;&#26377;&#38480;&#30340;&#37327;&#23376;&#36164;&#28304;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#38376;&#30340; QML &#27169;&#22411;&#23545;&#36719;&#20214;&#24037;&#31243;&#24072;&#24456;&#26041;&#20415;&#65292;&#20294;&#20854;&#34920;&#36798;&#33021;&#21147;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#30456;&#24178;&#26102;&#38388;&#20869;&#20801;&#35768;&#30340;&#30005;&#36335;&#28145;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#33033;&#20914;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#30456;&#24178;&#26102;&#38388;&#20869;&#26500;&#24314;&#8220;&#26080;&#38480;&#8221;&#28145;&#24230;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#33021;&#20026;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#37322;&#25918;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#20174;&#37327;&#23376;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#36825;&#20010;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#33033;&#20914;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26469;&#33258;&#32534;&#30721;&#36807;&#31243;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#22522;&#20110;&#38376;&#27169;&#22411;&#20013;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22522;&#30784;&#29289;&#29702;&#31995;&#32479;&#20855;&#26377;&#38598;&#21512;&#21487;&#25511;&#24615;&#30340;&#26465;&#20214;&#19979;&#65292;&#33033;&#20914;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#24847;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#65292;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#33033;&#20914;&#27169;&#22411;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#38376;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources. The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time. In contrast, pulse-based models enable the construction of "infinitely" deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks. In this paper, we investigate this potential from the perspective of quantum control theory. We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models. Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable. Under this condition, numerical si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02484</link><description>&lt;p&gt;
Weisfeiler Leman&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#31561;&#21464;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler Leman for Euclidean Equivariant Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;2-WL&#27979;&#35797;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#28857;&#20113;&#20013;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20462;&#25913;&#30340;PPGN&#26550;&#26500;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k-Weisfeiler Leman (k-WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#26159;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34920;&#36798;&#33021;&#21147;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#35777;&#26126;&#20102;2-WL&#27979;&#35797;&#22312;&#32534;&#30721;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#21152;&#26435;&#22270;&#19978;&#26159;&#23436;&#22791;&#30340;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#19982;2-WL&#27979;&#35797;&#31561;&#20215;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;GNNs&#21487;&#20197;&#34987;&#35777;&#26126;&#22312;&#28857;&#20113;&#19978;&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#38480;&#20110;&#28857;&#20113;&#19978;&#30340;&#19981;&#21464;&#36830;&#32493;&#20989;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#26041;&#38754;&#23545;&#36825;&#19968;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;:&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;2-WL&#27979;&#35797;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#28857;&#20113;&#65292;&#36825;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPGN (Maron&#31561;&#20154;&#65292;2019)&#21487;&#20197;&#22312;&#20302;&#22797;&#26434;&#24230;&#19979;&#22312;&#25152;&#26377;&#28857;&#20113;&#19978;&#19968;&#33268;&#22320;&#27169;&#25311;2-WL&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36825;&#20010;PPGN&#26550;&#26500;&#30340;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#19968;&#20010;&#21487;&#36817;&#20284;&#25152;&#26377;&#36830;&#32493;&#31561;&#21464;&#20989;&#25968;&#30340;&#36890;&#29992;&#31561;&#21464;&#26550;&#26500;&#12290;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16549</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16549
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#20013;&#39044;&#27979;&#22810;&#20010;&#26631;&#31614;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#28041;&#21450;&#22810;&#26631;&#31614;&#20998;&#31867;&#25110;&#25490;&#21517;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22810;&#26631;&#31614;&#20998;&#31867;&#38754;&#20020;&#30340;&#22256;&#38590;&#21253;&#25324;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#35299;&#20915;&#26631;&#31614;&#30456;&#20851;&#24615;&#21644;&#22788;&#29702;&#37096;&#20998;&#26631;&#31614;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#27604;&#36739;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11447</link><description>&lt;p&gt;
&#39044;&#27979;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#20013;&#24739;&#32773;&#20381;&#20174;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;(SCIT)&#26159;&#36807;&#25935;&#24615;&#40763;&#28814;&#30340;&#38271;&#25928;&#22240;&#26524;&#27835;&#30103;&#12290;&#22914;&#20309;&#25552;&#39640;&#24739;&#32773;&#23545;&#21464;&#24212;&#21407;&#20813;&#30123;&#27835;&#30103;(AIT)&#30340;&#20381;&#20174;&#24615;&#20197;&#26368;&#22823;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#22312;AIT&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;AIT&#30340;&#31649;&#29702;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#24207;&#21015;&#28508;&#22312;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#27169;&#22411;(SLAC)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;(LSTM)&#65292;&#24182;&#22522;&#20110;&#35780;&#20998;&#21644;&#20381;&#20174;&#24615;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;&#22312;&#25490;&#38500;&#31532;&#19968;&#26102;&#38388;&#27493;&#30340;&#20559;&#20506;&#26679;&#26412;&#21518;&#65292;SLAC&#27169;&#22411;&#30340;&#39044;&#27979;&#20381;&#20174;&#20934;&#30830;&#29575;&#20026;60%-72%&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;66%-84%&#65292;&#26681;&#25454;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;SLAC&#27169;&#22411;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#33539;&#22260;&#22312;0.93&#21040;2.22&#20043;&#38388;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;RMSE&#33539;&#22260;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
&lt;/p&gt;</description></item><item><title>FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09986</link><description>&lt;p&gt;
FLex&amp;Chill&#65306;&#36890;&#36807;Logit Chilling&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09986
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#38750;iid&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#32780;&#21463;&#21040;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;FLex&amp;Chill&#65292;&#21033;&#29992;&#20102;Logit Chilling&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#38750;iid&#25968;&#25454;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;&#20174;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20840;&#23616;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26102;&#38388;&#25552;&#39640;&#20102;6&#20493;&#65292;&#25512;&#29702;&#31934;&#24230;&#25552;&#39640;&#20102;3.37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05972</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;Hasegawa-Wakatani&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning physics-based reduced models from data for the Hasegawa-Wakatani equations. (arXiv:2401.05972v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#30340;&#32422;&#20943;&#27169;&#22411;&#65288;ROMs&#65289;&#26500;&#24314;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#12289;&#28151;&#27788;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;operator inference&#65288;OpInf&#65289;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;ROMs&#29992;&#20110;&#36825;&#31181;&#27169;&#25311;&#12290;&#20197;Hasegawa-Wakatani&#65288;HW&#65289;&#26041;&#31243;&#20026;&#20195;&#34920;&#65292;&#22312;&#24418;&#25104;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#32771;&#23519;&#20102;OpInf&#26500;&#24314;&#20934;&#30830;ROMs&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#20004;&#32452;&#23454;&#39564;&#12290;&#31532;&#19968;&#32452;&#23454;&#39564;&#21033;&#29992;&#36890;&#36807;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20174;&#29305;&#23450;&#21021;&#20540;&#26465;&#20214;&#24320;&#22987;&#30340;HW&#26041;&#31243;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;OpInf ROMs&#30340;&#35757;&#32451;&#20197;&#23454;&#29616;&#36229;&#36234;&#35757;&#32451;&#26102;&#38388;&#33539;&#22260;&#30340;&#39044;&#27979;&#12290;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31532;&#20108;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#23545;ROMs&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the construction of non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma turbulence simulations. In particular, we propose using Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we focus on the Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a comprehensive perspective of the potential of OpInf to construct accurate ROMs for this model, we consider a setup for the HW equations that leads to the formation of complex, nonlinear, and self-driven dynamics, and perform two sets of experiments. We first use the data obtained via a direct numerical simulation of the HW equations starting from a specific initial condition and train OpInf ROMs for predictions beyond the training time horizon. In the second, more challenging set of experiments, we train ROMs using the same datase
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2311.00721</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#25110;&#29983;&#29702;&#20449;&#21495;&#19978;&#36827;&#34892;&#20849;&#24773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals. (arXiv:2311.00721v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#19968;&#20010;&#31038;&#20132;&#25216;&#33021;&#65292;&#34920;&#26126;&#19968;&#20010;&#20010;&#20307;&#29702;&#35299;&#20182;&#20154;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20849;&#24773;&#24341;&#36215;&#20102;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#23398;&#31185;&#30340;&#20851;&#27880;&#12290;&#20849;&#24773;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#26415;&#35821;&#65292;&#22240;&#27492;&#26816;&#27979;&#25110;&#35782;&#21035;&#20849;&#24773;&#22312;&#31038;&#20250;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#28041;&#21450;&#33539;&#22260;&#24191;&#27867;&#19988;&#26377;&#37325;&#21472;&#65292;&#20294;&#20174;&#25972;&#20307;&#25991;&#29486;&#35282;&#24230;&#26469;&#30475;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20849;&#24773;&#26816;&#27979;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#25910;&#38598;&#21644;&#31579;&#36873;&#20102;&#26469;&#33258;10&#20010;&#30693;&#21517;&#25968;&#25454;&#24211;&#30340;801&#31687;&#35770;&#25991;&#65292;&#24182;&#20998;&#26512;&#20102;&#36873;&#23450;&#30340;54&#31687;&#35770;&#25991;&#12290;&#25105;&#20204;&#26681;&#25454;&#20849;&#24773;&#26816;&#27979;&#31995;&#32479;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#21363;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#65292;&#23545;&#35770;&#25991;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#20998;&#21035;&#30740;&#31350;&#20102;&#29305;&#23450;&#27169;&#24577;&#30340;&#39044;&#22788;&#29702;&#21644;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#21327;&#35758;&#12289;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#21644;&#21487;&#29992;&#24615;&#35814;&#24773;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a social skill that indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Empathy is a context-dependent term; thus, detecting or recognising empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection studies leveraging Machine Learning remains underexplored from a holistic literature perspective. To this end, we systematically collect and screen 801 papers from 10 well-known databases and analyse the selected 54 papers. We group the papers based on input modalities of empathy detection systems, i.e., text, audiovisual, audio and physiological signals. We examine modality-specific pre-processing and network architecture design protocols, popular dataset descriptions and availability details, and evaluation protocols. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19809</link><description>&lt;p&gt;
MgNO:&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
MgNO: Efficient Parameterization of Linear Operators via Multigrid. (arXiv:2310.19809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#26469;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#27604;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#31639;&#23376;&#23618;&#20013;&#31532;$i$&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#65292;&#35760;&#20316;$\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$&#12290;&#20854;&#20013;&#65292;$\mathcal W_{ij}$&#34920;&#31034;&#36830;&#25509;&#31532;$j$&#20010;&#36755;&#20837;&#31070;&#32463;&#20803;&#21644;&#31532;$i$&#20010;&#36755;&#20986;&#31070;&#32463;&#20803;&#30340;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#32780;&#20559;&#24046;$\mathcal B_{ij}$&#37319;&#29992;&#20989;&#25968;&#24418;&#24335;&#32780;&#38750;&#26631;&#37327;&#24418;&#24335;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31070;&#32463;&#20803;&#65288;Banach&#31354;&#38388;&#65289;&#20043;&#38388;&#26377;&#25928;&#21442;&#25968;&#21270;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;MgNO&#24341;&#20837;&#20102;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#26082;&#20855;&#22791;&#20102;&#25968;&#23398;&#20005;&#23494;&#24615;&#65292;&#21448;&#20855;&#22791;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;MgNO&#28040;&#38500;&#20102;&#23545;&#20256;&#32479;&#30340;lifting&#21644;projecting&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10157</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter. (arXiv:2309.10157v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CMS&#25506;&#27979;&#22120;&#26159;&#22312;LHC&#19978;&#25506;&#27979;&#39640;&#33021;&#30896;&#25758;&#20135;&#29983;&#30340;&#36890;&#29992;&#35013;&#32622;&#12290;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#22312;&#32447;&#25968;&#25454;&#36136;&#37327;&#30417;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25805;&#20316;&#24037;&#20855;&#65292;&#33021;&#22815;&#35753;&#25506;&#27979;&#22120;&#19987;&#23478;&#24555;&#36895;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#35786;&#26029;&#21487;&#33021;&#24433;&#21709;&#29289;&#29702;&#25968;&#25454;&#36136;&#37327;&#30340;&#21508;&#31181;&#25506;&#27979;&#22120;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#33258;&#32534;&#30721;&#22120;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;CMS&#30005;&#30913;&#37327;&#33021;&#22120;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#12290;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#30340;&#26102;&#22495;&#28436;&#21270;&#21644;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#26368;&#22823;&#21270;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#20010;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#31995;&#32479;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#24322;&#24120;&#65292;&#21516;&#26102;&#20445;&#25345;&#38750;&#24120;&#20302;&#30340;&#35823;&#25253;&#29575;&#12290;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#36890;&#36807;&#22312;2018&#24180;&#21644;2022&#24180;LHC&#30896;&#25758;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#24322;&#24120;&#36827;&#34892;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#39318;&#27425;&#25253;&#36947;&#20102;&#37096;&#32626;&#35813;&#31995;&#32479;&#21518;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.04055</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23450;&#20215;&#26159;&#20225;&#19994;&#24120;&#29992;&#30340;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#21046;&#23450;&#20215;&#26684;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20080;&#23478;&#20063;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#25968;&#25454;&#26469;&#33719;&#21462;&#26356;&#20302;&#30340;&#20215;&#26684;&#65292;&#20294;&#36825;&#20063;&#20250;&#23548;&#33268;&#29305;&#23450;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#20250;&#38459;&#30861;&#20225;&#19994;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#12290;&#21334;&#26041;&#26080;&#27861;&#35266;&#23519;&#21040;&#20080;&#23478;&#30340;&#30495;&#23454;&#29305;&#24449;&#65292;&#32780;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#26681;&#25454;&#31574;&#30053;&#34892;&#20026;&#25805;&#32437;&#21518;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21334;&#26041;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#20272;&#20540;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#20855;&#20307;&#25968;&#20540;&#65292;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#20108;&#36827;&#21046;&#30340;&#21709;&#24212;&#65292;&#34920;&#31034;&#26159;&#21542;&#21457;&#29983;&#38144;&#21806;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19981;&#32771;&#34385;&#31574;&#30053;&#24615;&#30340;&#23450;&#20215;&#31574;&#30053;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13928</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#30340;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems. (arXiv:2306.13928v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#35770;&#36848;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#22495;&#30340;&#36870;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#20174;&#35266;&#27979;&#20540;&#20013;&#25512;&#26029;&#20986;&#39537;&#21160;&#26234;&#33021;&#20307;&#34892;&#21160;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#36825;&#20010;&#25104;&#26412;&#26159;&#38750;&#20984;&#21644;&#38750;&#24179;&#31283;&#30340;&#65292;&#21516;&#26102;&#21463;&#21040;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#25104;&#26412;&#20272;&#35745;&#65292;&#21363;&#20351;&#20195;&#29702;&#25104;&#26412;&#19981;&#26159;&#20984;&#30340;&#65292;&#26412;&#25991;&#20063;&#33021;&#22815;&#29983;&#25104;&#20984;&#38382;&#39064;&#12290;&#20026;&#20102;&#24471;&#20986;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#20197;&#38543;&#26426;&#31574;&#30053;&#20026;&#20915;&#31574;&#21464;&#37327;&#30340;&#26377;&#38480;&#26102;&#22495;&#21069;&#21521;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26368;&#20248;&#35299;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#36716;&#21270;&#20026;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#36890;&#36807;&#34394;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#26377;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with a finite-horizon inverse control problem, which has the goal of inferring, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result that enables cost estimation by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. For this problem, we give an explicit expression for the optimal solution. Moreover, we turn our findings into algorithmic procedures and we show the effectiveness of our approach via both in-silico and experimental validations with real hardware. All the experiments confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11903</link><description>&lt;p&gt;
Deep Fusion&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#21033;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Deep Fusion&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#21644;T5&#27169;&#22411;&#22823;&#23567;&#19978;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Deep Fusion&#26159;&#19968;&#31181;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15598</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20419;&#36827;&#23398;&#20064;&#21333;&#25351;&#25968;&#21644;&#22810;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#22823;&#20110;&#20004;&#23618;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#31867;&#28145;&#24230;&#19981;&#21516;&#20294;&#23481;&#37327;&#30456;&#21516;&#30340;&#32593;&#32476;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#26174;&#24335;&#23450;&#20041;&#30340;&#34920;&#31034;&#25104;&#26412;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35825;&#23548;&#30340;&#20989;&#25968;&#30340;&#34920;&#31034;&#25104;&#26412;&#26159;&#32593;&#32476;&#34920;&#31034;&#35813;&#20989;&#25968;&#25152;&#38656;&#30340;&#24179;&#26041;&#26435;&#37325;&#20043;&#21644;&#30340;&#26368;&#23567;&#20540;&#65307;&#23427;&#21453;&#26144;&#20102;&#19982;&#35813;&#26550;&#26500;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32447;&#24615;&#23618;&#28155;&#21152;&#21040;ReLU&#32593;&#32476;&#20250;&#20135;&#29983;&#19968;&#20010;&#34920;&#31034;&#25104;&#26412;&#65292;&#36825;&#26377;&#21033;&#20110;&#20351;&#29992;&#20004;&#23618;&#32593;&#32476;&#26469;&#36924;&#36817;&#30001;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20855;&#26377;&#20302;&#34920;&#31034;&#25104;&#26412;&#30340;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26368;&#23567;&#30340;&#34920;&#31034;&#25104;&#26412;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#20250;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.08172</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#29702;&#35770;&#32467;&#26500;&#36880;&#28176;&#24471;&#21040;&#20102;&#38416;&#26126;&#12290;Imaizumi-Fukumizu&#65288;2019&#65289;&#21644;Suzuki&#65288;2019&#65289;&#25351;&#20986;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20809;&#28369;&#20989;&#25968;&#26102;&#65292;DNN&#30340;&#23398;&#20064;&#33021;&#21147;&#20248;&#20110;&#20808;&#21069;&#30340;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#20247;&#22810;&#30740;&#31350;&#23581;&#35797;&#22312;&#27809;&#26377;&#20219;&#20309;&#32479;&#35745;&#35770;&#35777;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#30740;&#31350;&#65292;&#25506;&#31350;&#30495;&#27491;&#33021;&#22815;&#24341;&#21457;&#26799;&#24230;&#19979;&#38477;&#30340;DNN&#26550;&#26500;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#24615;&#65292;&#36825;&#19968;&#23581;&#35797;&#20284;&#20046;&#26356;&#36148;&#36817;&#23454;&#38469;DNN&#12290;&#26412;&#25991;&#23558;&#30446;&#26631;&#20989;&#25968;&#38480;&#21046;&#20026;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#22312;ReLU-DNN&#20013;&#26500;&#36896;&#20102;&#19968;&#20010;&#31232;&#30095;&#19988;&#20855;&#26377;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05342</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Joint Representations for Reinforcement Learning with Multiple Sensors. (arXiv:2302.05342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#23545;&#22810;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#32467;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#65292;&#22914;&#26426;&#22120;&#20154;&#26412;&#20307;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#26412;&#20307;&#24863;&#30693;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#31639;&#27861;&#32858;&#28966;&#20110;&#30456;&#20851;&#26041;&#38754;&#65292;&#24182;&#25351;&#23548;&#20854;&#23547;&#25214;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20174;&#22810;&#20010;&#20256;&#24863;&#22120;&#20013;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32852;&#21512;&#34920;&#31034;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#26080;&#27169;&#22411;&#21644;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21253;&#21547;&#20998;&#25955;&#30340;&#35270;&#35273;&#20449;&#24687;&#25110;&#32570;&#23569;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining inputs from multiple sensor modalities effectively in reinforcement learning (RL) is an open problem. While many self-supervised representation learning approaches exist to improve performance and sample complexity for image-based RL, they usually neglect other available information, such as robot proprioception. However, using this proprioception for representation learning can help algorithms to focus on relevant aspects and guide them toward finding better representations. In this work, we systematically analyze representation learning for RL from multiple sensors by building on Recurrent State Space Models. We propose a combination of reconstruction-based and contrastive losses, which allows us to choose the most appropriate method for each sensor modality. We demonstrate the benefits of joint representations, particularly with distinct loss functions for each modality, for model-free and model-based RL on complex tasks. Those include tasks where the images contain distra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13152</link><description>&lt;p&gt;
STEEL: &#22855;&#24322;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#24635;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#30446;&#26631;&#31574;&#30053;&#35825;&#23548;&#30340;&#20998;&#24067;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#20197;&#20415;&#36890;&#36807;&#21464;&#25442;&#27979;&#24230;&#20351;&#29992;&#25209;&#37327;&#25968;&#25454;&#26469;&#26657;&#20934;&#30446;&#26631;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32477;&#23545;&#36830;&#32493;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31639;&#27861;&#20026;STEEL&#65306;SingulariTy-awarE rEinforcement Learning&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;&#20851;&#20110;&#31163;&#32447;&#35780;&#20272;&#30340;&#26032;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#65292;&#20197;&#21450;&#24102;&#26377;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31574;&#30053;&#23450;&#21521;&#35823;&#24046;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22855;&#24322;&#24773;&#20917;&#30340;&#23450;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
&lt;/p&gt;</description></item><item><title>ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13700</link><description>&lt;p&gt;
ES-GNN: &#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;
&lt;/p&gt;
&lt;p&gt;
ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13700
&lt;/p&gt;
&lt;p&gt;
ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29616;&#20195;&#21464;&#20307;&#20027;&#35201;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#36890;&#24120;&#21516;&#26102;&#26174;&#31034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;GNN&#22312;&#25972;&#20307;&#19978;&#24179;&#28369;&#33410;&#28857;&#25509;&#36817;&#24615;&#21487;&#33021;&#20250;&#32858;&#21512;&#20219;&#21153;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#65288;&#29978;&#33267;&#26377;&#23475;&#65289;&#30340;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36793;&#20998;&#21106;GNN&#65288;ES-GNN&#65289;&#26694;&#26550;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#33410;&#28857;&#38598;&#20294;&#20855;&#26377;&#29420;&#21344;&#36793;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#36825;&#20004;&#20010;&#23376;&#22270;&#19978;&#20998;&#21035;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#65292;&#20174;&#32780;&#20351;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#20132;&#26367;&#36827;&#34892;&#65292;&#23454;&#29616;&#20102;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have achieved enormous success in multiple graph analytical tasks, modern variants mostly rely on the strong inductive bias of homophily. However, real-world networks typically exhibit both homophilic and heterophilic linking patterns, wherein adjacent nodes may share dissimilar attributes and distinct labels. Therefore, GNNs smoothing node proximity holistically may aggregate both task-relevant and irrelevant (even harmful) information, limiting their ability to generalize to heterophilic graphs and potentially causing non-robustness. In this work, we propose a novel edge splitting GNN (ES-GNN) framework to adaptively distinguish between graph edges either relevant or irrelevant to learning tasks. This essentially transfers the original graph into two subgraphs with the same node set but exclusive edge sets dynamically. Given that, information propagation separately on these subgraphs and edge splitting are alternatively conducted, thus disentangling
&lt;/p&gt;</description></item></channel></rss>