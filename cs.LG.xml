<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00489</link><description>&lt;p&gt;
PROMPT-SAW&#65306;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#25552;&#31034;&#26159;LLM&#25512;&#29702;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36229;&#38271;&#25552;&#31034;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#23581;&#35797;&#23548;&#33268;&#21387;&#32553;&#25552;&#31034;&#22312;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23545;&#25552;&#31034;&#25928;&#29992;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROMPT-SAW&#65306;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25552;&#31034;&#21387;&#32553;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#24863;&#30693;&#25552;&#31034;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;PROMPT-SAW&#20351;&#29992;&#25552;&#31034;&#30340;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#22270;&#24418;&#65292;&#22312;&#22270;&#24418;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20803;&#32032;&#65292;&#20174;&#32780;&#24471;&#20986;&#21387;&#32553;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GSM8K-AUG&#65292;&#21363;&#29616;&#26377;GSM8k&#22522;&#20934;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#29992;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Moment Pooling&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#26174;&#33879;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#20869;&#37096;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08854</link><description>&lt;p&gt;
&#28165;&#26224;&#30636;&#38388;&#65306;&#20351;&#29992;Moment Pooling&#31616;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Moment Pooling&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#26174;&#33879;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#20869;&#37096;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#28041;&#21450;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36890;&#24120;&#26159;&#39640;&#32500;&#19988;&#38590;&#20197;&#30452;&#25509;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Moment Pooling&#8221;&#65292;&#36825;&#26159;Deep Sets&#32593;&#32476;&#30340;&#19968;&#20010;&#33258;&#28982;&#24310;&#20280;&#65292;&#21487;&#22823;&#24133;&#20943;&#23569;&#36825;&#20123;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#65292;&#21516;&#26102;&#32500;&#25345;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;Moment Pooling&#23558;Deep Sets&#20013;&#30340;&#27714;&#21644;&#27867;&#21270;&#20026;&#20219;&#24847;&#30340;&#22810;&#21464;&#37327;&#30697;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22266;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#26377;&#25928;&#28508;&#22312;&#32500;&#24230;&#12290;&#25105;&#20204;&#23558;Moment Pooling&#24212;&#29992;&#20110;&#22840;&#20811;/&#33014;&#23376;&#21943;&#27880;&#20998;&#31867;&#30340;&#23545;&#25758;&#26426;&#29289;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;Energy Flow Networks&#65288;EFNs&#65289;&#25193;&#23637;&#20026;Moment EFNs&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#23567;&#33267;1&#30340;&#28508;&#22312;&#32500;&#24230;&#30340;Moment EFNs&#34920;&#29616;&#19982;&#20855;&#26377;&#36739;&#39640;&#28508;&#22312;&#32500;&#24230;&#30340;&#26222;&#36890;EFNs&#31867;&#20284;&#12290;&#36825;&#31181;&#23567;&#28508;&#22312;&#32500;&#24230;&#20351;&#20869;&#37096;&#34920;&#31034;&#21487;&#20197;&#30452;&#25509;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08854v1 Announce Type: cross  Abstract: Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret. In this work, we propose "Moment Pooling", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance. Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension. We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension. This small latent dimension allows for the internal representation to be directly visualized and interpreted, w
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16731</link><description>&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16731
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25191;&#34892;&#28041;&#21450;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#65292;&#21518;&#32773;&#22312;&#24635;&#26102;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#21463;&#25968;&#25454;&#22312;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#20043;&#38388;&#31227;&#21160;&#30340;&#20005;&#37325;&#29942;&#39048;&#25152;&#38480;&#21046;&#12290;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#31616;&#21333;&#22788;&#29702;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyGim&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;PIM&#31995;&#32479;&#19978;&#21152;&#36895;GNNs&#12290;&#25105;&#20204;&#20026;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#23450;&#21046;&#30340;GNN&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#25552;&#20986;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#24320;&#21457;&#20102;&#26041;&#20415;&#30340;Python API&#12290;&#25105;&#20204;&#25552;&#20379;&#28151;&#21512;&#24335;GNN&#25191;&#34892;&#65292;&#20854;&#20013;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#20998;&#21035;&#22312;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#21644;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#25191;&#34892;&#65292;&#20197;&#21305;&#37197;&#23427;&#20204;&#30340;&#31639;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#21253;&#25324;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2402.14877</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#21253;&#25324;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#30340;&#20542;&#35206;&#21644;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;(AMOC)&#30340;&#26368;&#26032;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#20854;&#28508;&#22312;&#20542;&#35206;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#21271;&#22823;&#35199;&#27915;&#28129;&#27700;&#36755;&#20837;&#22686;&#21152;&#30340;&#19968;&#20010;&#20020;&#30028;&#28857;&#12290;&#39044;&#27979;&#30340;&#23849;&#28291;&#26102;&#38388;&#31383;&#21475;&#22823;&#32422;&#22312;&#26412;&#19990;&#32426;&#20013;&#21494;&#65292;&#26368;&#26089;&#21487;&#33021;&#22312;&#22823;&#32422;&#20004;&#24180;&#21518;&#24320;&#22987;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23545;&#31995;&#32479;&#20174;&#19968;&#20010;&#31283;&#23450;&#24179;&#34913;&#29366;&#24577;&#36716;&#21464;&#21040;&#21478;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#30340;&#20020;&#30028;&#28857;&#30340;&#39044;&#27979;&#23545;&#20110;&#24191;&#27867;&#39046;&#22495;&#37117;&#26159;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22024;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#20542;&#35206;&#65292;&#24182;&#22312;&#22810;&#20010;&#31995;&#32479;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;AMOC&#12289;&#29983;&#24577;&#32593;&#32476;&#12289;&#30005;&#21147;&#31995;&#32479;&#21644;&#27668;&#20505;&#27169;&#22411;&#12290;&#23545;&#20110;AMOC&#65292;&#25105;&#20204;&#22522;&#20110;&#27169;&#25311;&#25351;&#32441;&#25968;&#25454;&#21644;&#28023;&#34920;&#28201;&#24230;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#23558;&#28508;&#22312;&#20542;&#35206;&#30340;&#26102;&#38388;&#31383;&#21475;&#32622;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14877v1 Announce Type: cross  Abstract: Recent research on the Atlantic Meridional Overturning Circulation (AMOC) raised concern about its potential collapse through a tipping point due to the climate-change caused increase in the freshwater input into the North Atlantic. The predicted time window of collapse is centered about the middle of the century and the earliest possible start is approximately two years from now. More generally, anticipating a tipping point at which the system transitions from one stable steady state to another is relevant to a broad range of fields. We develop a machine-learning approach to predicting tipping in noisy dynamical systems with a time-varying parameter and test it on a number of systems including the AMOC, ecological networks, an electrical power system, and a climate model. For the AMOC, our prediction based on simulated fingerprint data and real data of the sea surface temperature places the time window of a potential collapse between 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;</title><link>https://arxiv.org/abs/2402.13251</link><description>&lt;p&gt;
FlashTex&#65306;&#20855;&#26377;LightControlNet&#30340;&#24555;&#36895;&#21487;&#37325;&#22609;&#32593;&#26684;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
FlashTex: Fast Relightable Mesh Texturing with LightControlNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#20026;3D&#32593;&#26684;&#21019;&#24314;&#32441;&#29702;&#36153;&#26102;&#36153;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#23478;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#32773;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#33258;&#21160;&#20026;&#36755;&#20837;&#30340;3D&#32593;&#26684;&#30528;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;/&#21453;&#23556;&#22312;&#29983;&#25104;&#30340;&#32441;&#29702;&#20013;&#35299;&#32806;&#65292;&#20197;&#20415;&#32593;&#26684;&#21487;&#20197;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#20013;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LightControlNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;ControlNet&#26550;&#26500;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20801;&#35768;&#23558;&#25152;&#38656;&#29031;&#26126;&#35268;&#26684;&#20316;&#20026;&#23545;&#27169;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#32441;&#29702;&#31649;&#36947;&#28982;&#21518;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#32441;&#29702;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;LightControlNet&#29983;&#25104;&#32593;&#26684;&#30340;&#19968;&#32452;&#31232;&#30095;&#30340;&#35270;&#35273;&#19968;&#33268;&#30340;&#21442;&#32771;&#35270;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#24212;&#29992;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#32441;&#29702;&#20248;&#21270;&#65292;&#36890;&#36807;LightControlNet&#26469;&#25552;&#39640;&#32441;&#29702;&#36136;&#37327;&#21516;&#26102;&#35299;&#32806;&#34920;&#38754;&#26448;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#22312;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#26102;&#65292;&#36890;&#36807;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#32780;&#19981;&#26159;&#27719;&#24635;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.02249</link><description>&lt;p&gt;
&#19981;&#35201;&#37325;&#22797;&#26631;&#35760;&#65306;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#27604;&#36739;&#20108;&#20803;&#20998;&#31867;&#22120;&#26102;&#65292;&#25968;&#37327;&#32988;&#36807;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02249
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#26102;&#65292;&#36890;&#36807;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#32780;&#19981;&#26159;&#27719;&#24635;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#39044;&#31639;&#26469;&#27604;&#36739;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#22810;&#27425;&#25910;&#38598;&#21644;&#27719;&#24635;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#22810;&#20010;&#22122;&#22768;&#26631;&#31614;&#65292;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#24418;&#25104;&#19968;&#20010;&#19981;&#22826;&#22122;&#22768;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#24120;&#35782;&#30456;&#21453;&#30340;&#23450;&#29702;&#12290;&#22914;&#26524;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;&#20998;&#31867;&#22120;&#20013;&#30340;&#36739;&#22909;&#32773;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#20570;&#27861;&#26159;&#23558;&#39044;&#31639;&#29992;&#20110;&#25910;&#38598;&#26356;&#22810;&#26679;&#26412;&#30340;&#21333;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#23545;Cram\'er&#23450;&#29702;&#30340;&#38750;&#24179;&#20961;&#24212;&#29992;&#65292;&#36825;&#26159;&#22823;&#20559;&#24046;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23427;&#20204;&#25512;&#32763;&#20102;&#19968;&#20123;&#21382;&#21490;&#19978;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#27604;Hoeffding&#30028;&#26356;&#20248;&#30340;&#26679;&#26412;&#22823;&#23567;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.01900</link><description>&lt;p&gt;
&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Distributional Off-policy Evaluation with Bellman Residual Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#23427;&#26159;&#35768;&#22810;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#65288;&#20381;&#36182;&#20110;&#26368;&#22823;&#20540;-&#25193;&#23637;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#22914;&#26368;&#22823;&#20540;Wasserstein&#36317;&#31163;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#37327;&#21270;&#20998;&#24067;&#24335;Bellman&#27531;&#24046;&#30340;&#26399;&#26395;-&#25193;&#23637;&#30340;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#21487;&#20197;&#19978;&#30028;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#30340;&#26399;&#26395;&#35823;&#24046;&#12290;&#22522;&#20110;&#36825;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#26694;&#26550;&#25512;&#24191;&#21040;DRL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33021;&#37327;Bellman&#27531;&#24046;&#26368;&#23567;&#21270;&#65288;EBRM&#65289;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#36820;&#22238;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;EBRM&#20272;&#35745;&#22120;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27493;&#24341;&#23548;&#36807;&#31243;&#30340;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#20197;&#23454;&#29616;&#22810;&#27493;&#25193;&#23637;&#12290;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#27493;&#38271;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distributional off-policy evaluation which serves as the foundation of many distributional reinforcement learning (DRL) algorithms. In contrast to most existing works (that rely on supremum-extended statistical distances such as supremum-Wasserstein distance), we study the expectation-extended statistical distance for quantifying the distributional Bellman residuals and show that it can upper bound the expected error of estimating the return distribution. Based on this appealing property, by extending the framework of Bellman residual minimization to DRL, we propose a method called Energy Bellman Residual Minimizer (EBRM) to estimate the return distribution. We establish a finite-sample error bound for the EBRM estimator under the realizability assumption. Furthermore, we introduce a variant of our method based on a multi-step bootstrapping procedure to enable multi-step extension. By selecting an appropriate step level, we obtain a better error bound for thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21453;&#24212;&#36335;&#24452;&#21021;&#22987;&#29468;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#26426;&#21453;&#24212;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10721</link><description>&lt;p&gt;
&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#26368;&#32456;&#29366;&#24577;&#30340;&#21453;&#24212;&#36335;&#24452;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Model for Constructing Reaction Path from Initial to Final States. (arXiv:2401.10721v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21453;&#24212;&#36335;&#24452;&#21021;&#22987;&#29468;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#26426;&#21453;&#24212;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32472;&#21046;&#21453;&#24212;&#36335;&#24452;&#21450;&#20854;&#30456;&#24212;&#30340;&#27963;&#21270;&#33021;&#22418;&#26159;&#20998;&#23376;&#27169;&#25311;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#65292;&#29978;&#33267;&#29983;&#25104;&#36825;&#20123;&#36335;&#24452;&#30340;&#21021;&#22987;&#29468;&#27979;&#37117;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36825;&#20123;&#21453;&#24212;&#36335;&#24452;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36755;&#20837;&#21021;&#22987;&#29366;&#24577;&#30340;&#22352;&#26631;&#65292;&#38543;&#21518;&#36880;&#27493;&#23545;&#20854;&#32467;&#26500;&#36827;&#34892;&#25913;&#21464;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#26368;&#32456;&#29983;&#25104;&#20102;&#23545;&#21453;&#24212;&#36335;&#24452;&#30340;&#36817;&#20284;&#34920;&#31034;&#20197;&#21450;&#26368;&#32456;&#29366;&#24577;&#30340;&#22352;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#26377;&#26426;&#21453;&#24212;&#25152;&#31034;&#30340;&#22797;&#26434;&#21453;&#24212;&#36335;&#24452;&#12290;&#35757;&#32451;&#26159;&#22312;Transition1x&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26377;&#26426;&#21453;&#24212;&#36335;&#24452;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#21453;&#24212;&#19982;&#30456;&#24212;&#30340;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#24403;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation. Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem. Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways. The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure. This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state. The application of this method extends to complex reaction pathways illustrated by organic reactions. Training was executed on the Transition1x dataset, an organic reaction pathway dataset. The results revealed generation of reactions that bore substantial similarities with the corresponding test data. The method's flexibility 
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;ODE&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#21644;RNN&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#27880;&#24847;&#26426;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#32452;&#20214;&#65292;&#20294;&#26159;&#35768;&#22810;Transformer&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#30456;&#27604;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#24182;&#23558;&#37096;&#20998;&#21367;&#31215;&#23618;&#26367;&#25442;&#20026;MHSA&#65288;&#22810;&#22836;&#33258;&#27880;&#24847;&#65289;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#65288;&#24120;&#24494;&#20998;&#26041;&#31243;&#65289;&#32780;&#19981;&#26159;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#65292;&#32780;&#19988;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#21488;&#36866;&#24230;&#35268;&#27169;&#30340;FPGA&#35774;&#22791;&#19978;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#30456;&#24212;&#30340;&#38376;&#22797;&#26434;&#24230;&#32447;&#24615;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#23384;&#22312;&#25351;&#25968;&#20851;&#31995;&#65292;&#36825;&#19968;&#32467;&#26524;&#38480;&#21046;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19882</link><description>&lt;p&gt;
&#23398;&#20064;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
Learning quantum states and unitaries of bounded gate complexity. (arXiv:2310.19882v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#26377;&#30028;&#38376;&#22797;&#26434;&#24230;&#30340;&#37327;&#23376;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#30456;&#24212;&#30340;&#38376;&#22797;&#26434;&#24230;&#32447;&#24615;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#23384;&#22312;&#25351;&#25968;&#20851;&#31995;&#65292;&#36825;&#19968;&#32467;&#26524;&#38480;&#21046;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37327;&#23376;&#29366;&#24577;&#37325;&#26500;&#38750;&#24120;&#22256;&#38590;&#65292;&#20294;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#26500;&#32773;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;&#29366;&#24577;&#30340;&#20852;&#36259;&#19981;&#22823;&#12290;&#37492;&#20110;&#33258;&#28982;&#30028;&#20013;&#20986;&#29616;&#30340;&#29366;&#24577;&#21644;&#37193;&#31639;&#31526;&#37117;&#20855;&#26377;&#26377;&#30028;&#30340;&#38376;&#22797;&#26434;&#24230;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#38382;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#23558;&#30001;$G$&#20010;&#20004;&#37327;&#23376;&#27604;&#29305;&#38376;&#29983;&#25104;&#30340;&#29366;&#24577;&#23398;&#20064;&#21040;&#23567;&#30340;&#36857;&#36317;&#31163;&#65292;&#38656;&#35201;&#21644;&#20805;&#20998;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;$G$&#32447;&#24615;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23398;&#20064;&#30001;$G$&#20010;&#38376;&#29983;&#25104;&#30340;&#37193;&#31639;&#31526;&#21040;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#30340;&#26368;&#20248;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;$G$&#32447;&#24615;&#27604;&#20363;&#12290;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#30340;&#23494;&#30721;&#23398;&#29468;&#24819;&#19979;&#65292;&#23398;&#20064;&#38376;&#22797;&#26434;&#24230;&#20026;$G$&#30340;&#29366;&#24577;&#21644;&#37193;&#31639;&#31526;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24517;&#39035;&#19982;$G$&#25351;&#25968;&#27604;&#20363;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#22914;&#20309;&#30830;&#23450;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#27835;&#30103;&#30340;&#38454;&#27573;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;DTR&#24182;&#20248;&#20808;&#32771;&#34385;&#27835;&#30103;&#36712;&#36857;&#19982;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#22312;&#20915;&#31574;&#38454;&#27573;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.19300</link><description>&lt;p&gt;
&#38024;&#23545;&#21160;&#24577;&#27835;&#30103;&#30340;&#38454;&#27573;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stage-Aware Learning for Dynamic Treatments. (arXiv:2310.19300v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#27835;&#30103;&#30340;&#38454;&#27573;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;DTR&#24182;&#20248;&#20808;&#32771;&#34385;&#27835;&#30103;&#36712;&#36857;&#19982;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#22312;&#20915;&#31574;&#38454;&#27573;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#65288;DTRs&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#24378;&#22823;&#30340;&#20248;&#21270;&#27835;&#30103;&#25628;&#32034;&#31639;&#27861;&#65292;&#26681;&#25454;&#20010;&#20307;&#20855;&#20307;&#38656;&#27714;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#33021;&#26368;&#22823;&#21270;&#20854;&#39044;&#26399;&#30340;&#20020;&#24202;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#20248;&#21270;&#27835;&#30103;&#19979;&#21487;&#33021;&#20250;&#21463;&#21040;&#26679;&#26412;&#37327;&#19981;&#36275;&#30340;&#22256;&#25200;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#38271;&#26102;&#38388;&#20915;&#31574;&#38454;&#27573;&#30340;&#24930;&#24615;&#30142;&#30149;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#20307;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20272;&#35745;DTR&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#36712;&#36857;&#19982;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#22312;&#20915;&#31574;&#38454;&#27573;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#25918;&#23485;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#24517;&#39035;&#23436;&#20840;&#19982;&#26368;&#20339;&#27835;&#30103;&#19968;&#33268;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#22522;&#20110;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#26696;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#27969;&#34892;&#30340;&#32467;&#26524;&#21152;&#26435;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals' specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framewo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15233</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#26356;&#39640;&#27425;&#35856;&#27874;&#30340;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#65306;&#36890;&#36807;&#21313;&#20493;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#25628;&#32034;&#20351;&#29992;&#20449;&#21495;&#27169;&#22411;&#25110;&#27169;&#26495;&#12290;&#30446;&#21069;&#22312;LIGO-Virgo-Kagra (LVK)&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#27169;&#26495;&#20165;&#27169;&#25311;&#20102;&#20449;&#21495;&#30340;&#20027;&#23548;&#22235;&#26497;&#27169;&#24335;$(\ell,m)=(2,2)$&#65292;&#24573;&#30053;&#20102;&#27425;&#35201;&#30340;&#39640;&#38454;&#27169;&#24335;(HM)&#20363;&#22914;$(\ell,m)=(3,3)$&#65292;$(4,4)$&#65292;&#36825;&#20123;&#27169;&#24335;&#26159;&#30001;&#24191;&#20041;&#30456;&#23545;&#35770;&#39044;&#27979;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25628;&#32034;&#21487;&#33021;&#20250;&#22312;&#21442;&#25968;&#31354;&#38388;&#30340;&#19968;&#20123;&#26377;&#36259;&#21306;&#22495;&#65292;&#22914;&#39640;&#36136;&#37327;&#21644;&#38750;&#23545;&#31216;&#36136;&#37327;&#27604;&#30340;&#31995;&#32479;&#20013;&#22833;&#21435;&#23545;&#40657;&#27934;&#21512;&#24182;&#30340;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;HM&#21253;&#21547;&#22312;&#27169;&#26495;&#24211;&#20013;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#29275;&#39039;&#38468;&#21152;&#20844;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#27169;&#25311;&#19982;&#32473;&#23450;$(2,2)$&#27874;&#24418;&#30456;&#23545;&#24212;&#30340;&#33258;&#26059;&#23545;&#40784;&#30340;$(3,3)$&#65292;$(4,4)$&#27874;&#24418;&#12290;&#21487;&#20197;&#23545;&#27599;&#20010;&#27169;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#21333;&#29420;&#28388;&#27874;&#65292;&#24471;&#21040;&#20449;&#22122;&#27604;(SNR)&#30340;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#21487;&#20197;&#20197;&#30456;&#23545;&#24265;&#20215;&#30340;&#26041;&#24335;&#23558;&#20854;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13193</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#26159;&#20132;&#36890;&#27969;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20132;&#36890;&#20998;&#37197;&#21644;&#20132;&#36890;&#27969;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#38142;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#36890;&#27169;&#24335;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22478;&#24066;&#20132;&#36890;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#35757;&#32451;&#25439;&#22833;&#21644;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32593;&#32476;&#25299;&#25169;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22797;&#26434;&#20132;&#36890;&#27969;&#20998;&#26512;&#21644;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.06119</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#21644;&#24322;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#31995;&#32479;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#24433;&#21709;&#36825;&#20123;&#31995;&#32479;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MTS&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#21644;&#26102;&#31354;&#39044;&#27979;&#65288;STF&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#21644;&#25216;&#26415;&#26041;&#27861;&#30340;&#36873;&#25321;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#20123;&#20105;&#35758;&#26174;&#33879;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#20105;&#35758;&#65292;&#20197;&#25552;&#20379;&#23545;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BasicTS&#65292;&#19968;&#20010;&#26088;&#22312;&#20844;&#24179;&#27604;&#36739;MTS&#39044;&#27979;&#30340;&#22522;&#20934;&#12290;BasicTS&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#27969;&#31243;&#21644;&#21512;&#29702;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#33021;&#22815;&#23545;30&#22810;&#31181;&#27969;&#34892;&#30340;MTS&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20844;&#27491;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
&lt;/p&gt;</description></item><item><title>D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16118</link><description>&lt;p&gt;
D$^3$Fields: &#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#29992;&#20110;&#38646;&#26679;&#26412;&#21487;&#27867;&#21270;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16118
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#31034;&#24212;&#35813;&#26159;&#19977;&#32500;&#30340;&#12289;&#21160;&#24577;&#30340;&#21644;&#35821;&#20041;&#21270;&#30340;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#21516;&#26102;&#32570;&#20047;&#36825;&#19977;&#20010;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D$^3$Fields&#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#12290;&#36825;&#20123;&#22330;&#25429;&#25417;&#20102;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#32534;&#30721;&#20102;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#21306;&#22495;&#20013;&#30340;&#20219;&#24847;&#19977;&#32500;&#28857;&#25237;&#24433;&#21040;&#22810;&#35270;&#35282;&#30340;&#20108;&#32500;&#35270;&#35273;&#35266;&#23519;&#20013;&#65292;&#24182;&#25554;&#20540;&#20174;&#22522;&#26412;&#27169;&#22411;&#20013;&#24471;&#21040;&#30340;&#29305;&#24449;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#34701;&#21512;&#25551;&#36848;&#31526;&#22330;&#21487;&#20197;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#28789;&#27963;&#22320;&#25351;&#23450;&#30446;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25551;&#36848;&#31526;&#22330;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#34920;&#31034;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#21644;&#27169;&#25311;&#20013;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#30340;&#26368;&#20339;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10340</link><description>&lt;p&gt;
&#23547;&#25214;&#24179;&#34913;&#65306;&#29992;&#20110;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#37319;&#38598;&#30340;&#36923;&#36753;&#22238;&#24402;&#30340;&#26368;&#20339;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression. (arXiv:2309.10340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#30340;&#26368;&#20339;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20174;&#38544;&#31169;&#25935;&#24863;&#21334;&#26041;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#25454;&#26159;&#31169;&#26377;&#30340;&#65292;&#21334;&#26041;&#24517;&#39035;&#36890;&#36807;&#25903;&#20184;&#26469;&#28608;&#21169;&#20182;&#20204;&#25552;&#20379;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#26426;&#21046;&#65292;&#20248;&#21270;&#27979;&#35797;&#25439;&#22833;&#12289;&#21334;&#26041;&#38544;&#31169;&#21644;&#25903;&#20184;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#21363;&#22312;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#20043;&#38388;&#23547;&#25214;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#21338;&#24328;&#35770;&#12289;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20080;&#26041;&#30340;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#38750;&#24120;&#38750;&#20984;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38382;&#39064;&#21442;&#25968;&#30340;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#30340;&#21464;&#25442;&#23558;&#38382;&#39064;&#20984;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24403;&#21334;&#26041;&#25968;&#37327;&#21464;&#22823;&#26102;&#65292;&#20080;&#26041;&#30340;&#27979;&#35797;&#35823;&#24046;&#21644;&#25903;&#20184;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of performing logistic regression on data collected from privacy-sensitive sellers. Since the data is private, sellers must be incentivized through payments to provide their data. Thus, the goal is to design a mechanism that optimizes a weighted combination of test loss, seller privacy, and payment, i.e., strikes a balance between multiple objectives of interest. We solve the problem by combining ideas from game theory, statistical learning theory, and differential privacy. The buyer's objective function can be highly non-convex. However, we show that, under certain conditions on the problem parameters, the problem can be convexified by using a change of variables. We also provide asymptotic results characterizing the buyer's test error and payments when the number of sellers becomes large. Finally, we demonstrate our ideas by applying them to a real healthcare data set.
&lt;/p&gt;</description></item><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04417</link><description>&lt;p&gt;
&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20844;&#27491;&#24863;&#30693;&#32852;&#37030;&#26497;&#23567;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#33258;&#30001;&#24230;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#20559;&#21521;&#20110;&#25935;&#24863;&#22240;&#32032;&#35832;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#24102;&#26377;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#20844;&#24179;&#32852;&#37030;&#24179;&#22343;&#27861; (FFALM)&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;FL&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#30446;&#26631;&#26045;&#21152;&#20102;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;FFALM&#30340;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;CelebA&#21644;UTKFace&#25968;&#25454;&#38598;&#20013;&#20805;&#20998;&#32771;&#34385;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;FFALM &#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04955</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#32593;&#32476;&#23545;&#38477;&#35299;&#22810;&#36793;&#24418;&#30340;&#24863;&#30693;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65306;&#20174;&#23545;&#25239;&#25915;&#20987;&#21040;&#22270;&#20687;&#25439;&#22351;&#65292;&#28145;&#24230;&#23398;&#20064;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#28982;&#32780;&#20154;&#31867;&#21364;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24674;&#22797;&#21463;&#25439;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#20154;&#31867;&#35270;&#35273;&#30340;&#8220;&#35782;&#21035;&#32452;&#20214;&#8221;&#29702;&#35770;&#20013;&#39318;&#27425;&#24341;&#20837;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#21160;&#21270;&#24418;&#29366;&#21487;&#24674;&#22797;&#24615;&#27979;&#35797;&#65292;&#24555;&#36895;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#21382;&#21490;&#19978;&#25163;&#21160;&#21019;&#24314;&#22270;&#20687;&#21487;&#24674;&#22797;&#24615;&#23454;&#39564;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29616;&#20195;&#21270;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22810;&#36793;&#24418;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01181</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#20026;&#19968;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21327;&#21161;&#26500;&#24314;&#30456;&#20851;&#20219;&#21153;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;&#35813;&#33539;&#20363;&#22312;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#23588;&#20854;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20844;&#24320;&#30340;&#65292;&#21482;&#26377;&#24494;&#35843;&#25968;&#25454;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#29702;&#30001;&#35748;&#20026;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20173;&#28982;&#26159;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#24517;&#39035;&#20102;&#35299;&#24494;&#35843;&#27169;&#22411;&#27844;&#38706;&#26377;&#20851;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#29702;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#35775;&#38382;&#24050;&#32463;&#24494;&#35843;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#24819;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20803;&#20998;&#31867;&#22120;&#30340;&#25915;&#20987;TMI&#65292;&#23427;&#21033;&#29992;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;TMI&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20165;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20197;&#21450;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#30340;&#26694;&#26550;&#65292;&#23558;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#21644;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#32852;&#31995;&#20102;&#36215;&#26469;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19674</link><description>&lt;p&gt;
&#22312;&#32447;&#21040;PAC&#30340;&#36716;&#25442;: &#36890;&#36807;&#36951;&#25022;&#20998;&#26512;&#24471;&#20986;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#30340;&#26694;&#26550;&#65292;&#23558;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#21644;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#32852;&#31995;&#20102;&#36215;&#26469;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#25512;&#23548;&#20986;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#31216;&#20026;&#8220;&#27867;&#21270;&#28216;&#25103;&#8221;&#65292;&#20854;&#20013;&#22312;&#32447;&#23398;&#20064;&#22120;&#35797;&#22270;&#19982;&#22266;&#23450;&#30340;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#31454;&#20105;&#65292;&#39044;&#27979;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#28857;&#35757;&#32451;&#38598;&#19978;&#30340;&#27867;&#21270;&#38388;&#38553;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#23384;&#22312;&#26377;&#30028;&#36951;&#25022;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#19982;&#32479;&#35745;&#23398;&#20064;&#35774;&#32622;&#20043;&#38388;&#30340;&#32852;&#31995;&#26469;&#24314;&#31435;&#36825;&#31181;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#38169;&#35823;&#23384;&#22312;&#19968;&#20010;&#30028;&#38480;&#65292;&#30452;&#21040;&#19982;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#26080;&#20851;&#30340;&#38789;&#27987;&#24230;&#39033;&#12290;&#36825;&#31181;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#24674;&#22797;&#20960;&#20010;&#26631;&#20934;&#30340;&#27867;&#21270;&#38480;&#21046;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;PAC-Bayesian&#20445;&#35777;&#21644;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the "generalization game", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.02662</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;
&lt;/p&gt;
&lt;p&gt;
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25429;&#25417;&#19990;&#30028;&#29289;&#29702;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#32780;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;GLAM&#65289;&#65306;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20351;&#29992;LLM&#20316;&#20026;&#31574;&#30053;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#38543;&#30528;&#20195;&#29702;&#31243;&#24207;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#32780;&#36880;&#27493;&#26356;&#26032;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#35299;&#20915;&#30446;&#26631;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25991;&#26412;&#29615;&#22659;&#35774;&#35745;&#26469;&#30740;&#31350;&#26356;&#39640;&#32423;&#24418;&#24335;&#30340;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#20197;&#21450;&#19968;&#32452;&#31354;&#38388;&#21644;&#23548;&#33322;&#20219;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;LLMs&#33021;&#21542;&#25552;&#39640;&#21508;&#31181;RL&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65311;2&#65289;&#23427;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#24418;&#24335;&#30340;&#27867;&#21270;&#65311;3&#65289;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#21151;&#33021;&#26041;&#24335;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
&lt;/p&gt;</description></item></channel></rss>