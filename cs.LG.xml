<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20212</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#23610;&#23544;&#21644;&#38590;&#24230;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20212
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#28909;&#22270;&#65292;&#25351;&#31034;&#27599;&#26465;&#36793;&#25104;&#20026;&#26368;&#20339;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#35757;&#32451;&#23454;&#20363;&#22823;&#23567;&#12289;&#23884;&#20837;&#32500;&#25968;&#21644;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#21508;&#31181;&#20998;&#24067;&#30340;&#38590;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20212v1 Announce Type: new  Abstract: We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results
&lt;/p&gt;</description></item><item><title>SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12977</link><description>&lt;p&gt;
SportsNGEN: &#25345;&#32493;&#29983;&#25104;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12977
&lt;/p&gt;
&lt;p&gt;
SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;SportsNGEN&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#36816;&#21160;&#21592;&#21644;&#29699;&#36861;&#36394;&#24207;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#25345;&#32493;&#30340;&#28216;&#25103;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#19987;&#19994;&#32593;&#29699;&#36861;&#36394;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;SportsNGEN&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#27169;&#25311;&#19982;&#23556;&#20987;&#20998;&#31867;&#22120;&#21644;&#36923;&#36753;&#30456;&#32467;&#21512;&#26469;&#24320;&#22987;&#21644;&#32467;&#26463;&#29699;&#36187;&#65292;&#31995;&#32479;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#12290;&#27492;&#22806;&#65292;SportsNGEN&#30340;&#36890;&#29992;&#29256;&#26412;&#21487;&#20197;&#36890;&#36807;&#22312;&#21253;&#21547;&#35813;&#29699;&#21592;&#30340;&#27604;&#36187;&#25968;&#25454;&#19978;&#24494;&#35843;&#26469;&#23450;&#21046;&#29305;&#23450;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21453;&#20107;&#23454;&#25110;&#20551;&#35774;&#36873;&#39033;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36136;&#37327;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36275;&#29699;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12977v1 Announce Type: cross  Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11046</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#38388;&#31454;&#20105;&#35843;&#25511;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Regulating Chatbot Output via Inter-Informational Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#24180;&#22810;&#30340;&#30417;&#31649;&#29378;&#28526;&#12290;&#28982;&#32780;&#65292;&#23569;&#25968;&#29616;&#26377;&#30740;&#31350;&#20005;&#26684;&#36136;&#30097;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#22914;&#26524;&#19981;&#32463;&#35268;&#33539;&#65292;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#20250;&#23545;&#20154;&#31867;&#20107;&#21153;&#36896;&#25104;&#23454;&#36136;&#19988;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#20851;&#38190;&#21487;&#33021;&#24615;&#65292;&#24182;&#22240;&#27492;&#20542;&#21521;&#20110;&#20351;&#29992;&#30417;&#31649;&#24037;&#20855;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#21508;&#31181;&#28192;&#36947;&#20043;&#38388;&#30340;&#20449;&#24687;&#31454;&#20105;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#37325;&#26032;&#35780;&#20272;AI&#30456;&#20851;&#20869;&#23481;&#39118;&#38505;&#21644;&#30456;&#24212;&#30417;&#31649;&#25552;&#35758;&#30340;&#26631;&#20934;&#12290;&#38271;&#36798;&#25968;&#21313;&#24180;&#30340;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30417;&#31649;&#21490;&#34920;&#26126;&#65292;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#22312;&#25552;&#20986;&#36807;&#24230;&#30340;&#30417;&#31649;&#25514;&#26045;&#26102;&#29359;&#38169;&#35823;&#12290;&#20107;&#23454;&#19978;&#65292;&#20016;&#23500;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#20102;&#20449;&#24687;&#24066;&#22330;&#26426;&#21046;&#22312;&#20449;&#24687;&#30417;&#31649;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.06807</link><description>&lt;p&gt;
&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multistep Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#35757;&#32451;&#65292;&#20294;&#29983;&#25104;&#26679;&#26412;&#38656;&#35201;&#35768;&#22810;&#27493;&#39588;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#26356;&#38590;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65306;&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;TRACT&#30340;&#32479;&#19968;&#65292;&#21487;&#20197;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65306;&#22312;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#20256;&#32479;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#23637;&#31034;&#20102;$\infty$&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#25193;&#25955;&#27169;&#22411;&#12290;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23558;&#26679;&#26412;&#39044;&#31639;&#20174;&#21333;&#27493;&#22686;&#21152;&#21040;2-8&#27493;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#37096;&#20998;&#37319;&#26679;&#36895;&#24230;&#20248;&#21183;&#12290;&#22312;Imagenet 64&#19978;8&#27493;&#36798;&#21040;1.4&#30340;FID&#65292;&#22312;Imagenet128&#19978;8&#27493;&#36798;&#21040;2.1&#30340;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
&lt;/p&gt;</description></item><item><title>&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.01420</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#19981;&#21464;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#38544;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Heterogeneity towards Invariance and Causality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01420
&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20351;&#29992;&#19968;&#31181;&#21464;&#20307;&#22238;&#24402;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;&#36825;&#19982;&#20256;&#32479;&#26234;&#24935;&#8220;&#20851;&#32852;&#19981;&#26159;&#22240;&#26524;&#8221;&#20197;&#21450;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#30456;&#21453;&#65292;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#35748;&#20026;&#20808;&#21069;&#30340;&#22240;&#26524;&#30693;&#35782;&#24212;&#35880;&#24910;&#22320;&#32435;&#20837;&#21040;&#26041;&#27861;&#35774;&#35745;&#20013;&#12290;&#20196;&#20154;&#22256;&#24785;&#30340;&#26159;&#65292;&#20026;&#20309;&#22312;&#36861;&#27714;&#20851;&#32852;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#33021;&#22815;&#20174;&#26356;&#39640;&#23618;&#27425;&#30340;&#29702;&#35299;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#12290;&#26412;&#25991;&#22768;&#31216;&#20174;&#38754;&#21521;&#20851;&#32852;&#30340;&#35757;&#32451;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#21487;&#20197;&#24402;&#22240;&#20110;&#28304;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#35757;&#32451;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21270;&#30340;&#32806;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#35265;&#22320;&#30340;&#27169;&#22411;&#26469;&#38416;&#37322;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22238;&#24402;&#25439;&#22833;&#23398;&#20064;&#19981;&#21464;&#24615;&#65292;&#19968;&#31181;&#20934;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01420v1 Announce Type: new  Abstract: It is observed empirically that the large language models (LLM), trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation'' and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be spec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00570</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32858;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking cluster-conditioned diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#30340;&#22270;&#29255;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20851;&#20110;&#22270;&#29255;&#32858;&#31867;&#30340;&#20010;&#21035;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#29255;&#21512;&#25104;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#32771;&#34385;&#21040;&#22270;&#29255;&#21512;&#25104;&#65288;&#35270;&#35273;&#32452;&#65289;&#30340;&#26368;&#20339;&#31751;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32858;&#31867;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;FID&#65288;&#21363;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#20998;&#21035;&#20026;1.67&#21644;2.17&#65289;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#24378;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#32858;&#31867;&#26469;&#25512;&#23548;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#38480;&#31751;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#19982;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#29255;&#29983;&#25104;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#32852;&#31995;&#12290;&#20195;&#30721;&#21644;&#32858;&#31867;&#20998;&#37197;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.15000</link><description>&lt;p&gt;
&#21010;&#20998;&#36824;&#26159;&#24449;&#26381;&#65311;&#20320;&#24212;&#35813;&#25552;&#28860;LLM&#30340;&#21738;&#19968;&#37096;&#20998;&#65311;
&lt;/p&gt;
&lt;p&gt;
Divide-or-Conquer? Which Part Should You Distill Your LLM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34987;&#40723;&#21169;&#20808;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#30340;&#23376;&#20219;&#21153;&#26102;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#35813;&#31574;&#30053;&#33021;&#22815;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#19982;&#35299;&#20915;&#38382;&#39064;&#30456;&#27604;&#65292;&#20998;&#35299;&#38454;&#27573;&#26356;&#23481;&#26131;&#34987;&#25552;&#28860;&#20026;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#21069;&#32773;&#21482;&#38656;&#35201;&#23398;&#20064;&#19968;&#33324;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#28860;&#36825;&#20004;&#31181;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25512;&#29702;&#32467;&#26524;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#25552;&#28860;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#65292;&#24182;&#21516;&#26102;&#22312;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#28860;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#23601;&#26356;&#22256;&#38590;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25552;&#20379;&#19987;&#38376;&#25968;&#25454;&#38598;&#24182;&#24314;&#31435;&#20102;PQA&#26694;&#26550;&#65292;&#21253;&#21547;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13653</link><description>&lt;p&gt;
PQA&#65306;&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#30001;&#31185;&#23398;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13653
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25552;&#20379;&#19987;&#38376;&#25968;&#25454;&#38598;&#24182;&#24314;&#31435;&#20102;PQA&#26694;&#26550;&#65292;&#21253;&#21547;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#65288;PQA&#65289;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#30001;&#24418;&#24335;&#31185;&#23398;&#25506;&#31350;&#12290;&#32473;&#23450;&#19968;&#20010;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#25552;&#20379;&#19968;&#20010;&#31185;&#23398;&#19978;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#36825;&#19968;&#20219;&#21153;&#19981;&#20165;&#25903;&#25345;&#26410;&#26469;&#30340;&#29983;&#29289;&#30740;&#31350;&#65292;&#36824;&#21487;&#20197;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31185;&#23398;&#31934;&#24230;&#25552;&#20379;&#19968;&#20010;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;PQA&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;257K&#20010;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#27880;&#37322;&#26377;1.97M&#20010;&#31185;&#23398;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#19982;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#12290;&#36890;&#36807;&#20004;&#31181;&#24378;&#22823;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;PQA&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;PQA&#26694;&#26550;&#65292;&#21517;&#20026;Pika&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#12289;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13653v1 Announce Type: new  Abstract: We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09122</link><description>&lt;p&gt;
&#28151;&#21512;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed-Output Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#20449;&#21495;&#20998;&#31163;&#26041;&#27861;&#65292;&#20854;&#20013;&#20449;&#21495;&#21487;&#20197;&#26681;&#25454;&#28508;&#21464;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22686;&#21152;&#20102;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#65292;&#20197;&#21253;&#25324;&#27599;&#20010;&#25968;&#25454;&#28857;&#30001;&#24050;&#30693;&#25968;&#37327;&#30340;&#32431;&#32452;&#20998;&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#22810;&#20010;&#36755;&#20837;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#20851;&#20110;&#27599;&#20010;&#35266;&#27979;&#26435;&#37325;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#20998;&#25968;&#32452;&#25104;&#30340;&#24635;&#21644;&#20026;&#19968;&#32422;&#26463;&#21644;&#29992;&#20110;&#20998;&#31867;&#30340;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23545;&#20110;&#20809;&#35889;&#23398;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#25913;&#21464;&#26465;&#20214;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#32431;&#32452;&#20998;&#20449;&#21495;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#23637;&#31034;&#23545;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#24212;&#29992;&#65306;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#28201;&#24230;&#30340;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36817;&#20284;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#38024;&#23545;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;&#34892;&#27874;&#35299;&#65292;&#24341;&#20837;&#20102;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#26469;&#25552;&#39640;&#35299;&#30340;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08313</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;Fisher&#26041;&#31243;&#30340;&#35299;&#30340;&#26063;&#32676;
&lt;/p&gt;
&lt;p&gt;
Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36817;&#20284;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#38024;&#23545;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;&#34892;&#27874;&#35299;&#65292;&#24341;&#20837;&#20102;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#26469;&#25552;&#39640;&#35299;&#30340;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#21453;&#24212;&#25193;&#25955;&#31995;&#32479;&#30340;&#22522;&#26412;&#34920;&#31034;&#12290;&#37325;&#28857;&#26159;&#30740;&#31350;&#22312;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;Fisher&#26041;&#31243;&#65292;&#20854;&#20013;&#35299;&#21576;&#29616;&#20026;&#34892;&#27874;&#65292;&#30001;&#20110;&#27874;&#21069;&#30340;&#38497;&#23789;&#24615;&#65292;&#23545;&#25968;&#20540;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#20934;PINN&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#20013;&#30340;&#21453;&#24212;&#39033;&#26469;&#22686;&#24378;&#34892;&#27874;&#21069;&#30340;&#36319;&#36394;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19987;&#38376;&#38024;&#23545;&#34892;&#27874;&#24418;&#24335;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23558;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#32593;&#32476;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper employs physics-informed neural networks (PINNs) to solve Fisher's equation, a fundamental representation of a reaction-diffusion system with both simplicity and significance. The focus lies specifically in investigating Fisher's equation under conditions of large reaction rate coefficients, wherein solutions manifest as traveling waves, posing a challenge for numerical methods due to the occurring steepness of the wave front. To address optimization challenges associated with the standard PINN approach, a residual weighting scheme is introduced. This scheme is designed to enhance the tracking of propagating wave fronts by considering the reaction term in the reaction-diffusion equation. Furthermore, a specific network architecture is studied which is tailored for solutions in the form of traveling waves. Lastly, the capacity of PINNs to approximate an entire family of solutions is assessed by incorporating the reaction rate coefficient as an additional input to the network 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12044</link><description>&lt;p&gt;
XLand-MiniGrid: &#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12044
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;XLand&#30340;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#20197;&#21450;MiniGrid&#30340;&#31616;&#21333;&#21644;&#31616;&#32422;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XLand-MiniGrid&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#24037;&#20855;&#22871;&#20214;&#21644;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#12290;XLand-MiniGrid&#37319;&#29992;JAX&#32534;&#20889;&#65292;&#26088;&#22312;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;GPU&#25110;TPU&#21152;&#36895;&#22120;&#19978;&#36816;&#34892;&#65292;&#29992;&#26377;&#38480;&#36164;&#28304;&#23454;&#29616;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#27665;&#20027;&#21270;&#12290;&#38500;&#20102;&#29615;&#22659;&#22806;&#65292;XLand-MiniGrid&#36824;&#25552;&#20379;&#20102;&#39044;&#37319;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#29420;&#29305;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#24320;&#22987;&#35757;&#32451;&#33258;&#36866;&#24212;&#20195;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35268;&#27169;&#21270;&#21644;&#27867;&#21270;&#30340;&#21021;&#27493;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#35757;&#32451;&#20013;&#21487;&#20197;&#36798;&#21040;&#27599;&#31186;&#25968;&#30334;&#19975;&#27493;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.12843</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#35745;&#31639;&#30340;&#26102;&#38388;&#22270;
&lt;/p&gt;
&lt;p&gt;
An embedding-based distance for temporal graphs. (arXiv:2401.12843v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#26102;&#38388;&#22270;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#20351;&#29992;&#26102;&#38388;&#23562;&#37325;&#30340;&#38543;&#26426;&#28216;&#36208;&#26500;&#24314;&#30340;&#22270;&#23884;&#20837;&#26469;&#23450;&#20041;&#20102;&#19968;&#31181;&#26102;&#38388;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21305;&#37197;&#22270;&#21644;&#19981;&#21305;&#37197;&#22270;&#30340;&#24773;&#20917;&#65292;&#24403;&#23384;&#22312;&#24050;&#30693;&#30340;&#33410;&#28857;&#20851;&#31995;&#26102;&#65292;&#20197;&#21450;&#24403;&#19981;&#23384;&#22312;&#35813;&#20851;&#31995;&#24182;&#19988;&#22270;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#26102;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#26102;&#38388;&#32593;&#32476;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#36317;&#31163;&#23450;&#20041;&#30340;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#26102;&#38388;&#23646;&#24615;&#30340;&#22270;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#30340;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05972</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;Hasegawa-Wakatani&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning physics-based reduced models from data for the Hasegawa-Wakatani equations. (arXiv:2401.05972v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#30340;&#32422;&#20943;&#27169;&#22411;&#65288;ROMs&#65289;&#26500;&#24314;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#12289;&#28151;&#27788;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;operator inference&#65288;OpInf&#65289;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;ROMs&#29992;&#20110;&#36825;&#31181;&#27169;&#25311;&#12290;&#20197;Hasegawa-Wakatani&#65288;HW&#65289;&#26041;&#31243;&#20026;&#20195;&#34920;&#65292;&#22312;&#24418;&#25104;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#32771;&#23519;&#20102;OpInf&#26500;&#24314;&#20934;&#30830;ROMs&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#20004;&#32452;&#23454;&#39564;&#12290;&#31532;&#19968;&#32452;&#23454;&#39564;&#21033;&#29992;&#36890;&#36807;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20174;&#29305;&#23450;&#21021;&#20540;&#26465;&#20214;&#24320;&#22987;&#30340;HW&#26041;&#31243;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;OpInf ROMs&#30340;&#35757;&#32451;&#20197;&#23454;&#29616;&#36229;&#36234;&#35757;&#32451;&#26102;&#38388;&#33539;&#22260;&#30340;&#39044;&#27979;&#12290;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31532;&#20108;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#23545;ROMs&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the construction of non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma turbulence simulations. In particular, we propose using Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we focus on the Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a comprehensive perspective of the potential of OpInf to construct accurate ROMs for this model, we consider a setup for the HW equations that leads to the formation of complex, nonlinear, and self-driven dynamics, and perform two sets of experiments. We first use the data obtained via a direct numerical simulation of the HW equations starting from a specific initial condition and train OpInf ROMs for predictions beyond the training time horizon. In the second, more challenging set of experiments, we train ROMs using the same datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18160</link><description>&lt;p&gt;
&#23545;&#31561;&#20844;&#24179;&#24615;&#8212;&#8212;&#35299;&#20915;&#20844;&#24179;&#35780;&#20272;&#20013;&#32676;&#20307;&#20043;&#38388;&#31995;&#32479;&#24046;&#24322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation. (arXiv:2305.18160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#20915;&#31574;&#26102;&#65292;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21363;&#19981;&#27495;&#35270;&#29305;&#23450;&#20010;&#20307;/&#32676;&#20307;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#24369;&#21183;&#32676;&#20307;&#30340;&#20154;&#12290;&#29616;&#26377;&#30340;&#32676;&#20307;&#20844;&#24179;&#26041;&#27861;&#35201;&#27714;&#36827;&#34892;&#24179;&#31561;&#30340;&#32676;&#20307;&#27979;&#37327;&#65292;&#20294;&#26410;&#32771;&#34385;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#34429;&#28982;&#19982;&#25935;&#24863;&#21464;&#37327;&#26080;&#20851;&#65292;&#20294;&#34920;&#29616;&#20986;&#31995;&#32479;&#24046;&#24322;&#65292;&#20250;&#23545;&#20844;&#24179;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#20844;&#24179;&#27979;&#37327;&#24212;&#35813;&#22522;&#20110;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#20110;&#24863;&#20852;&#36259;&#20219;&#21153;&#30340;&#23545;&#31561;&#20154;&#65288;&#21363;&#24444;&#27492;&#30456;&#20284;&#30340;&#20010;&#20307;&#65289;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20854;&#32676;&#20307;&#36523;&#20221;&#19981;&#21487;&#36890;&#36807;&#25506;&#32034;&#28151;&#28102;&#22240;&#32032;&#31639;&#27861;&#22320;&#21306;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#20197;&#36991;&#20813;&#20844;&#24179;&#35780;&#20272;&#27604;&#36739;&#8220;&#27225;&#23376;&#8221;&#21644;&#8220;&#33529;&#26524;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using machine learning (ML) to aid decision-making, it is critical to ensure that an algorithmic decision is fair, i.e., it does not discriminate against specific individuals/groups, particularly those from underprivileged populations. Existing group fairness methods require equal group-wise measures, which however fails to consider systematic between-group differences. The confounding factors, which are non-sensitive variables but manifest systematic differences, can significantly affect fairness evaluation. To tackle this problem, we believe that a fairness measurement should be based on the comparison between counterparts (i.e., individuals who are similar to each other with respect to the task of interest) from different groups, whose group identities cannot be distinguished algorithmically by exploring confounding factors. We have developed a propensity-score-based method for identifying counterparts, which prevents fairness evaluation from comparing "oranges" with "apples". 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.13723</link><description>&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#39592;&#24178;&#23398;&#20064;&#22810;&#20010;&#30446;&#26631;&#12290;&#19982;&#21333;&#29420;&#35757;&#32451;&#20219;&#21153;&#30456;&#27604;&#65292;MTL&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#26469;&#28508;&#22312;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#20854;&#20013;&#65292;MTL&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#38598;&#20013;&#22312;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#25512;&#23548;&#20986;&#23545;&#25152;&#26377;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#32456;&#26799;&#24230;&#19979;&#38477;&#26041;&#21521;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#32780;&#19981;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#35757;&#32451;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#25439;&#22833;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item></channel></rss>