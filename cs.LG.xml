<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01077</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Predictive Modeling with Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20351;&#24471;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#24739;&#32773;&#25968;&#25454;&#24471;&#20197;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#25324;&#21307;&#30103;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;EHR&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;EHR&#25968;&#25454;&#30340;&#32972;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#24314;&#27169;&#20219;&#21153;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#39044;&#27979;&#28145;&#24230;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19982;&#21307;&#30103;&#39044;&#27979;&#24314;&#27169;&#30456;&#20851;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.16133</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;SSHPool
&lt;/p&gt;
&lt;p&gt;
SSHPool: The Separated Subgraph-based Hierarchical Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16133
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#65292;&#29992;&#20110;&#22270;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26679;&#26412;&#22270;&#30340;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#20998;&#38548;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;&#26412;&#22320;&#22270;&#21367;&#31215;&#21333;&#20803;&#20316;&#20026;&#23616;&#37096;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23558;&#27599;&#20010;&#23376;&#22270;&#21387;&#32553;&#25104;&#19968;&#20010;&#31895;&#31961;&#33410;&#28857;&#65292;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#31895;&#31961;&#22270;&#12290;&#30001;&#20110;&#36825;&#20123;&#23376;&#22270;&#30001;&#19981;&#21516;&#30340;&#31751;&#20998;&#38548;&#24320;&#65292;&#32467;&#26500;&#20449;&#24687;&#26080;&#27861;&#22312;&#23427;&#20204;&#20043;&#38388;&#20256;&#25773;&#65292;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#21487;&#20197;&#26174;&#33879;&#36991;&#20813;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#32467;&#26524;&#31895;&#31961;&#22270;&#19978;&#23618;&#27425;&#22320;&#25191;&#34892;&#25152;&#25552;&#35758;&#30340;&#31243;&#24207;&#65292;SSHPool&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16133v1 Announce Type: new  Abstract: In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.16130</link><description>&lt;p&gt;
AKBR: &#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AKBR: Learning Adaptive Kernel-based Representations for Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;&#65288;AKBR&#65289;&#12290;&#19982;&#20165;&#36890;&#36807;&#35745;&#31639;&#22270;&#20043;&#38388;&#21516;&#26500;&#23376;&#32467;&#26500;&#23545;&#30340;&#25968;&#37327;&#26469;&#23450;&#20041;&#30340;&#26368;&#20808;&#36827;&#30340; R-&#21367;&#31215;&#22270;&#26680;&#19981;&#21516;&#65292;&#26080;&#27861;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#31471;&#21040;&#31471;&#23398;&#20064;&#26426;&#21046;&#65292;&#25152;&#25552;&#20986;&#30340;AKBR&#26041;&#27861;&#26088;&#22312;&#23450;&#20041;&#19968;&#20010;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22270;&#26500;&#24314;&#33258;&#36866;&#24212;&#26680;&#30697;&#38453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#21407;&#22987;&#22270;&#20013;&#19981;&#21516;&#23376;&#32467;&#26500;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;AKBR&#27169;&#22411;&#22240;&#27492;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#19981;&#21516;&#23376;&#32467;&#26500;&#30340;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#30001;&#20854;&#32467;&#26500;&#27880;&#24847;&#21147;&#25351;&#23450;&#30340;&#26356;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#25104;&#23545;&#22270;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;&#30001;&#20110;&#32467;&#26524;&#26680;&#30697;&#38453;&#30340;&#27599;&#19968;&#34892;...&#65288;&#27492;&#22788;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16130v1 Announce Type: cross  Abstract: In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel mat
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13369</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13369
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#20214;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#20449;&#24687;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#25152;&#38656;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#25104;&#26412;&#12289;&#27169;&#22411;&#39044;&#27979;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#12289;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#38544;&#31169;&#27861;&#35268;&#12290;&#26368;&#36817;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#25552;&#31034;&#26041;&#27861;&#19978;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#23569;&#36164;&#28304;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#21307;&#29983;&#20449;&#20214;&#19978;&#36827;&#34892;&#22810;&#31867;&#21035;&#27573;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#31867;&#21035;&#32423;&#35780;&#20272;&#65292;&#25903;&#25345; Shapley &#20540;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20165;&#20165;&#25552;&#31034;&#20102; 20 &#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13369v1 Announce Type: new  Abstract: Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classificatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#31616;&#21270;&#35299;&#20915;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#21512;&#29702;&#20934;&#30830;&#22320;&#25429;&#25417;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17232</link><description>&lt;p&gt;
&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Two-scale Neural Networks for Partial Differential Equations with Small Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17232
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#31616;&#21270;&#35299;&#20915;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#21512;&#29702;&#20934;&#30830;&#22320;&#25429;&#25417;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#21452;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#23567;&#21442;&#25968;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#20197;&#31616;&#21333;&#26041;&#24335;&#35299;&#20915;&#20855;&#26377;&#23567;&#21442;&#25968;&#30340;PDE&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#20613;&#37324;&#21494;&#29305;&#24449;&#25110;&#20854;&#20182;&#35745;&#31639;&#32321;&#29712;&#30340;&#25130;&#26029;&#21442;&#25968;&#25628;&#32034;&#12290;&#22810;&#20010;&#25968;&#20540;&#20363;&#23376;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#30001;&#23567;&#21442;&#25968;&#24341;&#36215;&#30340;&#35299;&#20013;&#22823;&#23548;&#25968;&#29305;&#24449;&#26102;&#30340;&#21512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17232v1 Announce Type: cross  Abstract: We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs). We directly incorporate the small parameters into the architecture of neural networks. The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters. Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters.
&lt;/p&gt;</description></item><item><title>Transformers&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20013;&#65292;&#20851;&#38190;&#30340;&#35777;&#25454;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;</title><link>https://arxiv.org/abs/2402.14735</link><description>&lt;p&gt;
Transformers&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
How Transformers Learn Causal Structure with Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14735
&lt;/p&gt;
&lt;p&gt;
Transformers&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20013;&#65292;&#20851;&#38190;&#30340;&#35777;&#25454;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#21151;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#20449;&#24687;&#22312;&#24207;&#21015;&#30340;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20256;&#36882;&#12290;&#33258;&#27880;&#24847;&#26426;&#21046;&#20351;&#24471;transformers&#33021;&#22815;&#32534;&#30721;&#22240;&#26524;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#20854;&#29305;&#21035;&#36866;&#21512;&#24207;&#21015;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;transformers&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;&#23398;&#20064;&#36825;&#31181;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38656;&#35201;&#23398;&#20064;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21270;&#30340;&#20004;&#23618;transformer&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#23398;&#20250;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#31532;&#19968;&#23618;&#27880;&#24847;&#21147;&#20013;&#32534;&#30721;&#28508;&#22312;&#22240;&#26524;&#22270;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#30001;&#20110;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#32467;&#26524;&#65292;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#26368;&#22823;&#30340;&#26465;&#30446;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14735v1 Announce Type: new  Abstract: The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13033</link><description>&lt;p&gt;
&#29992;&#36229;&#36793;&#22686;&#24378;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Real-World Complex Network Representations with Hyperedge Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22270;&#22686;&#24378;&#26041;&#27861;&#22312;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#25200;&#21160;&#22270;&#32467;&#26500;&#65292;&#36890;&#24120;&#38480;&#20110;&#25104;&#23545;&#33410;&#28857;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#20110;&#24418;&#25104;&#39640;&#38454;&#36793;&#30340;&#25968;&#25454;&#65292;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#20027;&#35201;&#34987;&#24314;&#27169;&#20026;&#31616;&#21333;&#22270;&#12290;&#22240;&#27492;&#65292;&#23558;&#39640;&#38454;&#36793;&#37325;&#26032;&#37197;&#32622;&#20026;&#22270;&#22686;&#24378;&#31574;&#30053;&#30340;&#19968;&#37096;&#20998;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36335;&#24452;&#65292;&#21487;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36793;&#22686;&#24378;&#65288;HyperAug&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#26500;&#24314;&#34394;&#25311;&#36229;&#36793;&#65292;&#24182;&#20135;&#29983;&#36741;&#21161;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 Announce Type: new  Abstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary nod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11196</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20445;&#25345;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Adversarial Robustness in Continuous Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#38450;&#24481;&#31639;&#27861;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#28436;&#21270;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24456;&#23481;&#26131;&#34987;&#25273;&#21435;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#20197;&#36890;&#36807;&#22521;&#20859;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#26469;&#35299;&#20915;&#65292;&#31216;&#20026;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#23427;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20851;&#27880;&#21069;&#26399;&#20219;&#21153;&#30340;(&#20998;&#31867;)&#24615;&#33021;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26799;&#24230;&#27491;&#20132;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#19978; -- &#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24179;&#28369;&#26679;&#26412;&#26799;&#24230;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#36755;&#20986;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#23545;&#24378;&#23545;&#25239;&#24615;&#30340;&#25345;&#32493;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;</title><link>https://arxiv.org/abs/2402.07867</link><description>&lt;p&gt;
PoisonedRAG: &#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20063;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#32570;&#20047;&#26368;&#26032;&#30340;&#30693;&#35782;&#21644;&#34394;&#26500;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;RAG&#20174;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#36755;&#20837;&#12290;&#20363;&#22914;&#65292;&#24403;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;&#20174;&#32500;&#22522;&#30334;&#31185;&#25910;&#38598;&#30340;&#25968;&#30334;&#19975;&#20010;&#25991;&#26412;&#26102;&#65292;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#21487;&#20197;&#26159;&#19982;&#32473;&#23450;&#38382;&#39064;&#22312;&#35821;&#20041;&#19978;&#26368;&#30456;&#20284;&#30340;&#21069;K&#20010;&#25991;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;LLM&#21487;&#20197;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#20316;&#20026;&#19978;&#19979;&#25991;&#20026;&#32473;&#23450;&#38382;&#39064;&#29983;&#25104;&#31572;&#26696;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;RAG&#30340;&#20934;&#30830;&#24615;&#25110;&#25928;&#29575;&#65292;&#32780;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge pois
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.05071</link><description>&lt;p&gt;
&#25193;&#23637;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#28385;&#36275;rho-&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#25110;&#22312;rho-&#24369;Minty&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;MVI&#65289;&#20013;&#23384;&#22312;&#35299;&#30340;&#32422;&#26463;&#65292;L-&#20809;&#28369;&#30340;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#25968;rho&gt;0&#30340;&#36739;&#22823;&#20540;&#23545;&#24212;&#26356;&#39640;&#30340;&#38750;&#20984;&#24615;&#31243;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#31867;&#21253;&#25324;&#20004;&#20010;&#29609;&#23478;&#24378;&#21270;&#23398;&#20064;&#65292;&#20132;&#20114;&#20027;&#23548;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20197;&#21450;&#26576;&#20123;&#32463;&#20856;&#26497;&#23567;&#26497;&#22823;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#21512;&#25104;&#27979;&#35797;&#38382;&#39064;&#12290;&#24050;&#26377;&#29468;&#24819;&#35748;&#20026;&#19968;&#38454;&#26041;&#27861;&#21487;&#23481;&#24525;&#26368;&#22823;rho&#20026;1/L&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#24050;&#20572;&#28382;&#22312;&#26356;&#20005;&#26684;&#30340;&#35201;&#27714;rho&lt;1/2L&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#25110;&#24369;MVI&#26465;&#20214;&#19979;&#65292;rho&lt;1/L&#30340;&#26368;&#20248;&#25110;&#26368;&#20339;&#24050;&#30693;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#31639;&#27861;&#26159;Halpern&#21644;Krasnosel'ski&#301;-Mann (KM)&#36845;&#20195;&#30340;&#38750;&#31934;&#30830;&#21464;&#31181;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;g...
&lt;/p&gt;
&lt;p&gt;
We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\rho$-cohypomonotonicity or admitting a solution to the $\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\rho&gt;0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\rho &lt; \frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\rho &lt; \frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\u{\i}-Mann (KM) iterations. We also provide algorithms and complexity g
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.14209</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Continual Driving Policy Optimization with Closed-Loop Individualized Curricula
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14209
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#22836;&#31561;&#20851;&#27880;&#28857;&#65292;&#26681;&#28304;&#20110;&#38271;&#23614;&#33258;&#28982;&#39550;&#39542;&#20998;&#24067;&#20013;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#32570;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#22522;&#20110;&#22330;&#26223;&#30340;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#29983;&#25104;&#39640;&#39118;&#38505;&#39550;&#39542;&#22330;&#26223;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23545;AV&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#37325;&#22797;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;AV&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20174;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#20854;&#20182;AV&#27169;&#22411;&#25910;&#38598;&#30340;&#24040;&#22823;&#22330;&#26223;&#24211;&#20013;&#28388;&#20986;&#21487;&#20256;&#36882;&#20449;&#24687;&#20197;&#25913;&#36827;&#24403;&#21069;AV&#20173;&#28982;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#29305;&#28857;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#26631;&#20934;&#21270;&#30340;&#23376;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14209v3 Announce Type: replace-cross  Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.11798</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23545;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25913;&#21892;&#26657;&#20934;&#27169;&#22411;&#30340;&#38271;&#26399;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20248;&#21270;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#25968;&#20540;&#23454;&#20363;&#65292;&#21253;&#25324;&#31896;&#24615;Burgers&#26041;&#31243;&#12289;Navier-Stokes&#26041;&#31243;&#21644;Kuramoto-Sivashinsky&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#31354;&#38388;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00105</link><description>&lt;p&gt;
&#28508;&#31354;&#38388;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00105
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26126;&#30830;&#30693;&#36947;&#23545;&#31216;&#32676;&#12290;&#33258;&#21160;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#26088;&#22312;&#25918;&#23485;&#36825;&#20010;&#32422;&#26463;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#22312;&#25628;&#32034;&#31354;&#38388;&#20013;&#20165;&#38480;&#20110;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#26080;&#27861;&#22788;&#29702;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23545;&#31216;&#24615;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#26159;&#39640;&#32500;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#21040;&#28508;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#20854;&#20013;&#23545;&#31216;&#24615;&#21464;&#24471;&#32447;&#24615;&#65292;&#24182;&#21516;&#26102;&#21457;&#29616;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#39640;&#32500;&#35266;&#27979;&#20013;&#30340;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LaLiGAN&#22312;&#25913;&#36827;&#26041;&#31243;&#21457;&#29616;&#26041;&#38754;&#30340;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks require explicit knowledge of the symmetry group. Automatic symmetry discovery methods aim to relax this constraint and learn invariance and equivariance from data. However, existing symmetry discovery methods are limited to linear symmetries in their search space and cannot handle the complexity of symmetries in real-world, often high-dimensional data. We propose a novel generative model, Latent LieGAN (LaLiGAN), which can discover nonlinear symmetries from data. It learns a mapping from data to a latent space where the symmetries become linear and simultaneously discovers symmetries in the latent space. Theoretically, we show that our method can express any nonlinear symmetry under certain conditions. Experimentally, our method can capture the intrinsic symmetry in high-dimensional observations, which results in a well-structured latent space that is useful for other downstream tasks. We demonstrate the use cases for LaLiGAN in improving equation discovery
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#35780;&#20998;&#28388;&#27874;&#22120;&#65288;EnSF&#65289;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#38750;&#32447;&#24615;&#28388;&#27874;&#38382;&#39064;&#26102;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;EnSF&#21033;&#29992;&#35780;&#20998;&#27169;&#22411;&#22312;&#20266;&#26102;&#22495;&#20013;&#25551;&#36848;&#28388;&#27874;&#23494;&#24230;&#30340;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#23384;&#20648;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26679;&#26412;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00983</link><description>&lt;p&gt;
&#29992;&#20110;&#36319;&#36394;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#38598;&#25104;&#35780;&#20998;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems. (arXiv:2309.00983v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00983
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#35780;&#20998;&#28388;&#27874;&#22120;&#65288;EnSF&#65289;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#38750;&#32447;&#24615;&#28388;&#27874;&#38382;&#39064;&#26102;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;EnSF&#21033;&#29992;&#35780;&#20998;&#27169;&#22411;&#22312;&#20266;&#26102;&#22495;&#20013;&#25551;&#36848;&#28388;&#27874;&#23494;&#24230;&#30340;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#23384;&#20648;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26679;&#26412;&#30340;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#35780;&#20998;&#28388;&#27874;&#22120;&#65288;EnSF&#65289;&#26469;&#35299;&#20915;&#39640;&#32500;&#38750;&#32447;&#24615;&#28388;&#27874;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#28388;&#27874;&#26041;&#27861;&#65288;&#22914;&#31890;&#23376;&#28388;&#27874;&#22120;&#25110;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#22312;&#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#24230;&#38750;&#32447;&#24615;&#38382;&#39064;&#26102;&#23384;&#22312;&#20302;&#20934;&#30830;&#24615;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;EnSF&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20266;&#26102;&#22495;&#20013;&#23450;&#20041;&#65292;&#26469;&#25551;&#36848;&#28388;&#27874;&#23494;&#24230;&#30340;&#28436;&#21270;&#65292;&#20174;&#32780;&#25915;&#20811;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;EnSF&#22312;&#35780;&#20998;&#20989;&#25968;&#20013;&#23384;&#20648;&#20102;&#36882;&#24402;&#26356;&#26032;&#30340;&#28388;&#27874;&#23494;&#24230;&#20989;&#25968;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#33945;&#29305;&#21345;&#32599;&#26679;&#26412;&#20013;&#23384;&#20648;&#20449;&#24687;&#65288;&#29992;&#20110;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#12290;&#19982;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35780;&#20998;&#20989;&#25968;&#30340;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35780;&#20998;&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#23567;&#25209;&#37327;&#30340;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#26469;&#30452;&#25509;&#36817;&#20284;&#20219;&#20309;&#20266;&#31354;&#38388;-&#26102;&#38388;&#20301;&#32622;&#22788;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#36275;&#22815;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an ensemble score filter (EnSF) for solving high-dimensional nonlinear filtering problems with superior accuracy. A major drawback of existing filtering methods, e.g., particle filters or ensemble Kalman filters, is the low accuracy in handling high-dimensional and highly nonlinear problems. EnSF attacks this challenge by exploiting the score-based diffusion model, defined in a pseudo-temporal domain, to characterizing the evolution of the filtering density. EnSF stores the information of the recursively updated filtering density function in the score function, in stead of storing the information in a set of finite Monte Carlo samples (used in particle filters and ensemble Kalman filters). Unlike existing diffusion models that train neural networks to approximate the score function, we develop a training-free score estimation that uses mini-batch-based Monte Carlo estimator to directly approximate the score function at any pseudo-spatial-temporal location, which provides suf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#20102;&#31034;&#20363;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.16422</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Neural networks can detect model-free static arbitrage strategies. (arXiv:2306.16422v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26816;&#27979;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#20102;&#31034;&#20363;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29702;&#35770;&#21644;&#25968;&#20540;&#26041;&#27861;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#24066;&#22330;&#23384;&#22312;&#22871;&#21033;&#26426;&#20250;&#26102;&#26816;&#27979;&#20986;&#26080;&#27169;&#22411;&#38745;&#24577;&#22871;&#21033;&#26426;&#20250;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20132;&#26131;&#35777;&#21048;&#25968;&#37327;&#36739;&#22810;&#30340;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#30830;&#20445;&#30456;&#24212;&#20132;&#26131;&#31574;&#30053;&#30340;&#20960;&#20046;&#21363;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26131;&#22788;&#29702;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#30340;&#31034;&#20363;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;&#19968;&#31867;&#20984;&#21322;&#26080;&#38480;&#35268;&#21010;&#38382;&#39064;&#65292;&#36825;&#26159;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we demonstrate both theoretically as well as numerically that neural networks can detect model-free static arbitrage opportunities whenever the market admits some. Due to the use of neural networks, our method can be applied to financial markets with a high number of traded securities and ensures almost immediate execution of the corresponding trading strategies. To demonstrate its tractability, effectiveness, and robustness we provide examples using real financial data. From a technical point of view, we prove that a single neural network can approximately solve a class of convex semi-infinite programs, which is the key result in order to derive our theoretical results that neural networks can detect model-free static arbitrage strategies whenever the financial market admits such opportunities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01094</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30001;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#30340;&#24773;&#24418;&#19979;&#39044;&#27979;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#23454;&#29616;&#24335;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#20854;&#20551;&#35774;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23454;&#29616;&#24335;&#39044;&#27979;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#20984;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20984;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.11140</link><description>&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#38543;&#26426;&#22270;&#19978;&#30340;&#36890;&#29992;&#32858;&#21512;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs. (arXiv:2304.11140v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24403;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#38480;&#26102;&#65292;&#35813;&#32593;&#32476;&#27169;&#22411;&#33021;&#25910;&#25947;&#20110;&#20854;&#36830;&#32493;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#35813;&#25910;&#25947;&#24615;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#20540;&#24418;&#24335;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#25193;&#23637;&#21040;&#21253;&#21547;&#25152;&#26377;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#31867;&#32858;&#21512;&#20989;&#25968;&#19978;&#65292;&#20363;&#22914;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#26368;&#22823;&#21367;&#31215;&#30340;&#32593;&#32476;&#12290;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#39640;&#27010;&#29575;&#30340;&#38750;&#28176;&#36827;&#19978;&#38480;&#26469;&#37327;&#21270;&#36825;&#31181;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;McDiarmid&#19981;&#31561;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#29305;&#21035;&#22788;&#29702;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#38750;&#24120;&#19981;&#21516;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#24182;&#20135;&#29983;&#20102;&#23450;&#24615;&#19981;&#21516;&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence of message passing graph neural networks on random graph models to their continuous counterpart as the number of nodes tends to infinity. Until now, this convergence was only known for architectures with aggregation functions in the form of degree-normalized means. We extend such results to a very large class of aggregation functions, that encompasses all classically used message passing graph neural networks, such as attention-based mesage passing or max convolutional message passing on top of (degree-normalized) convolutional message passing. Under mild assumptions, we give non asymptotic bounds with high probability to quantify this convergence. Our main result is based on the McDiarmid inequality. Interestingly, we treat the case where the aggregation is a coordinate-wise maximum separately, at it necessitates a very different proof technique and yields a qualitatively different convergence rate.
&lt;/p&gt;</description></item></channel></rss>