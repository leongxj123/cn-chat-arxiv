<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.15734</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#32676;&#20449;&#24687;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Space Group Informed Transformer for Crystalline Materials Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15734
&lt;/p&gt;
&lt;p&gt;
CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;CrystalFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#12290;&#31354;&#38388;&#32676;&#23545;&#31216;&#24615;&#26174;&#33879;&#31616;&#21270;&#20102;&#26230;&#20307;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;Wyckoff&#20301;&#32622;&#30340;&#26174;&#33879;&#31163;&#25955;&#21644;&#39034;&#24207;&#29305;&#24615;&#65292;CrystalFormer&#23398;&#20250;&#20102;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CrystalFormer&#22312;&#29983;&#25104;&#30340;&#26230;&#20307;&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#19982;&#26631;&#20934;&#22522;&#20934;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;CrystalFormer&#20174;&#25968;&#25454;&#20013;&#21560;&#25910;&#20102;&#21512;&#29702;&#30340;&#22266;&#20307;&#21270;&#23398;&#20449;&#24687;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#12290;CrystalFormer&#32479;&#19968;&#20102;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#32467;&#26500;&#25628;&#32034;&#21644;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15734v1 Announce Type: cross  Abstract: We introduce CrystalFormer, a transformer-based autoregressive model specifically designed for space group-controlled generation of crystalline materials. The space group symmetry significantly simplifies the crystal space, which is crucial for data and compute efficient generative modeling of crystalline materials. Leveraging the prominent discrete and sequential nature of the Wyckoff positions, CrystalFormer learns to generate crystals by directly predicting the species and locations of symmetry-inequivalent atoms in the unit cell. Our results demonstrate that CrystalFormer matches state-of-the-art performance on standard benchmarks for both validity, novelty, and stability of the generated crystalline materials. Our analysis also shows that CrystalFormer ingests sensible solid-state chemistry information from data for generative modeling. The CrystalFormer unifies symmetry-based structure search and generative pre-training in the re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.15455</link><description>&lt;p&gt;
&#25913;&#36827;&#25991;&#26412;&#27969;&#20013;&#29992;&#20110;&#24494;&#35843;SentenceBERT&#30340;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31934;&#32454;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25991;&#26412;&#25968;&#25454;&#30340;&#28608;&#22686;&#20026;&#26426;&#26500;&#21644;&#20844;&#21496;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#30417;&#27979;&#20844;&#20247;&#23545;&#20854;&#26381;&#21153;&#21644;&#20135;&#21697;&#30340;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#24555;&#36895;&#29983;&#25104;&#65292;&#22788;&#29702;&#20381;&#27425;&#21040;&#36798;&#12289;&#28508;&#22312;&#26080;&#38480;&#30340;&#25991;&#26412;&#27969;&#30340;&#25991;&#26412;&#27969;&#25366;&#25496;&#35774;&#32622;&#36890;&#24120;&#27604;&#20256;&#32479;&#30340;&#25209;&#37327;&#23398;&#20064;&#26356;&#21512;&#36866;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22240;&#20854;&#22312;&#27969;&#24335;&#20869;&#23481;&#20013;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#65288;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#29616;&#35937;&#65289;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#19971;&#31181;&#25991;&#26412;&#37319;&#26679;&#26041;&#27861;&#23545;&#31934;&#24515;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#20934;&#30830;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#30340;SBERT&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ
&lt;/p&gt;</description></item><item><title>PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.13108</link><description>&lt;p&gt;
&#20998;&#26512;&#37096;&#20998;&#20849;&#20139;&#23545;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13108
&lt;/p&gt;
&lt;p&gt;
PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23457;&#26597;&#20102;&#37096;&#20998;&#20849;&#20139;&#30340;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PSO-Fed&#65289;&#31639;&#27861;&#23545;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290; PSO-Fed&#36890;&#36807;&#20351;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#26356;&#26032;&#36718;&#27425;&#20165;&#19982;&#26381;&#21153;&#22120;&#20132;&#25442;&#37096;&#20998;&#27169;&#22411;&#20272;&#35745;&#26469;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#12290;&#27169;&#22411;&#20272;&#35745;&#30340;&#37096;&#20998;&#20849;&#20139;&#36824;&#22686;&#24378;&#20102;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24378;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PSO-Fed&#31639;&#27861;&#22312;&#23384;&#22312;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#22312;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20043;&#21069;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#36731;&#24494;&#31713;&#25913;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PSO-Fed&#22312;&#22343;&#20540;&#21644;&#22343;&#26041;&#24847;&#20041;&#19978;&#37117;&#33021;&#20445;&#25345;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#21387;&#21147;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;PSO-Fed&#30340;&#29702;&#35770;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#23558;&#20854;&#19982;&#27493;&#38271;&#12289;&#25915;&#20987;&#27010;&#29575;&#12289;&#25968;&#23383;&#31561;&#21508;&#31181;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13108v1 Announce Type: new  Abstract: We scrutinize the resilience of the partial-sharing online federated learning (PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the communication load by enabling clients to exchange only a fraction of their model estimates with the server at each update round. Partial sharing of model estimates also enhances the robustness of the algorithm against model-poisoning attacks. To gain better insights into this phenomenon, we analyze the performance of the PSO-Fed algorithm in the presence of Byzantine clients, malicious actors who may subtly tamper with their local models by adding noise before sharing them with the server. Through our analysis, we demonstrate that PSO-Fed maintains convergence in both mean and mean-square senses, even under the strain of model-poisoning attacks. We further derive the theoretical mean square error (MSE) of PSO-Fed, linking it to various parameters such as stepsize, attack probability, numb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.12641</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Contrastive Learning Strategy Search for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#20027;&#35201;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#25163;&#21160;&#26500;&#24314;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;CLS&#65289;&#20197;&#24212;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#24320;&#21457;CLS&#36890;&#24120;&#38656;&#35201;&#23545;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#26377;&#36807;&#22810;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23545;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#30340;&#19987;&#19994;&#35748;&#30693;&#65292;&#20197;&#21450;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#22823;&#37327;&#23454;&#39564;&#26469;&#30830;&#23450;&#35814;&#32454;&#30340;&#23398;&#20064;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#23454;&#36341;&#65292;&#35813;&#23454;&#36341;&#33258;&#21160;&#23398;&#20064;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#21363;&#33258;&#21160;&#21270;&#23545;&#27604;&#23398;&#20064;&#65288;AutoCL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12641v1 Announce Type: new  Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive 
&lt;/p&gt;</description></item><item><title>CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11960</link><description>&lt;p&gt;
CASPER&#65306;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11960
&lt;/p&gt;
&lt;p&gt;
CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#26159;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#24433;&#21709;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#36890;&#36807;&#25918;&#32622;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#25910;&#38598;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#21508;&#31181;&#25925;&#38556;&#32780;&#23548;&#33268;&#30340;&#32570;&#22833;&#20540;&#65292;&#36825;&#23545;&#25968;&#25454;&#20998;&#26512;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#22312;&#24674;&#22797;&#29305;&#23450;&#25968;&#25454;&#28857;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#32771;&#34385;&#19982;&#35813;&#28857;&#30456;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#19968;&#20123;&#26410;&#30693;&#28151;&#26434;&#22240;&#32032;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#26500;&#24314;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#38750;&#22240;&#26524;&#24555;&#25463;&#36793;&#12290;&#36825;&#20123;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24320;&#36767;&#21453;&#21521;&#36335;&#24452;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#24314;&#31435;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
&lt;/p&gt;</description></item><item><title>&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09066</link><description>&lt;p&gt;
Continual Learning&#20013;&#30340;&#36229;&#21442;&#25968;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Continual Learning: a Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09066
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#26088;&#22312;&#22312;CL&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32531;&#35299;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35843;&#25972;&#27599;&#31181;&#31639;&#27861;&#30340;&#36866;&#24403;&#36229;&#21442;&#25968;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20027;&#24352;&#29616;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#26082;&#19981;&#20999;&#23454;&#38469;&#65292;&#20063;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.04720</link><description>&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#24322;&#36136;&#24615;&#34920;&#26684;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#20803;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20803;&#29305;&#24449;&#65292;&#20363;&#22914;&#65292;&#32479;&#35745;&#37327;&#25110;&#26631;&#24535;&#28857;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22914;Dataset2Vec&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;liltab&#21253;&#20013;&#65292;&#35813;&#21253;&#21487;&#22312;GitHub&#19978;&#25214;&#21040;https://github.com/azoz01/liltab&#12290;&#25105;&#20204;&#30340;&#21253;&#22522;&#20110;[Iwata and Kumagai, 2020]&#25552;&#20986;&#30340;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#34920;&#26684;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#22914;Dataset2Vec &#30340;&#32534;&#30721;&#29305;&#24449;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#35780;&#20215;&#20102;Dataset2Vec&#21644;liltab
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#26469;&#35299;&#20915;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#22312;&#31283;&#23450;&#32852;&#21512;&#20248;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;IP-CAEs&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.00563</link><description>&lt;p&gt;
&#38388;&#25509;&#21442;&#25968;&#21270;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Indirectly Parameterized Concrete Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#26469;&#35299;&#20915;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#22312;&#31283;&#23450;&#32852;&#21512;&#20248;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;IP-CAEs&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#22312;&#25968;&#25454;&#39640;&#32500;&#25110;&#33719;&#21462;&#23436;&#25972;&#29305;&#24449;&#38598;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30340;&#21457;&#23637;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#34987;&#35748;&#20026;&#26159;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#21487;&#33021;&#38590;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#35757;&#32451;&#26102;&#38388;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#21457;&#29616;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#19982;CAE&#23398;&#20064;&#37325;&#22797;&#36873;&#25321;&#26377;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25913;&#36827;&#65306;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#12290;IP-CAEs&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#21644;&#20174;&#23427;&#21040;Gumbel-Softmax&#20998;&#24067;&#21442;&#25968;&#30340;&#26144;&#23556;&#12290;&#23613;&#31649;&#23454;&#29616;&#31616;&#21333;&#65292;IP-CAE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26500;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#26080;&#35770;&#26159;&#22312;&#27867;&#21270;&#36824;&#26159;&#35757;&#32451;&#26102;&#38388;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi
&lt;/p&gt;</description></item><item><title>CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.19105</link><description>&lt;p&gt;
CollaFuse&#65306;&#22312;&#21327;&#20316;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#23548;&#33322;&#26377;&#38480;&#36164;&#28304;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19105
&lt;/p&gt;
&lt;p&gt;
CollaFuse&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#25193;&#25955;&#24335;&#27169;&#22411;&#22312;&#25968;&#25454;&#38656;&#27714;&#21644;&#38544;&#31169;&#26041;&#38754;&#32473;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#24102;&#26469;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#32852;&#37030;&#23398;&#20064;&#20998;&#21457;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#20250;&#32473;&#20010;&#21035;&#23458;&#25143;&#24102;&#26469;&#21387;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CollaFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#25286;&#20998;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#26377;&#25928;&#21327;&#20316;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;CollaFuse&#23454;&#29616;&#20102;&#20849;&#20139;&#26381;&#21153;&#22120;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36825;&#36890;&#36807;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#26412;&#22320;&#20445;&#30041;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;GPU&#36827;&#31243;&#65292;&#21516;&#26102;&#23558;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#36827;&#31243;&#22806;&#21253;&#32473;&#20849;&#20139;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#23637;&#31034;&#65292;CollaFuse&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#23545;&#25935;&#24863;&#20449;&#24687;&#20849;&#20139;&#30340;&#38656;&#27714;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19105v1 Announce Type: cross  Abstract: In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabiliti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>GNN&#22312;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#22833;&#36133;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31283;&#20581;GNN&#12290;</title><link>https://arxiv.org/abs/2402.11494</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#29616;&#22270;&#24418;&#30340;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Out-of-Distribution Generalization via Causal Intervention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11494
&lt;/p&gt;
&lt;p&gt;
GNN&#22312;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#22833;&#36133;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31283;&#20581;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20174;&#33258;&#19979;&#32780;&#19978;&#30340;&#25968;&#25454;&#29983;&#25104;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;GNN&#22312;OOD&#27867;&#21270;&#20013;&#22833;&#36133;&#30340;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#12290;&#21518;&#32773;&#35823;&#23548;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#22270;&#29305;&#24449;&#21644;&#30446;&#26631;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#29615;&#22659;&#25935;&#24863;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#26032;&#30340;&#26410;&#35265;&#33410;&#28857;&#19978;&#19981;&#33391;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#33410;&#28857;&#32423;&#21035;&#20998;&#24067;&#36716;&#31227;&#19979;&#35757;&#32451;&#31283;&#20581;GNN&#30340;&#27010;&#24565;&#31616;&#21333;&#32780;&#21448;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11494v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11124</link><description>&lt;p&gt;
&#36890;&#36807;&#24320;&#20851;&#21464;&#37327;&#22312;&#38544;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#35299;&#24320;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in Implicit Causal Models via Switch Variable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#65292;&#22312;&#27809;&#26377;&#24050;&#30693;&#30340;&#22320;&#38754;&#30495;&#23454;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#38544;&#24335;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;&#36890;&#24120;&#28041;&#21450;&#20004;&#31867;&#24178;&#39044;&#25968;&#25454;&#65306;&#30828;&#24178;&#39044;&#21644;&#36719;&#24178;&#39044;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36719;&#24178;&#39044;&#36890;&#24120;&#27604;&#30828;&#24178;&#39044;&#26356;&#29616;&#23454;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#23436;&#20840;&#21463;&#25511;&#30340;&#29615;&#22659;&#12290;&#19982;&#30452;&#25509;&#24378;&#21046;&#25913;&#21464;&#22240;&#26524;&#21464;&#37327;&#30340;&#30828;&#24178;&#39044;&#19981;&#21516;&#65292;&#36719;&#24178;&#39044;&#36890;&#36807;&#24433;&#21709;&#22240;&#26524;&#26426;&#21046;&#38388;&#25509;&#22320;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#20013;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26088;&#22312;&#22312;&#19981;&#21516;&#22240;&#26524;&#26426;&#21046;&#20043;&#38388;&#20999;&#25442;&#30340;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#26469;&#24314;&#27169;&#36719;&#24178;&#39044;&#25928;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11124v1 Announce Type: new  Abstract: Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistentl
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.13913</link><description>&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#65288;D2C&#65289;&#36890;&#24120;&#36890;&#36807;Wasserstein&#36136;&#24515;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#32858;&#31867;&#21487;&#20197;&#36890;&#36807;&#36136;&#24515;&#24456;&#22909;&#22320;&#34920;&#31034;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;Wasserstein&#36317;&#31163;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;D2C&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#22320;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20445;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#32858;&#31867;&#20998;&#24067;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#29983;&#38271;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#29702;&#35770;&#21644;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#20026;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#30340;&#27835;&#30103;&#35774;&#35745;&#25552;&#20379;&#20102;&#20851;&#38190;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2311.16536</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#28024;&#28070;:&#25968;&#23398;&#27169;&#22411;&#12289;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#27169;&#24577;&#25195;&#25551;
&lt;/p&gt;
&lt;p&gt;
Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans. (arXiv:2311.16536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#29983;&#38271;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#29702;&#35770;&#21644;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#20026;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#30340;&#27835;&#30103;&#35774;&#35745;&#25552;&#20379;&#20102;&#20851;&#38190;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#65288;GBM&#65289;&#20174;&#21307;&#23398;MRI&#25195;&#25551;&#20013;&#30340;&#28024;&#28070;&#23545;&#20110;&#29702;&#35299;&#32959;&#30244;&#29983;&#38271;&#21160;&#21147;&#23398;&#21644;&#35774;&#35745;&#20010;&#20307;&#21270;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;GBM&#29983;&#38271;&#30340;&#25968;&#23398;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#32959;&#30244;&#32454;&#32990;&#30340;&#31354;&#38388;&#20998;&#24067;&#20013;&#34917;&#20805;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#65292;&#30001;&#20110;&#26102;&#38388;&#25968;&#25454;&#26377;&#38480;&#19988;&#25104;&#20687;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#26102;&#38388;&#26377;&#38480;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21453;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#21333;&#20010;&#19977;&#32500;&#32467;&#26500;MRI&#24555;&#29031;&#20013;&#20272;&#35745;GBM&#29983;&#38271;&#21453;&#24212;&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#24322;&#24615;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;PINNs&#23558;&#25968;&#25454;&#21644;PDE&#23884;&#20837;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#25972;&#21512;&#20102;&#29702;&#35770;&#21644;&#25968;&#25454;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#29305;&#24449;&#26080;&#37327;&#32434;&#21442;&#25968;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#65292;&#21033;&#29992;&#26080;&#37327;&#32434;&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#20197;&#21450;&#24494;&#35843;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is crucial for understanding tumor growth dynamics and designing personalized radiotherapy treatment plans.Mathematical models of GBM growth can complement the data in the prediction of spatial distributions of tumor cells. However, this requires estimating patient-specific parameters of the model from clinical data, which is a challenging inverse problem due to limited temporal data and the limited time between imaging and diagnosis. This work proposes a method that uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific parameters of a reaction-diffusion PDE model of GBM growth from a single 3D structural MRI snapshot. PINNs embed both the data and the PDE into a loss function, thus integrating theory and data. Key innovations include the identification and estimation of characteristic non-dimensional parameters, a pre-training step that utilizes the non-dimensional parameters and a fine-tunin
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01205</link><description>&lt;p&gt;
&#20351;&#29992;&#20301;&#21453;&#36716;&#25915;&#20987;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;Weisfeiler&#21644;Lehman&#21464;&#24471;&#20919;&#28448;&#20102;
&lt;/p&gt;
&lt;p&gt;
Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25915;&#20987;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#27602;&#21270;&#21644;&#35268;&#36991;&#19978;&#65292;&#24573;&#30053;&#20102;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#22914;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#65288;Injectivity Bit Flip Attack&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#38024;&#23545;&#37327;&#21270;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#23398;&#20064;&#37051;&#22495;&#32858;&#21512;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#20854;&#21306;&#20998;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;Weisfeiler-Lehman&#27979;&#35797;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#26576;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25968;&#23398;&#23646;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#21487;&#20197;&#23558;&#21508;&#31181;&#22270;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#22823;&#34920;&#36798;&#24615;&#21516;&#26500;&#32593;&#32476;&#38477;&#32423;&#20026;&#38543;&#26426;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to rando
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00201</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#33258;&#24049;&#30340;&#31169;&#26377;&#22870;&#21169;&#20989;&#25968;&#23545;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#20849;&#20139;&#29615;&#22659;&#30340;&#30456;&#21516;&#36716;&#31227;&#26680;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#27493;&#26631;&#35760;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20986;&#19968;&#31181;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#65292;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#25240;&#25187;&#24635;&#22870;&#21169;&#20043;&#21644;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#20165;&#19982;&#20854;&#22312;&#32473;&#23450;&#22270;&#25299;&#25169;&#20013;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312; softmax &#21442;&#25968;&#21270;&#19979;&#24320;&#23637;&#20102;&#32852;&#37030;&#32431;&#31929;&#21644;&#29109;&#27491;&#21017;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#26799;&#24230;&#36319;&#36394;&#24212;&#29992;&#20110;&#20840;&#23616; Q &#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#19979;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#20960;&#20046;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly indep
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.15290</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#21487;&#38752;&#22320;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#20016;&#23500;&#30340;&#24739;&#32773;&#32423;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#26816;&#39564;&#12289;&#33647;&#29289;&#21644;&#35786;&#26029;&#65292;&#20026;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#24120;&#24120;&#38480;&#21046;&#20102;&#23545;EHR&#30340;&#35775;&#38382;&#65292;&#38459;&#30861;&#20102;&#19979;&#28216;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;EHR&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#19971;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#26469;&#22686;&#24378;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12842</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#26080;&#20851;&#21464;&#37327;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic variable importance for predictive uncertainty: an entropy-based approach. (arXiv:2310.12842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30456;&#20449;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24517;&#39035;&#29702;&#35299;&#23548;&#33268;&#36825;&#20123;&#39044;&#27979;&#30340;&#22240;&#32032;&#12290;&#23545;&#20110;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#19981;&#20165;&#38656;&#35201;&#29702;&#35299;&#39044;&#27979;&#26412;&#36523;&#30340;&#21407;&#22240;&#65292;&#36824;&#35201;&#29702;&#35299;&#27169;&#22411;&#23545;&#36825;&#20123;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#25193;&#23637;&#21040;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#65292;&#24182;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25193;&#23637;&#26469;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#25913;&#32534;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#34913;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#35813;&#20998;&#24067;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the model's level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model's predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>S-GBDT&#26159;&#19968;&#31181;&#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#23398;&#20064;&#22120;&#65292;&#21033;&#29992;&#20102;&#22235;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21253;&#25324;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#26356;&#32039;&#23494;&#35745;&#31639;&#21644;&#25972;&#21512;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#20197;&#23398;&#20064;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.12041</link><description>&lt;p&gt;
S-GBDT: &#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees. (arXiv:2309.12041v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12041
&lt;/p&gt;
&lt;p&gt;
S-GBDT&#26159;&#19968;&#31181;&#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#23398;&#20064;&#22120;&#65292;&#21033;&#29992;&#20102;&#22235;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21253;&#25324;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#26356;&#32039;&#23494;&#35745;&#31639;&#21644;&#25972;&#21512;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#20197;&#23398;&#20064;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#22312;&#34920;&#26684;&#25968;&#25454;(&#22914;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#25110;&#21307;&#30103;&#20803;&#25968;&#25454;)&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#28508;&#21147;&#65306;&#32463;&#20856;&#30340;GBDT&#23398;&#20064;&#22120;&#21487;&#20197;&#20174;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#21487;&#35777;&#26126;&#20855;&#26377;&#38544;&#31169;&#24615;&#36136;&#30340;&#24403;&#21069;&#26041;&#27861;&#26159;&#24046;&#20998;&#38544;&#31169;&#65292;&#35813;&#26041;&#27861;&#35201;&#27714;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#26377;&#38480;&#19988;&#21487;&#21542;&#35748;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GBDT&#23398;&#20064;&#22120;&#65292;&#24182;&#21033;&#29992;&#22235;&#31181;&#20027;&#35201;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;(1)&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22122;&#22768;&#32553;&#25918;&#26041;&#27861;&#65292;&#26356;&#32039;&#23494;&#22320;&#35745;&#31639;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#20915;&#31574;&#26641;&#21494;&#23376;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#20174;&#32780;&#23548;&#33268;&#22122;&#22768;&#30340;&#26399;&#26395;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;n&#30340;&#27604;&#20363;&#20026;$O(1/n)$&#65292;&#20854;&#20013;n&#20026;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;(2)&#25105;&#20204;&#23558;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#20197;&#20174;&#22312;&#36845;&#20195;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#20013;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#20110;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving learning of gradient boosting decision trees (GBDT) has the potential for strong utility-privacy tradeoffs for tabular data, such as census data or medical meta data: classical GBDT learners can extract non-linear patterns from small sized datasets. The state-of-the-art notion for provable privacy-properties is differential privacy, which requires that the impact of single data points is limited and deniable. We introduce a novel differentially private GBDT learner and utilize four main techniques to improve the utility-privacy tradeoff. (1) We use an improved noise scaling approach with tighter accounting of privacy leakage of a decision tree leaf compared to prior work, resulting in noise that in expectation scales with $O(1/n)$, for $n$ data points. (2) We integrate individual R\'enyi filters to our method to learn from data points that have been underutilized during an iterative training process, which -- potentially of independent interest -- results in a natura
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#36827;&#34892;&#35745;&#31639;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.12157</link><description>&lt;p&gt;
&#22312;&#22810;&#23618;&#32423;&#29615;&#22659;&#20013;&#35782;&#21035;&#20379;&#24212;&#38142;&#32467;&#26524;&#30340;&#36129;&#29486;&#32773;&#65306;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach. (arXiv:2307.12157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12157
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#36827;&#34892;&#35745;&#31639;&#65292;&#26080;&#38656;&#25968;&#25454;&#20849;&#20139;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#32463;&#24120;&#38590;&#20197;&#30830;&#23450;&#20135;&#21697;&#36136;&#37327;&#21644;&#20132;&#36135;&#26102;&#38388;&#31561;&#25351;&#26631;&#21464;&#21270;&#30340;&#21407;&#22240;&#12290;&#24403;&#21407;&#22240;&#20301;&#20110;&#20844;&#21496;&#36793;&#30028;&#20043;&#22806;&#65292;&#22788;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#23618;&#32423;&#20379;&#24212;&#38142;&#20013;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#20379;&#24212;&#38142;&#31649;&#29702;&#20027;&#24352;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#33719;&#24471;&#26356;&#22909;&#30340;&#27934;&#23519;&#21147;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#23454;&#29616;&#23545;&#22810;&#38454;&#27573;&#29983;&#20135;&#36807;&#31243;&#20013;&#24863;&#20852;&#36259;&#25351;&#26631;&#30340;&#20272;&#35745;&#36129;&#29486;&#30340;&#21435;&#20013;&#24515;&#21270;&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#35828;&#26381;&#20379;&#24212;&#38142;&#21442;&#19982;&#32773;&#20849;&#20139;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#25152;&#26377;&#35745;&#31639;&#37117;&#26159;&#20197;&#21435;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#38454;&#27573;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#36827;&#34892;&#32463;&#39564;&#35777;&#23454;&#30340;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20351;&#29992;&#38598;&#20013;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#36136;&#37327;&#21464;&#21270;&#30340;&#21407;&#22240;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05319</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#24211;&#19978;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;LM&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#24182;&#32463;&#24120;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#26412;&#30740;&#31350;&#27839;&#30528;&#20004;&#20010;&#37325;&#35201;&#26041;&#21521;&#30740;&#31350;&#20102;LM&#30340;&#23433;&#20840;&#24615;:(i)&#23433;&#20840;&#21152;&#22266;&#65292;&#26088;&#22312;&#22686;&#24378;LM&#22312;&#29983;&#25104;&#23433;&#20840;&#20195;&#30721;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;;(ii)&#23545;&#25239;&#27979;&#35797;&#65292;&#26088;&#22312;&#22312;&#23545;&#25239;&#24615;&#31435;&#22330;&#35780;&#20272;LM&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#39033;&#31216;&#20026;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#23433;&#20840;&#20219;&#21153;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#23558;&#19968;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25351;&#23548;LM&#29983;&#25104;&#23433;&#20840;&#25110;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#30041;LM&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SVEN&#30340;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;SVEN&#21033;&#29992;&#23646;&#24615;&#29305;&#23450;&#30340;&#36830;&#32493;&#21521;&#37327;&#26469;&#24341;&#23548;&#31243;&#24207;&#29983;&#25104;&#36798;&#21040;&#32473;&#23450;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#20462;&#25913;LM&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#25237;&#24433;&#25439;&#22833;&#26469;&#20248;&#21270;&#36825;&#20123;&#36830;&#32493;&#21521;&#37327;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SVEN&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#24403;&#21069;&#30340;LM&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22312;&#27979;&#35797;&#26102;&#20462;&#25913;&#23427;&#20204;&#30340;&#36755;&#20837;&#32780;&#20445;&#30041;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#38656;&#35201;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#65292;&#24182;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2204.08335</link><description>&lt;p&gt;
&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#39640;&#26031;&#36807;&#31243;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning with Weak Supervision for Gaussian Processes. (arXiv:2204.08335v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#65292;&#24182;&#22312;&#39640;&#26031;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#25104;&#26412;&#12290;&#24403;&#27880;&#37322;&#39044;&#31639;&#26377;&#38480;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#36873;&#25321;&#21644;&#27880;&#37322;&#37027;&#20123;&#21487;&#33021;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#33719;&#24471;&#26368;&#22823;&#25910;&#30410;&#30340;&#35266;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#38500;&#20102;&#36873;&#25321;&#35201;&#27880;&#37322;&#30340;&#35266;&#27979;&#32467;&#26524;&#22806;&#65292;&#36824;&#36873;&#25321;&#35201;&#33719;&#24471;&#30340;&#27880;&#37322;&#31934;&#24230;&#12290;&#20551;&#23450;&#20855;&#26377;&#20302;&#31934;&#24230;&#30340;&#27880;&#37322;&#27604;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#27880;&#37322;&#26356;&#20415;&#23452;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#30340;&#26356;&#22823;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#25552;&#20986;&#30340;BALD&#30446;&#26631;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#33719;&#21462;&#20989;&#25968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#33021;&#22815;&#35843;&#25972;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#30340;&#27880;&#37322;&#31934;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating data for supervised learning can be costly. When the annotation budget is limited, active learning can be used to select and annotate those observations that are likely to give the most gain in model performance. We propose an active learning algorithm that, in addition to selecting which observation to annotate, selects the precision of the annotation that is acquired. Assuming that annotations with low precision are cheaper to obtain, this allows the model to explore a larger part of the input space, with the same annotation budget. We build our acquisition function on the previously proposed BALD objective for Gaussian Processes, and empirically demonstrate the gains of being able to adjust the annotation precision in the active learning loop.
&lt;/p&gt;</description></item></channel></rss>