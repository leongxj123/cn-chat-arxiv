<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18222</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#24456;&#22823;&#24076;&#26395;&#65307;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#27867;&#21270;&#21040;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26657;&#20934;&#30340;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20505;&#36873;&#21160;&#20316;&#30340;&#26412;&#22320;&#20449;&#24687;&#26469;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#19977;&#20010;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#28508;&#21147;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#38468;&#24102;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18222v1 Announce Type: cross  Abstract: Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;</description></item><item><title>&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15312</link><description>&lt;p&gt;
&#27700;&#26031;&#22374;&#35270;&#35282;&#19979;&#30340;&#26222;&#36890; GANs
&lt;/p&gt;
&lt;p&gt;
A Wasserstein perspective of Vanilla GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15312
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#23454;&#35777;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#29702;&#35770;&#30740;&#31350;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#32479;&#35745;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#27700;&#26031;&#22374;GANs&#21450;&#20854;&#25193;&#23637;&#19978;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#20855;&#26377;&#33391;&#22909;&#30340;&#38477;&#32500;&#29305;&#24615;&#12290;&#23545;&#20110;&#26222;&#36890;GANs&#65292;&#21363;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#32479;&#35745;&#32467;&#26524;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#65292;&#38656;&#35201;&#20551;&#35774;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#21644;&#28508;&#31354;&#38388;&#19982;&#21608;&#22260;&#31354;&#38388;&#30340;&#32500;&#24230;&#30456;&#31561;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#29616;&#26377;&#30340;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;&#26222;&#36890;GANs&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27700;&#26031;&#22374;&#36317;&#31163;&#20013;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;&#36825;&#20010;&#31070;&#35861;&#19981;&#31561;&#24335;&#30340;&#20551;&#35774;&#26088;&#22312;&#30001;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#65292;&#22914;&#21069;&#39304;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15312v1 Announce Type: cross  Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.14050</link><description>&lt;p&gt;
&#20351;&#29992;BART&#20174;&#25512;&#25991;&#20013;&#25552;&#21462;&#24773;&#32490;&#30701;&#35821;
&lt;/p&gt;
&lt;p&gt;
Extracting Emotion Phrases from Tweets using BART
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BART&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38382;&#31572;&#26694;&#26550;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#24773;&#32490;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#31572;&#26696;&#36328;&#24230;&#20301;&#32622;&#65292;&#23454;&#29616;&#23545;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#39033;&#26088;&#22312;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#20027;&#35201;&#26159;&#23545;&#25991;&#26412;&#30340;&#25972;&#20307;&#26497;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#20256;&#36798;&#24773;&#32490;&#30340;&#20855;&#20307;&#30701;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#26694;&#26550;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#21452;&#21521;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;BART&#65289;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20174;&#32473;&#23450;&#25991;&#26412;&#20013;&#25552;&#21462;&#25918;&#22823;&#32473;&#23450;&#24773;&#24863;&#26497;&#24615;&#30340;&#30701;&#35821;&#12290;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#30830;&#23450;&#35201;&#25552;&#21462;&#30340;&#29305;&#23450;&#24773;&#32490;&#65292;&#28982;&#21518;&#24341;&#23548;BART&#19987;&#27880;&#20110;&#25991;&#26412;&#20013;&#30456;&#20851;&#30340;&#24773;&#24863;&#32447;&#32034;&#12290;&#25105;&#20204;&#22312;BART&#20013;&#20351;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#25991;&#26412;&#20013;&#31572;&#26696;&#36328;&#24230;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#20301;&#32622;&#65292;&#20174;&#32780;&#24110;&#21161;&#30830;&#23450;&#25552;&#21462;&#30340;&#24773;&#32490;&#30701;&#35821;&#30340;&#31934;&#30830;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14050v2 Announce Type: replace  Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our
&lt;/p&gt;</description></item><item><title>SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12977</link><description>&lt;p&gt;
SportsNGEN: &#25345;&#32493;&#29983;&#25104;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12977
&lt;/p&gt;
&lt;p&gt;
SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;SportsNGEN&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#36816;&#21160;&#21592;&#21644;&#29699;&#36861;&#36394;&#24207;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#25345;&#32493;&#30340;&#28216;&#25103;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#19987;&#19994;&#32593;&#29699;&#36861;&#36394;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;SportsNGEN&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#27169;&#25311;&#19982;&#23556;&#20987;&#20998;&#31867;&#22120;&#21644;&#36923;&#36753;&#30456;&#32467;&#21512;&#26469;&#24320;&#22987;&#21644;&#32467;&#26463;&#29699;&#36187;&#65292;&#31995;&#32479;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#12290;&#27492;&#22806;&#65292;SportsNGEN&#30340;&#36890;&#29992;&#29256;&#26412;&#21487;&#20197;&#36890;&#36807;&#22312;&#21253;&#21547;&#35813;&#29699;&#21592;&#30340;&#27604;&#36187;&#25968;&#25454;&#19978;&#24494;&#35843;&#26469;&#23450;&#21046;&#29305;&#23450;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21453;&#20107;&#23454;&#25110;&#20551;&#35774;&#36873;&#39033;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36136;&#37327;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36275;&#29699;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12977v1 Announce Type: cross  Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10997</link><description>&lt;p&gt;
&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#23618;&#27425;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10997
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29702;&#35299;&#22810;&#23618;&#25277;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330; (N2F2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#30417;&#30563;&#26469;&#23398;&#20064;&#21333;&#20010;&#29305;&#24449;&#22330;&#65292;&#22312;&#21516;&#19968;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#19981;&#21516;&#32500;&#24230;&#32534;&#30721;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#28789;&#27963;&#23450;&#20041;&#23618;&#27425;&#65292;&#21487;&#20197;&#26681;&#25454;&#29289;&#29702;&#32500;&#24230;&#12289;&#35821;&#20041;&#32500;&#24230;&#25110;&#20004;&#32773;&#22343;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22330;&#26223;&#30340;&#20840;&#38754;&#21644;&#32454;&#33268;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;2D&#31867;&#21035;&#26080;&#20851;&#20998;&#21106;&#27169;&#22411;&#22312;&#22270;&#20687;&#31354;&#38388;&#30340;&#20219;&#24847;&#23610;&#24230;&#25552;&#20379;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20687;&#32032;&#20998;&#32452;&#65292;&#24182;&#26597;&#35810;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20026;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#33719;&#24471;&#19982;&#35821;&#35328;&#23545;&#40784;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#30417;&#30563;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#23884;&#22871;&#29305;&#24449;&#22330;&#32500;&#24230;&#20998;&#37197;&#32473;&#25552;&#21462;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26469;&#33258;Reddit&#21644;Twitter&#30340;&#20004;&#20010;&#26032;&#22270;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#22312;&#27492;&#38382;&#39064;&#19978;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.15988</link><description>&lt;p&gt;
&#26397;&#21521;&#20844;&#24179;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65306;&#38382;&#39064;&#12289;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15988
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26469;&#33258;Reddit&#21644;Twitter&#30340;&#20004;&#20010;&#26032;&#22270;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#22312;&#27492;&#38382;&#39064;&#19978;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#26088;&#22312;&#22312;&#36755;&#20837;&#22270;&#20013;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#36991;&#20813;&#38024;&#23545;&#26469;&#33258;&#25935;&#24863;&#20122;&#32452;&#22914;&#24615;&#21035;&#25110;&#25919;&#27835;&#20542;&#21521;&#30340;&#20010;&#20307;&#30340;&#20559;&#35265;&#39044;&#27979;&#12290;&#22270;&#20013;&#30340;&#20844;&#24179;&#24615;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#65292;&#27604;&#22914;&#22312;&#25628;&#32034;/&#25490;&#21517;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#20915;&#31574;&#32467;&#26524;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#20010;&#20154;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#29486;&#27809;&#26377;&#20840;&#38754;&#35752;&#35770;&#36825;&#20010;&#38382;&#39064;&#65292;&#20063;&#27809;&#26377;&#25552;&#20379;&#36148;&#36817;&#23454;&#38469;&#22270;&#32467;&#26500;&#12289;&#24322;&#24120;&#26631;&#31614;&#21644;&#25935;&#24863;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;FairGAD&#30740;&#31350;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FairGAD&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#20110;&#20840;&#29699;&#30693;&#21517;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;Reddit&#21644;Twitter&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15988v1 Announce Type: cross  Abstract: The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14236</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#30340;&#33258;&#21160;&#35774;&#35745;&#19982;&#20248;&#21270;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;(DFC)&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#30005;&#36335;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#30005;&#23376;&#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;DFC&#30340;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#24037;&#31243;&#24072;&#35774;&#35745;&#32463;&#39564;&#30340;&#20381;&#36182;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#30005;&#36335;&#35774;&#35745;&#30456;&#20851;&#30340;&#20027;&#35266;&#24615;&#21644;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19982;&#20256;&#32479;&#30340;&#24037;&#31243;&#24072;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35774;&#35745;&#25928;&#29575;&#21644;&#36136;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#25913;&#21892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35774;&#35745;&#22797;&#26434;&#25110;&#24555;&#36895;&#21457;&#23637;&#30340;DFC&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
&lt;/p&gt;</description></item><item><title>&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;</title><link>https://arxiv.org/abs/2402.13700</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#21644;&#23398;&#20064;&#30340;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
On the Conflict of Robustness and Learning in Collaborative Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13700
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#20801;&#35768;&#21442;&#19982;&#32773;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#12290;&#22312;&#38544;&#31169;&#26159;&#19968;&#20010;&#24378;&#28872;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#20581;&#24247;&#30456;&#20851;&#24212;&#29992;&#20013;&#65292;&#23433;&#20840;&#20063;&#26159;&#39318;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#20445;&#25252;&#38544;&#31169;&#30340;CML&#27969;&#31243;&#24517;&#39035;&#20135;&#29983;&#33021;&#22815;&#36755;&#20986;&#27491;&#30830;&#21487;&#38752;&#20915;&#31574;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#21487;&#33021;&#19981;&#21463;&#20449;&#20219;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20381;&#36182;&#20110;&#24110;&#21161;&#36807;&#28388;&#21487;&#33021;&#21361;&#21450;&#35757;&#32451;&#36807;&#31243;&#30340;&#24694;&#24847;&#36129;&#29486;&#30340;&#24230;&#37327;&#30340;&#8220;&#31283;&#20581;&#32858;&#21512;&#22120;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#35268;&#33539;&#21270;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#35268;&#33539;&#21270;&#33021;&#22815;&#34920;&#26126;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65306;&#26080;&#35770;&#26159;&#23427;&#20204;&#20351;&#29992;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#65307;&#36824;&#26159;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#35774;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Generated Hypotheses by Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#22686;&#24378;&#24615;&#33021;&#21152;&#36895;&#20102;&#20854;&#34701;&#20837;&#31185;&#23398;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21019;&#24314;&#31185;&#23398;&#20551;&#35774;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20551;&#35774;&#36827;&#34892;&#20851;&#38190;&#20915;&#31574;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#26102;&#65292;&#39564;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#37327;&#21270;&#20854;&#21487;&#38752;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#36825;&#19968;&#20107;&#23454;&#26465;&#20214;&#19979;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25511;&#21046;&#38169;&#35823;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2401.17823</link><description>&lt;p&gt;
&#37319;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21457;&#24067;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving data release leveraging optimal transport and particle gradient descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25919;&#24220;&#65289;&#20013;&#38544;&#31169;&#30340;&#34920;&#26684;&#25968;&#25454;&#24046;&#20998;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#65292;&#20174;&#31169;&#26377;&#36793;&#38469;&#20272;&#35745;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PrivPGD&#65292;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#26032;&#19968;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22823;&#33539;&#22260;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#32467;&#21512;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2311.14120</link><description>&lt;p&gt;
(&#28145;)&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21644;&#36870;&#26041;&#24046;&#24179;&#30452;&#20851;&#31995;&#30340;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#21512;&#25104;&#39640;&#26031;&#25968;&#25454;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#36830;&#32493;&#26497;&#38480;&#20869;&#65292;&#30740;&#31350;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#27424;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#65288;&#26411;&#24577;&#65289;&#35757;&#32451;&#35268;&#21017;&#12290;&#23545;&#20110; schwach&#27424;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#21333;&#23618;&#32593;&#32476;&#65292;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#26126;&#26174;&#20559;&#31163;Hessian&#65292;&#21487;&#20197;&#24402;&#22240;&#20110;SGD&#21160;&#24577;&#30340;&#30772;&#22351;&#35814;&#32454;&#24179;&#34913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26435;&#37325;&#27874;&#21160;&#36890;&#24120;&#26159;&#21508;&#21521;&#24322;&#24615;&#30340;&#65292;&#20294;&#21463;&#21508;&#21521;&#21516;&#24615;&#25439;&#22833;&#38480;&#21046;&#12290;&#23545;&#20110;&#21452;&#23618;&#32593;&#32476;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#20851;&#30340;&#31283;&#23450;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23618;&#38388;&#32806;&#21512;&#20316;&#20026;&#26435;&#37325;&#27874;&#21160;&#30340;&#21508;&#21521;&#24322;&#24615;&#30340;&#26032;&#26469;&#28304;&#12290;&#19982;&#21333;&#23618;&#24773;&#20917;&#30456;&#21453;&#65292;&#26435;&#37325;&#27874;&#21160;&#32463;&#21382;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#65292;&#20854;&#24179;&#30452;&#24230;&#19982;&#27874;&#21160;&#30340;&#26041;&#24046;&#25104;&#21453;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14120v2 Announce Type: replace  Abstract: We investigate the stationary (late-time) training regime of single- and two-layer linear underparameterized neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but are subject to an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation varian
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2311.09922</link><description>&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25972;&#25968;&#21015;&#34920;&#20316;&#20026;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#30340;&#38598;&#21512;&#26469;&#23454;&#29616;&#24555;&#36895;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25972;&#25968;&#21015;&#34920;&#34920;&#31034;&#30340;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#20056;&#27861;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;python&#20195;&#30721;&#23454;&#29616;&#20102;&#19968;&#32452;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#26576;&#19968;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#25968;&#35770;&#21464;&#25442;(NTT)&#21644;&#21345;&#25289;&#33576;&#24052;(Karatsuba)&#20056;&#27861;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#29992;python&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#65292;&#19982;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25972;&#25968;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#25972;&#25968;&#25110;&#23454;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#34920;&#31034;&#20108;&#36827;&#21046;&#20013;&#30340;&#26377;&#38480;&#32423;&#25968;&#12290;&#35813;&#25968;&#23383;&#30340;&#25972;&#25968;&#32034;&#24341;&#26377;&#38480;&#32423;&#25968;&#21487;&#20197;&#23384;&#20648;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;CPU / GPU&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#27861;&#21644;&#20056;&#27861;&#36816;&#31639;&#21487;&#20197;&#24212;&#29992;&#20110;&#20316;&#20026;&#32034;&#24341;&#25972;&#25968;&#34920;&#31034;&#30340;&#20004;&#20010;&#34917;&#30721;&#21152;&#27861;&#65292;&#24182;&#21487;&#20197;&#23436;&#20840;&#20998;&#24067;&#22312;&#32473;&#23450;&#30340;CPU / GPU&#26550;&#26500;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23436;&#20840;&#30340;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distribute
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.12944</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning in Mean Field Games: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12944
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21512;&#20316;&#21644;&#21512;&#20316;&#28216;&#25103;&#22312;&#25317;&#26377;&#22823;&#37327;&#29609;&#23478;&#26102;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#22343;&#22330;&#21338;&#24328;(Mean Field Games, MFGs)&#30001;Lasry&#21644;Lions&#20197;&#21450;Huang&#65292;Caines&#21644;Malham\'e&#24341;&#20837;&#65292;&#20381;&#38752;&#22343;&#22330;&#36817;&#20284;&#20801;&#35768;&#29609;&#23478;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#20123;&#28216;&#25103;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35299;&#20915;&#24102;&#26377;&#23545;&#27169;&#22411;&#30340;&#23436;&#20840;&#20102;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#20986;&#29616;&#22312;&#35299;&#20915;&#35268;&#27169;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;RL&#21644;MFGs&#30340;&#32467;&#21512;&#26377;&#26395;&#35299;&#20915;&#22312;&#20154;&#21475;&#35268;&#27169;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#24222;&#22823;&#30340;&#28216;&#25103;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#36805;&#36895;&#22686;&#38271;&#30340;&#20851;&#20110;RL&#26041;&#27861;&#22312;MFGs&#20013;&#23398;&#20064;&#22343;&#34913;&#21644;&#31038;&#20132;&#26368;&#20248;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;M&#20013;&#26368;&#24120;&#35265;&#30340;&#35774;&#32622;(&#38745;&#24577;&#12289;&#31283;&#24577;&#21644;&#36827;&#21270;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.06834</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Tries&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Sequential Pattern Mining with Hybrid Tries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.06834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23545;&#20110;&#33021;&#22815;&#22788;&#29702;&#22914;&#27492;&#24222;&#22823;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#25366;&#25496;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#36843;&#20999;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;SPM&#65289;&#65292;&#36825;&#26159;&#30693;&#35782;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#65292;&#38754;&#20020;&#30528;&#38024;&#23545;&#22823;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#20869;&#23384;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;trie&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#37325;&#22797;&#27169;&#24335;&#32039;&#20945;&#22320;&#23384;&#20648;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#38598;; &#20197;&#21450;&#19968;&#20010;&#30456;&#24212;&#30340;&#25366;&#25496;&#31639;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#27492;&#32039;&#20945;&#34920;&#31034;&#20013;&#25552;&#21462;&#27169;&#24335;&#12290;&#23545;&#30495;&#23454;&#27979;&#35797;&#23454;&#20363;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#23545;&#20110;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20869;&#23384;&#28040;&#32791;&#24179;&#22343;&#25552;&#39640;&#20102;88&#65285;&#65292;&#35745;&#31639;&#26102;&#38388;&#25552;&#39640;&#20102;41&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21807;&#19968;&#19968;&#20010;&#22312;&#31995;&#32479;&#20869;&#23384;&#20026;256GB&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#30340;SPM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09031</link><description>&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65306;&#26102;&#38388;&#27493;&#24341;&#36215;&#30340;&#23545;&#24433;&#21709;&#20272;&#35745;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#34892;&#20026;&#36861;&#28335;&#21040;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20026;&#29702;&#35299;&#8220;&#40657;&#31665;&#8221;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#37327;&#21270;&#32852;&#31995;&#65292;&#20294;&#22312;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#25193;&#25955;&#27169;&#22411;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#38754;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#31995;&#21015;&#26102;&#38388;&#27493;&#39588;&#32780;&#19981;&#26159;&#20043;&#21069;&#30340;&#30636;&#26102;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#25805;&#20316;&#65292;&#23545;&#30452;&#25509;&#23558;&#29616;&#26377;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diffusion-TracIn&#65292;&#23427;&#21253;&#21547;&#20102;&#36825;&#31181;&#26102;&#38388;&#21160;&#21147;&#23398;&#65292;&#24182;&#35266;&#23519;&#21040;&#26679;&#26412;&#30340;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#12290;&#36825;&#31181;&#36235;&#21183;&#23548;&#33268;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#23588;&#20026;&#26126;&#26174;&#65292;&#23548;&#33268;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.</title><link>http://arxiv.org/abs/2401.04301</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;Transformer&#36807;&#24230;&#24179;&#28369;&#30340;&#30495;&#30456;&#12299;
&lt;/p&gt;
&lt;p&gt;
Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04301
&lt;/p&gt;
&lt;p&gt;
Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#20250;&#36880;&#28176;&#36807;&#24230;&#24179;&#28369;&#36755;&#20837;&#25968;&#25454;&#65292;&#38477;&#20302;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#23384;&#22312;&#36825;&#20010;&#32570;&#38519;&#30340;&#24773;&#20917;&#19979;&#65292;Transformer&#26159;&#22914;&#20309;&#21462;&#24471;&#36825;&#20123;&#25104;&#21151;&#30340;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20107;&#23454;&#19978;Transformer&#24182;&#19981;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;&#30456;&#21453;&#65292;Transformer&#26159;&#21542;&#36807;&#24230;&#24179;&#28369;&#21462;&#20915;&#20110;&#20854;&#26356;&#26032;&#26041;&#31243;&#30340;&#29305;&#24449;&#35889;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#20102;&#20043;&#21069;&#20851;&#20110;&#36807;&#24230;&#24179;&#28369;&#21644;&#30456;&#20851;&#29616;&#35937;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;Transformer&#27169;&#22411;&#20855;&#26377;&#28385;&#36275;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#26465;&#20214;&#30340;&#27880;&#24847;&#21147;&#21644;&#26435;&#37325;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23545;Transformer&#26356;&#26032;&#26041;&#31243;&#30340;&#26435;&#37325;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20351;&#20854;&#21487;&#20197;&#25511;&#21046;&#20854;&#35889;&#29305;&#24615;&#65292;&#30830;&#20445;&#19981;&#20250;&#21457;&#29983;&#36807;&#24230;&#24179;&#28369;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Deep-ELA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#65292;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;ELA&#29305;&#24449;&#23384;&#22312;&#30340;&#24378;&#30456;&#20851;&#24615;&#21644;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01192</link><description>&lt;p&gt;
Deep-ELA: &#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#36827;&#34892;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems. (arXiv:2401.01192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Deep-ELA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#65292;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;ELA&#29305;&#24449;&#23384;&#22312;&#30340;&#24378;&#30456;&#20851;&#24615;&#21644;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65288;ELA&#65289;&#29305;&#24449;&#30340;&#28508;&#21147;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23545;&#29305;&#23450;&#30340;&#21333;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#34920;&#24449;&#12290;&#36825;&#20123;&#25968;&#20540;&#29305;&#24449;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#36755;&#20837;&#65292;&#21253;&#25324;&#39640;&#32423;&#23646;&#24615;&#39044;&#27979;&#12289;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#21644;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#31561;&#12290;&#27809;&#26377;ELA&#29305;&#24449;&#65292;&#20998;&#26512;&#21644;&#29702;&#35299;&#21333;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#29305;&#24615;&#23558;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#26080;&#21487;&#20105;&#35758;&#65292;ELA&#29305;&#24449;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#20854;&#20013;&#21253;&#25324;&#65288;1.&#65289;&#22810;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#65288;2.&#65289;&#20854;&#22312;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#20316;&#20026;ELA&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#20363;&#22914;&#65292;&#28857;&#20113;&#21464;&#25442;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated. These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration. Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems would be impossible.  Yet, despite their undisputed usefulness, ELA features suffer from several drawbacks. These include, in particular, (1.) a strong correlation between multiple features, as well as (2.) its very limited applicability to multi-objective continuous optimization problems. As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA. In these works, e.g., point-cloud transformers were
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#36866;&#24212;&#24615;&#31574;&#30053;&#36873;&#25321;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#19979;&#30028;&#21644;&#31639;&#27861;&#30340;&#21305;&#37197;&#19978;&#30028;&#12290;&#30740;&#31350;&#21457;&#29616;&#31119;&#21033;&#26368;&#22823;&#21270;&#27604;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#26356;&#22256;&#38590;&#65292;&#20294;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#22686;&#38271;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09597</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Adaptive maximization of social welfare. (arXiv:2310.09597v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09597
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#36866;&#24212;&#24615;&#31574;&#30053;&#36873;&#25321;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#19979;&#30028;&#21644;&#31639;&#27861;&#30340;&#21305;&#37197;&#19978;&#30028;&#12290;&#30740;&#31350;&#21457;&#29616;&#31119;&#21033;&#26368;&#22823;&#21270;&#27604;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#26356;&#22256;&#38590;&#65292;&#20294;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#22686;&#38271;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#37325;&#22797;&#36873;&#25321;&#25919;&#31574;&#20197;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#30340;&#38382;&#39064;&#12290;&#31119;&#21033;&#26159;&#20010;&#20154;&#25928;&#29992;&#21644;&#20844;&#20849;&#25910;&#20837;&#30340;&#21152;&#26435;&#21644;&#12290;&#26089;&#26399;&#30340;&#32467;&#26524;&#24433;&#21709;&#21518;&#32493;&#30340;&#25919;&#31574;&#36873;&#25321;&#12290;&#25928;&#29992;&#19981;&#21487;&#35266;&#27979;&#65292;&#20294;&#21487;&#20197;&#38388;&#25509;&#25512;&#26029;&#12290;&#21709;&#24212;&#20989;&#25968;&#36890;&#36807;&#23454;&#39564;&#23398;&#20064;&#33719;&#24471;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#23545;&#20110;&#19968;&#31181;Exp3&#31639;&#27861;&#30340;&#21305;&#37197;&#23545;&#31574;&#23545;&#31435;&#19978;&#30028;&#12290;&#32047;&#31215;&#36951;&#25022;&#20197;$T^{2/3}$&#30340;&#36895;&#29575;&#22686;&#38271;&#12290;&#36825;&#24847;&#21619;&#30528;(i)&#31119;&#21033;&#26368;&#22823;&#21270;&#27604;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#26356;&#22256;&#38590;&#65288;&#23545;&#20110;&#26377;&#38480;&#30340;&#25919;&#31574;&#38598;&#26469;&#35828;&#65292;&#22686;&#38271;&#36895;&#29575;&#20026;$T^{1/2}$&#65289;&#65292;&#21644;(ii)&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#22686;&#38271;&#36895;&#29575;&#12290;&#23545;&#20110;&#38543;&#26426;&#35774;&#32622;&#65292;&#22914;&#26524;&#31038;&#20250;&#31119;&#21033;&#26159;&#20985;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20108;&#20998;&#25628;&#32034;&#31639;&#27861;&#22312;&#36830;&#32493;&#25919;&#31574;&#38598;&#19978;&#23454;&#29616;$T^{1/2}$&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#32447;&#24615;&#25910;&#20837;&#31246;&#25193;&#23637;&#65292;&#24182;&#27010;&#36848;&#20102;&#21830;&#21697;&#31246;&#25193;&#23637;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35774;&#32622;&#19982;&#22404;&#26029;&#23450;&#20215;&#65288;&#26356;&#23481;&#26131;&#65289;&#21644;&#21452;&#36793;&#20132;&#26131;&#30340;&#23450;&#20215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of repeatedly choosing policies to maximize social welfare. Welfare is a weighted sum of private utility and public revenue. Earlier outcomes inform later policies. Utility is not observed, but indirectly inferred. Response functions are learned through experimentation.  We derive a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This implies that (i) welfare maximization is harder than the multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our algorithm achieves the optimal rate. For the stochastic setting, if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy sets), using a dyadic search algorithm.  We analyze an extension to nonlinear income taxation, and sketch an extension to commodity taxation. We compare our setting to monopoly pricing (which is easier), and price setting for bilateral tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.04328</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Robust Losses for Decision-Focused Learning. (arXiv:2310.04328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#36827;&#34892;&#31163;&#25955;&#20915;&#31574;&#30340;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#22522;&#20110;&#39044;&#27979;&#20272;&#35745;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#19981;&#30830;&#23450;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#32771;&#34385;&#22522;&#20110;&#39044;&#27979;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;&#31471;&#21040;&#31471;&#39044;&#27979;-&#20248;&#21270;&#65289;&#26088;&#22312;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#21363;&#36890;&#36807;&#36827;&#34892;&#27425;&#20248;&#20915;&#31574;&#32780;&#20135;&#29983;&#30340;&#25439;&#22833;&#12290;&#23613;&#31649;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#21487;&#33021;&#26159;&#38750;&#20984;&#21644;&#19968;&#33324;&#19981;&#21487;&#23548;&#30340;&#65292;&#20294;&#24050;&#32463;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#26399;&#26395;&#25439;&#22833;&#65292;&#20351;&#29992;&#32463;&#39564;&#25439;&#22833;&#20316;&#20026;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#36951;&#25022;&#21487;&#33021;&#19981;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26367;&#20195;&#65292;&#22240;&#20026;&#20248;&#21270;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#32463;&#39564;&#36951;&#25022;&#19982;&#26399;&#26395;&#36951;&#25022;&#22312;&#26399;&#26395;&#19978;&#19981;&#30456;&#31561;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#31181;&#19981;&#31561;&#24335;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26426;&#20250;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#33021;&#22815;&#26356;&#22909;&#22320;&#36817;&#20284;&#26399;&#26395;&#36951;&#25022;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25351;&#23548;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization models used to make discrete decisions often contain uncertain parameters that are context-dependent and are estimated through prediction. To account for the quality of the decision made based on the prediction, decision-focused learning (end-to-end predict-then-optimize) aims at training the predictive model to minimize regret, i.e., the loss incurred by making a suboptimal decision. Despite the challenge of this loss function being possibly non-convex and in general non-differentiable, effective gradient-based learning approaches have been proposed to minimize the expected loss, using the empirical loss as a surrogate. However, empirical regret can be an ineffective surrogate because the uncertainty in the optimization model makes the empirical regret unequal to the expected regret in expectation. To illustrate the impact of this inequality, we evaluate the effect of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a surrogate. Next, we propose 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03447</link><description>&lt;p&gt;
FLAIM: &#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03447
&lt;/p&gt;
&lt;p&gt;
FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21516;&#26102;&#23454;&#29616;&#21327;&#21516;&#25968;&#25454;&#20849;&#20139;&#23545;&#32452;&#32455;&#33267;&#20851;&#37325;&#35201;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20135;&#29983;&#19982;&#31169;&#26377;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#30456;&#20284;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20551;&#35774;&#25968;&#25454;&#26159;&#38598;&#20013;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#32852;&#37030;&#26041;&#24335;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#32852;&#37030;&#21512;&#25104;&#34920;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;AIM&#36825;&#20010;&#20808;&#36827;&#30340;&#20013;&#24515;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistAIM&#21644;FLAIM&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#21457;AIM&#26159;&#31616;&#21333;&#30340;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#20351;&#20854;&#22312;&#32852;&#37030;&#22330;&#26223;&#20013;&#19981;&#22826;&#36866;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#32852;&#37030;AIM&#21487;&#33021;&#23548;&#33268;&#22312;&#24322;&#26500;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25928;&#29992;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;FLAIM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.00115</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#26500;&#35937;&#38598;&#21512;&#65306;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks. (arXiv:2310.00115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#37238;&#35774;&#35745;&#31561;&#20247;&#22810;&#29983;&#29289;&#21270;&#23398;&#24212;&#29992;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#24573;&#30053;&#20102;&#20998;&#23376;&#30340;&#28789;&#27963;&#24615;&#65292;&#20998;&#23376;&#36890;&#36807;&#21270;&#23398;&#38190;&#26059;&#36716;&#21644;&#24494;&#23567;&#25391;&#21160;&#25200;&#21160;&#19981;&#26029;&#22312;&#26500;&#35937;&#20043;&#38388;&#30456;&#20114;&#36716;&#21270;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#32771;&#34385;&#20998;&#23376;&#30340;&#28789;&#27963;&#24615;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#23558;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23450;&#20041;&#20026;&#19968;&#20010;&#38598;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#19987;&#27880;&#20110;&#20174;&#19968;&#32452;&#26500;&#35937;&#32467;&#26500;&#20013;&#26126;&#30830;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;MoleculAR Conformer Ensemble Learning&#65288;MARCEL&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22312;&#26500;&#35937;&#38598;&#21512;&#19978;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;MARCEL&#21253;&#25324;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#20998;&#23376;&#21644;&#21453;&#24212;&#27700;&#24179;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;</title><link>http://arxiv.org/abs/2309.15169</link><description>&lt;p&gt;
&#25581;&#31034;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#28041;&#21450;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#24320;&#21457;&#33021;&#22815;&#26126;&#30830;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#31354;&#38388;-&#26102;&#38388;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;MTS&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#25552;&#39640;&#31354;&#38388;-&#26102;&#38388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;STMAE&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#32534;&#30721;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#22522;&#20110;&#20559;&#32622;&#38543;&#26426;&#28216;&#36208;&#30340;&#31354;&#38388;&#25513;&#34109;&#21644;&#22522;&#20110;&#34917;&#19969;&#30340;&#26102;&#38388;&#25513;&#34109;&#12290;&#38543;&#21518;&#65292;&#35299;&#30721;&#22120;&#26088;&#22312;&#37325;&#26500;&#20004;&#20010;&#25513;&#34109;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#20013;&#29699;&#30340;&#27010;&#24565;&#31867;&#20013;&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.02876</link><description>&lt;p&gt;
&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#22312;&#22270;&#20013;&#29699;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-Clashing Teaching Maps for Balls in Graphs. (arXiv:2309.02876v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#20013;&#29699;&#30340;&#27010;&#24565;&#31867;&#20013;&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Kirkpatrick&#31561;&#20154;[ALT 2019]&#21644;Fallat&#31561;&#20154;[JMLR 2023]&#24341;&#20837;&#20102;&#38750;&#20914;&#31361;&#25945;&#23398;&#65292;&#24182;&#34920;&#26126;&#23427;&#26159;&#28385;&#36275;Goldman&#21644;Mathias&#25552;&#20986;&#30340;&#38450;&#27490;&#21246;&#32467;&#22522;&#20934;&#30340;&#26368;&#39640;&#25928;&#30340;&#26426;&#22120;&#25945;&#23398;&#27169;&#22411;&#12290;&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867;$\cal{C}$&#26469;&#35828;&#65292;&#25945;&#23398;&#22270;$T$&#23558;&#19968;&#20010;&#65288;&#25945;&#23398;&#65289;&#38598;&#21512;$T(C)$&#20998;&#37197;&#32473;&#27599;&#20010;&#27010;&#24565;$C \in \cal{C}$&#12290;&#22914;&#26524;&#27809;&#26377;&#19968;&#23545;&#27010;&#24565;&#19982;&#23427;&#20204;&#30340;&#25945;&#23398;&#38598;&#21512;&#30340;&#24182;&#19968;&#33268;&#65292;&#21017;&#25945;&#23398;&#22270;&#26159;&#38750;&#20914;&#31361;&#30340;&#12290;&#38750;&#20914;&#31361;&#25945;&#23398;&#22270;&#65288;NCTM&#65289;$T$&#30340;&#22823;&#23567;&#26159;$T(C)$&#20013;&#30340;&#26368;&#22823;&#22823;&#23567;&#65292;&#20854;&#20013;$C \in \cal{C}$&#12290;&#27010;&#24565;&#31867;$\mathcal{B}(G)$&#30340;&#38750;&#20914;&#31361;&#25945;&#23398;&#32500;&#24230;NCTD$(\cal{C})$&#26159;$\cal{C}$&#30340;&#19968;&#20010;NCTM&#30340;&#26368;&#23567;&#22823;&#23567;&#12290;&#31867;&#20284;&#22320;&#65292;NCTM$^+$&#21644;NCTD$^+(\cal{C})$&#30340;&#23450;&#20041;&#26159;&#31867;&#20284;&#30340;&#65292;&#21482;&#26159;&#25945;&#24072;&#21482;&#33021;&#20351;&#29992;&#27491;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#22270;$G$&#30340;&#25152;&#26377;&#29699;&#32452;&#25104;&#30340;&#27010;&#24565;&#31867;$\mathcal{B}(G)$&#30340;NCTMs&#21644;NCTM$^+$s&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;NCTD$^+$&#30340;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;{\sc B-NCTD$^+$}&#26159;NP&#23436;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\cal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \in \cal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a $T(C)$, $C \in \cal{C}$. The non-clashing teaching dimension NCTD$(\cal{C})$ of $\cal{C}$ is the minimum size of an NCTM for $\cal{C}$. NCTM$^+$ and NCTD$^+(\cal{C})$ are defined analogously, except the teacher may only use positive examples.  We study NCTMs and NCTM$^+$s for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in spl
&lt;/p&gt;</description></item><item><title>GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.16891</link><description>&lt;p&gt;
GNFactor&#65306;&#20855;&#26377;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#22810;&#20219;&#21153;&#30495;&#23454;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16891
&lt;/p&gt;
&lt;p&gt;
GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32467;&#26500;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#24320;&#21457;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#35821;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNFactor&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35270;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#65288;GNF&#65289;&#20316;&#20026;&#37325;&#24314;&#27169;&#22359;&#65292;Perceiver Transformer&#20316;&#20026;&#20915;&#31574;&#27169;&#22359;&#65292;&#20849;&#20139;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#12290;&#20026;&#20102;&#23558;&#35821;&#20041;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#65292;&#37325;&#24314;&#27169;&#22359;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#65289;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21040;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#20013;&#12290;&#25105;&#20204;&#22312;3&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;GNFactor&#65292;&#24182;&#23545;10&#20010;RLBench&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#21482;&#20351;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11247</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#26159;&#36807;&#31243;&#30417;&#35270;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#22522;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#25925;&#38556;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#30001;&#20110;&#30417;&#27979;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#22914;&#25805;&#20316;&#27169;&#24335;&#30340;&#25913;&#21464;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#21270;&#23398;&#24037;&#19994;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30000;&#32435;&#35199;-&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#22312;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.08538</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proprioceptive Learning with Soft Polyhedral Networks. (arXiv:2308.08538v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#22791;&#23884;&#20837;&#24335;&#35270;&#35273;&#30340;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#26412;&#20307;&#24863;&#30693;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#12290;&#35813;&#35774;&#35745;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20840;&#21521;&#20132;&#20114;&#30340;&#34987;&#21160;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#20869;&#23884;&#30340;&#24494;&#22411;&#39640;&#36895;&#36816;&#21160;&#36319;&#36394;&#31995;&#32479;&#20197;&#35270;&#35273;&#26041;&#24335;&#25429;&#33719;&#26412;&#20307;&#24863;&#30693;&#30340;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#33021;&#22815;&#20197;0.25/0.24/0.35 N&#21644;0.025/0.034/0.006 Nm&#30340;&#31934;&#24230;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#22312;&#21160;&#24577;&#20132;&#20114;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#28155;&#21152;&#31896;&#24377;&#24615;&#24863;&#21463;&#24615;&#26469;&#22312;&#38745;&#24577;&#36866;&#24212;&#20013;&#22686;&#21152;&#26412;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proprioception is the "sixth sense" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04428</link><description>&lt;p&gt;
&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#20803;&#23398;&#20064;&#25805;&#20316;&#31526;&#21040;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Operators to Optimality from Multi-Task Non-IID Data. (arXiv:2308.04428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#20010;&#24378;&#22823;&#27010;&#24565;&#26159;&#20174;&#24322;&#26500;&#26469;&#28304;&#25110;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#21516;&#29305;&#24449;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#20989;&#25968;&#65292;&#26082;&#26377;&#21161;&#20110;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#26377;&#21161;&#20110;&#32479;&#35745;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#23569;&#35201;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#22312;&#29702;&#35770;&#19978;&#20570;&#20986;&#36825;&#20123;&#20248;&#28857;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22122;&#22768;&#21521;&#37327;&#27979;&#37327;$y = Mx + w$&#20013;&#22238;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;$M$&#30340;&#19968;&#33324;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#21327;&#21464;&#37327;$x$&#26082;&#21487;&#20197;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#36825;&#23548;&#33268;&#22122;&#22768;&#39033;&#30340;&#32553;&#25918;&#19981;&#20877;&#26377;&#21033;&#20110;&#28304;&#20219;&#21153;&#25968;&#37327;&#12290;&#36825;&#21453;&#36807;&#26469;&#20250;&#23548;&#33268;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21463;&#21040;&#21333;&#20219;&#21153;&#25968;&#25454;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias &amp; Feature-Whiten}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#23436;&#32654;&#21516;&#27493;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#40065;&#26834;&#30340;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#21644;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#37051;&#22495;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2307.11617</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#29992;&#20110;&#36890;&#29992;&#26550;&#26500;&#19978;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Fully-Asynchronous Methods for Distributed Training over General Architecture. (arXiv:2307.11617v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#23436;&#32654;&#21516;&#27493;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#40065;&#26834;&#30340;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#21644;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#37051;&#22495;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#23436;&#32654;&#30340;&#21516;&#27493;&#26159;&#20302;&#25928;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#65292;&#30001;&#20110;&#24310;&#36831;&#12289;&#25968;&#25454;&#20002;&#22833;&#21644;&#24310;&#36831;&#36739;&#22823;&#30340;&#35774;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20840;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#36319;&#36394;&#26041;&#27861;&#65288;R-FAST&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#20197;&#33258;&#24049;&#30340;&#36895;&#24230;&#36827;&#34892;&#26412;&#22320;&#35745;&#31639;&#21644;&#36890;&#20449;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#21516;&#27493;&#12290;&#19982;&#29616;&#26377;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#19981;&#21516;&#65292;R-FAST&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#35774;&#35745;&#33391;&#22909;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#36319;&#36394;&#21644;&#32531;&#20914;&#25972;&#20307;&#26799;&#24230;&#21521;&#37327;&#30340;&#40065;&#26834;&#26799;&#24230;&#36319;&#36394;&#31574;&#30053;&#65292;&#28040;&#38500;&#35774;&#22791;&#38388;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#20801;&#35768;&#25968;&#25454;&#21253;&#20002;&#22833;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#26641;&#22270;&#36827;&#34892;&#36890;&#20449;&#65292;&#21482;&#35201;&#20004;&#32773;&#20849;&#20139;&#33267;&#23569;&#19968;&#20010;&#20849;&#21516;&#30340;&#26681;&#33410;&#28857;&#65292;&#23601;&#33021;&#23454;&#29616;&#28789;&#27963;&#30340;&#36890;&#20449;&#26550;&#26500;&#35774;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;R-FAST&#23545;&#20110;&#24179;&#28369;&#21644;&#24378;&#20984;&#38382;&#39064;&#65292;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26399;&#26395;&#37051;&#22495;&#65292;&#24182;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05908</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65306;&#20934;&#30830;LLM&#35299;&#30721;&#20013;&#30340;&#35745;&#31639;&#24310;&#36831;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;"&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#21407;&#22987;&#35299;&#30721;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#19981;&#21516;&#65292;PPD&#21033;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#22312;&#24403;&#21069;&#20196;&#29260;&#35299;&#30721;&#26399;&#38388;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20943;&#23569;&#20102;&#35299;&#30721;&#24310;&#36831;&#65292;&#24182;&#37325;&#26032;&#22609;&#36896;&#20102;LLM&#35299;&#30721;&#31574;&#30053;&#20013;&#30340;&#26435;&#34913;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#35745;&#31639;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#29575;&#65288;&#34920;&#31034;&#20026;p_correct&#65289;&#26469;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#33021;&#30340;&#24310;&#36831;&#20943;&#23569;&#36827;&#34892;&#20998;&#26512;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#28508;&#21147;&#21152;&#36895;LLM&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03201</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#30340;&#24130;&#24459;&#20851;&#31995;&#65292;&#23427;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#27169;&#22411;&#35774;&#35745;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#65288;&#25110;&#27169;&#22411;&#21442;&#25968;&#31561;&#65289;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#23558;&#30456;&#24212;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21516;&#26102;&#65292;&#36825;&#31181;&#32553;&#25918;&#23450;&#24459;&#20851;&#31995;&#24573;&#35270;&#20102;&#29992;&#20110;&#34913;&#37327;&#24615;&#33021;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#21644;&#26377;&#20105;&#35758;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#31526;&#21512;&#19981;&#21516;&#20154;&#32676;&#23545;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#38543;&#30528;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#22686;&#38271;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#19981;&#21516;&#31038;&#32676;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#65289;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#27599;&#20010;&#31038;&#32676;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20013;&#25152;&#20195;&#34920;&#30340;&#31038;&#32676;&#21487;&#33021;&#23384;&#22312;&#20215;&#20540;&#35266;&#25110;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
&lt;/p&gt;</description></item><item><title>GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11670</link><description>&lt;p&gt;
GIO&#65306;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#30340;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11670
&lt;/p&gt;
&lt;p&gt;
GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26377;&#21033;&#20110;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#20026;&#36825;&#20123;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#24076;&#26395;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#65288;GIO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#23567;&#32452;&#65288;&#26410;&#26631;&#35760;&#30340;&#65289;&#26679;&#26412;&#26469;&#20195;&#34920;&#30446;&#26631;&#20998;&#24067;&#12290;GIO&#20174;&#19968;&#20010;&#23454;&#36341;&#20013;&#38590;&#20197;&#22788;&#29702;&#30340;&#33258;&#28982;&#30340;&#20449;&#24687;&#29702;&#35770;&#30446;&#26631;&#24320;&#22987;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20986;&#36890;&#36807;&#23545;&#30446;&#26631;&#36827;&#34892;&#31616;&#21333;&#30340;&#25918;&#26494;&#21644;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#23427;&#21487;&#20197;&#34987;&#39640;&#24230;&#25193;&#23637;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25340;&#20889;&#32416;&#27491;&#21644;&#22270;&#20687;&#35782;&#21035;&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GIO&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;GIO&#26412;&#36523;&#30340;&#19981;&#21516;&#34920;&#31034;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#12290;GIO&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06048</link><description>&lt;p&gt;
&#24494;&#35843;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#26159;&#24590;&#26679;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22806;&#20998;&#24067;&#26816;&#27979;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#20869;&#20998;&#24067;&#20934;&#30830;&#24615;&#24448;&#24448;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#65292;&#24050;&#32463;&#22312;&#23384;&#22312;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#20869;&#20998;&#24067;&#20998;&#31867;&#21644;&#22806;&#20998;&#24067;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#23545;&#20110;&#27809;&#26377;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#35821;&#20041;&#36716;&#31227;&#26159;&#21542;&#21487;&#38752;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#23545;&#24494;&#35843;&#23545;&#20110;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#21270;&#20026;&#22810;&#27169;&#24335;&#27010;&#24565;&#21305;&#37197;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24494;&#35843;&#26041;&#27861;&#21644;&#21508;&#31181;&#22806;&#20998;&#24067;&#20998;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consiste
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAPI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#65292;&#36890;&#36807;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#12290;SAPI&#33021;&#22815;&#22312;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01812</link><description>&lt;p&gt;
SAPI:&#29615;&#22659;&#24863;&#30693;&#36710;&#36742;&#22312;&#36335;&#21475;&#30340;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SAPI: Surroundings-Aware Vehicle Trajectory Prediction at Intersections. (arXiv:2306.01812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAPI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#65292;&#36890;&#36807;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#12290;SAPI&#33021;&#22815;&#22312;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;SAPI&#65292;&#29992;&#20110;&#22312;&#36335;&#21475;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;SAPI&#20351;&#29992;&#25277;&#35937;&#30340;&#26041;&#24335;&#26469;&#34920;&#31034;&#21644;&#32534;&#30721;&#21608;&#22260;&#29615;&#22659;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#22320;&#22270;&#12289;&#20248;&#20808;&#26435;&#21644;&#21608;&#22260;&#20132;&#36890;&#20449;&#24687;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#21367;&#31215;&#32593;&#32476;(CNN)&#21644;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#32534;&#30721;&#22120;&#20197;&#21450;&#19968;&#20010;&#35299;&#30721;&#22120;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21407;&#22987;&#21382;&#21490;&#36712;&#36857;&#20449;&#24687;&#65292;&#22312;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#21270;&#22120;&#26469;&#36827;&#34892;&#22238;&#28335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#20027;&#36710;&#36742;&#22312;&#23454;&#38469;&#36335;&#21475;&#37319;&#38598;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;SAPI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#27979;&#36335;&#21475;&#36710;&#36742;&#36712;&#36857;&#26102;&#65292;SAPI&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;6&#31186;&#39044;&#27979;&#30340;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;(ADE)&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;(FDE)&#20998;&#21035;&#20026;1.84&#31859;&#21644;4.32&#31859;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a deep learning model, i.e., SAPI, to predict vehicle trajectories at intersections. SAPI uses an abstract way to represent and encode surrounding environment by utilizing information from real-time map, right-of-way, and surrounding traffic. The proposed model consists of two convolutional network (CNN) and recurrent neural network (RNN)-based encoders and one decoder. A refiner is proposed to conduct a look-back operation inside the model, in order to make full use of raw history trajectory information. We evaluate SAPI on a proprietary dataset collected in real-world intersections through autonomous vehicles. It is demonstrated that SAPI shows promising performance when predicting vehicle trajectories at intersection, and outperforms benchmark methods. The average displacement error(ADE) and final displacement error(FDE) for 6-second prediction are 1.84m and 4.32m respectively. We also show that the proposed model can accurately predict vehicle trajectories i
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10413</link><description>&lt;p&gt;
&#20351;&#29992;Lasso&#30340;&#31614;&#21517;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31614;&#21517;&#21464;&#25442;&#26159;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36845;&#20195;&#36335;&#24452;&#31215;&#20998;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#38750;&#32447;&#24615;&#36890;&#36807;&#32447;&#24615;&#21270;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26356;&#25509;&#36817;&#24067;&#26391;&#36816;&#21160;&#25110;&#20855;&#26377;&#36739;&#24369;&#36328;&#32500;&#24230;&#30456;&#20851;&#24615;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#31614;&#21517;&#23450;&#20041;&#20026;It\^o&#31215;&#20998;&#30340;Lasso&#22238;&#24402;&#26356;&#20855;&#19968;&#33268;&#24615;&#65307;&#23545;&#20110;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#31614;&#21517;&#23450;&#20041;&#20026;Stratonovich&#31215;&#20998;&#22312;Lasso&#22238;&#24402;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#21644;&#38543;&#26426;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06625</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;Dropout&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dropout Regularization in Extended Generalized Linear Models based on Double Exponential Families. (arXiv:2305.06625v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;dropout&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21452;&#25351;&#25968;&#26063;&#30340;&#25193;&#23637;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;dropout&#27491;&#21017;&#21270;&#65292;&#20854;&#20013;&#31163;&#25955;&#21442;&#25968;&#21487;&#20197;&#38543;&#29305;&#24449;&#21464;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;dropout&#27491;&#21017;&#21270;&#20559;&#22909;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22312;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#26041;&#38754;&#37117;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#36825;&#25193;&#23637;&#20102;&#20043;&#21069;&#38024;&#23545;&#20256;&#32479;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#32467;&#26524; &#12290;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;dropout&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;B&#26679;&#26465;&#24179;&#28369;&#65292;&#20854;&#20013;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#21442;&#25968;&#37117;&#34987;&#28789;&#27963;&#22320;&#24314;&#27169;&#12290;&#37325;&#35201;&#30340;B&#26679;&#26465;&#22522;&#30784;&#20989;&#25968;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#32597;&#35265;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#65292;dropout&#26159;&#19968;&#31181;&#25913;&#21892;&#20102;&#32602;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#30340;&#26174;&#24335;&#24179;&#28369;&#24615;&#30340;&#22343;&#20540;&#21644;&#31163;&#25955;&#24230;&#21442;&#25968;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though dropout is a popular regularization technique, its theoretical properties are not fully understood. In this paper we study dropout regularization in extended generalized linear models based on double exponential families, for which the dispersion parameter can vary with the features. A theoretical analysis shows that dropout regularization prefers rare but important features in both the mean and dispersion, generalizing an earlier result for conventional generalized linear models. Training is performed using stochastic gradient descent with adaptive learning rate. To illustrate, we apply dropout to adaptive smoothing with B-splines, where both the mean and dispersion parameters are modelled flexibly. The important B-spline basis functions can be thought of as rare features, and we confirm in experiments that dropout is an effective form of regularization for mean and dispersion parameters that improves on a penalized maximum likelihood approach with an explicit smoothness p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01618</link><description>&lt;p&gt;
ContactArt&#65306;&#23398;&#20064;&#31867;&#21035;&#32423;&#32852;&#32467;&#29289;&#20307;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25163;&#37096;&#21644;&#32852;&#32467;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#36965;&#25805;&#20316;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#30452;&#25509;&#22312;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#28216;&#25103;&#26469;&#25805;&#32437;&#32852;&#32467;&#23545;&#35937;&#12290; &#25105;&#20204;&#35760;&#24405;&#25968;&#25454;&#24182;&#20174;&#27169;&#25311;&#22120;&#33719;&#24471;&#26377;&#20851;&#30446;&#26631;&#23039;&#24577;&#21644;&#25509;&#35302;&#20449;&#24687;&#30340;&#20813;&#36153;&#21644;&#20934;&#30830;&#27880;&#37322;&#12290; &#25105;&#20204;&#30340;&#31995;&#32479;&#20165;&#38656;&#35201;&#20351;&#29992;iPhone&#26469;&#35760;&#24405;&#20154;&#25163;&#36816;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#24182;&#22823;&#22823;&#38477;&#20302;&#25968;&#25454;&#21644;&#27880;&#37322;&#25910;&#38598;&#30340;&#25104;&#26412;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;&#65292;&#21253;&#25324;&#25429;&#33719;&#23545;&#35937;&#37096;&#20214;&#25490;&#21015;&#20998;&#24067;&#30340;&#37492;&#21035;&#22120;&#65288;&#22312;GAN&#20013;&#65289;&#65292;&#20197;&#21450;&#29983;&#25104;&#32852;&#32467;&#23545;&#35937;&#19978;&#25509;&#35302;&#21306;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25351;&#23548;&#25163;&#21183;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26500;&#21644;&#25509;&#35302;&#20808;&#39564;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#23398;&#20064;&#30340;&#20808;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.17674</link><description>&lt;p&gt;
&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#12290;&#21487;&#36798;&#38598;&#22312;&#25511;&#21046;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#35745;&#31639;&#36215;&#26469;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#36807;&#36924;&#36817;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#65292;&#23558;&#20854;&#34920;&#31034;&#25104;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#36825;&#20010;&#26377;&#38480;&#32500;&#30340;&#21051;&#30011;&#24320;&#21551;&#20102;&#19968;&#31181;&#32039;&#23494;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#65292;&#19988;&#25104;&#26412;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20302;&#12289;&#26356;&#31934;&#20934;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#31070;&#32463;&#21453;&#39304;&#29615;&#20998;&#26512;&#21644;&#40065;&#26834;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.13462</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#20960;&#20309;&#36827;&#34892;&#23398;&#20064;&#24186;&#27491;&#21464;&#25442;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;(DQFIM)&#26469;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#33021;&#22815;&#27867;&#21270;&#12290;&#23545;&#20110;&#24186;&#27491;&#21464;&#25442;&#30340;&#21487;&#21464;&#23398;&#20064;&#65292;DQFIM&#37327;&#21270;&#20102;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;DQFIM&#26469;&#35299;&#37322;&#20309;&#26102;&#24658;&#23450;&#25968;&#37327;&#30340;&#35757;&#32451;&#29366;&#24577;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#21442;&#25968;&#36275;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#30340;&#33021;&#21147;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25552;&#39640;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#35299;&#20915;&#38750;&#20809;&#28369;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#36845;&#20195;&#20013;&#37319;&#29992;ADMM&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915; PDE-constrained optimization problems&#12290;</title><link>http://arxiv.org/abs/2302.08309</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20809;&#28369;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
The ADMM-PINNs Algorithmic Framework for Nonsmooth PDE-Constrained Optimization: A Deep Learning Approach. (arXiv:2302.08309v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#35299;&#20915;&#38750;&#20809;&#28369;PDE&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#36845;&#20195;&#20013;&#37319;&#29992;ADMM&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915; PDE-constrained optimization problems&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#27861;(ADMM)&#21644;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#19968;&#33324;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#24403;&#38656;&#35201;&#23545;&#25511;&#21046;&#25110;&#35774;&#35745;&#21464;&#37327;&#36827;&#34892;&#38480;&#21046;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#38468;&#21152;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;PINNs&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#30340;PDE&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#37319;&#29992;ADMM&#21487;&#20197;&#23558;PDE&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#20998;&#31163;&#20986;&#26469;&#65292;&#20351;&#36845;&#20195;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#23376;&#38382;&#39064;&#26159;&#24179;&#28369;&#30340;PDE&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;PINNs&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#32780;&#21478;&#19968;&#20010;&#26159;&#31616;&#21333;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#24120;&#26377;&#23553;&#38381;&#24418;&#24335;&#35299;&#25110;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26631;&#20934;&#20248;&#21270;&#31639;&#27861;&#25110;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;ADMM-PINNs&#31639;&#27861;&#26694;&#26550;&#22312;&#35299;&#20915;PDE&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the combination of the alternating direction method of multipliers (ADMM) with physics-informed neural networks (PINNs) for a general class of nonsmooth partial differential equation (PDE)-constrained optimization problems, where additional regularization can be employed for constraints on the control or design variables. The resulting ADMM-PINNs algorithmic framework substantially enlarges the applicable range of PINNs to nonsmooth cases of PDE-constrained optimization problems. The application of the ADMM makes it possible to untie the PDE constraints and the nonsmooth regularization terms for iterations. Accordingly, at each iteration, one of the resulting subproblems is a smooth PDE-constrained optimization which can be efficiently solved by PINNs, and the other is a simple nonsmooth optimization problem which usually has a closed-form solution or can be efficiently solved by various standard optimization algorithms or pre-trained neural networks. The ADMM-PINNs algorithmi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.07890</link><description>&lt;p&gt;
&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#21453;&#24212;&#24335;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Policy Blending as Inference for Reactive Robot Control. (arXiv:2210.07890v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#12289;&#23494;&#38598;&#21644;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#19968;&#26041;&#38754;&#65292;&#21453;&#24212;&#24335;&#31574;&#30053;&#20445;&#35777;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#24555;&#36895;&#21709;&#24212;&#65292;&#20294;&#20197;&#27425;&#20248;&#30340;&#34892;&#20026;&#20316;&#20026;&#20195;&#20215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#36816;&#21160;&#29983;&#25104;&#25552;&#20379;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20294;&#39640;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#20250;&#38480;&#21046;&#25511;&#21046;&#39057;&#29575;&#65292;&#20174;&#32780;&#29306;&#29298;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#32467;&#21512;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#25512;&#29702;&#26041;&#27861;&#26469;&#27491;&#24335;&#21270;&#20998;&#23618;&#27169;&#22411;&#21644;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20026;&#38543;&#26426;&#21453;&#24212;&#24335;&#19987;&#23478;&#31574;&#30053;&#30340;&#21152;&#26435;&#20056;&#31215;&#65292;&#20854;&#20013;&#35268;&#21010;&#34987;&#29992;&#20110;&#33258;&#36866;&#24212;&#35745;&#31639;&#20219;&#21153;&#21608;&#26399;&#20869;&#30340;&#26368;&#20248;&#26435;&#37325;&#12290;&#36825;&#31181;&#38543;&#26426;&#20248;&#21270;&#36991;&#20813;&#20102;&#23616;&#37096;&#26368;&#20248;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#34892;&#30340;&#21453;&#24212;&#24335;&#35745;&#21010;&#65292;&#25214;&#21040;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion generation in cluttered, dense, and dynamic environments is a central topic in robotics, rendered as a multi-objective decision-making problem. Current approaches trade-off between safety and performance. On the one hand, reactive policies guarantee fast response to environmental changes at the risk of suboptimal behavior. On the other hand, planning-based motion generation provides feasible trajectories, but the high computational cost may limit the control frequency and thus safety. To combine the benefits of reactive policies and planning, we propose a hierarchical motion generation method. Moreover, we adopt probabilistic inference methods to formalize the hierarchical model and stochastic optimization. We realize this approach as a weighted product of stochastic, reactive expert policies, where planning is used to adaptively compute the optimal weights over the task horizon. This stochastic optimization avoids local optima and proposes feasible reactive plans that find path
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.14233</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;Bandits&#36890;&#36807;&#20581;&#22766;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#32463;&#24120;&#21516;&#26102;&#38754;&#23545;&#35768;&#22810;&#30456;&#20851;&#20294;&#24322;&#36136;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#23398;&#20064;&#23454;&#20363;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65292;&#20351;&#29992;&#20581;&#22766;&#32479;&#35745;&#23398;&#65288;&#22312;&#30456;&#20284;&#23454;&#20363;&#19978;&#23398;&#20064;&#65289;&#21644;LASSO&#22238;&#24402;&#65288;&#21435;&#20559;&#24046;&#32467;&#26524;&#65289;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.03860</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#39640;&#26031;&#21270;&#23618;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;GAN&#12289;&#26631;&#20934;&#21270;&#27969;&#21644;&#25193;&#25955;&#27169;&#22411;&#26159;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#19981;&#36866;&#23450;&#24615;&#24182;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36870;&#25512;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#24352;&#37327;&#21487;&#33021;&#20250;&#20174;&#26399;&#26395;&#30340;&#39640;&#32500;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#20013;&#33073;&#31163;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#30340;&#27491;&#21521;&#27169;&#22411;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20250;&#23548;&#33268;&#20302;&#20445;&#30495;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26032;&#39062;&#30340;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#65292;&#20854;&#20013;&#20351;&#29992;&#33258;&#23450;&#20041;&#25805;&#20316;&#31526;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#25311;&#35758;&#30340;&#23618;&#23558;&#36870;&#38382;&#39064;&#32422;&#26463;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21453;&#28436;&#20219;&#21153;&#65288;&#21387;&#32553;&#24863;&#30693;MRI&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#20934;&#30830;&#24230;&#21463;&#38480;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21453;&#28436;&#38382;&#39064;&#8220;eikonal tomography&#8221;&#65289;&#19978;&#20351;&#29992;&#20004;&#31181;&#20856;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models
&lt;/p&gt;</description></item></channel></rss>