<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#21644;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#20415;&#25429;&#33719;&#29983;&#29289;&#32452;&#32455;&#20013;&#30340;&#32420;&#32500;&#26041;&#21521;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.18597</link><description>&lt;p&gt;
&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;: &#20174;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#27979;&#37327;&#20013;&#21457;&#29616;&#29983;&#29289;&#32452;&#32455;&#26412;&#26500;&#23450;&#24459;&#21644;&#24494;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Peridynamic Neural Operators: Discover Biotissue Constitutive Law and Microstructure From Digital Image Correlation Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#21644;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#20415;&#25429;&#33719;&#29983;&#29289;&#32452;&#32455;&#20013;&#30340;&#32420;&#32500;&#26041;&#21521;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32452;&#32455;&#26159;&#39640;&#24230;&#26377;&#26426;&#21270;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#29305;&#23450;&#30340;&#33014;&#21407;&#32420;&#32500;&#25490;&#21015;&#65292;&#20174;&#28857;&#21040;&#28857;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#23545;&#32452;&#32455;&#21151;&#33021;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#21457;&#29616;&#21644;&#29702;&#35299;&#36825;&#31181;&#32420;&#32500;&#26041;&#21521;&#30340;&#20998;&#24067;&#20174;&#23454;&#39564;&#27979;&#37327;&#25968;&#25454;&#65292;&#22914;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#25968;&#25454;&#20013;&#23588;&#20026;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#36136;Peridynamic&#31070;&#32463;&#31639;&#23376;&#65288;HeteroPNO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#21508;&#21521;&#24322;&#24615;&#26448;&#26009;&#26412;&#26500;&#24314;&#27169;&#12290;&#26088;&#22312;&#20174;&#21152;&#36733;&#22330;&#20301;&#31227;&#22330;&#27979;&#37327;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#20197;&#21450;&#26448;&#26009;&#24494;&#32467;&#26500;&#65292;&#20197;&#24322;&#36136;&#32420;&#32500;&#23450;&#21521;&#22330;&#30340;&#24418;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#26680;&#20989;&#25968;&#21644;&#38750;&#23616;&#37096;&#38190;&#21147;&#30340;&#22343;&#21248;&#26412;&#26500;&#23450;&#24459;&#65292;&#20197;&#25429;&#25417;&#23436;&#25972;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18597v1 Announce Type: cross  Abstract: Human tissues are highly organized structures with specific collagen fiber arrangements varying from point to point. The effects of such heterogeneity play an important role for tissue function, and hence it is of critical to discover and understand the distribution of such fiber orientations from experimental measurements, such as the digital image correlation data. To this end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO) approach, for data-driven constitutive modeling of heterogeneous anisotropic materials. The goal is to learn both a nonlocal constitutive law together with the material microstructure, in the form of a heterogeneous fiber orientation field, from loading field-displacement field measurements. To this end, we propose a two-phase learning approach. Firstly, we learn a homogeneous constitutive law in the form of a neural network-based kernel function and a nonlocal bond force, to capture comple
&lt;/p&gt;</description></item><item><title>Jetfire&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;INT8&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#20026;&#26631;&#20934;transformer&#22359;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;</title><link>https://arxiv.org/abs/2403.12422</link><description>&lt;p&gt;
Jetfire: &#20351;&#29992;INT8&#25968;&#25454;&#27969;&#21644;&#25353;&#22359;&#37327;&#21270;&#30340;&#39640;&#25928;&#20934;&#30830;Transformer&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12422
&lt;/p&gt;
&lt;p&gt;
Jetfire&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;INT8&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#20026;&#26631;&#20934;transformer&#22359;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;transformer&#36890;&#24120;&#32791;&#26102;&#36739;&#38271;&#12290;&#23436;&#20840;&#37327;&#21270;&#35757;&#32451;&#65288;FQT&#65289;&#26159;&#19968;&#31181;&#21152;&#36895;&#39044;&#35757;&#32451;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;FQT&#26041;&#27861;&#37319;&#29992;&#37327;&#21270;-&#35745;&#31639;-&#21453;&#37327;&#21270;&#30340;&#36807;&#31243;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#22312;transformers&#20013;&#20351;&#29992;&#26102;&#20986;&#29616;&#27425;&#20248;&#30340;&#21152;&#36895;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#38477;&#32423;&#65292;&#21407;&#22240;&#26159;&#39640;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#21644;&#20302;&#31934;&#24230;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jetfire&#65292;&#19968;&#31181;&#38024;&#23545;transformers&#30340;&#39640;&#25928;&#20934;&#30830;INT8&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;INT8&#25968;&#25454;&#27969;&#26469;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#65292;&#24182;&#37319;&#29992;&#25353;&#22359;&#37327;&#21270;&#26041;&#27861;&#26469;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#30340;&#20934;&#30830;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;INT8 FQT&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;FP16&#35757;&#32451;&#22522;&#32447;&#30456;&#24403;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;transformers&#30340;INT8&#35757;&#32451;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26631;&#20934;&#30340;transformer&#22359;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;1.42&#20493;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#21152;&#36895;&#21644;1.49&#20493;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12422v1 Announce Type: new  Abstract: Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09549</link><description>&lt;p&gt;
&#23558;&#21435;&#22122;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#20197;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09549
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;3D&#21407;&#23376;&#20307;&#31995;&#20013;&#30340;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#20174;&#22836;&#31639;&#35745;&#31639;&#65292;&#22240;&#27492;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#38750;&#24179;&#34913;&#32467;&#26500;&#65288;DeNS&#65289;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;DeNS&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21521;&#20854;3D&#22352;&#26631;&#28155;&#21152;&#22122;&#22768;&#26469;&#30772;&#22351;3D&#32467;&#26500;&#65292;&#28982;&#21518;&#39044;&#27979;&#22122;&#22768;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#20165;&#38480;&#20110;&#24179;&#34913;&#32467;&#26500;&#30340;&#21435;&#22122;&#24037;&#20316;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#21435;&#22122;&#27867;&#21270;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#38750;&#24179;&#34913;&#32467;&#26500;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#38750;&#24179;&#34913;&#32467;&#26500;&#19981;&#23545;&#24212;&#20110;&#23616;&#37096;&#33021;&#37327;&#26368;&#23567;&#20540;&#65292;&#20855;&#26377;&#38750;&#38646;&#21147;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#21407;&#23376;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2403.05181</link><description>&lt;p&gt;
Adversarial Sparse Teacher: &#23545;&#25239;&#25932;&#23545;&#31034;&#20363;&#65292;&#38450;&#24481;&#29992;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#30340;&#22522;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20419;&#36827;&#20102;&#23558;&#39640;&#32423;&#25945;&#24072;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#36716;&#31227;&#21040;&#26356;&#31616;&#21333;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#30830;&#20445;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#23427;&#20063;&#34987;&#29992;&#20110;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20351;&#29992;KD&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#21533;&#21868;&#25945;&#24072;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#31232;&#30095;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#30693;&#35782;&#20135;&#26435;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#20445;&#25252;&#20854;logits&#65292;&#21463;&#8220;&#24694;&#27602;&#25945;&#24072;&#8221;&#29702;&#24565;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#21152;&#24378;&#25945;&#24072;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#20943;&#23569;&#20102;&#30456;&#23545;&#30340;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05181v1 Announce Type: new  Abstract: Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation. Our approach carefully reduces the relative e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18919</link><description>&lt;p&gt;
Decompose-and-Compose: &#19968;&#31181;&#32452;&#21512;&#26041;&#27861;&#26469;&#20943;&#36731;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#24067;&#36716;&#31227;&#26469;&#28304;&#26159;&#22270;&#20687;&#30340;&#32452;&#25104;&#24615;&#36136;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#30830;&#23450;&#26631;&#31614;&#30340;&#20027;&#35201;&#23545;&#35937;&#25110;&#32452;&#20214;&#22806;&#65292;&#36890;&#24120;&#36824;&#23384;&#22312;&#19968;&#20123;&#20854;&#20182;&#22270;&#20687;&#32452;&#20214;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#19982;&#26631;&#31614;&#20855;&#26377;&#20266;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decompose-and-Compose&#65288;DaC&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#32452;&#21512;&#22270;&#20687;&#20803;&#32032;&#30340;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20351;&#29992;ERM&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#39640;&#24230;&#20851;&#27880;&#35201;&#20040;&#26159;&#22240;&#26524;&#32452;&#20214;&#65292;&#35201;&#20040;&#26159;&#19982;&#26631;&#31614;&#20855;&#26377;&#39640;&#20266;&#30456;&#20851;&#24615;&#30340;&#32452;&#20214;&#65288;&#23588;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18919v1 Announce Type: cross  Abstract: While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especia
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.16822</link><description>&lt;p&gt;
&#24425;&#34425;&#22242;&#38431;&#65306;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16822
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29702;&#35299;&#21644;&#22686;&#24378;&#23427;&#20204;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35782;&#21035;&#25932;&#23545;&#25552;&#31034;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25110;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24425;&#34425;&#22242;&#38431;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#24425;&#34425;&#22242;&#38431;&#23558;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#36136;&#37327; - &#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#26412;&#25991;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#30001;&#24425;&#34425;&#22242;&#38431;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
&lt;/p&gt;</description></item><item><title>CoLoRA&#36890;&#36807;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#39044;&#27979;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#28436;&#21464;&#30340;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14646</link><description>&lt;p&gt;
CoLoRA:&#29992;&#20110;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#31616;&#21270;&#38544;&#24335;&#31070;&#32463;&#24314;&#27169;&#30340;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14646
&lt;/p&gt;
&lt;p&gt;
CoLoRA&#36890;&#36807;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#39044;&#27979;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#28436;&#21464;&#30340;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;CoLoRA&#65289;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#23427;&#39044;&#20808;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#19978;&#36830;&#32493;&#22320;&#35843;&#25972;&#20302;&#31209;&#26435;&#37325;&#65292;&#20197;&#24555;&#36895;&#39044;&#27979;&#26032;&#29289;&#29702;&#21442;&#25968;&#21644;&#26032;&#21021;&#22987;&#26465;&#20214;&#19979;&#35299;&#22330;&#30340;&#28436;&#21464;&#12290;&#33258;&#36866;&#24212;&#21487;&#20197;&#26159;&#32431;&#31929;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26041;&#31243;&#39537;&#21160;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#25552;&#20379;Galerkin&#26368;&#20248;&#30340;&#36924;&#36817;&#12290;&#30001;&#20110;CoLoRA&#22312;&#26102;&#38388;&#19978;&#23616;&#37096;&#36924;&#36817;&#35299;&#22330;&#65292;&#26435;&#37325;&#30340;&#31209;&#21487;&#20197;&#20445;&#25345;&#36739;&#23567;&#65292;&#36825;&#24847;&#21619;&#30528;&#21482;&#38656;&#35201;&#31163;&#32447;&#35757;&#32451;&#20960;&#26465;&#36712;&#36857;&#65292;&#22240;&#27492;CoLoRA&#38750;&#24120;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;CoLoRA&#30340;&#39044;&#27979;&#36895;&#24230;&#24555;&#19978;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20854;&#20934;&#30830;&#24230;&#21644;&#21442;&#25968;&#25928;&#29575;&#20063;&#27604;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14646v1 Announce Type: new  Abstract: This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations. Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches.
&lt;/p&gt;</description></item><item><title>D-Flow&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#65292;&#20248;&#21270;&#28304;&#28857;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#20013;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14017</link><description>&lt;p&gt;
D-Flow: &#36890;&#36807;&#27969;&#24418;&#36827;&#34892;&#21306;&#20998;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
D-Flow: Differentiating through Flows for Controlled Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14017
&lt;/p&gt;
&lt;p&gt;
D-Flow&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#65292;&#20248;&#21270;&#28304;&#28857;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#20013;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#65288;FM&#65289;&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#21453;&#38382;&#39064;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#19968;&#33324;&#25511;&#21046;&#29983;&#25104;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D-Flow&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#36890;&#36807;&#27969;&#24418;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#28304;&#65288;&#22122;&#22768;&#65289;&#28857;&#36827;&#34892;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26469;&#28608;&#21457;&#36825;&#19968;&#26694;&#26550;&#65292;&#35813;&#35266;&#23519;&#25351;&#20986;&#65292;&#23545;&#20110;&#20351;&#29992;&#39640;&#26031;&#27010;&#29575;&#36335;&#24452;&#35757;&#32451;&#30340;&#25193;&#25955;/FM&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#21306;&#20998;&#20250;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#25237;&#24433;&#26799;&#24230;&#65292;&#23558;&#20808;&#39564;&#38544;&#24335;&#27880;&#20837;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22270;&#20687;&#21644;&#38899;&#39057;&#21453;&#38382;&#39064;&#20197;&#21450;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#22312;&#20869;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14017v1 Announce Type: new  Abstract: Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.
&lt;/p&gt;</description></item><item><title>&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12562</link><description>&lt;p&gt;
&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing and Learning with Long-term Reference Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#23545;&#24403;&#21069;&#20215;&#26684;&#30340;&#21453;&#24212;&#21463;&#21040;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#65292;&#21363;&#21442;&#32771;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#20854;&#20013;&#21442;&#32771;&#20215;&#26684;&#26159;&#21334;&#23478;&#36807;&#21435;&#25552;&#20379;&#30340;&#20215;&#26684;&#30340;&#24179;&#22343;&#20540;&#12290;&#19982;&#26356;&#24120;&#35265;&#30340;&#25351;&#25968;&#24179;&#28369;&#26426;&#21046;&#30456;&#21453;&#65292;&#22312;&#25105;&#20204;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#20013;&#65292;&#21334;&#23478;&#25552;&#20379;&#30340;&#20215;&#26684;&#23545;&#26410;&#26469;&#39038;&#23458;&#26399;&#26395;&#26377;&#26356;&#38271;&#26399;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36825;&#31181;&#26426;&#21046;&#19979;&#65292;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#19981;&#21463;&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#36825;&#31526;&#21512;&#19968;&#20010;&#24120;&#35265;&#30340;&#30452;&#35273;&#65292;&#21363;&#21334;&#23478;&#21487;&#20197;&#36890;&#36807;&#20197;&#36739;&#39640;&#30340;&#20215;&#26684;&#20986;&#21457;&#65292;&#28982;&#21518;&#36880;&#28176;&#38477;&#20302;&#20215;&#26684;&#65292;&#22240;&#20026;&#39038;&#23458;&#20250;&#35273;&#24471;&#20182;&#20204;&#27491;&#22312;&#36141;&#20080;&#36890;&#24120;&#26356;&#26114;&#36149;&#30340;&#29289;&#21697;&#19978;&#30340;&#20415;&#23452;&#36135;&#12290;&#23545;&#20110;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#30340;&#35814;&#32454;&#29305;&#24449;&#24615;&#25551;&#36848;&#20197;&#21450;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12562v1 Announce Type: new  Abstract: We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11904</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#35770;&#22363; &#25688;&#35201;: &#33258;&#21160;&#21270;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#39640;&#25910;&#20837;&#21644;&#28608;&#21169;&#20860;&#23481;&#30340;&#26426;&#21046;&#12290;&#30830;&#20445;&#20027;&#23548;&#25112;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23558;&#26426;&#21046;&#38480;&#21046;&#22312;&#20223;&#23556;&#26368;&#22823;&#21270;&#25293;&#21334;&#65288;AMAs&#65289;&#33539;&#22260;&#20869;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AMA&#30340;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65288;&#30001;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#23548;&#33268;&#65289;&#21644;&#25910;&#20837;&#30340;&#19981;&#21487;&#24494;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;AMA&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25293;&#21334;&#26426;&#21046;&#38480;&#21046;&#22312;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65288;VVCAs&#65289;&#33539;&#22260;&#20869;&#65292;&#36825;&#26159;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;AMAs&#23376;&#38598;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24182;&#34892;&#21270;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#35745;&#31639;VVCA&#30340;&#33719;&#32988;&#20998;&#37197;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#38646;&#38454;&#21644;&#19968;&#38454;&#25216;&#26415;&#30340;&#26032;&#22411;&#20248;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;VVCA&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 Announce Type: cross  Abstract: Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning. Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs). Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue. In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters. Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA. Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters. Extens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08991</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#20581;&#22766;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#36716;&#31227;&#21160;&#21147;&#23398;&#21487;&#20197;&#34987;&#23545;&#25163;&#30772;&#22351;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20581;&#22766;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36827;&#34892;&#20540;&#20989;&#25968;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#36716;&#31227;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#24773;&#20917;&#12290;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#30340;&#20449;&#24687;&#27604;&#29575;&#20316;&#20026;MLE&#30340;&#19981;&#30830;&#23450;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CR-OMLE&#30340;&#36951;&#25022;&#24230;&#20026;$ \tilde {\mathcal {O}}&#65288;\sqrt {T} + C&#65289;$&#65292;&#20854;&#20013;$ C $&#34920;&#31034;&#32463;&#36807;$ T $&#20010;&#22238;&#21512;&#21518;&#30340;&#32047;&#35745;&#30772;&#22351;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08991v1 Announce Type: cross Abstract: This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04924</link><description>&lt;p&gt;
&#20004;&#20010;&#20132;&#26131;&#19981;&#20250;&#22256;&#25200;&#65306;&#36890;&#36807;&#26500;&#36896;&#21512;&#29702;&#30340;&#26799;&#24230;&#21305;&#37197;&#26469;&#21387;&#32553;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#35757;&#32451;&#24050;&#32463;&#22312;&#22270;&#34920;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#23436;&#25972;&#30340;&#22270;&#34920;&#21387;&#32553;&#25104;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#38598;&#12290;&#23613;&#31649;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#24378;&#35843;&#26799;&#24230;&#30340;&#21305;&#37197;&#26041;&#21521;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#36712;&#36857;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#36827;&#19968;&#27493;&#30001;&#21387;&#32553;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#24322;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#32047;&#31215;&#35823;&#24046;&#65292;&#23545;&#21387;&#32553;&#22270;&#34920;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory&#65288;\textbf{CTRL}&#65289;&#30340;&#26032;&#22411;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#24067;&#30340;&#20248;&#21270;&#36215;&#28857;&#21644;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.03119</link><description>&lt;p&gt;
&#22909;&#30340;&#25945;&#24072;&#35299;&#37322;: &#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#25945;&#24072;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#24050;&#32463;&#21457;&#29616;&#23398;&#29983;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#23398;&#21040;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30456;&#20284;&#23646;&#24615;&#65292;&#22914;&#22522;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#24120;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#22240;&#20026;&#36825;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#8220;&#27491;&#30830;&#30340;&#29305;&#24449;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#21450;&#25945;&#24072;&#21644;&#23398;&#29983;&#25152;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#31616;&#21333;&#19988;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;e$^2$KD&#65289;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#22987;&#32456;&#25552;&#20379;&#20102;&#22823;&#24133;&#24230;&#30340;&#22686;&#30410;&#65292;&#65288;2&#65289;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.02866</link><description>&lt;p&gt;
&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Normalizing Flows for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#65288;&#20363;&#22914;&#27491;&#24577;&#65289;&#20998;&#24067;&#30340;&#21452;&#23556;&#26144;&#23556;&#12290;&#19968;&#26086;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#26144;&#23556;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#37327;&#23376;&#26550;&#26500;&#30340;&#26631;&#20934;&#21270;&#27969;&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#24314;&#27169;&#21644;&#20248;&#21270;&#36825;&#26679;&#30340;&#27969;&#65292;&#24182;&#22312;&#31034;&#20363;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#23396;&#31435;&#26862;&#26519;&#12289;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#25110;&#21333;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23436;&#20840;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#34987;&#23631;&#34109;&#29305;&#24449;&#65292;&#32467;&#21512;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.17052</link><description>&lt;p&gt;
&#20351;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#20877;&#27425;&#21464;&#24471;&#38750;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again. (arXiv:2401.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#34987;&#23631;&#34109;&#29305;&#24449;&#65292;&#32467;&#21512;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#28145;&#24230;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#25928;&#26524;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;Transformer&#27169;&#22411;&#23398;&#20064;&#37325;&#24314;&#8220;&#27491;&#24120;&#8221;&#26679;&#26412;&#30340;&#34987;&#23631;&#34109;&#29305;&#24449;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22522;&#20110;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited. Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression. In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data. We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \textit{normal} samples. We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample. Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.12476</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38477;&#38454;&#24314;&#27169;&#36827;&#34892;&#36125;&#21494;&#26031;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#35782;&#21035;&#21644;&#22810;&#39033;&#24335;&#22122;&#22768; (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20351;&#29992;&#38543;&#26426;&#21160;&#21147;&#27169;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27979;&#37327;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#30340;&#20284;&#28982;&#20989;&#25968;&#25152;&#38656;&#30340;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#39640;&#26031;&#28388;&#27874;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#39640;&#32500;&#31995;&#32479;&#36827;&#34892;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#31995;&#32479;&#35782;&#21035;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#25152;&#25552;&#35758;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#20030;&#20363;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#25105;&#20204;&#23558;&#36125;&#21494;&#26031;&#26041;&#27861;&#19982;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#20010;&#20856;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#27169;&#22411;&#21644;&#24102;&#26377;&#23567;&#22411;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28151;&#27788;&#21452;&#25670;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08734</link><description>&lt;p&gt;
&#25552;&#39640;&#23545;&#25239;&#36716;&#31227;&#33021;&#21147;&#30340;&#19968;&#31995;&#21015;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks to Boost Adversarial Transferability. (arXiv:2401.08734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#30333;&#30418;&#35774;&#32622;&#19979;&#29983;&#25104;&#30340;&#32431;&#31929;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#20256;&#36882;&#33021;&#21147;&#36890;&#24120;&#36739;&#20302;&#12290;&#30001;&#20110;&#23545;&#25239;&#24615;&#36716;&#31227;&#23545;&#23454;&#38469;&#24212;&#29992;&#36896;&#25104;&#26356;&#20005;&#37325;&#30340;&#23041;&#32961;&#65292;&#22240;&#27492;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36755;&#20837;&#36716;&#25442;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25915;&#20987;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#30340;&#20960;&#20010;&#24494;&#23567;&#25913;&#21464;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25915;&#20987;&#24615;&#33021;&#65292;&#20363;&#22914;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#12290;&#22522;&#20110;&#23545;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20180;&#32454;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#21160;&#37327;&#21021;&#22987;&#21270;&#12289;&#23450;&#26399;&#35843;&#25972;&#27493;&#38271;&#12289;&#23545;&#25239;&#31034;&#20363;&#12289;&#22522;&#20110;&#35889;&#30340;&#36755;&#20837;&#36716;&#25442;&#20197;&#21450;&#20960;&#31181;&#38598;&#25104;&#31574;&#30053;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely known to be vulnerable to adversarial examples. However, vanilla adversarial examples generated under the white-box setting often exhibit low transferability across different models. Since adversarial transferability poses more severe threats to practical applications, various approaches have been proposed for better transferability, including gradient-based, input transformation-based, and model-related attacks, \etc. In this work, we find that several tiny changes in the existing adversarial attacks can significantly affect the attack performance, \eg, the number of iterations and step size. Based on careful studies of existing adversarial attacks, we propose a bag of tricks to enhance adversarial transferability, including momentum initialization, scheduled step size, dual example, spectral-based input transformation, and several ensemble strategies. Extensive experiments on the ImageNet dataset validate the high effectiveness of our proposed tricks a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#20984;&#12289;&#21487;&#33021;&#19981;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#19968;&#33324;&#30340;&#32463;&#24120;&#24615;&#25277;&#26679;&#26041;&#26696;&#19979;&#65292;&#21487;&#20197;&#20197;&#26368;&#20339;&#36895;&#29575;&#25910;&#25947;&#65307;&#21516;&#26102;&#25351;&#20986;&#20102;&#25910;&#25947;&#36895;&#24230;&#19982;"&#32463;&#24120;&#24615;&#30340;&#36895;&#24230;"&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.07694</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#32463;&#24120;&#24615;&#25968;&#25454;&#25277;&#26679;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization with arbitrary recurrent data sampling. (arXiv:2401.07694v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#20984;&#12289;&#21487;&#33021;&#19981;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#19968;&#33324;&#30340;&#32463;&#24120;&#24615;&#25277;&#26679;&#26041;&#26696;&#19979;&#65292;&#21487;&#20197;&#20197;&#26368;&#20339;&#36895;&#29575;&#25910;&#25947;&#65307;&#21516;&#26102;&#25351;&#20986;&#20102;&#25910;&#25947;&#36895;&#24230;&#19982;"&#32463;&#24120;&#24615;&#30340;&#36895;&#24230;"&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#33719;&#24471;&#38543;&#26426;&#20248;&#21270;&#30340;&#26368;&#20339;&#19968;&#38454;&#25910;&#25947;&#20445;&#35777;&#65292;&#38656;&#35201;&#20351;&#29992;&#19968;&#20010;&#32463;&#24120;&#24615;&#25968;&#25454;&#25277;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#36275;&#22815;&#30340;&#39057;&#29575;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#25277;&#26679;&#12290;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#25968;&#25454;&#25277;&#26679;&#31639;&#27861;&#65288;&#22914;i.i.d.&#65292;MCMC&#65292;&#38543;&#26426;&#37325;&#25490;&#65289;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#30830;&#23454;&#26159;&#32463;&#24120;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#19968;&#31867;&#29305;&#27530;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#25105;&#20204;&#26080;&#38656;&#38500;&#20102;&#25968;&#25454;&#25277;&#26679;&#31639;&#27861;&#20013;&#30340;&#32463;&#24120;&#24615;&#20043;&#22806;&#30340;&#20219;&#20309;&#20854;&#20182;&#23646;&#24615;&#65288;&#22914;&#29420;&#31435;&#24615;&#65292;&#25351;&#25968;&#28151;&#21512;&#21644;&#37325;&#25490;&#65289;&#26469;&#20445;&#35777;&#26368;&#20339;&#30340;&#19968;&#38454;&#25910;&#25947;&#36895;&#29575;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20351;&#29992;Minimization by Incremental Surrogate Optimization (MISO)&#30340;&#27491;&#21017;&#21270;&#29256;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#20984;&#30340;&#12289;&#21487;&#33021;&#19981;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26399;&#26395;&#30340;&#26368;&#20248;&#24615;&#24046;&#24322;&#22312;&#19968;&#33324;&#30340;&#32463;&#24120;&#24615;&#25277;&#26679;&#26041;&#26696;&#19979;&#25910;&#25947;&#20110;&#26368;&#20339;&#36895;&#29575;$O(n^{-1/2})$&#12290;&#27492;&#22806;&#65292;&#26263;&#31034;&#30340;&#24120;&#25968;&#26126;&#30830;&#21462;&#20915;&#20110;"&#32463;&#24120;&#24615;&#30340;&#36895;&#24230;"&#65292;&#30001;&#25351;&#25968;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
For obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. Most commonly used data sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under mild assumptions. In this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. Namely, using regularized versions of Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes. Furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02938</link><description>&lt;p&gt;
&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35268;&#27169;&#24222;&#22823;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#36825;&#26159;&#20026;&#20102;&#24674;&#22797;&#22240;&#21024;&#38500;&#26435;&#37325;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#24573;&#30053;&#20102;&#24494;&#35843;&#65292;&#19987;&#27880;&#20110;&#39640;&#25928;&#30340;&#20462;&#21098;&#26631;&#20934;&#65292;&#35201;&#20040;&#23581;&#35797;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#65292;&#20445;&#25345;&#27599;&#20010;&#23618;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#23545;LLMs&#26469;&#35828;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#24471;&#19981;&#37319;&#29992;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#12290;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24191;&#27867;&#30340;LLMs&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/fmfi-compbio/admm-pruning&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#21270;&#30340;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#21518;&#39564;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#23450;&#20041;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#35780;&#20272;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#21487;&#20197;&#38598;&#25104;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#20197;&#20445;&#35777;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.02413</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation-Based Inference with Quantile Regression. (arXiv:2401.02413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#21270;&#30340;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#21518;&#39564;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#23450;&#20041;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#35780;&#20272;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#21487;&#20197;&#38598;&#25104;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#20197;&#20445;&#35777;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#25311;&#25512;&#26029;&#65288;Simulation-Based Inference&#65292;SBI&#65289;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#20998;&#20301;&#25968;&#20272;&#35745;&#65288;Neural Quantile Estimation&#65292;NQE&#65289;&#12290;NQE&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#23398;&#20064;&#27599;&#20010;&#21518;&#39564;&#32500;&#24230;&#30340;&#21333;&#19968;&#32500;&#24230;&#20998;&#20301;&#25968;&#65292;&#20197;&#25968;&#25454;&#21644;&#20043;&#21069;&#30340;&#21518;&#39564;&#32500;&#24230;&#20026;&#26465;&#20214;&#12290;&#21518;&#39564;&#26679;&#26412;&#36890;&#36807;&#20351;&#29992;&#21333;&#35843;&#19977;&#27425;&#22467;&#23572;&#31859;&#29305;&#26679;&#26465;&#25554;&#20540;&#39044;&#27979;&#20998;&#20301;&#25968;&#36827;&#34892;&#33719;&#21462;&#65292;&#24182;&#23545;&#23614;&#37096;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#36827;&#34892;&#20102;&#29305;&#27530;&#22788;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#36125;&#21494;&#26031;&#21487;&#20449;&#21306;&#38388;&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#20854;&#35780;&#20272;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#26368;&#39640;&#21518;&#39564;&#23494;&#24230;&#21306;&#22495;&#65288;HPDR&#65289;&#24555;&#24471;&#22810;&#12290;&#22312;&#27169;&#25311;&#39044;&#31639;&#26377;&#38480;&#21644;/&#25110;&#24050;&#30693;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23558;&#21518;&#22788;&#29702;&#25193;&#23637;&#27493;&#39588;&#38598;&#25104;&#21040;NQE&#20013;&#65292;&#20197;&#30830;&#20445;&#21518;&#39564;&#20272;&#35745;&#30340;&#26080;&#20559;&#24615;&#65292;&#19988;&#38468;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;NQE&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing broadening step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that the proposed NQE method achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;ALICE&#23454;&#39564;&#20013;&#30001;&#20110;&#19981;&#21516;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32780;&#20135;&#29983;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01905</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Machine-learning-based particle identification with missing data. (arXiv:2401.01905v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#31890;&#23376;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;ALICE&#23454;&#39564;&#20013;&#30001;&#20110;&#19981;&#21516;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32780;&#20135;&#29983;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#30340;Large Hadron Collider&#30340;ALICE&#23454;&#39564;&#33539;&#22260;&#20869;&#36827;&#34892;&#31890;&#23376;&#37492;&#21035;&#65288;PID&#65289;&#12290;&#37492;&#23450;LHC&#25552;&#20379;&#30340;&#36229;&#30456;&#23545;&#35770;&#30896;&#25758;&#20135;&#29289;&#26159;ALICE&#30340;&#20851;&#38190;&#30446;&#26631;&#20043;&#19968;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;PID&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#36873;&#25321;&#65292;&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#29702;&#35770;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25552;&#39640;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#27491;&#30830;&#30340;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#23376;&#25506;&#27979;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#21450;&#26377;&#38480;&#30340;&#25506;&#27979;&#22120;&#25928;&#29575;&#21644;&#25509;&#21463;&#24230;&#65292;&#20135;&#29983;&#30340;&#31890;&#23376;&#24182;&#19981;&#24635;&#26159;&#22312;ALICE&#30340;&#25152;&#26377;&#32452;&#20214;&#20013;&#20135;&#29983;&#20449;&#21495;&#12290;&#36825;&#23548;&#33268;&#25968;&#25454;&#20013;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26080;&#27861;&#35757;&#32451;&#36825;&#20123;&#31034;&#20363;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36339;&#36807;&#20102;&#25968;&#25454;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;PID&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel method for Particle Identification (PID) within the scope of the ALICE experiment at the Large Hadron Collider at CERN. Identifying products of ultrarelativisitc collisions delivered by the LHC is one of the crucial objectives of ALICE. Typically employed PID methods rely on hand-crafted selections, which compare experimental data to theoretical simulations. To improve the performance of the baseline methods, novel approaches use machine learning models that learn the proper assignment in a classification task. However, because of the various detection techniques used by different subdetectors, as well as the limited detector efficiency and acceptance, produced particles do not always yield signals in all of the ALICE components. This results in data with missing values. Machine learning techniques cannot be trained with such examples, so a significant part of the data is skipped during training. In this work, we propose the first method for PID that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00280</link><description>&lt;p&gt;
&#25512;&#36827;TTP&#20998;&#26512;&#65306;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#24182;&#25552;&#21319;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation. (arXiv:2401.00280v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#65292;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#27010;&#36848;&#20102;&#25915;&#20987;&#32773;&#21033;&#29992;&#28431;&#27934;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20551;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#65292;&#23545;MITRE ATT&#65286;CK&#26694;&#26550;&#20013;&#30340;TTPs&#30340;&#35299;&#37322;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#26368;&#36817;&#22312;&#30740;&#31350;&#20013;&#25506;&#32034;&#20854;&#22312;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#20013;&#30340;&#29992;&#36884;&#30340;&#28608;&#22686;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#30340;&#30097;&#38382;&#65292;&#20165;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;RoBERTa&#65289;&#21644;&#20165;&#35299;&#30721;&#22120;&#65288;&#20363;&#22914;GPT-3.5&#65289;LLMs&#23545;&#20110;&#29702;&#35299;&#21644;&#24635;&#32467;TTPs&#20197;&#36890;&#30693;&#20998;&#26512;&#20154;&#21592;&#26377;&#20851;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#39044;&#26399;&#30446;&#30340;&#65288;&#21363;&#31574;&#30053;&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#12290;&#26368;&#20808;&#36827;&#30340;LLMs&#24050;&#32463;&#26174;&#31034;&#20986;&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#65292;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#31561;&#20851;&#38190;&#39046;&#22495;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26469;&#20026;&#20165;&#35299;&#30721;&#22120;&#30340;LLMs&#25552;&#21462;&#27599;&#20010;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&amp;CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity. Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning)
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.05910</link><description>&lt;p&gt;
&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#19982;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#38750;&#22343;&#22330;&#21644;&#22312;&#32447;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05910
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#21407;&#21017;&#24615;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GPSSMs&#21464;&#20998;&#23398;&#20064;&#21644;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20248;&#21270;&#22823;&#37327;&#21464;&#20998;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#25928;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#65288;EnKF&#65289;&#65292;&#19968;&#31181;&#25104;&#29087;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#25216;&#26415;&#65292;&#32435;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#20013;&#65292;&#20197;&#36817;&#20284;&#28508;&#22312;&#29366;&#24577;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#31181;&#21033;&#29992;EnKF&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#21644;GP&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#21464;&#20998;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23545;&#22810;&#20010;&#39033;&#36827;&#34892;&#27714;&#21644;&#26469;&#30452;&#25509;&#35780;&#20272;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#36817;&#20284;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;&#65292;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17800</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interacting Diffusion Processes for Event Sequence Forecasting. (arXiv:2310.17800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#20114;&#25193;&#25955;&#36807;&#31243;&#65292;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPPs&#65289;&#24050;&#25104;&#20026;&#39044;&#27979;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#20013;&#21457;&#29983;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#20027;&#35201;&#26694;&#26550;&#65292;&#20294;&#20854;&#39034;&#24207;&#24615;&#21487;&#33021;&#20250;&#24433;&#21709;&#38271;&#26399;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#32435;&#20837;&#20854;&#20013;&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#24207;&#21015;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#22810;&#20010;&#20107;&#20214;&#31867;&#22411;&#21644;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#32500;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#25193;&#25955;&#36807;&#31243;&#32452;&#25104;&#65292;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#38388;&#38548;&#65292;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#31867;&#22411;&#12290;&#36825;&#20123;&#36807;&#31243;&#36890;&#36807;&#21508;&#33258;&#30340;&#21435;&#22122;&#20989;&#25968;&#36827;&#34892;&#20132;&#20114;&#65292;&#21487;&#20197;&#25509;&#21463;&#26469;&#33258;&#20004;&#20010;&#36807;&#31243;&#30340;&#20013;&#38388;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Temporal Point Processes (TPPs) have emerged as the primary framework for predicting sequences of events that occur at irregular time intervals, but their sequential nature can hamper performance for long-horizon forecasts. To address this, we introduce a novel approach that incorporates a diffusion generative model. The model facilitates sequence-to-sequence prediction, allowing multi-step predictions based on historical event sequences. In contrast to previous approaches, our model directly learns the joint probability distribution of types and inter-arrival times for multiple events. This allows us to fully leverage the high dimensional modeling capability of modern generative models. Our model is composed of two diffusion processes, one for the time intervals and one for the event types. These processes interact through their respective denoising functions, which can take as input intermediate representations from both processes, allowing the model to learn complex interacti
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#22122;&#22768;&#21442;&#25968;&#33539;&#22260;&#20869;&#36798;&#21040;&#36739;&#20302;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15411</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24102;&#26377;Tsybakov&#22122;&#22768;&#30340;&#21322;&#31354;&#38388;&#20027;&#21160;&#23398;&#20064;&#65306;&#19968;&#31181;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex Optimization Approach. (arXiv:2310.15411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#22122;&#22768;&#21442;&#25968;&#33539;&#22260;&#20869;&#36798;&#21040;&#36739;&#20302;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#23545;&#20110;&#20855;&#26377;Tsybakov&#22122;&#22768;&#30340;$d$&#32500;&#21322;&#31354;&#38388;&#65292;&#35745;&#31639;&#21644;&#26631;&#31614;&#30340;&#39640;&#25928;PAC&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#21463;&#21040;\cite{diakonikolas2020learning}&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24179;&#28369;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#20309;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#37117;&#20250;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#20302;&#36807;&#37327;&#35823;&#24046;&#20445;&#35777;&#30340;&#21322;&#31354;&#38388;&#12290;&#26681;&#25454;&#19978;&#36848;&#32467;&#26500;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#26631;&#31614;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$&#65292;&#22312;Tsybakov&#22122;&#22768;&#21442;&#25968;$\alpha \in (\frac13, 1]$&#30340;&#20551;&#35774;&#19979;&#65292;&#36825;&#32553;&#23567;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#39640;&#25928;&#34987;&#21160;&#25110;&#20027;&#21160;&#31639;&#27861;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of computationally and label efficient PAC active learning $d$-dimensional halfspaces with Tsybakov Noise~\citep{tsybakov2004optimal} under structured unlabeled data distributions. Inspired by~\cite{diakonikolas2020learning}, we prove that any approximate first-order stationary point of a smooth nonconvex loss function yields a halfspace with a low excess error guarantee. In light of the above structural result, we design a nonconvex optimization-based algorithm with a label complexity of $\tilde{O}(d (\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$\footnote{In the main body of this work, we use $\tilde{O}(\cdot), \tilde{\Theta}(\cdot)$ to hide factors of the form $\polylog(d, \frac{1}{\epsilon}, \frac{1}{\delta})$}, under the assumption that the Tsybakov noise parameter $\alpha \in (\frac13, 1]$, which narrows down the gap between the label complexities of the previously known efficient passive or active algorithms~\citep{diakonikolas2020polynomial,zhang2021im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.10818</link><description>&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#36328;&#20219;&#21153;&#20256;&#36882;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#26679;&#26412;&#25928;&#29575;&#23545;&#20110;&#24320;&#21457;&#23454;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#26469;&#33258;&#20808;&#21069;&#32463;&#39564;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21518;&#32487;&#29305;&#24449;&#65288;SF&#65289;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#20294;&#30456;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#27867;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#32467;&#21512;&#27169;&#22411;&#22522;&#20110;&#65288;MB&#65289;&#26041;&#27861;&#21644;SF&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22266;&#23450;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21478;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#23558;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#65288;MB-SF&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#20010;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#20219;&#21153;&#26679;&#26412;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this pa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.14928</link><description>&lt;p&gt;
&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#26377;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#27880;&#65292;&#36825;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;NtUA&#20316;&#20026;&#19968;&#20010;&#38190;&#20540;&#32531;&#23384;&#65292;&#23558;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#39044;&#27979;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#38190;&#20540;&#23545;&#36827;&#34892;&#24314;&#27169;&#12290;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#35774;&#35745;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#65292;&#36890;&#36807;&#26681;&#25454;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#23545;&#38190;&#20540;&#23545;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23545;&#25239;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#26159;&#20266;&#26631;&#31614;&#20462;&#27491;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38190;&#20540;&#23545;&#30340;&#26435;&#37325;&#26469;&#20462;&#27491;&#20266;&#26631;&#31614;&#20197;&#21450;&#32531;&#23384;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
&lt;/p&gt;</description></item><item><title>&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.12226</link><description>&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65306;&#31639;&#27861;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Smooth Nash Equilibria: Algorithms and Complexity. (arXiv:2309.12226v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12226
&lt;/p&gt;
&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#22522;&#26412;&#32570;&#28857;&#26159;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65306;&#22312;&#27491;&#21017;&#24418;&#24335;&#30340;&#21338;&#24328;&#20013;&#65292;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#26159;PPAD&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#24179;&#28369;&#20998;&#26512;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#26494;&#24347;&#21464;&#31181;&#65292;&#20854;&#20013;$\sigma$&#26159;&#20809;&#28369;&#24615;&#21442;&#25968;&#12290;&#22312;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#29609;&#23478;&#20204;&#21482;&#38656;&#35201;&#23454;&#29616;&#33267;&#23569;&#19982;&#20182;&#20204;&#26368;&#20339;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#65292;&#32780;&#36825;&#20010;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#26159;&#19981;&#20250;&#23545;&#20219;&#20309;&#22266;&#23450;&#21160;&#20316;&#20135;&#29983;&#36807;&#22810;&#36136;&#37327;&#65288;&#26681;&#25454;$\sigma$&#21442;&#25968;&#21270;&#65289;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#31181;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#21464;&#31181;&#65306;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29609;&#23478;&#20204;&#38656;&#35201;&#22312;&#22343;&#34913;&#20013;&#37319;&#29992;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#36827;&#34892;&#28216;&#25103;&#65307;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#26159;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#36824;&#26159;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#37117;&#27604;&#32435;&#20160;&#22343;&#34913;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.  We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibri
&lt;/p&gt;</description></item><item><title>RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00169</link><description>&lt;p&gt;
RepCodec:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#26631;&#35760;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RepCodec: A Speech Representation Codec for Speech Tokenization. (arXiv:2309.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00169
&lt;/p&gt;
&lt;p&gt;
RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#22312;&#23558;&#35821;&#38899;&#27880;&#20837;LLMs&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23548;&#33268;&#20102;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepCodec&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#35821;&#38899;&#26631;&#35760;&#30340;&#26032;&#22411;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#12290;&#19982;&#37325;&#26032;&#26500;&#24314;&#21407;&#22987;&#38899;&#39057;&#30340;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#19981;&#21516;&#65292;RepCodec&#36890;&#36807;&#20174;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#22914;HuBERT&#25110;data2vec&#65289;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#26469;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#12290;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;&#32534;&#35299;&#30721;&#22120;&#21644;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#20849;&#21516;&#26500;&#25104;&#19968;&#20010;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#22686;&#24378;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#65292;&#36890;&#36807;&#39030;&#28857;&#24230;&#26680;&#21644;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.07867</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#30340;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes. (arXiv:2308.07867v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#65292;&#36890;&#36807;&#39030;&#28857;&#24230;&#26680;&#21644;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#32467;&#26500;&#26680;&#35774;&#35745;&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21151;&#29575;&#27969;&#23398;&#20064;&#12290;&#35813;&#26680;&#34987;&#21629;&#21517;&#20026;&#39030;&#28857;&#24230;&#26680;&#65288;VDK&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;&#32593;&#32476;&#22270;&#25110;&#25299;&#25169;&#30340;&#30005;&#21387;&#27880;&#20837;&#20851;&#31995;&#30340;&#28508;&#22312;&#20998;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;VDK&#35774;&#35745;&#36991;&#20813;&#20102;&#38656;&#35201;&#35299;&#20915;&#26680;&#25628;&#32034;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#36739;&#23569;&#39033;&#30340;VDK&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25195;&#25551;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#26234;&#33021;&#22320;&#36873;&#25321;&#39034;&#24207;&#35757;&#32451;&#36755;&#20837;&#65292;&#21152;&#36895;VDK&#30340;&#23398;&#20064;&#12290;&#21033;&#29992;VDK&#30340;&#21487;&#21152;&#24615;&#32467;&#26500;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#23545;GP&#30340;&#39044;&#27979;&#26041;&#24046;&#36827;&#34892;&#20102;&#22359;&#19979;&#38477;&#31867;&#22411;&#30340;&#36807;&#31243;&#65292;&#20316;&#20026;&#20449;&#24687;&#22686;&#30410;&#30340;&#20195;&#29702;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;VDK-GP&#19982;&#20013;&#31561;&#35268;&#27169;500&#20010;&#33410;&#28857;&#21644;&#22823;&#35268;&#27169;1354&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;GP&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#20004;&#20493;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04428</link><description>&lt;p&gt;
&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#20803;&#23398;&#20064;&#25805;&#20316;&#31526;&#21040;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Operators to Optimality from Multi-Task Non-IID Data. (arXiv:2308.04428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#20010;&#24378;&#22823;&#27010;&#24565;&#26159;&#20174;&#24322;&#26500;&#26469;&#28304;&#25110;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#21516;&#29305;&#24449;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#20989;&#25968;&#65292;&#26082;&#26377;&#21161;&#20110;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#26377;&#21161;&#20110;&#32479;&#35745;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#23569;&#35201;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#22312;&#29702;&#35770;&#19978;&#20570;&#20986;&#36825;&#20123;&#20248;&#28857;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22122;&#22768;&#21521;&#37327;&#27979;&#37327;$y = Mx + w$&#20013;&#22238;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;$M$&#30340;&#19968;&#33324;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#21327;&#21464;&#37327;$x$&#26082;&#21487;&#20197;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#36825;&#23548;&#33268;&#22122;&#22768;&#39033;&#30340;&#32553;&#25918;&#19981;&#20877;&#26377;&#21033;&#20110;&#28304;&#20219;&#21153;&#25968;&#37327;&#12290;&#36825;&#21453;&#36807;&#26469;&#20250;&#23548;&#33268;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21463;&#21040;&#21333;&#20219;&#21153;&#25968;&#25454;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias &amp; Feature-Whiten}
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item><item><title>VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.10167</link><description>&lt;p&gt;
VITS: &#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10167
&lt;/p&gt;
&lt;p&gt;
VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20256;&#32479;&#30340;TS&#31639;&#27861;&#22312;&#27599;&#36718;&#38656;&#35201;&#20174;&#24403;&#21069;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#24182;&#25552;&#20379;&#25509;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36817;&#20284;&#25216;&#26415;&#35201;&#20040;&#20272;&#35745;&#19981;&#20934;&#30830;&#65288;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65288;MCMC&#26041;&#27861;&#65292;&#38598;&#25104;&#25277;&#26679;...&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#25512;&#29702;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;VITS&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#24182;&#19988;&#23481;&#26131;&#20174;&#20013;&#25277;&#26679;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#26159;TS&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#65292;VITS&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#19982;&#32500;&#24230;&#21644;&#22238;&#21512;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05722</link><description>&lt;p&gt;
&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#20272;&#35745;&#23725;
&lt;/p&gt;
&lt;p&gt;
Estimation of Ridge Using Nonlinear Transformation on Density Function. (arXiv:2306.05722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#23545;&#23725;&#30340;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#20197;&#25913;&#36827;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#20197;&#21450;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23725;&#22312;&#20934;&#30830;&#36817;&#20284;&#27969;&#24418;&#30340;&#22522;&#30784;&#32467;&#26500;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20985;&#38750;&#32447;&#24615;&#21464;&#25442;&#24212;&#29992;&#20110;&#23494;&#24230;&#20989;&#25968;&#20197;&#25506;&#32034;&#23725;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#30340;&#25512;&#23548;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#21464;&#25442;&#20135;&#29983;&#20102;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#12290;&#21033;&#29992;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#21464;&#20998;&#24615;&#36136;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30456;&#24212;&#23725;&#20043;&#38388;&#30340;&#20559;&#24207;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#30452;&#35266;&#22320;&#21457;&#29616;&#65292;&#36890;&#36807;Hessian&#30697;&#38453;&#30340;&#31209;&#19968;&#20462;&#25913;&#65292;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#23545;&#20999;&#31354;&#38388;&#30340;&#20272;&#35745;&#25913;&#36827;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;&#27969;&#24418;&#25311;&#21512;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21464;&#25442;&#26041;&#27861;&#24471;&#21040;&#30340;&#23725;&#22312;&#36817;&#20284;&#24213;&#23618;&#30495;&#23454;&#27969;&#24418;&#26041;&#38754;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ridges play a vital role in accurately approximating the underlying structure of manifolds. In this paper, we explore the ridge's variation by applying a concave nonlinear transformation to the density function. Through the derivation of the Hessian matrix, we observe that nonlinear transformations yield a rank-one modification of the Hessian matrix. Leveraging the variational properties of eigenvalue problems, we establish a partial order inclusion relationship among the corresponding ridges. We intuitively discover that the transformation can lead to improved estimation of the tangent space via rank-one modification of the Hessian matrix. To validate our theories, we conduct extensive numerical experiments on synthetic and real-world datasets that demonstrate the superiority of the ridges obtained from our transformed approach in approximating the underlying truth manifold compared to other manifold fitting algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07408</link><description>&lt;p&gt;
&#38754;&#21521;&#20989;&#25968;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Gradient Descent for Functional Learning. (arXiv:2305.07408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#22240;&#20854;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#20449;&#24687;&#26041;&#38754;&#30340;&#24040;&#22823;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38024;&#23545;&#26368;&#36817;&#20174;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#20135;&#29983;&#30340;&#22823;&#25968;&#25454;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;DGDFL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20247;&#22810;&#26412;&#22320;&#26426;&#22120;&#65288;&#22788;&#29702;&#22120;&#65289;&#30340;&#20989;&#25968;&#25968;&#25454;&#12290;&#22522;&#20110;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;DGDFL&#31639;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#31532;&#19968;&#20010;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#29702;&#35299;DGDFL&#30340;&#36807;&#31243;&#20013;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20840;&#38754;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#28176;&#36827;&#24335;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;GDFL&#65289;&#31639;&#27861;&#19982;&#21333;&#26426;&#27169;&#22411;&#30456;&#20851;&#32852;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#24471;&#21040;&#20102;DGDFL&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#22312;&#27491;&#21017;&#24615;&#32034;&#24341;&#19978;&#36973;&#21463;&#30340;&#39281;&#21644;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, different types of distributed learning schemes have received increasing attention for their strong advantages in handling large-scale data information. In the information era, to face the big data challenges which stem from functional data analysis very recently, we propose a novel distributed gradient descent functional learning (DGDFL) algorithm to tackle functional data across numerous local machines (processors) in the framework of reproducing kernel Hilbert space. Based on integral operator approaches, we provide the first theoretical understanding of the DGDFL algorithm in many different aspects in the literature. On the way of understanding DGDFL, firstly, a data-based gradient descent functional learning (GDFL) algorithm associated with a single-machine model is proposed and comprehensively studied. Under mild conditions, confidence-based optimal learning rates of DGDFL are obtained without the saturation boundary on the regularity index suffered in previous w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#35299;&#20915;&#20102;&#31895;&#31890;&#21270;&#27169;&#22411;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01243</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#36870;&#31895;&#31890;&#21270;&#22810;&#23610;&#24230;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Machine-Learned Invertible Coarse Graining for Multiscale Molecular Modeling. (arXiv:2305.01243v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#35299;&#20915;&#20102;&#31895;&#31890;&#21270;&#27169;&#22411;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#20998;&#23376;&#24314;&#27169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#23376;&#22312;&#22823;&#26102;&#38388;&#21644;&#38271;&#24230;&#23610;&#24230;&#19979;&#30340;&#24615;&#36136;&#12290;&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#31895;&#31890;&#21270;&#65288;CG&#65289;&#27169;&#22411;&#30340;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312; CCG &#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#20248;&#21270;&#36807;&#31243;&#23454;&#29616;&#37325;&#26500;&#65292;&#20174; CG &#27169;&#25311;&#20013;&#24674;&#22797;&#32454;&#33410;&#65292;&#36827;&#32780;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale molecular modeling is widely applied in scientific research of molecular properties over large time and length scales. Two specific challenges are commonly present in multiscale modeling, provided that information between the coarse and fine representations of molecules needs to be properly exchanged: One is to construct coarse grained (CG) models by passing information from the fine to coarse levels; the other is to restore finer molecular details given CG configurations. Although these two problems are commonly addressed independently, in this work, we present a theory connecting them, and develop a methodology called Cycle Coarse Graining (CCG) to solve both problems in a unified manner. In CCG, reconstruction can be achieved via a tractable optimization process, leading to a general method to retrieve fine details from CG simulations, which in turn, delivers a new solution to the CG problem, yielding an efficient way to calculate free energies in a rare-event-free manner
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11884</link><description>&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20854;&#40657;&#30418;&#24615;&#36136;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#21518;&#32493;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23545;&#27169;&#22411;&#20915;&#31574;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#22522;&#20934;&#24402;&#22240;&#65292;&#22240;&#27492;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#20351;&#35270;&#35273;&#26816;&#26597;&#26356;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.05037</link><description>&lt;p&gt;
&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#30340;&#37327;&#35268;&#21644;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets. (arXiv:2303.05037v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#27010;&#24565;&#19982;&#23427;&#20204;&#21463;&#27426;&#36814;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#30456;&#20284;&#65292;&#20294;&#22312;&#19968;&#38454;&#20248;&#21270;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#32447;&#24615;&#20248;&#21270;&#25110;&#25237;&#24433;&#39044;&#35328;&#26426;&#65292;&#20165;&#20351;&#29992;&#20415;&#23452;&#30340;&#19968;&#32500;&#32447;&#25628;&#32034;&#21644;&#27861;&#21521;&#37327;&#35745;&#31639;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22522;&#20110;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#30340;&#38389;&#21487;&#22827;&#26031;&#22522;&#37327;&#30340;&#26032;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65306;&#23613;&#31649;&#37327;&#35268;&#26082;&#19981;&#26159;&#24179;&#28369;&#30340;&#20063;&#19981;&#26159;&#24378;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#20102;&#35268;&#27169;&#30340;&#21152;&#24179;&#26041;&#22312;&#38598;&#21512;&#20013;&#32487;&#25215;&#20219;&#20309;&#23384;&#22312;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider feasibility and constrained optimization problems defined over smooth and/or strongly convex sets. These notions mirror their popular function counterparts but are much less explored in the first-order optimization literature. We propose new scalable, projection-free, accelerated first-order methods in these settings. Our methods avoid linear optimization or projection oracles, only using cheap one-dimensional linesearches and normal vector computations. Despite this, we derive optimal accelerated convergence guarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth problems, and accelerated linear convergence given both. Our algorithms and analysis are based on novel characterizations of the Minkowski gauge of smooth and/or strongly convex sets, which may be of independent interest: although the gauge is neither smooth nor strongly convex, we show the gauge squared inherits any structure present in the set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.13395</link><description>&lt;p&gt;
&#20351;&#29992;Davis-Yin&#20998;&#35010;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Davis-Yin&#20998;&#35010;&#26041;&#27861;&#23454;&#29616;&#26356;&#24555;&#30340;&#39044;&#27979;&#19982;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#19978;&#36731;&#26494;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#20855;&#26377;&#30456;&#20284;&#20294;&#19981;&#21516;&#21442;&#25968;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;$w$&#24182;&#38750;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#65307;&#21482;&#26377;&#19982;$w$&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;$d$&#21487;&#29992;&#12290;&#25105;&#20204;&#24456;&#23481;&#26131;&#23601;&#20250;&#24819;&#21040;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26681;&#25454;$d$&#39044;&#27979;$w$&#65292;&#20294;&#26159;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#23558;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#24615;&#19982;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#12290;&#24403;&#25152;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26102;&#65292;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#32452;&#21512;&#38382;&#39064;&#30340;&#36830;&#32493;&#25918;&#26494;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#23567;&#22411;&#38382;&#39064;&#65288;10-100&#20010;&#21464;&#37327;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#39640;&#24230;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22823;&#22411;&#38382;&#39064;&#19978;&#25193;&#23637;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#29616;&#20195;&#20984;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32593;&#32476;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#21315;&#20010;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.12195</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22320;&#35753;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#21407;&#21017;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#20854;&#26631;&#20934;&#35757;&#32451;&#33539;&#24335;&#35201;&#27714;&#23458;&#25143;&#31471;&#36890;&#36807;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#30001;&#20110;&#36825;&#20123;&#23458;&#25143;&#31471;&#36890;&#24120;&#26159;&#36793;&#32536;&#35774;&#22791;&#32780;&#19981;&#26159;&#23436;&#20840;&#21463;&#20449;&#20219;&#30340;&#65292;&#22240;&#27492;&#22312;&#23427;&#20204;&#19978;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#20250;&#20135;&#29983;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#20197;&#21450;&#30333;&#30418;&#28431;&#27934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#31216;&#20026;BAFFLE&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#25442;&#20026;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20197;&#20272;&#35745;&#26799;&#24230;&#12290;BAFFLE&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;1&#65289;&#20869;&#23384;&#25928;&#29575;&#39640;&#24182;&#19988;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65307;2&#65289;&#19982;&#20165;&#25512;&#29702;&#30828;&#20214;&#20248;&#21270;&#20197;&#21450;&#27169;&#22411;&#37327;&#21270;&#25110;&#20462;&#21098;&#20860;&#23481;&#65307;3&#65289;&#38750;&#24120;&#36866;&#21512;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#22240;&#20026;BAFFLE&#20013;&#30340;&#23458;&#25143;&#31471;&#20165;&#25191;&#34892;&#27491;&#21521;&#20256;&#25773;&#24182;&#36820;&#22238;&#19968;&#32452;&#26631;&#37327;&#21040;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;BAFFLE&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
&lt;/p&gt;</description></item></channel></rss>