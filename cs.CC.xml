<rss version="2.0"><channel><title>Chat Arxiv cs.CC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CC</description><item><title>&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.17805</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Arithmetic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17805
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#36981;&#24490;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#38480;&#20110;&#32858;&#21512;-&#32452;&#21512;GNN&#25110;&#20854;&#20182;&#29305;&#23450;&#31867;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#30005;&#36335;&#20013;&#30340;&#38376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#24120;&#25968;&#28145;&#24230;&#30005;&#36335;&#21644;&#32593;&#32476;&#23478;&#26063;&#22343;&#25104;&#31435;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#33268;&#36824;&#26159;&#38750;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#25152;&#26377;&#24120;&#35265;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
&lt;/p&gt;</description></item></channel></rss>