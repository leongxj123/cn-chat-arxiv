<rss version="2.0"><channel><title>Chat Arxiv cs.CC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CC</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.12860</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee. (arXiv:2210.12860v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#19981;&#31934;&#30830;&#27491;&#21017;&#21270;&#29275;&#39039;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#38797;&#28857;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#20110;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#33719;&#24471;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#19981;&#31934;&#30830;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#36845;&#20195;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#24182;&#19988;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010; $\epsilon$-&#38797;&#28857;&#65292;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#20026; $O(\epsilon^{-2/3})$&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#21463;&#38480;&#38388;&#38553;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#35813;&#39046;&#22495;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20108;&#38454;&#26041;&#27861;&#25910;&#25947;&#20998;&#26512;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#30028;&#24615;&#35201;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
We propose and analyze exact and inexact regularized Newton-type methods for finding a global saddle point of \emph{convex-concave} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information is much more involved. In this paper, we examine how second-order information can be used to speed up extra-gradient methods, even under inexactness. Specifically, we show that the proposed algorithms generate iterates that remain within a bounded set and the averaged iterates converge to an $\epsilon$-saddle point within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gap function. Our algorithms match the theoretically established lower bound in this context and our analysis provides a simple and intuitive convergence analysis for second-order methods without any boundedness requirements. Finally, we present a 
&lt;/p&gt;</description></item></channel></rss>