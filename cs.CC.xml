<rss version="2.0"><channel><title>Chat Arxiv cs.CC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CC</description><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#26159;&#19968;&#20010;&#24179;&#22343;&#22797;&#26434;&#24230;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.05765</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the average-case complexity of learning output distributions of quantum circuits. (arXiv:2305.05765v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#26159;&#19968;&#20010;&#24179;&#22343;&#22797;&#26434;&#24230;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#30340;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#19979;&#65292;&#35813;&#38382;&#39064;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;&#26159;&#22256;&#38590;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#28145;&#24230;&#20026;$d$&#12289;&#30001;$n$&#20010;&#37327;&#23376;&#27604;&#29305;&#26500;&#25104;&#30340;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;&#22312;&#36229;&#23545;&#25968;&#30005;&#36335;&#28145;&#24230;$d=\omega(\log(n))$&#26102;&#65292;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;&#24658;&#23450;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#23384;&#22312;&#19968;&#20010;$d=O(n)$&#65292;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#36827;&#34892;$\Omega(2^n)$&#27425;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;$O(2^{-n})$&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#22312;&#26080;&#38480;&#30005;&#36335;&#28145;&#24230;$d\to\infty$&#26102;&#65292;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892;$2^{2^{\Omega(n)}}$&#27425;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;$2^{-2^{\Omega(n)}}$&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#36741;&#21161;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;......&#65288;&#25991;&#31456;&#20869;&#23481;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
In this work, we show that learning the output distributions of brickwork random quantum circuits is average-case hard in the statistical query model. This learning model is widely used as an abstract computational model for most generic learning algorithms. In particular, for brickwork random quantum circuits on $n$ qubits of depth $d$, we show three main results:  - At super logarithmic circuit depth $d=\omega(\log(n))$, any learning algorithm requires super polynomially many queries to achieve a constant probability of success over the randomly drawn instance.  - There exists a $d=O(n)$, such that any learning algorithm requires $\Omega(2^n)$ queries to achieve a $O(2^{-n})$ probability of success over the randomly drawn instance.  - At infinite circuit depth $d\to\infty$, any learning algorithm requires $2^{2^{\Omega(n)}}$ many queries to achieve a $2^{-2^{\Omega(n)}}$ probability of success over the randomly drawn instance.  As an auxiliary result of independent interest, we show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33756;&#21333;&#25551;&#36848;&#22312;&#23637;&#31034;&#26426;&#21046;&#31574;&#30053;&#26080;&#25032;&#21487;&#20987;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#33756;&#21333;&#25551;&#36848;&#30340;&#24310;&#36831;&#25509;&#21463;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#33756;&#21333;&#25551;&#36848;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.13148</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#26080;&#25032;&#21487;&#20987;&#24615;&#30340;&#26292;&#38706;&#26426;&#21046;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Strategyproofness-Exposing Mechanism Descriptions. (arXiv:2209.13148v2 [econ.TH] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33756;&#21333;&#25551;&#36848;&#22312;&#23637;&#31034;&#26426;&#21046;&#31574;&#30053;&#26080;&#25032;&#21487;&#20987;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#33756;&#21333;&#25551;&#36848;&#30340;&#24310;&#36831;&#25509;&#21463;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#33756;&#21333;&#25551;&#36848;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33756;&#21333;&#25551;&#36848;&#20197;&#20004;&#20010;&#27493;&#39588;&#21521;&#29609;&#23478;i&#23637;&#31034;&#26426;&#21046;&#12290;&#31532;&#19968;&#27493;&#20351;&#29992;&#20854;&#20182;&#29609;&#23478;&#30340;&#25253;&#21578;&#25551;&#36848;i&#30340;&#33756;&#21333;&#65306;&#21363;i&#30340;&#28508;&#22312;&#32467;&#26524;&#38598;&#21512;&#12290;&#31532;&#20108;&#27493;&#20351;&#29992;i&#30340;&#25253;&#21578;&#20174;&#22905;&#30340;&#33756;&#21333;&#20013;&#36873;&#25321;i&#26368;&#21916;&#27426;&#30340;&#32467;&#26524;&#12290;&#33756;&#21333;&#25551;&#36848;&#33021;&#26356;&#22909;&#22320;&#26292;&#38706;&#31574;&#30053;&#26080;&#25032;&#21487;&#20987;&#24615;&#21527;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#31616;&#21333;&#24615;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#33756;&#21333;&#25551;&#36848;&#30340;&#24310;&#36831;&#25509;&#21463;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#19982;&#20854;&#20182;&#24120;&#35265;&#30340;&#21305;&#37197;&#26426;&#21046;&#30456;&#27604;&#65292;&#36825;&#31181;&#33756;&#21333;&#25551;&#36848;&#24517;&#39035;&#19982;&#30456;&#24212;&#30340;&#20256;&#32479;&#25551;&#36848;&#26377;&#30528;&#23454;&#36136;&#24615;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#26412;&#26426;&#21046;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#35777;&#26126;&#20102;&#33756;&#21333;&#25551;&#36848;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
A menu description presents a mechanism to player $i$ in two steps. Step (1) uses the reports of other players to describe $i$'s menu: the set of $i$'s potential outcomes. Step (2) uses $i$'s report to select $i$'s favorite outcome from her menu. Can menu descriptions better expose strategyproofness, without sacrificing simplicity? We propose a new, simple menu description of Deferred Acceptance. We prove that -- in contrast with other common matching mechanisms -- this menu description must differ substantially from the corresponding traditional description. We demonstrate, with a lab experiment on two elementary mechanisms, the promise and challenges of menu descriptions.
&lt;/p&gt;</description></item></channel></rss>