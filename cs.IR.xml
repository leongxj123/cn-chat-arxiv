<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#20811;&#26381;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#26102;&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.11202</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#23454;&#29616;&#26597;&#35810;&#37325;&#26500;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Scalability and Extensibility of Query Reformulation Modeling in E-commerce Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#20811;&#26381;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#26102;&#38754;&#20020;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#35265;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20851;&#32852;&#30340;&#34892;&#20026;&#25968;&#25454;&#24448;&#24448;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#26080;&#27861;&#20026;&#25628;&#32034;&#26426;&#21046;&#25552;&#20379;&#36275;&#22815;&#30340;&#25903;&#25345;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#26597;&#35810;&#37325;&#26500;&#30340;&#27010;&#24565;&#12290;&#23427;&#24314;&#35758;&#23569;&#35265;&#26597;&#35810;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#31867;&#20284;&#21547;&#20041;&#30340;&#28909;&#38376;&#23545;&#24212;&#26597;&#35810;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#37325;&#26500;&#24050;&#26174;&#31034;&#20986;&#22312;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#22686;&#24378;&#25972;&#20307;&#25910;&#20837;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#27969;&#37327;&#36739;&#20302;&#19988;&#22797;&#26434;&#30340;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36816;&#33829;&#30340;&#36739;&#23567;&#25110;&#26032;&#20852;&#20225;&#19994;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;&#37325;&#26500;&#26041;&#26696;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#21363;&#20351;&#38754;&#23545;&#26377;&#38480;&#30340;&#20132;&#26131;&#37327;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11202v1 Announce Type: new  Abstract: Customer behavioral data significantly impacts e-commerce search systems. However, in the case of less common queries, the associated behavioral data tends to be sparse and noisy, offering inadequate support to the search mechanism. To address this challenge, the concept of query reformulation has been introduced. It suggests that less common queries could utilize the behavior patterns of their popular counterparts with similar meanings. In Amazon product search, query reformulation has displayed its effectiveness in improving search relevance and bolstering overall revenue. Nonetheless, adapting this method for smaller or emerging businesses operating in regions with lower traffic and complex multilingual settings poses the challenge in terms of scalability and extensibility. This study focuses on overcoming this challenge by constructing a query reformulation solution capable of functioning effectively, even when faced with limited tra
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05967</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20998;&#22359;&#23545;&#35282;&#27491;&#20132;&#20851;&#31995;&#21644;&#30697;&#38453;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#65292;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#20851;&#31995;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#27867;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#34920;&#31034;&#20197;&#39044;&#27979;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#26059;&#36716;-based&#26041;&#27861;&#22914;RotatE&#21644;QuatE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#19982;&#23454;&#20307;&#32500;&#24230;&#25104;&#27604;&#20363;&#22320;&#22686;&#21152;&#20851;&#31995;&#22823;&#23567;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26059;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OrthogonalE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30697;&#38453;&#34920;&#31034;&#23454;&#20307;&#21644;&#22359;&#23545;&#35282;&#27491;&#20132;&#30697;&#38453;&#19982;Riemannian&#20248;&#21270;&#34920;&#31034;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;OrthogonalE&#26082;&#20855;&#26377;&#24191;&#27867;&#24615;&#21448;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#20851;&#31995;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>