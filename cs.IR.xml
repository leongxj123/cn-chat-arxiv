<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06737</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36827;&#34892;&#21518;&#35757;&#32451;&#23646;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Post-Training Attribute Unlearning in Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25512;&#33616;&#31995;&#32479;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#25512;&#33616;&#21462;&#28040;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#27169;&#22411;&#36755;&#20837;&#65292;&#20316;&#20026;&#21462;&#28040;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#26126;&#30830;&#36935;&#21040;&#65292;&#25915;&#20987;&#32773;&#20173;&#21487;&#20197;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#31169;&#20154;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26410;&#35265;&#20449;&#24687;&#31216;&#20026;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#21462;&#28040;&#23398;&#20064;&#30446;&#26631;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;AU&#65289;&#26088;&#22312;&#20351;&#30446;&#26631;&#23646;&#24615;&#38590;&#20197;&#20998;&#36776;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;AU&#30340;&#19968;&#20010;&#20005;&#26684;&#20294;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#65292;&#20854;&#20013;&#21462;&#28040;&#23398;&#20064;&#21482;&#33021;&#22312;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#25191;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;PoT-AU&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#12290;&#31532;&#19968;&#37096;&#20998;&#26159;&#21487;&#21306;&#20998;&#24615;&#25439;&#22833;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06737v1 Announce Type: new  Abstract: With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based meas
&lt;/p&gt;</description></item><item><title>LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#20026;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#26356;&#26032;&#25552;&#20379;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22806;&#31185;&#39046;&#22495;&#20013;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16664</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16664
&lt;/p&gt;
&lt;p&gt;
LLM&#36741;&#21161;&#30340;&#22810;&#25945;&#24072;&#25345;&#32493;&#23398;&#20064;&#20026;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#26356;&#26032;&#25552;&#20379;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22806;&#31185;&#39046;&#22495;&#20013;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#22312;&#20419;&#36827;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#25945;&#32946;&#26041;&#38754;&#21487;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23398;&#21592;&#30340;&#38656;&#27714;&#19981;&#26029;&#21457;&#23637;&#65292;&#27604;&#22914;&#23398;&#20064;&#26356;&#22810;&#31181;&#31867;&#30340;&#25163;&#26415;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#21450;&#20026;&#19968;&#31181;&#25163;&#26415;&#23398;&#20064;&#26032;&#30340;&#22806;&#31185;&#22120;&#26800;&#21644;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#38656;&#35201;&#36890;&#36807;&#22810;&#20010;&#36164;&#28304;&#30340;&#39034;&#24207;&#25968;&#25454;&#27969;&#25345;&#32493;&#26356;&#26032;VQA&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#22312;&#22806;&#31185;&#22330;&#26223;&#20013;&#65292;&#23384;&#20648;&#25104;&#26412;&#21644;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#36890;&#24120;&#38480;&#21046;&#20102;&#22312;&#26356;&#26032;&#27169;&#22411;&#26102;&#26087;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26080;&#26679;&#26412;&#30340;&#25345;&#32493;&#23398;&#20064;(CL)&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#22806;&#31185;&#39046;&#22495;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;i)&#26469;&#33258;&#19981;&#21516;&#31185;&#23460;&#25110;&#20020;&#24202;&#20013;&#24515;&#25910;&#38598;&#30340;&#21508;&#31181;&#22806;&#31185;&#25163;&#26415;&#30340;&#22823;&#39046;&#22495;&#36716;&#21464;&#65292;ii)&#30001;&#20110;&#22806;&#31185;&#22120;&#26800;&#25110;&#27963;&#21160;&#30340;&#19981;&#22343;&#21248;&#20986;&#29616;&#32780;&#23548;&#33268;&#30340;&#20005;&#37325;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16664v1 Announce Type: new  Abstract: Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities du
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14622</link><description>&lt;p&gt;
&#20174;&#20851;&#38190;&#35789;&#21040;&#32467;&#26500;&#21270;&#25688;&#35201;: &#31934;&#31616;&#23398;&#26415;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25991;&#24378;&#35843;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#26085;&#30410;&#37325;&#35201;&#65292;&#25351;&#20986;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#30001;&#20110;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25903;&#25345;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20202;&#34920;&#26495;&#65292;&#20197;&#24443;&#24213;&#25913;&#21464;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#65292;&#21462;&#20195;&#20256;&#32479;&#30340;&#25991;&#26412;&#23494;&#38598;&#22411;&#26041;&#27861;&#12290;&#36825;&#19968;&#24895;&#26223;&#36890;&#36807;&#19968;&#20010;&#20197;&#8220;&#20256;&#26579;&#30149;&#30340;&#32321;&#27542;&#25968;&#20272;&#35745;&#8221;&#30740;&#31350;&#20027;&#39064;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#39564;&#35777;&#24471;&#20197;&#20307;&#29616;&#65292;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#21019;&#24314;&#32467;&#26500;&#21270;&#35760;&#24405;&#20197;&#22635;&#20805;&#19968;&#20010;&#36229;&#36234;&#20851;&#38190;&#35789;&#30340;&#21518;&#31471;&#25968;&#25454;&#24211;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;https://orkg.org/usecases/r0-estimates &#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
&lt;/p&gt;</description></item></channel></rss>