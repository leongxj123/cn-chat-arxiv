<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item></channel></rss>