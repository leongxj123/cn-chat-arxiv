<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#24213;&#23618;&#35268;&#21017;&#35821;&#35328;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;</title><link>https://arxiv.org/abs/2010.13442</link><description>&lt;p&gt;
&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Purely Regular Approach to Non-Regular Core Spanners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.13442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#35268;&#21017;&#30340;&#38750;&#35268;&#21017;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#24213;&#23618;&#35268;&#21017;&#35821;&#35328;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;vset-&#33258;&#21160;&#26426;&#29305;&#24449;&#21270;&#30340;&#65292;&#23427;&#20204;&#23545;&#24182;&#38598;&#12289;&#36830;&#25509;&#21644;&#25237;&#24433;&#31561;&#20195;&#25968;&#25805;&#20316;&#23553;&#38381;&#65292;&#24182;&#20855;&#26377;&#29702;&#24819;&#30340;&#31639;&#27861;&#23646;&#24615;&#12290;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#20316;&#20026;IBM SystemT&#20013;&#26597;&#35810;&#35821;&#35328;AQL&#30340;&#26680;&#24515;&#21151;&#33021;&#30340;&#24418;&#24335;&#21270;&#24341;&#20837;&#65292;&#38500;&#20102;&#38656;&#35201;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#22806;&#65292;&#36824;&#34987;&#35777;&#26126;&#20250;&#23548;&#33268;&#38745;&#24577;&#20998;&#26512;&#21644;&#26597;&#35810;&#35780;&#20272;&#20013;&#20856;&#22411;&#38382;&#39064;&#30340;&#39640;&#22797;&#26434;&#24615;&#29978;&#33267;&#19981;&#21487;&#21028;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#26041;&#27861;&#65306;&#23558;&#23383;&#31526;&#20018;&#30456;&#31561;&#36873;&#25321;&#30452;&#25509;&#32435;&#20837;&#34920;&#31034;&#24213;&#23618;&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#35268;&#21017;&#35821;&#35328;&#20013;&#65288;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#22312;&#35268;&#21017;&#36328;&#24230;&#29983;&#25104;&#22120;&#25552;&#21462;&#30340;&#34920;&#19978;&#30340;&#20195;&#25968;&#25805;&#20316;&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#30053;&#24494;&#36739;&#24369;&#34920;&#36798;&#33021;&#21147;&#30340;&#26680;&#24515;&#36328;&#24230;&#29983;&#25104;&#22120;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The regular spanners (characterised by vset-automata) are closed under the algebraic operations of union, join and projection, and have desirable algorithmic properties. The core spanners (introduced by Fagin, Kimelfeld, Reiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core functionality of the query language AQL used in IBM's SystemT) additionally need string-equality selections and it has been shown by Freydenberger and Holldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high complexity and even undecidability of the typical problems in static analysis and query evaluation. We propose an alternative approach to core spanners: by incorporating the string-equality selections directly into the regular language that represents the underlying regular spanner (instead of treating it as an algebraic operation on the table extracted by the regular spanner), we obtain a fragment of core spanners that, while having slightly weaker expressive power t
&lt;/p&gt;</description></item></channel></rss>