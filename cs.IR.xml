<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25490;&#24207;&#25439;&#22833;&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#26799;&#24230;&#26469;&#25913;&#21892;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14144</link><description>&lt;p&gt;
&#20102;&#35299;&#24102;&#26377;&#31232;&#30095;&#29992;&#25143;&#21453;&#39304;&#30340;&#25512;&#33616;&#25490;&#24207;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Understanding the Ranking Loss for Recommendation with Sparse User Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14144
&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#25439;&#22833;&#19982;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#26799;&#24230;&#26469;&#25913;&#21892;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14144v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22312;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#65292;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;BCE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20294;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23558;BCE&#25439;&#22833;&#19982;&#25490;&#24207;&#25439;&#22833;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#25439;&#22833;&#30340;&#23436;&#25972;&#21151;&#25928;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23384;&#22312;&#31232;&#30095;&#27491;&#21453;&#39304;&#22330;&#26223;&#65288;&#22914;CTR&#39044;&#27979;&#65289;&#20013;&#19982;BCE&#25439;&#22833;&#30456;&#20851;&#30340;&#19968;&#20010;&#26032;&#25361;&#25112;&#65306;&#36127;&#26679;&#26412;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#25490;&#24207;&#25439;&#22833;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#36127;&#26679;&#26412;&#19978;&#29983;&#25104;&#26356;&#22823;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#25913;&#21892;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#24471;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14144v1 Announce Type: new  Abstract: Click-through rate (CTR) prediction holds significant importance in the realm of online advertising. While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements. However, the full efficacy of this combination loss remains incompletely understood. In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples. Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability. Our perspective is supported by extensive the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;</title><link>https://arxiv.org/abs/2403.00793</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;&#28151;&#20081;&#32780;&#32416;&#32544;&#30340;&#19990;&#30028;&#20013;&#30340;&#24191;&#21578;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ad Recommendation in a Collapsed and Entangled World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00793
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23637;&#31034;&#22914;&#20309;&#22312;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#23884;&#20837;&#34920;&#31034;&#26102;&#20445;&#30041;&#20808;&#39564;&#24320;&#22987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24207;&#21015;&#29305;&#24449;&#12289;&#25968;&#20540;&#29305;&#24449;&#12289;&#39044;&#35757;&#32451;&#23884;&#20837;&#29305;&#24449;&#20197;&#21450;&#31232;&#30095;ID&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#29305;&#24449;&#34920;&#31034;&#30456;&#20851;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#22810;&#20010;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#20248;&#21270;&#65292;&#20943;&#23569;&#20559;&#24046;&#24182;&#22686;&#24378;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20840;&#38754;&#30740;&#31350;&#29305;&#24449;&#30456;&#20851;&#24615;&#12289;&#32500;&#24230;&#22349;&#32553;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#22240;&#26524;&#25512;&#26029;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#26412;&#25991;&#35797;&#22270;&#25552;&#39640;&#35835;&#32773;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35748;&#35782;&#65292;&#24182;&#20026;&#20170;&#21518;&#30340;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11666</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22240;&#26524;&#25512;&#26029;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Inference for Recommendation. (arXiv:2303.11666v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#22240;&#26524;&#25512;&#26029;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#26412;&#25991;&#35797;&#22270;&#25552;&#39640;&#35835;&#32773;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35748;&#35782;&#65292;&#24182;&#20026;&#20170;&#21518;&#30340;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22240;&#26524;&#25512;&#26029;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#22240;&#26524;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#26524;&#25512;&#26029;&#21487;&#20197;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#22914;&#28151;&#28102;&#25928;&#24212;&#65292;&#24182;&#22788;&#29702;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#26377;&#20215;&#20540;&#30340;&#22240;&#26524;&#25512;&#33616;&#32508;&#36848;&#65292;&#20294;&#26159;&#36825;&#20123;&#32508;&#36848;&#30456;&#23545;&#23396;&#31435;&#22320;&#20171;&#32461;&#20102;&#26041;&#27861;&#65292;&#24182;&#32570;&#20047;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#30001;&#20110;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#20174;&#22240;&#26524;&#29702;&#35770;&#30340;&#35282;&#24230;&#20840;&#38754;&#23457;&#26597;&#30456;&#20851;&#30740;&#31350;&#23545;&#20110;&#25552;&#20986;&#26032;&#30340;&#23454;&#36341;&#26041;&#27861;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#65292;&#20063;&#26159;&#24517;&#35201;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#31687;&#32508;&#36848;&#35797;&#22270;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, causal inference has attracted increasing attention from researchers of recommender systems (RS), which analyzes the relationship between a cause and its effect and has a wide range of real-world applications in multiple fields. Causal inference can model the causality in recommender systems like confounding effects and deal with counterfactual problems such as offline policy evaluation and data augmentation. Although there are already some valuable surveys on causal recommendations, these surveys introduce approaches in a relatively isolated way and lack theoretical analysis of existing methods. Due to the unfamiliarity with causality to RS researchers, it is both necessary and challenging to comprehensively review the relevant studies from the perspective of causal theory, which might be instructive for the readers to propose new approaches in practice. This survey attempts to provide a systematic review of up-to-date papers in this area from a theoretical standpoint. First
&lt;/p&gt;</description></item></channel></rss>