<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17372</link><description>&lt;p&gt;
&#35757;&#32451;&#29420;&#31435;&#20110;ID&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#22120;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#35768;&#22810;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#38598;&#20013;&#22312;&#29992;&#25143;ID&#21644;&#29289;&#21697;ID&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#24863;&#30693;&#19990;&#30028;&#30340;&#26041;&#24335;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22914;&#20309;&#26500;&#24314;&#19981;&#20351;&#29992;ID&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#65288;MMSR&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#23558;&#31934;&#21326;&#25552;&#28860;&#25104;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#12290;&#27839;&#30528;&#36825;&#20123;&#32500;&#24230;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#27169;&#22411;&#35774;&#35745;&#65292;&#24182;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item></channel></rss>