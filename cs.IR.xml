<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.02059</link><description>&lt;p&gt;
IISAN&#65306;&#20351;&#29992;&#35299;&#32806;PEFT&#26377;&#25928;&#22320;&#35843;&#25972;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02059
&lt;/p&gt;
&lt;p&gt;
IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#36716;&#21464;&#24615;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#24120;&#29992;&#20110;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#20197;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20248;&#20808;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#36890;&#24120;&#24573;&#30053;GPU&#20869;&#23384;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;IISAN&#65288;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#20391;&#38754;&#36866;&#24212;&#32593;&#32476;&#65289;&#65292;&#19968;&#20010;&#20351;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#30340;&#31616;&#21333;&#21363;&#25554;&#21363;&#29992;&#26550;&#26500;&#12290;IISAN&#19982;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;GPU&#20869;&#23384;&#20351;&#29992;&#37327; - &#23545;&#20110;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;47GB&#38477;&#20302;&#21040;&#20165;3GB&#12290;&#27492;&#22806;&#65292;&#19982;FFT&#30456;&#27604;&#65292;&#23427;&#23558;&#27599;&#20010;&#26102;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;443&#31186;&#21152;&#36895;&#21040;22&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02059v1 Announce Type: new  Abstract: Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#24207;&#21015;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11879</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning. (arXiv:2303.11879v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#24207;&#21015;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#24207;&#21015;&#20132;&#20114;&#20316;&#20026;&#20027;&#35201;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#21916;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#29983;&#25104;&#19981;&#23613;&#22914;&#20154;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#24207;&#21015;&#28151;&#21512;&#65288;MSM4SR&#65289;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#65288;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MSM4SR&#23558;&#27599;&#20010;&#29289;&#21697;&#22270;&#20687;&#26631;&#35760;&#25104;&#22810;&#20010;&#25991;&#26412;&#20851;&#38190;&#35789;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#33719;&#21462;&#29289;&#21697;&#30340;&#21021;&#22987;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20197;&#28040;&#38500;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#21363;&#22810;&#27169;&#24577;&#28151;&#21512;&#24207;&#21015;&#32534;&#30721;&#22120;&#65288;M $^2$ SE&#65289;&#65292;&#23427;&#20351;&#29992;&#20114;&#34917;&#30340;&#24207;&#21015;&#28151;&#21512;&#31574;&#30053;&#26469;&#24357;&#21512;&#29289;&#21697;&#22810;&#27169;&#24577;&#20869;&#23481;&#21644;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#26469;&#24378;&#21046;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#26377;&#21306;&#20998;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24207;&#21015;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation systems utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences. However, existing methods usually generate unsatisfactory results due to the sparsity of user behavior data. To address this issue, we propose a novel pre-training framework, named Multimodal Sequence Mixup for Sequential Recommendation (MSM4SR), which leverages both users' sequential behaviors and items' multimodal content (\ie text and images) for effectively recommendation. Specifically, MSM4SR tokenizes each item image into multiple textual keywords and uses the pre-trained BERT model to obtain initial textual and visual features of items, for eliminating the discrepancy between the text and image modalities. A novel backbone network, \ie Multimodal Mixup Sequence Encoder (M$^2$SE), is proposed to bridge the gap between the item multimodal content and the user behavior, using a complementary sequence mixup strategy. In addition,
&lt;/p&gt;</description></item></channel></rss>