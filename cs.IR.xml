<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17372</link><description>&lt;p&gt;
&#35757;&#32451;&#29420;&#31435;&#20110;ID&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#22120;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#35768;&#22810;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#38598;&#20013;&#22312;&#29992;&#25143;ID&#21644;&#29289;&#21697;ID&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#24863;&#30693;&#19990;&#30028;&#30340;&#26041;&#24335;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22914;&#20309;&#26500;&#24314;&#19981;&#20351;&#29992;ID&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#65288;MMSR&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#23558;&#31934;&#21326;&#25552;&#28860;&#25104;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#12290;&#27839;&#30528;&#36825;&#20123;&#32500;&#24230;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#27169;&#22411;&#35774;&#35745;&#65292;&#24182;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13033</link><description>&lt;p&gt;
&#29992;&#36229;&#36793;&#22686;&#24378;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Real-World Complex Network Representations with Hyperedge Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22270;&#22686;&#24378;&#26041;&#27861;&#22312;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#25200;&#21160;&#22270;&#32467;&#26500;&#65292;&#36890;&#24120;&#38480;&#20110;&#25104;&#23545;&#33410;&#28857;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#20110;&#24418;&#25104;&#39640;&#38454;&#36793;&#30340;&#25968;&#25454;&#65292;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#20027;&#35201;&#34987;&#24314;&#27169;&#20026;&#31616;&#21333;&#22270;&#12290;&#22240;&#27492;&#65292;&#23558;&#39640;&#38454;&#36793;&#37325;&#26032;&#37197;&#32622;&#20026;&#22270;&#22686;&#24378;&#31574;&#30053;&#30340;&#19968;&#37096;&#20998;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36335;&#24452;&#65292;&#21487;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36793;&#22686;&#24378;&#65288;HyperAug&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#26500;&#24314;&#34394;&#25311;&#36229;&#36793;&#65292;&#24182;&#20135;&#29983;&#36741;&#21161;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 Announce Type: new  Abstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102; ChatGPT &#26159;&#21542;&#33021;&#22815;&#35780;&#20272;&#26032;&#38395;&#26426;&#26500;&#30340;&#21487;&#20449;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126; ChatGPT &#21487;&#20197;&#20026;&#19981;&#21516;&#35821;&#35328;&#21644;&#35773;&#21050;&#24615;&#36164;&#28304;&#30340;&#26032;&#38395;&#26426;&#26500;&#25552;&#20379;&#35780;&#32423;&#21450;&#20854;&#32972;&#26223;&#35828;&#26126;&#65292;&#24182;&#19988;&#36825;&#20123;&#35780;&#32423;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#32423;&#30456;&#20851;&#12290;LLMs&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#26816;&#26597;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#20449;&#24230;&#35780;&#32423;&#30340;&#32463;&#27982;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2304.00228</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#35780;&#20272;&#26032;&#38395;&#26426;&#26500;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models can rate news outlet credibility. (arXiv:2304.00228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102; ChatGPT &#26159;&#21542;&#33021;&#22815;&#35780;&#20272;&#26032;&#38395;&#26426;&#26500;&#30340;&#21487;&#20449;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126; ChatGPT &#21487;&#20197;&#20026;&#19981;&#21516;&#35821;&#35328;&#21644;&#35773;&#21050;&#24615;&#36164;&#28304;&#30340;&#26032;&#38395;&#26426;&#26500;&#25552;&#20379;&#35780;&#32423;&#21450;&#20854;&#32972;&#26223;&#35828;&#26126;&#65292;&#24182;&#19988;&#36825;&#20123;&#35780;&#32423;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#32423;&#30456;&#20851;&#12290;LLMs&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#26816;&#26597;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#20449;&#24230;&#35780;&#32423;&#30340;&#32463;&#27982;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35937;&#12290;&#29616;&#20195;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;&#26032;&#30340; Bing&#65292;&#23581;&#35797;&#36890;&#36807;&#30452;&#25509;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21306;&#20998;&#20540;&#24471;&#20449;&#36182;&#30340;&#20449;&#24687;&#28304;&#23545;&#20110;&#21521;&#29992;&#25143;&#25552;&#20379;&#36866;&#24403;&#30340;&#20934;&#30830;&#24615;&#32972;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#30693;&#21517;&#30340;LLM ChatGPT&#26159;&#21542;&#33021;&#22815;&#35780;&#20272;&#26032;&#38395;&#26426;&#26500;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#36866;&#24403;&#30340;&#25351;&#23548;&#19979;&#65292;ChatGPT&#21487;&#20197;&#20026;&#19981;&#21516;&#35821;&#35328;&#21644;&#35773;&#21050;&#24615;&#36164;&#28304;&#30340;&#26032;&#38395;&#26426;&#26500;&#25552;&#20379;&#35780;&#32423;&#21450;&#20854;&#32972;&#26223;&#35828;&#26126;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#35780;&#32423;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#32423;&#30456;&#20851;&#65288;Spearmam's $\rho=0.54, p&lt;0.001$&#65289;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#26816;&#26597;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#20449;&#24230;&#35780;&#32423;&#30340;&#32463;&#27982;&#21442;&#32771;&#12290;&#26410;&#26469;&#30340;LLMs&#24212;&#22686;&#24378;&#23427;&#20204;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations. State-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue by gathering information directly from the internet to ground their answers. In this setting, the capacity to distinguish trustworthy sources is critical for providing appropriate accuracy contexts to users. Here we assess whether ChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With appropriate instructions, ChatGPT can provide ratings for a diverse set of news outlets, including those in non-English languages and satirical sources, along with contextual explanations. Our results show that these ratings correlate with those from human experts (Spearmam's $\rho=0.54, p&lt;0.001$). These findings suggest that LLMs could be an affordable reference for credibility ratings in fact-checking applications. Future LLMs should enhance their align
&lt;/p&gt;</description></item></channel></rss>