<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item></channel></rss>