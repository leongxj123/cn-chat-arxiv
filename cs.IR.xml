<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16504</link><description>&lt;p&gt;
LARA&#65306;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#29992;&#20110;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16504
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#35821;&#35328;&#12289;&#21333;&#36718;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LARA&#65288;Linguistic-Adaptive Retrieval-Augmented Language Models&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#35821;&#35328;&#22810;&#36718;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36866;&#24212;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#20247;&#22810;&#24847;&#22270;&#12290;&#30001;&#20110;&#20250;&#35805;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#24615;&#36136;&#65292;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;LARA&#36890;&#36807;&#23558;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#32467;&#21512;&#65292;&#23884;&#20837;LLMs&#30340;&#26550;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;LARA&#33021;&#22815;&#21160;&#24577;&#21033;&#29992;&#36807;&#21435;&#30340;&#23545;&#35805;&#21644;&#30456;&#20851;&#24847;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26816;&#32034;&#25216;&#26415;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16504v1 Announce Type: new  Abstract: Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15925</link><description>&lt;p&gt;
MultiContrievers: &#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MultiContrievers: Analysis of Dense Retrieval Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#22120;&#23558;&#28304;&#25991;&#26723;&#21387;&#32553;&#20026;&#65288;&#21487;&#33021;&#26159;&#26377;&#25439;&#30340;&#65289;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;&#22833;&#21435;&#21644;&#20445;&#30041;&#30340;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#26512;&#36739;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27604;&#31264;&#23494;&#26816;&#32034;&#22120;&#25429;&#33719;&#30340;&#20449;&#24687;&#19982;&#23427;&#20204;&#22522;&#20110;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#19982;Contriever&#65289;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;25&#20010;MultiBert&#26816;&#26597;&#28857;&#20316;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#26469;&#35757;&#32451;MultiContrievers&#65292;&#36825;&#26159;&#19968;&#32452;25&#20010;contriever&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#29305;&#23450;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#21644;&#32844;&#19994;&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26723;&#30340;contriever&#21521;&#37327;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#25506;&#27979;&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#25552;&#21462;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#32467;&#26524;&#23545;&#35768;&#22810;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#27927;&#29260;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;contriever&#27169;&#22411;&#26377;&#26174;&#33879;&#22686;&#21152;&#30340;&#21487;&#25552;&#21462;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
&lt;/p&gt;</description></item><item><title>DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.11035</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65306;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#26159;&#21542;&#22312;&#26816;&#32034;&#20013;&#65311;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11035
&lt;/p&gt;
&lt;p&gt;
DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#26159;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#20013;&#30340;&#31532;&#19968;&#27493;&#12290; DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;&#23545;DPR&#24494;&#35843;&#30340;&#28145;&#20837;&#29702;&#35299;&#23558;&#38656;&#35201;&#20174;&#26681;&#26412;&#19978;&#37322;&#25918;&#35813;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#12289;&#23618;&#28608;&#27963;&#20998;&#26512;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#32452;&#21512;&#65292;&#26426;&#26800;&#22320;&#25506;&#32034;&#20102;DPR&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPR&#35757;&#32451;&#20351;&#32593;&#32476;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#21435;&#20013;&#24515;&#21270;&#65292;&#21019;&#24314;&#20102;&#35775;&#38382;&#30456;&#21516;&#20449;&#24687;&#30340;&#22810;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36825;&#31181;&#35757;&#32451;&#39118;&#26684;&#30340;&#23616;&#38480;&#24615;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#38480;&#21046;&#20102;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26816;&#32034;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23494;&#38598;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#26041;&#21521;&#65306;&#65288;1&#65289;&#26292;&#38706;DPR&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
&lt;/p&gt;</description></item></channel></rss>