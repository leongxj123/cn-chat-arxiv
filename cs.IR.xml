<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item></channel></rss>