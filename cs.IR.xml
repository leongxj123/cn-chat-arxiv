<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#32508;&#21512;&#25506;&#32034;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#36712;&#36857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.00621</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#29992;&#20110;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00621
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#25506;&#32034;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#36712;&#36857;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#20316;&#20026;&#29992;&#25143;&#21457;&#29616;&#26681;&#25454;&#20854;&#20852;&#36259;&#23450;&#21046;&#30340;&#20449;&#24687;&#25110;&#29289;&#21697;&#30340;&#26222;&#36941;&#28192;&#36947;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#21807;&#19968;ID&#21644;&#20998;&#31867;&#29305;&#24449;&#36827;&#34892;&#29992;&#25143;-&#39033;&#30446;&#21305;&#37197;&#65292;&#21487;&#33021;&#24573;&#35270;&#36328;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#21407;&#22987;&#39033;&#30446;&#20869;&#23481;&#30340;&#24494;&#22937;&#26412;&#36136;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20302;&#21033;&#29992;&#29575;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#38395;&#12289;&#38899;&#20048;&#21644;&#30701;&#35270;&#39057;&#24179;&#21488;&#31561;&#22810;&#23186;&#20307;&#26381;&#21153;&#20013;&#12290;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#24320;&#21457;&#20869;&#23481;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#20840;&#38754;&#25506;&#35752;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#12289;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00621v1 Announce Type: new  Abstract: Personalized recommendation serves as a ubiquitous channel for users to discover information or items tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in pretrained multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications to recommender systems. Furthermore, we discuss open chal
&lt;/p&gt;</description></item></channel></rss>