<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05063</link><description>&lt;p&gt;
&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Controllable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24322;&#24120;&#30340;&#26234;&#33021;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#24320;&#22987;&#25506;&#32034;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#24320;&#21019;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479; - &#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#23545;&#35805;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25511;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25972;&#21512;&#21040;LLMs&#20013;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#32452;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#26631;&#35760;&#26469;&#28304;&#20110;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#30340;&#26631;&#31614;&#65292;&#26088;&#22312;&#26126;&#30830;&#25913;&#21892;LLMs&#36981;&#24490;&#29305;&#23450;&#25512;&#33616;&#25351;&#20196;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#31243;&#24207;&#65292;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;LLMs&#22312;&#21709;&#24212;&#29992;&#25143;&#24847;&#22270;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26631;&#35760;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v1 Announce Type: cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.02875</link><description>&lt;p&gt;
&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21457;&#23637;&#31934;&#32454;&#30340;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#36890;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#38543;&#26426;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20960;&#20046;&#21482;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#27010;&#24565;&#36827;&#34892;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21512;&#25104;&#30340;&#30828;&#36127;&#25991;&#26412;&#31034;&#20363;&#12290;&#36825;&#20123;&#30828;&#36127;&#26679;&#26412;&#23545;&#24212;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#25490;&#21015;&#65292;&#23548;&#33268;&#26356;&#31934;&#32454;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InpaintCOCO&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;COCO&#22270;&#20687;&#29983;&#25104;&#30340;&#20449;&#24687;&#22635;&#20805;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#65292;&#20351;&#22270;&#20687;&#19981;&#20877;&#19982;&#20854;&#21407;&#22987;&#26631;&#39064;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.07769</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#29992;&#20110;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#20013;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation. (arXiv:2401.07769v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#27969;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#22330;&#26223;&#65292;&#31216;&#20026;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35302;&#21457;&#39033;&#26126;&#30830;&#34920;&#36798;&#20182;&#20204;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22312;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65288;&#22914;&#38463;&#37324;&#24052;&#24052;&#21644;&#20122;&#39532;&#36874;&#65289;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#26126;&#30830;&#24314;&#27169;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22240;&#27492;&#22312;TIR&#20013;&#33719;&#24471;&#27425;&#20248;&#32467;&#26524;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#21516;&#26102;&#32771;&#34385;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20173;&#26410;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#29992;&#25143;&#21521;&#19979;&#28378;&#21160;&#26102;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;--&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#65292;&#29992;&#20110;TIR&#22330;&#26223;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation has been playing a key role in many industries, e.g., e-commerce, streaming media, social media, etc. Recently, a new recommendation scenario, called Trigger-Induced Recommendation (TIR), where users are able to explicitly express their instant interests via trigger items, is emerging as an essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon. Without explicitly modeling the user's instant interest, traditional recommendation methods usually obtain sub-optimal results in TIR. Even though there are a few methods considering the trigger and target items simultaneously to solve this problem, they still haven't taken into account temporal information of user behaviors, the dynamic change of user instant interest when the user scrolls down and the interactions between the trigger and target items. To tackle these problems, we propose a novel method -- Deep Evolutional Instant Interest Network (DEI2N), for click-through rate prediction in TIR scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03531</link><description>&lt;p&gt;
&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;&#65306;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25193;&#23637;&#30001;&#23567;&#30340;&#31181;&#23376;&#23454;&#20307;&#38598;&#25551;&#36848;&#30340;&#30446;&#26631;&#35821;&#20041;&#31867;&#30340;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ESE&#26041;&#27861;&#26159;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#25552;&#21462;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#31181;&#23376;&#23454;&#20307;&#21644;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#30340;&#65292;&#23427;&#20204;&#24517;&#39035;&#36845;&#20195;&#22320;&#36941;&#21382;&#35821;&#26009;&#24211;&#21644;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#23454;&#20307;&#35789;&#27719;&#65292;&#23548;&#33268;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;ESE&#26041;&#27861;&#28040;&#32791;&#30340;&#26102;&#38388;&#19982;&#23454;&#20307;&#35789;&#27719;&#21644;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;ESE&#26694;&#26550;&#65292;Generative Entity Set Expansion (GenExpan)&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;ESE&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21069;&#32512;&#26641;&#26469;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
&lt;/p&gt;</description></item></channel></rss>