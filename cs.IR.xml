<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11480</link><description>&lt;p&gt;
&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pattern-wise Transparent Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11480
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#26469;&#35828;&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#20851;&#38190;&#39033;&#30446;&#20316;&#20026;&#20854;&#25512;&#33616;&#32467;&#26524;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#25512;&#33616;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23558;&#25972;&#20010;&#39033;&#30446;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#21152;&#31579;&#36873;&#30340;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTSR&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#23427;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20316;&#20026;&#25972;&#20010;&#25512;&#33616;&#36807;&#31243;&#30340;&#21407;&#23376;&#21333;&#20803;&#12290;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#24471;&#21040;&#37327;&#21270;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#21152;&#26435;&#26657;&#27491;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#30495;&#23454;&#20851;&#38190;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23398;&#20064;&#27169;&#24335;&#30340;&#36129;&#29486;&#12290;&#26368;&#32456;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2312.06355</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Linguistic and Structural Basis of Engineering Design Knowledge. (arXiv:2312.06355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21697;&#25551;&#36848;&#26159;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#20027;&#35201;&#36733;&#20307;&#65292;&#26082;&#26159;&#35774;&#35745;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#20063;&#26159;&#39537;&#21160;&#35774;&#35745;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#23613;&#31649;&#29289;&#21697;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#20869;&#28085;&#36827;&#34892;&#25551;&#36848;&#65292;&#20294;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#19968;&#31181;&#25551;&#36848;&#26469;&#20307;&#29616;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#36825;&#36890;&#36807;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#23433;&#25490;&#22312;&#25991;&#26412;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#25991;&#26412;&#20013;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#29983;&#25104;&#20307;&#29616;&#26126;&#30830;&#30340;&#24037;&#31243;&#35774;&#35745;&#20107;&#23454;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#29702;&#35770;&#24456;&#23569;&#33021;&#25351;&#23548;&#30446;&#21069;&#20165;&#38480;&#20110;&#26500;&#24605;&#21644;&#23398;&#20064;&#30446;&#30340;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20174;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#22823;&#26679;&#26412;&#20013;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#29702;&#35299;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artefact descriptions are the primary carriers of engineering design knowledge that is both an outcome and a driver of the design process. While an artefact could be described in different connotations, the design process requires a description to embody engineering design knowledge, which is expressed in the text through intricate placement of entities and relationships. As large-language models learn from all kinds of text merely as a sequence of characters/tokens, these are yet to generate text that embodies explicit engineering design facts. Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes. In this article, we explicate engineering design knowledge as knowledge graphs from a large sample of 33,881 patent documents. We examine the constituents of these knowledge graphs to understand the linguistic and structural basis of engineering design knowledge. In terms of linguist
&lt;/p&gt;</description></item><item><title>IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.10323</link><description>&lt;p&gt;
IncDSI&#65306;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10323
&lt;/p&gt;
&lt;p&gt;
IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#25628;&#32034;&#32034;&#24341;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#25991;&#26723;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#25991;&#26723;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#65292;&#24182;&#30452;&#25509;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#22312;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#28155;&#21152;&#26032;&#25991;&#26723;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IncDSI&#65292;&#19968;&#31181;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#30340;&#26041;&#27861;&#65288;&#27599;&#20010;&#25991;&#26723;&#32422;20-50&#27627;&#31186;&#65289;&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#65288;&#29978;&#33267;&#37096;&#20998;&#25968;&#25454;&#38598;&#65289;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#28155;&#21152;&#25991;&#26723;&#30340;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32593;&#32476;&#21442;&#25968;&#19978;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#36895;&#24230;&#26356;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;IncDSI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;G2P2&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10230</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;Prompt&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning on Graph-augmented Low-resource Text Classification. (arXiv:2307.10230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;G2P2&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#39044;&#27979;&#22312;&#32447;&#25991;&#31456;&#30340;&#20027;&#39064;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#65292;&#21363;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#26631;&#27880;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#23545;&#30417;&#30563;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#38382;&#39064;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#25991;&#26412;&#25968;&#25454;&#26412;&#36136;&#19978;&#37117;&#24314;&#31435;&#22312;&#32593;&#32476;&#32467;&#26500;&#19978;&#65292;&#20363;&#22914;&#22312;&#32447;&#25991;&#31456;&#30340;&#36229;&#38142;&#25509;/&#24341;&#29992;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#30340;&#29992;&#25143;-&#29289;&#21697;&#36141;&#20080;&#32593;&#32476;&#12290;&#36825;&#20123;&#22270;&#32467;&#26500;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph-Grounded Pre-training and Prompting (G2P2)&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20004;&#26041;&#38754;&#26041;&#27861;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22270;&#20132;&#20114;&#30340;&#23545;&#27604;&#31574;&#30053;&#65292;&#20849;&#21516;&#39044;&#35757;&#32451;&#22270;&#25991;&#27169;&#22411;&#65307;&#22312;&#19979;&#28216;&#20998;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with no or few labeled samples, presents a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore handcrafted 
&lt;/p&gt;</description></item></channel></rss>