<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25581;&#31034;&#29992;&#25143;&#36141;&#20080;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#20010;&#24615;&#21270;&#30340;&#36141;&#20080;&#21407;&#22240;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.13417</link><description>&lt;p&gt;
&#35299;&#38145;&#36141;&#20080;&#30340;&#8220;&#20026;&#20309;&#8221;&#65306;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#36141;&#20080;&#21407;&#22240;&#19982;&#21518;&#36141;&#20080;&#20307;&#39564;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13417
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25581;&#31034;&#29992;&#25143;&#36141;&#20080;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#20010;&#24615;&#21270;&#30340;&#36141;&#20080;&#21407;&#22240;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#23545;&#20110;&#25552;&#39640;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#20449;&#20219;&#21644;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#26500;&#24314;&#30495;&#27491;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#38656;&#35201;&#33021;&#38416;&#26126;&#29992;&#25143;&#20026;&#20309;&#20570;&#20986;&#36873;&#25321;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36141;&#20080;&#21407;&#22240;&#35299;&#37322;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#19968;&#20010;&#30001;&#30495;&#23454;&#29992;&#25143;&#35299;&#37322;&#20026;&#20309;&#20570;&#20986;&#26576;&#20123;&#36141;&#20080;&#20915;&#31574;&#30340;&#25991;&#26412;&#35299;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35825;&#23548;LLM&#26126;&#30830;&#21306;&#20998;&#29992;&#25143;&#35780;&#35770;&#20013;&#36141;&#20080;&#20135;&#21697;&#32972;&#21518;&#30340;&#21407;&#22240;&#21644;&#36141;&#20080;&#21518;&#30340;&#20307;&#39564;&#12290;&#33258;&#21160;&#21270;&#30340;LLM&#39537;&#21160;&#35780;&#20272;&#20197;&#21450;&#23567;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#33719;&#21462;&#39640;&#36136;&#37327;&#12289;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13417v1 Announce Type: new  Abstract: Explanations are crucial for enhancing user trust and understanding within modern recommendation systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy.   In our work, we propose a novel purchase reason explanation task. To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We benchmark this dataset on two personalized 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item></channel></rss>