<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Mamba4Rec&#26159;&#39318;&#20010;&#25506;&#32034;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#24207;&#21015;&#25512;&#33616;&#30340;&#24037;&#20316;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#25512;&#26029;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03900</link><description>&lt;p&gt;
Mamba4Rec&#65306;&#38024;&#23545;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#39640;&#25928;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03900
&lt;/p&gt;
&lt;p&gt;
Mamba4Rec&#26159;&#39318;&#20010;&#25506;&#32034;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#24207;&#21015;&#25512;&#33616;&#30340;&#24037;&#20316;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#25512;&#26029;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#26088;&#22312;&#20272;&#35745;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#21644;&#21382;&#21490;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Mamba4Rec&#65292;&#36825;&#26159;&#39318;&#20010;&#25506;&#32034;&#36873;&#25321;&#24615;SSM&#28508;&#21147;&#20197;&#23454;&#29616;&#39640;&#25928;&#24207;&#21015;&#25512;&#33616;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#22522;&#26412;&#30340;Mamba&#22359;&#26500;&#24314;&#65292;&#32467;&#21512;&#19968;&#31995;&#21015;&#39034;&#24207;&#24314;&#27169;&#25216;&#26415;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25512;&#26029;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mamba4Rec&#33021;&#22815;&#24456;&#22909;&#22320;&#22788;&#29702;&#24207;&#21015;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03900v1 Announce Type: new  Abstract: Sequential recommendation aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors. Although Transformer-based models have proven to be effective for sequential recommendation, they suffer from the inference inefficiency problem stemming from the quadratic computational complexity of attention operators, especially for long-range behavior sequences. Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential recommendation. Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we incorporate a series of sequential modeling techniques to further promote the model performance and meanwhile maintain the inference efficiency. Experiments on two public datasets demonstrate that Mamba4Rec is able to well address the effectiven
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;E2URec&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36951;&#24536;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03536</link><description>&lt;p&gt;
&#20026;&#25512;&#33616;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Effective Unlearning of Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;E2URec&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36951;&#24536;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#20135;&#29983;&#20102;&#19968;&#39033;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;LLMs&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#65288;LLMRec&#65289;&#12290; LLMRec&#30340;&#26377;&#25928;&#24615;&#28304;&#33258;LLMs&#22266;&#26377;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290; LLMRec&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#25351;&#23548;&#35843;&#25972;&#33719;&#24471;&#25512;&#33616;&#21151;&#33021;&#12290; &#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20248;&#21270;&#25928;&#29992;&#65292;LLMRec&#36824;&#24517;&#39035;&#26377;&#24847;&#24536;&#35760;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#31216;&#20026;&#25512;&#33616;&#36951;&#24536;&#12290; &#22312;LLMs&#26102;&#20195;&#65292;&#25512;&#33616;&#36951;&#24536;&#22312;\textit{&#25928;&#29575;}&#21644;\textit{&#26377;&#25928;&#24615;}&#26041;&#38754;&#20026;LLMRec&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#12290; &#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#38656;&#35201;&#26356;&#26032;LLMRec&#20013;&#25968;&#21313;&#20159;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290; &#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#24635;&#26159;&#24433;&#21709;&#27169;&#22411;&#25928;&#29992;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{E2URec}&#65292;&#31532;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03536v1 Announce Type: cross  Abstract: The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \textbf{E2URec}, the first
&lt;/p&gt;</description></item><item><title>MuGI&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#38598;&#25104;&#20197;&#25552;&#21319;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MuGI&#27169;&#22411;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;18%&#20197;&#19978;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;</title><link>http://arxiv.org/abs/2401.06311</link><description>&lt;p&gt;
MuGI:&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#22686;&#24378;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models. (arXiv:2401.06311v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06311
&lt;/p&gt;
&lt;p&gt;
MuGI&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#38598;&#25104;&#20197;&#25552;&#21319;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MuGI&#27169;&#22411;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;18%&#20197;&#19978;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#25104;&#20026;&#35821;&#35328;&#25216;&#26415;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#21147;&#37327;&#12290;&#23427;&#20204;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#30693;&#35782;&#24211;&#20351;&#20854;&#22312;&#21508;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#26041;&#38754;&#20855;&#22791;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;IR&#20013;&#30340;&#23454;&#29992;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#65288;MuGI&#65289;&#65292;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;IR&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#23548;LLM&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#36827;&#34892;&#38598;&#25104;&#20197;&#36827;&#34892;&#26816;&#32034;&#12290;&#26080;&#38656;&#35757;&#32451;&#30340;MuGI&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26597;&#35810;&#25193;&#23637;&#31574;&#30053;&#65292;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;&#36890;&#36807;MuGI&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24555;&#36895;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ran
&lt;/p&gt;</description></item><item><title>Autumn&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#38754;&#21521;&#35835;&#25805;&#20316;&#20248;&#21270;&#30340;LSM-tree&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#30456;&#37051;&#20004;&#23618;&#20043;&#38388;&#30340;&#23481;&#37327;&#27604;&#26469;&#19981;&#26029;&#25552;&#39640;&#35835;&#24615;&#33021;&#65292;&#20351;&#24471;&#28857;&#35835;&#21644;&#21306;&#38388;&#35835;&#25104;&#26412;&#20174;&#20043;&#21069;&#26368;&#20248;&#30340;$O(logN)$&#22797;&#26434;&#24230;&#20248;&#21270;&#21040;&#20102;$O(\sqrt{logN})$&#12290;</title><link>http://arxiv.org/abs/2305.05074</link><description>&lt;p&gt;
Autumn&#65306;&#22522;&#20110;LSM-tree&#30340;&#21487;&#25193;&#23637;&#30340;&#38754;&#21521;&#35835;&#25805;&#20316;&#20248;&#21270;&#30340;&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Autumn: A Scalable Read Optimized LSM-tree based Key-Value Stores with Fast Point and Range Read Speed. (arXiv:2305.05074v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05074
&lt;/p&gt;
&lt;p&gt;
Autumn&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#38754;&#21521;&#35835;&#25805;&#20316;&#20248;&#21270;&#30340;LSM-tree&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#30456;&#37051;&#20004;&#23618;&#20043;&#38388;&#30340;&#23481;&#37327;&#27604;&#26469;&#19981;&#26029;&#25552;&#39640;&#35835;&#24615;&#33021;&#65292;&#20351;&#24471;&#28857;&#35835;&#21644;&#21306;&#38388;&#35835;&#25104;&#26412;&#20174;&#20043;&#21069;&#26368;&#20248;&#30340;$O(logN)$&#22797;&#26434;&#24230;&#20248;&#21270;&#21040;&#20102;$O(\sqrt{logN})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Log Structured Merge Trees (LSM-tree)&#30340;&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#23384;&#20648;&#31995;&#32479;&#20013;&#65292;&#20197;&#25903;&#25345;&#26356;&#26032;&#12289;&#28857;&#35835;&#21644;&#21306;&#38388;&#35835;&#31561;&#21508;&#31181;&#25805;&#20316;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Autumn&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#38754;&#21521;&#35835;&#25805;&#20316;&#20248;&#21270;&#30340;&#22522;&#20110;LSM-tree&#30340;&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;&#65292;&#23427;&#20855;&#26377;&#26368;&#23569;&#30340;&#28857;&#35835;&#21644;&#21306;&#38388;&#35835;&#25104;&#26412;&#12290;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#30456;&#37051;&#20004;&#23618;&#20043;&#38388;&#30340;&#23481;&#37327;&#27604;&#26469;&#19981;&#26029;&#25552;&#39640;&#35835;&#24615;&#33021;&#65292;&#28857;&#35835;&#21644;&#21306;&#38388;&#35835;&#25104;&#26412;&#20174;&#20043;&#21069;&#26368;&#20248;&#30340;$O(logN)$&#22797;&#26434;&#24230;&#20248;&#21270;&#21040;&#20102;$O(\sqrt{logN})$&#65292;&#24182;&#24212;&#29992;&#20102;&#26032;&#30340;Garnering&#21512;&#24182;&#31574;&#30053;&#12290;Autumn&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#38754;&#21521;&#35835;&#25805;&#20316;&#20248;&#21270;&#30340;LSM-tree&#38190;&#20540;&#23384;&#20648;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Log Structured Merge Trees (LSM-tree) based key-value stores are widely used in many storage systems to support a variety of operations such as updates, point reads, and range reads. Traditionally, LSM-tree's merge policy organizes data into multiple levels of exponentially increasing capacity to support high-speed writes. However, we contend that the traditional merge policies are not optimized for reads. In this work, we present Autumn, a scalable and read optimized LSM-tree based key-value stores with minimal point and range read cost. The key idea in improving the read performance is to dynamically adjust the capacity ratio between two adjacent levels as more data are stored. As a result, smaller levels gradually increase their capacities and merge more often. In particular, the point and range read cost improves from the previous best known $O(logN)$ complexity to $O(\sqrt{logN})$ in Autumn by applying the new novel Garnering merge policy. While Garnering merge policy optimize
&lt;/p&gt;</description></item></channel></rss>