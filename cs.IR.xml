<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.14263</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions. (arXiv:2308.14263v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#21333;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#35775;&#38382;&#30340;&#38656;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#24212;&#36816;&#32780;&#29983;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#20419;&#36827;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#25991;&#29486;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20294;&#23384;&#22312;&#30528;&#20851;&#20110;&#21450;&#26102;&#24615;&#12289;&#20998;&#31867;&#20307;&#31995;&#21644;&#20840;&#38754;&#24615;&#31561;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#25216;&#26415;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28436;&#36827;&#12290;&#25991;&#31456;&#39318;&#20808;&#20174;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12289;&#26426;&#21046;&#21644;&#27169;&#22411;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#27010;&#36848;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12289;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#21644;&#24120;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users demanding access to data from various modalities. To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. Although prior literature undertook a review of the cross-modal retrieval field, it exhibits numerous deficiencies pertaining to timeliness, taxonomy, and comprehensiveness. This paper conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the paper then delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. Furthermore, it offers an overview of widel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item></channel></rss>