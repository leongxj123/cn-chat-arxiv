<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"RepPad"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22635;&#20805;&#31354;&#38388;&#26469;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06372</link><description>&lt;p&gt;
&#37325;&#22797;&#22635;&#20805;&#20316;&#20026;&#39034;&#24207;&#25512;&#33616;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Repeated Padding as Data Augmentation for Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"RepPad"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22635;&#20805;&#31354;&#38388;&#26469;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20114;&#21160;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#22312;&#35757;&#32451;&#39034;&#24207;&#27169;&#22411;&#26102;&#65292;&#22635;&#20805;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;1&#65289;&#32477;&#22823;&#22810;&#25968;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#22266;&#23450;&#38271;&#24230;&#30340;&#24207;&#21015;&#65307;2&#65289;&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#35757;&#32451;&#38656;&#35201;&#30830;&#20445;&#27599;&#20010;&#25209;&#27425;&#20013;&#30340;&#24207;&#21015;&#20855;&#26377;&#30456;&#21516;&#30340;&#38271;&#24230;&#12290;&#36890;&#24120;&#20351;&#29992;&#29305;&#27530;&#20540;0&#20316;&#20026;&#22635;&#20805;&#20869;&#23481;&#65292;&#19981;&#21253;&#21547;&#23454;&#38469;&#20449;&#24687;&#24182;&#22312;&#27169;&#22411;&#35745;&#31639;&#20013;&#34987;&#24573;&#30053;&#12290;&#36825;&#31181;&#24120;&#35782;&#22635;&#20805;&#31574;&#30053;&#24341;&#20986;&#20102;&#19968;&#20010;&#20197;&#21069;&#20174;&#26410;&#25506;&#35752;&#36807;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#22635;&#20805;&#20854;&#20182;&#20869;&#23481;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#38386;&#32622;&#36755;&#20837;&#31354;&#38388;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#65311; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#21517;&#20026;RepPad (&#37325;&#22797;&#22635;&#20805;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06372v1 Announce Type: new  Abstract: Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: \emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?}   In this paper, we propose a simple yet effective padding method called \textbf{Rep}eated \textbf{Pad}ding (\textbf{RepPad}). Specifically, we use the original interaction sequences as
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item></channel></rss>