<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17372</link><description>&lt;p&gt;
&#35757;&#32451;&#29420;&#31435;&#20110;ID&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#22120;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#25552;&#28860;&#20986;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#36825;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#35768;&#22810;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#38598;&#20013;&#22312;&#29992;&#25143;ID&#21644;&#29289;&#21697;ID&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#24863;&#30693;&#19990;&#30028;&#30340;&#26041;&#24335;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22914;&#20309;&#26500;&#24314;&#19981;&#20351;&#29992;ID&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#65288;MMSR&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#23558;&#31934;&#21326;&#25552;&#28860;&#25104;&#22235;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#39034;&#24207;&#26550;&#26500;&#12290;&#27839;&#30528;&#36825;&#20123;&#32500;&#24230;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#27169;&#22411;&#35774;&#35745;&#65292;&#24182;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;PrepRec&#65292;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#21487;&#20197;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01497</link><description>&lt;p&gt;
&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#21160;&#24577;&#30340;&#38646;-shot&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Sequential Recommendation Framework: Popularity Dynamics for Zero-shot Transfer. (arXiv:2401.01497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;PrepRec&#65292;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#21487;&#20197;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#23545;&#20110;&#22312;&#32447;&#24212;&#29992;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#35270;&#39057;&#27969;&#23186;&#20307;&#21644;&#31038;&#20132;&#23186;&#20307;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#27169;&#22411;&#26550;&#26500;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;&#27599;&#20010;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#26032;&#27169;&#22411;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#24050;&#32463;&#22312;&#38646;-shot&#25110;&#23569;-shot&#36866;&#24212;&#26032;&#24212;&#29992;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#21463;&#21040;&#21516;&#34892;AI&#39046;&#22495;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65306;PrepRec&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#27969;&#34892;&#24230;&#21160;&#24577;&#26469;&#23398;&#20064;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#12290;&#36890;&#36807;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;PrepRec&#22312;&#27809;&#26377;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#19981;&#20165;&#33021;&#22815;&#38646;-shot&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#24182;&#19988;&#19982;&#21516;&#31867;&#26368;&#20808;&#36827;&#30340;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30456;&#27604;&#65292;&#27169;&#22411;&#22823;&#23567;&#20165;&#30456;&#24403;&#19968;&#23567;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders are crucial to the success of online applications, \eg e-commerce, video streaming, and social media. While model architectures continue to improve, for every new application domain, we still have to train a new model from scratch for high quality recommendations. On the other hand, pre-trained language and vision models have shown great success in zero-shot or few-shot adaptation to new application domains. Inspired by the success of pre-trained models in peer AI fields, we propose a novel pre-trained sequential recommendation framework: PrepRec. We learn universal item representations by modeling item popularity dynamics. Through extensive experiments on five real-world datasets, we show that PrepRec, without any auxiliary information, can not only zero-shot transfer to a new domain, but achieve competitive performance compared to state-of-the-art sequential recommender models with only a fraction of the model size. In addition, with a simple post-hoc interpol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18952</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#26356;&#26032;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;(IR)&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#35821;&#26009;&#24211;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#26159;&#19981;&#26029;&#26356;&#26032;&#30340;&#12290;&#26412;&#25991;&#23558;&#30693;&#35782;&#30340;&#21160;&#24577;&#24615;&#24341;&#20837;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#23558;&#26816;&#32034;&#35270;&#20026;&#21160;&#24577;&#30340;&#30693;&#35782;&#24211;&#65292;&#26356;&#31526;&#21512;&#30495;&#23454;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21452;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#26816;&#32034;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;StreamingQA&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#26816;&#32034;&#23545;&#26032;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Dynamic Generative Retrieval (DynamicGR)&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#21457;&#29616;&#12290;&#23427;&#33021;&#22815;&#22312;&#20854;&#20869;&#37096;&#32034;&#24341;&#20013;&#39640;&#25928;&#21387;&#32553;&#26032;&#30340;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
&lt;/p&gt;</description></item></channel></rss>