<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11060</link><description>&lt;p&gt;
Persona-DB&#65306;&#29992;&#20110;&#21709;&#24212;&#39044;&#27979;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#19982;&#21327;&#21516;&#25968;&#25454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20010;&#24615;&#21270;&#20132;&#20114;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#35782;&#21035;&#29992;&#25143;&#24847;&#35265;&#21644;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#29992;&#25143;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26816;&#32034;&#38454;&#27573;&#65292;&#24182;&#23545;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#31561;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20110;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#20415;&#22312;LLM&#23450;&#21046;&#30340;&#24773;&#22659;&#19979;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Persona-DB&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#20998;&#23618;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#36328;&#20219;&#21153;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item></channel></rss>