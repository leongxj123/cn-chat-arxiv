<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03032</link><description>&lt;p&gt;
&#22270;&#22686;&#24378;&#20248;&#21270;&#22120;&#29992;&#20110;&#32467;&#26500;&#24863;&#30693;&#25512;&#33616;&#23884;&#20837;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#34394;&#25311;&#34920;&#31034;&#65292;&#24182;&#19988;&#26159;&#21518;&#32493;&#20915;&#31574;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#26356;&#26032;&#26426;&#21046;&#65292;&#31216;&#20026;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#65292;&#20197;&#40723;&#21169;&#30456;&#20851;&#33410;&#28857;&#22312;&#27599;&#19968;&#27493;&#20013;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#19982;&#36890;&#24120;&#20316;&#20026;&#20013;&#38388;&#37096;&#20998;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;SEvo&#33021;&#22815;&#30452;&#25509;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#21487;&#24573;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;SEvo&#30340;&#25910;&#25947;&#24615;&#36136;&#21450;&#20854;&#21487;&#33021;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20197;&#35777;&#26126;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;SEvo&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30697;&#20272;&#35745;&#26657;&#27491;&#30340;SEvo&#22686;&#24378;AdamW&#20013;&#65292;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#25928;&#26524;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26377;&#25928;&#25512;&#33616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
&lt;/p&gt;</description></item></channel></rss>