<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10081</link><description>&lt;p&gt;
DRAGIN&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#26816;&#32034;&#12290;&#35813;&#33539;&#24335;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#30830;&#23450;&#28608;&#27963;&#26816;&#32034;&#27169;&#22359;&#30340;&#26368;&#20339;&#26102;&#26426;&#65288;&#20915;&#23450;&#20309;&#26102;&#26816;&#32034;&#65289;&#20197;&#21450;&#19968;&#26086;&#35302;&#21457;&#26816;&#32034;&#65292;&#21046;&#23450;&#36866;&#24403;&#30340;&#26597;&#35810;&#65288;&#30830;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21160;&#24577;RAG&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#26816;&#32034;&#30340;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20915;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#30340;&#31574;&#30053;&#36890;&#24120;&#23616;&#38480;&#20110;LLM&#30340;&#26368;&#36817;&#19968;&#21477;&#25110;&#26368;&#21518;&#20960;&#20010;&#26631;&#35760;&#65292;&#32780;LLM&#30340;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DRAGIN&#65292; &#21363;&#22522;&#20110;LLMs&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item></channel></rss>