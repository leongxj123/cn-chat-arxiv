<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item></channel></rss>