<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#31995;&#32479;&#26159;&#29992;&#20110;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#24182;&#22788;&#29702;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#27169;&#22411;&#26469;&#20445;&#35777;&#20219;&#21153;&#23548;&#21521;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#30340;&#24615;&#33021;&#21644;&#20302;&#24310;&#36831;&#65292;&#20197;&#21450;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#30340;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#12290;&#35813;&#35770;&#25991;&#23545;&#20110;&#26500;&#24314;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.07647</link><description>&lt;p&gt;
GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65306;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#24314;&#31435;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#30340;&#32463;&#39564;&#19982;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GRILLBot&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#31995;&#32479;&#26159;&#29992;&#20110;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#24182;&#22788;&#29702;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#27169;&#22411;&#26469;&#20445;&#35777;&#20219;&#21153;&#23548;&#21521;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#30340;&#24615;&#33021;&#21644;&#20302;&#24310;&#36831;&#65292;&#20197;&#21450;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#30340;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#12290;&#35813;&#35770;&#25991;&#23545;&#20110;&#26500;&#24314;&#36866;&#24212;&#24615;&#20250;&#35805;&#20219;&#21153;&#21161;&#25163;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#26500;&#24314;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#23454;&#38469;&#22810;&#27169;&#24577;&#21161;&#25163;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;GRILLBot&#30340;&#23454;&#36341;&#24615;&#21644;&#25361;&#25112;&#24615;&#65292;&#35813;&#31995;&#32479;&#26159;Alexa Prize TaskBot&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21644;&#31532;&#20108;&#21517;&#30340;&#31995;&#32479;&#65288;&#20998;&#21035;&#22312;2022&#24180;&#21644;2023&#24180;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#24320;&#25918;&#21161;&#25163;&#24037;&#20855;&#21253;&#65288;OAT&#65289;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20026;&#38656;&#35201;&#38750;&#24120;&#20302;&#24310;&#36831;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#35843;&#20248;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;OAT&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32467;&#26500;&#21270;&#19988;&#21487;&#37096;&#32626;&#30340;&#26041;&#24335;&#23450;&#20041;&#20309;&#26102;&#12289;&#22914;&#20309;&#20197;&#21450;&#20351;&#29992;&#21738;&#20123;LLMs&#12290;&#23545;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#21644;&#23454;&#26102;&#20219;&#21153;&#35843;&#25972;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#20219;&#21153;&#32972;&#26223;&#21644;&#19990;&#30028;&#30693;&#35782;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#36229;&#36807;&#24310;&#36831;&#38382;&#39064;&#12290;&#23545;&#20110;&#23545;&#35805;&#29366;&#24577;&#31649;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#20855;&#26377;84&#65285;&#30340;&#26377;&#25928;&#24615;&#21644;100&#20493;&#30340;&#20302;&#24310;&#36831;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27934;&#35265;&#65292;&#24182;&#35752;&#35770;&#20102;&#26435;&#34913;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.10563</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MESc&#30340;&#20998;&#23618;&#31070;&#32463;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#35299;&#37322;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#12290;&#36890;&#36807;&#23558;&#25991;&#20214;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#20174;&#38271;&#25991;&#26723;&#20013;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21450;&#20854;&#35299;&#37322;&#24120;&#24120;&#38754;&#20020;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#32479;&#19968;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#19978;&#39044;&#27979;&#21028;&#20915;&#24182;&#25552;&#21462;&#35299;&#37322;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23558;&#36825;&#19968;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#31232;&#32570;&#26631;&#27880;&#27861;&#24459;&#25991;&#20214;&#8221;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;MESc&#65288;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26694;&#26550;&#26469;&#25506;&#32034;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#21644;&#38271;&#25991;&#26723;&#30340;&#29305;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#20010;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#35797;&#22270;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21478;&#19968;&#32452;Transformer&#32534;&#30721;&#22120;&#23618;&#23398;&#20064;&#37096;&#20998;&#20043;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
&lt;/p&gt;</description></item></channel></rss>