<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#25512;&#26029;&#20986;&#20102;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.11742</link><description>&lt;p&gt;
&#30693;&#35782;&#23548;&#33322;&#65306;&#20174;&#30740;&#31350;&#36712;&#36857;&#20013;&#25512;&#26029;&#30693;&#35782;&#30340;&#20132;&#38169;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Knowledge Navigation: Inferring the Interlocking Map of Knowledge from Research Trajectories. (arXiv:2401.11742v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#25512;&#26029;&#20986;&#20102;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#22914;&#26524;&#25105;&#30475;&#24471;&#26356;&#36828;&#65292;&#37027;&#26159;&#22240;&#20026;&#25105;&#31449;&#22312;&#24040;&#20154;&#30340;&#32937;&#33152;&#19978;&#12290;"&#33406;&#33832;&#20811;&#183;&#29275;&#39039;&#30340;&#33879;&#21517;&#22768;&#26126;&#26263;&#31034;&#20102;&#26032;&#30693;&#35782;&#24314;&#31435;&#22312;&#29616;&#26377;&#22522;&#30784;&#20043;&#19978;&#30340;&#20107;&#23454;&#65292;&#36825;&#24847;&#21619;&#30528;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#22312;&#31185;&#23398;&#20307;&#31995;&#30340;&#21382;&#21490;&#21457;&#23637;&#20013;&#19968;&#30452;&#26410;&#34987;&#25581;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#26088;&#22312;&#25512;&#26029;&#8220;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#8221;&#12290;&#36825;&#20010;&#22320;&#22270;&#26159;&#20174;&#25968;&#30334;&#19975;&#23398;&#32773;&#30340;&#30740;&#31350;&#36712;&#36857;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25512;&#26029;&#20986;&#30340;&#22320;&#22270;&#26377;&#25928;&#22320;&#21246;&#30011;&#20102;&#23398;&#31185;&#36793;&#30028;&#65292;&#24182;&#25429;&#25417;&#21040;&#20102;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20132;&#38169;&#22320;&#22270;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#23637;&#31034;&#20986;&#26469;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#31354;&#38388;&#20013;&#30340;&#22810;&#27493;&#31867;&#27604;&#25512;&#29702;&#21644;&#27010;&#24565;&#20043;&#38388;&#30340;&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
"If I have seen further, it is by standing on the shoulders of giants," Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the "knowledge interlocking map." This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. The utility of the interlocking map is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoSAM&#30340;&#33258;&#21160;&#37319;&#26679;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#38750;&#22343;&#21248;&#22788;&#29702;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#21382;&#21490;&#34892;&#20026;&#30340;&#20559;&#26012;&#20998;&#24067;&#65292;&#24182;&#37319;&#26679;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#20197;&#26500;&#24314;&#26356;&#20855;&#21487;&#27867;&#21270;&#24615;&#30340;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2311.00388</link><description>&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#37319;&#26679;&#23545;&#20110;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Sampling of User Behaviors for Sequential Recommender Systems. (arXiv:2311.00388v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoSAM&#30340;&#33258;&#21160;&#37319;&#26679;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#38750;&#22343;&#21248;&#22788;&#29702;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#21382;&#21490;&#34892;&#20026;&#30340;&#20559;&#26012;&#20998;&#24067;&#65292;&#24182;&#37319;&#26679;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#20197;&#26500;&#24314;&#26356;&#20855;&#21487;&#27867;&#21270;&#24615;&#30340;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#24403;&#21069;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#20010;&#40664;&#35748;&#35774;&#32622;&#26159;&#23558;&#27599;&#20010;&#21382;&#21490;&#34892;&#20026;&#22343;&#21248;&#22320;&#35270;&#20026;&#27491;&#21521;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#36825;&#31181;&#35774;&#32622;&#26377;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#27599;&#20010;&#21830;&#21697;&#23545;&#29992;&#25143;&#30340;&#20852;&#36259;&#26377;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#20363;&#22914;&#65292;&#36141;&#20080;&#30340;&#21830;&#21697;&#24212;&#35813;&#27604;&#28857;&#20987;&#30340;&#21830;&#21697;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#21160;&#37319;&#26679;&#26694;&#26550;&#65292;&#21517;&#20026;AutoSAM&#65292;&#29992;&#20110;&#38750;&#22343;&#21248;&#22320;&#22788;&#29702;&#21382;&#21490;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoSAM&#36890;&#36807;&#22312;&#26631;&#20934;&#30340;&#36830;&#32493;&#25512;&#33616;&#26550;&#26500;&#20013;&#22686;&#21152;&#19968;&#20010;&#37319;&#26679;&#22120;&#23618;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#21407;&#22987;&#36755;&#20837;&#30340;&#20559;&#26012;&#20998;&#24067;&#65292;&#24182;&#37319;&#26679;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#20197;&#26500;&#24314;&#26356;&#20855;&#21487;&#27867;&#21270;&#24615;&#30340;&#36830;&#32493;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#20102;&#20811;&#26381;&#38750;&#21487;&#24494;&#20998;&#37319;&#26679;&#25805;&#20316;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20915;&#31574;&#22240;&#32032;&#36827;&#34892;&#37319;&#26679;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems (SRS) have gained widespread popularity in recommendation due to their ability to effectively capture dynamic user preferences. One default setting in the current SRS is to uniformly consider each historical behavior as a positive interaction. Actually, this setting has the potential to yield sub-optimal performance, as each item makes a distinct contribution to the user's interest. For example, purchased items should be given more importance than clicked ones. Hence, we propose a general automatic sampling framework, named AutoSAM, to non-uniformly treat historical behaviors. Specifically, AutoSAM augments the standard sequential recommendation architecture with an additional sampler layer to adaptively learn the skew distribution of the raw input, and then sample informative sub-sets to build more generalizable SRS. To overcome the challenges of non-differentiable sampling actions and also introduce multiple decision factors for sampling, we further int
&lt;/p&gt;</description></item><item><title>CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11916</link><description>&lt;p&gt;
CompoDiff: &#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#22810;&#21151;&#33021;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion. (arXiv:2303.11916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11916
&lt;/p&gt;
&lt;p&gt;
CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411; CompoDiff&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#65288;CIR&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001; 1800 &#19975;&#20010;&#21442;&#32771;&#22270;&#20687;&#12289;&#26465;&#20214;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#22270;&#20687;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;CompoDiff &#19981;&#20165;&#22312;&#20687; FashionIQ &#36825;&#26679;&#30340; CIR &#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#25509;&#25910;&#21508;&#31181;&#26465;&#20214;&#65288;&#22914;&#36127;&#25991;&#26412;&#21644;&#22270;&#20687;&#36974;&#32617;&#26465;&#20214;&#65289;&#65292;&#20351;&#24471; CIR &#26356;&#21152;&#22810;&#21151;&#33021;&#65292;&#36825;&#26159;&#29616;&#26377; CIR &#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;&#27492;&#22806;&#65292;CompoDiff &#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#26435;&#37325;&#21487;&#22312; https://github.com/navervision/CompoDiff &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at https://github.com/navervision/CompoDiff
&lt;/p&gt;</description></item></channel></rss>