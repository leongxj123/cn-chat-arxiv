<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#31038;&#21306;&#24212;&#35813;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#28040;&#38500;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;&#20449;&#24687;&#26816;&#32034;&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#65292;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#26500;&#24314;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17901</link><description>&lt;p&gt;
&#25628;&#32034;&#19982;&#31038;&#20250;&#65306;&#37325;&#26032;&#26500;&#24819;&#28608;&#36827;&#26410;&#26469;&#30340;&#20449;&#24687;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Search and Society: Reimagining Information Access for Radical Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17901
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#24212;&#35813;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#28040;&#38500;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;&#20449;&#24687;&#26816;&#32034;&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#65292;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#26500;&#24314;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2403.17901v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#25216;&#26415;&#21644;&#30740;&#31350;&#27491;&#22312;&#32463;&#21382;&#21464;&#38761;&#12290;&#25105;&#20204;&#35748;&#20026;&#31038;&#21306;&#24212;&#35813;&#25235;&#20303;&#36825;&#20010;&#26426;&#20250;&#65292;&#37325;&#26032;&#23558;&#30740;&#31350;&#35758;&#31243;&#32858;&#28966;&#20110;&#31038;&#20250;&#38656;&#27714;&#65292;&#21516;&#26102;&#28040;&#38500;IR&#30340;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#36879;&#26126;&#24230;&#21644;&#36947;&#24503;&#30740;&#31350;&#19982;IR&#20854;&#20182;&#39046;&#22495;&#20043;&#38388;&#30340;&#20154;&#20026;&#38548;&#31163;&#12290;&#31038;&#21306;&#19981;&#24212;&#37319;&#21462;&#35797;&#22270;&#20943;&#36731;&#26032;&#20852;&#25216;&#26415;&#21487;&#33021;&#24102;&#26469;&#31038;&#20250;&#23475;&#22788;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#32780;&#24212;&#35813;&#31215;&#26497;&#35774;&#23450;&#30740;&#31350;&#35758;&#31243;&#65292;&#28608;&#21169;&#25105;&#20204;&#26500;&#24314;&#21508;&#31181;&#26126;&#30830;&#38472;&#36848;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#25152;&#21551;&#21457;&#30340;&#31995;&#32479;&#31867;&#22411;&#12290;&#25903;&#25745;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#31038;&#20250;&#25216;&#26415;&#24819;&#35937;&#21147;&#38656;&#35201;&#26126;&#30830;&#34920;&#36798;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#36825;&#20123;&#19981;&#21516;&#35270;&#35282;&#30340;&#32972;&#26223;&#19979;&#21457;&#23637;&#21464;&#38761;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#25351;&#23548;&#26410;&#26469;&#24819;&#35937;&#21147;&#24517;&#39035;&#21463;&#21040;&#20854;&#20182;&#23398;&#26415;&#39046;&#22495;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17901v1 Announce Type: new  Abstract: Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic field
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16504</link><description>&lt;p&gt;
LARA&#65306;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#29992;&#20110;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16504
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#35821;&#35328;&#12289;&#21333;&#36718;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LARA&#65288;Linguistic-Adaptive Retrieval-Augmented Language Models&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#35821;&#35328;&#22810;&#36718;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36866;&#24212;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#20247;&#22810;&#24847;&#22270;&#12290;&#30001;&#20110;&#20250;&#35805;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#24615;&#36136;&#65292;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;LARA&#36890;&#36807;&#23558;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#32467;&#21512;&#65292;&#23884;&#20837;LLMs&#30340;&#26550;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;LARA&#33021;&#22815;&#21160;&#24577;&#21033;&#29992;&#36807;&#21435;&#30340;&#23545;&#35805;&#21644;&#30456;&#20851;&#24847;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26816;&#32034;&#25216;&#26415;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16504v1 Announce Type: new  Abstract: Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03025</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#29702;&#35299;&#21644;&#24341;&#23548;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#26159;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#35782;&#21035;&#31561;&#20215;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22522;&#20110;&#32858;&#21512;&#30340;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#25773;&#35270;&#35282;&#26469;&#20998;&#26512;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#65292;&#24182;&#35299;&#37322;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#23547;&#25214;&#29992;&#20110;&#23545;&#23454;&#20307;&#30456;&#20284;&#24230;&#36827;&#34892;&#20256;&#25773;&#30340;&#25805;&#20316;&#31526;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#23613;&#31649;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#23384;&#22312;&#32467;&#26500;&#24322;&#36136;&#24615;&#65292;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#65292;&#36825;&#26159;&#23454;&#20307;&#23545;&#40784;&#30340;&#26680;&#24515;&#21069;&#25552;&#65292;&#20294;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#36328;&#30693;&#35782;&#22270;&#35889;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;PipEA&#65292;&#23454;&#29616;&#20102;&#25928;&#26524;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.04408</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems. (arXiv:2401.04408v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#22823;&#22411;&#23884;&#20837;&#34920;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#38656;&#35201;&#36807;&#22823;&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#20943;&#23567;&#35757;&#32451;&#26102;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861; (FIITED)&#12290;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#21516;&#65292;FIITED&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36830;&#32493;&#35843;&#25972;&#27599;&#20010;&#23884;&#20837;&#21521;&#37327;&#30340;&#32500;&#24230;&#65292;&#23558;&#26356;&#37325;&#35201;&#30340;&#23884;&#20837;&#21521;&#37327;&#20998;&#37197;&#26356;&#38271;&#30340;&#32500;&#24230;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#25968;&#25454;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21704;&#24076;&#30340;&#29289;&#29702;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#23884;&#20837;&#32500;&#24230;&#30340;&#35843;&#25972;&#24182;&#26377;&#25928;&#22320;&#33410;&#30465;&#20869;&#23384;&#12290;&#23545;&#20004;&#20010;&#34892;&#19994;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIITED&#33021;&#22815;&#23558;&#23884;&#20837;&#30340;&#22823;&#23567;&#20943;&#23567;&#36229;&#36807;65%&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#27604;&#29616;&#26377;&#30340;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#23884;&#20837;&#20462;&#21098;&#30340;&#26041;&#27861;&#33410;&#30465;&#26356;&#22810;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16605</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#22270;&#30340;&#20998;&#24067;&#40065;&#26834;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs. (arXiv:2310.16605v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Web-DRO&#65292;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#22312;&#23545;&#27604;&#35757;&#32451;&#26399;&#38388;&#37325;&#26032;&#21152;&#26435;&#30340;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#32593;&#32476;&#22270;&#38142;&#25509;&#24182;&#23545;&#38170;&#28857;-&#25991;&#26723;&#23545;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#26469;&#37325;&#26032;&#21152;&#26435;&#19981;&#21516;&#30340;&#38170;&#28857;-&#25991;&#26723;&#23545;&#32676;&#32452;&#65292;&#36825;&#25351;&#23548;&#27169;&#22411;&#23558;&#26356;&#22810;&#26435;&#37325;&#20998;&#37197;&#32473;&#23545;&#27604;&#25439;&#22833;&#26356;&#39640;&#30340;&#32676;&#32452;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#22312;MS MARCO&#21644;BEIR&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Web-DRO&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#23545;&#32858;&#31867;&#25216;&#26415;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#35777;&#23454;&#20102;&#32676;&#32452;&#26435;&#37325;&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#20102;&#19968;&#33268;&#30340;&#27169;&#22411;&#20559;&#22909;&#20197;&#21450;&#23545;&#26377;&#20215;&#20540;&#25991;&#26723;&#30340;&#26377;&#25928;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable 
&lt;/p&gt;</description></item><item><title>EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08891</link><description>&lt;p&gt;
EHI: &#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#30340;&#23618;&#27425;&#32034;&#24341;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval. (arXiv:2310.08891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08891
&lt;/p&gt;
&lt;p&gt;
EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#23884;&#20837;&#24335;&#26816;&#32034;&#29616;&#24050;&#25104;&#20026;&#35821;&#20041;&#25628;&#32034;&#21644;&#25490;&#21517;&#38382;&#39064;&#30340;&#34892;&#19994;&#26631;&#20934;&#65292;&#22914;&#33719;&#21462;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#32593;&#32476;&#25991;&#26723;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;(a)&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120;&#20197;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#20197;&#21450;(b)&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#20197;&#26597;&#25214;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20284;&#25991;&#26723;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#26159;&#19981;&#30456;&#20132;&#30340;&#65307;&#23398;&#24471;&#30340;&#23884;&#20837;&#21487;&#33021;&#19981;&#36866;&#21512;ANNS&#26041;&#27861;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31471;&#21040;&#31471;&#23618;&#27425;&#32034;&#24341;(EHI)&#30340;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;EHI&#20351;&#29992;&#26631;&#20934;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#20498;&#25490;&#25991;&#20214;&#32034;&#24341;(IVF)&#39118;&#26684;&#30340;&#26641;&#29366;&#32467;&#26500;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;ANNS&#12290;&#20026;&#20102;&#30830;&#20445;&#31163;&#25955;&#22522;&#20110;&#26641;&#30340;ANNS&#32467;&#26500;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#23398;&#20064;&#65292;EHI&#24341;&#20837;&#20102;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#30340;&#27010;&#24565;&#65292;&#29992;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that capture
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00976</link><description>&lt;p&gt;
&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#21487;&#20197;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#36335;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22914;&#20849;&#21516;&#37051;&#23621;&#65288;CN&#65289;&#25152;&#36229;&#36234;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65306;&#23613;&#31649;MPNN&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32534;&#30721;&#38142;&#36335;&#39044;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65288;&#22914;CN&#65289;&#26041;&#38754;&#21017;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#65292;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#30830;&#23454;&#21487;&#20197;&#25429;&#25417;&#21040;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MPNN&#22312;&#36817;&#20284;CN&#21551;&#21457;&#24335;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#28040;&#24687;&#20256;&#36882;&#38142;&#36335;&#39044;&#27979;&#22120;&#65288;MPLP&#65289;&#12290;MPLP&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#25417;&#32467;&#26500;&#29305;&#24449;&#33021;&#22815;&#25913;&#21892;&#38142;&#36335;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
&lt;/p&gt;</description></item></channel></rss>