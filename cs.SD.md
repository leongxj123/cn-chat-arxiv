# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Streaming Sequence Transduction through Dynamic Compression](https://rss.arxiv.org/abs/2402.01172) | STAR是一种新型的Transformer模型，通过动态压缩和优化延迟、内存占用和质量，实现对流的高效序列转导，并在自动语音识别领域表现出色。 |

# 详细

[^1]: 流式序列转导通过动态压缩

    Streaming Sequence Transduction through Dynamic Compression

    [https://rss.arxiv.org/abs/2402.01172](https://rss.arxiv.org/abs/2402.01172)

    STAR是一种新型的Transformer模型，通过动态压缩和优化延迟、内存占用和质量，实现对流的高效序列转导，并在自动语音识别领域表现出色。

    

    我们引入了STAR（带有锚定表示的流式转导），这是一种基于Transformer的新型模型，旨在实现对流的高效序列转导。STAR动态地对输入流进行分段，创建压缩的锚定表示，实现近乎无损的压缩（12倍）在自动语音识别（ASR）中，并优于现有方法。此外，STAR在同时进行语音到文本任务中展示出优越的分割和延迟-质量折衷，优化延迟、内存占用和质量。

    We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
    

