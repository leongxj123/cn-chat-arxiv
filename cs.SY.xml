<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.04272</link><description>&lt;p&gt;
&#23398;&#20064;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#65288;LQ&#65289;&#21338;&#24328;&#22312;&#26368;&#20248;&#25511;&#21046;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#65288;i&#65289;&#39118;&#38505;&#25935;&#24863;&#25110;&#40065;&#26834;&#25511;&#21046;&#30340;&#21160;&#24577;&#21338;&#24328;&#24418;&#24335;&#65292;&#25110;&#32773;&#65288;ii&#65289;&#20316;&#20026;&#36830;&#32493;&#29366;&#24577;-&#25511;&#21046;&#31354;&#38388;&#20013;&#20004;&#20010;&#31454;&#20105;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#35774;&#32622;&#12290;&#19982;&#24191;&#27867;&#30740;&#31350;&#30340;&#21333;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#19981;&#21516;&#65292;&#38646;&#21644;LQ&#21338;&#24328;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#32570;&#20047;&#24378;&#21046;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38750;&#20985;&#26368;&#23567;-&#26368;&#22823;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#20013;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#65292;&#20197;&#36798;&#21040;Nash&#22343;&#34913;&#30340;&#949;-&#37051;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#29702;&#24819;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01312</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control. (arXiv:2307.01312v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23454;&#39564;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35843;&#25972;PID&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#23384;&#22312;&#65292;&#23454;&#38469;&#31995;&#32479;&#65288;&#22914;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#65289;&#38656;&#35201;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;PID&#25511;&#21046;&#22120;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#12290;&#37319;&#29992;&#20102;&#22686;&#37327;&#24335;PID&#25511;&#21046;&#22120;&#65292;&#24182;&#20165;&#23545;&#21487;&#21464;&#22686;&#30410;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#20026;&#20102;&#35843;&#25972;&#21160;&#24577;&#22686;&#30410;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;Actor-Critic&#28151;&#21512;&#31070;&#32463;&#32467;&#26500;&#65292;&#33021;&#22815;&#36866;&#24403;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#21516;&#26102;&#20805;&#24403;&#26368;&#20339;&#35782;&#21035;&#22120;&#12290;&#22312;&#35843;&#25972;&#21644;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#21644;Sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#65288;ADAM&#65289;&#20248;&#21270;&#22120;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagatio
&lt;/p&gt;</description></item></channel></rss>