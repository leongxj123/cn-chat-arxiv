<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01758</link><description>&lt;p&gt;
ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21270;&#65306;&#26368;&#20339;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#31995;&#32479;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;&#36755;&#20837;&#20449;&#24687;&#65292;&#24182;&#20197;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23545;&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#30005;&#32593;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#26159;&#22312;&#24378;&#21046;&#38480;&#21046;&#21516;&#19968;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#35299;&#20915;&#30340;&#21464;&#37327;&#12290;&#36825;&#23558;&#20250;&#20135;&#29983;&#19968;&#20010;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65307;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#20351;&#36825;&#31867;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#26032;&#20852;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32447;&#24615;&#21270;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#24191;&#27867;&#20351;&#29992;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#26412;&#25991;&#24320;&#21457;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22235;&#31181;&#36866;&#29992;&#20110;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a
&lt;/p&gt;</description></item></channel></rss>