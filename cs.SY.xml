<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20803;-&#24378;&#21270;&#23398;&#20064;RNN&#26550;&#26500;&#12289;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;RFLO&#23616;&#37096;&#22312;&#32447;&#23398;&#20064;&#65292;&#25104;&#21151;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;BPTT&#25110;RTRL&#26367;&#20195;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24182;&#19981;&#33021;&#25552;&#39640;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2311.04830</link><description>&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Real-Time Recurrent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20803;-&#24378;&#21270;&#23398;&#20064;RNN&#26550;&#26500;&#12289;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;RFLO&#23616;&#37096;&#22312;&#32447;&#23398;&#20064;&#65292;&#25104;&#21151;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;BPTT&#25110;RTRL&#26367;&#20195;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24182;&#19981;&#33021;&#25552;&#39640;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892;&#27714;&#35299;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#26041;&#27861;&#12290;RTRRL&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#20010;&#20803;-&#24378;&#21270;&#23398;&#20064;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#65292;&#29420;&#31435;&#23454;&#29616;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65307;&#65288;2&#65289;&#19968;&#20010;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#21644;&#33655;&#20848;&#36164;&#26684;&#36861;&#36394;&#26469;&#35757;&#32451;&#20803;-&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65307;&#21644;&#65288;3&#65289;&#38543;&#26426;&#21453;&#39304;&#23616;&#37096;&#22312;&#32447;&#65288;RFLO&#65289;&#23398;&#20064;&#65292;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#21442;&#25968;&#26799;&#24230;&#30340;&#22312;&#32447;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#26367;&#25442;&#20026;&#29983;&#29289;&#19981;&#21512;&#29702;&#30340;&#26102;&#24310;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#25110;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#24182;&#19981;&#33021;&#25913;&#21892;&#22238;&#25253;&#65292;&#21516;&#26102;&#22312;&#21305;&#37197;BPTT&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#20250;&#22686;&#21152;&#36820;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04830v2 Announce Type: replace  Abstract: In this paper we propose real-time recurrent reinforcement learning (RTRRL), a biologically plausible approach to solving discrete and continuous control tasks in partially-observable markov decision processes (POMDPs). RTRRL consists of three parts: (1) a Meta-RL RNN architecture, implementing on its own an actor-critic algorithm; (2) an outer reinforcement learning algorithm, exploiting temporal difference learning and dutch eligibility traces to train the Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an online automatic differentiation algorithm for computing the gradients with respect to parameters of the network.Our experimental results show that by replacing the optimization algorithm in RTRRL with the biologically implausible back propagation through time (BPTT), or real-time recurrent learning (RTRL), one does not improve returns, while matching the computational complexity for BPTT, and even increasi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).</title><link>http://arxiv.org/abs/2304.08845</link><description>&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08845
&lt;/p&gt;
&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340; $\textit{&#30452;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20250;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#19968;&#30452;&#20351;&#29992;&#21407;&#22987;&#32422;&#26463;&#12290;&#23427;&#20204;&#25110;&#32773;&#32570;&#20047;&#31574;&#30053;&#36845;&#20195;&#26399;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25110;&#32773;&#36973;&#36935;&#19981;&#21487;&#34892;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;&#65288;FPI&#65289;&#30340; $\textit{&#38388;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#22495;&#30001;&#19968;&#20010;&#21483;&#20570;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#21487;&#34892;&#24615;&#20989;&#25968;&#34920;&#31034;&#12290;FPI &#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#30340;&#21306;&#22495;&#24615;&#31574;&#30053;&#26356;&#26032;&#35268;&#21017;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270; CDF&#12290;&#36825;&#20010;&#26356;&#26032;&#35268;&#21017;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#30830;&#20445;&#21487;&#34892;&#22495;&#21333;&#35843;&#22320;&#25193;&#23637;&#65292;&#29366;&#24577;&#20540;&#20989;&#25968;&#21333;&#35843;&#22320;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
&lt;/p&gt;</description></item></channel></rss>