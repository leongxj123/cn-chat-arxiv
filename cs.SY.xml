<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15780</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#65292;&#21253;&#25324;&#37027;&#20123;&#30452;&#25509;&#28041;&#21450;&#20154;&#31867;&#30340;&#39046;&#22495;&#65292;&#24179;&#31561;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#30028;&#24840;&#21457;&#31361;&#20986;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#23548;&#21521;&#26041;&#27861;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#25506;&#35752;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#24615;&#33021;&#20248;&#21270;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q-Learning&#31639;&#27861;&#65292;&#21033;&#29992;&#20854;&#25910;&#25947;&#20445;&#35777;&#26469;&#30830;&#20445;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31449;&#28857;&#31867;&#21035;&#65288;&#20013;&#24515;&#12289;&#36793;&#32536;&#21644;&#36828;&#31243;&#65289;&#20043;&#38388;&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#36890;&#36807;&#22522;&#23612;&#31995;&#25968;&#26469;&#34913;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15780v1 Announce Type: cross  Abstract: As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through stra
&lt;/p&gt;</description></item></channel></rss>