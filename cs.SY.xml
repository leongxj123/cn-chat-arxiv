<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.08448</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#32773;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic Physics-informed Neural Lyapunov Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#21487;&#35777;&#20445;&#35777;&#30340;&#31283;&#23450;&#21270;&#20219;&#21153;&#25511;&#21046;&#31574;&#30053;&#26159;&#38750;&#32447;&#24615;&#25511;&#21046;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20851;&#38190;&#30340;&#24615;&#33021;&#25351;&#26631;&#26159;&#20135;&#29983;&#21306;&#22495;&#21560;&#24341;&#21147;&#30340;&#22823;&#23567;&#65292;&#36825;&#22522;&#26412;&#19978;&#20805;&#24403;&#20102;&#23553;&#38381;&#29615;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#24377;&#24615;&#8220;&#36793;&#30028;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#21450;&#20854;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20135;&#29983;&#30340;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#21516;&#26102;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#35813;&#26041;&#31243;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#32473;&#23450;&#25511;&#21046;&#31574;&#30053;&#30340;&#30495;&#23454;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#28436;&#21592;&#35780;&#35770;&#32773;&#27169;&#24335;&#65292;&#25105;&#20204;&#22312;&#25913;&#36827;&#25511;&#21046;&#31574;&#30053;&#65288;&#28436;&#21592;&#65289;&#21644;&#23398;&#20064;&#31062;&#21338;&#22827;&#20989;&#25968;&#65288;&#35780;&#35770;&#32773;&#65289;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#29992;SMT&#27714;&#35299;&#22120;&#35745;&#31639;&#20986;&#26368;&#22823;&#30340;&#21487;&#35777;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08448v1 Announce Type: new  Abstract: Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness "margin" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09640</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20572;&#30005;&#20943;&#36731;
&lt;/p&gt;
&lt;p&gt;
Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20026;&#20102;&#38450;&#27490;&#40657;&#26263;&#27169;&#24335;&#32780;&#22312;&#31995;&#32479;&#24322;&#24120;&#26102;&#36827;&#34892;&#24207;&#21015;&#35774;&#35745;&#30340;&#34917;&#25937;&#25511;&#21046;&#34892;&#21160;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#32771;&#34385;&#31995;&#32479;&#31283;&#23450;&#24615;&#38271;&#26399;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#24207;&#21015;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#31163;&#25955;&#20540;&#20256;&#36755;&#32447;&#24320;&#20851;&#20915;&#31574;&#65288;&#32447;&#36335;&#37325;&#26032;&#36830;&#25509;&#21644;&#31227;&#38500;&#65289;&#21644;&#36830;&#32493;&#20540;&#21457;&#30005;&#26426;&#35843;&#25972;&#30340;&#25511;&#21046;&#34892;&#21160;&#31354;&#38388;&#12290;&#20026;&#20102;&#30830;&#23450;&#26377;&#25928;&#30340;&#20572;&#30005;&#20943;&#36731;&#31574;&#30053;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;&#19982;&#30005;&#21147;&#20256;&#36755;&#32593;&#32476;&#30456;&#20851;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26399;&#38388;&#30340;&#25506;&#32034;&#12290;&#20351;&#29992;&#24320;&#28304;Grid2Op&#24179;&#21488;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23558;&#29289;&#29702;&#20449;&#21495;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-gu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.01674</link><description>&lt;p&gt;
&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#32463;&#27982;&#65289;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;&#65288;e&#65289;NMPC&#65289;&#38656;&#35201;&#22312;&#25152;&#26377;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#37117;&#20855;&#26377;&#36275;&#22815;&#20934;&#30830;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#24517;&#39035;&#35745;&#31639;&#25104;&#26412;&#36275;&#22815;&#20302;&#20197;&#30830;&#20445;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26426;&#21046;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#65288;e&#65289;NMPC&#30340;&#35745;&#31639;&#36127;&#25285;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#36776;&#35782;&#20197;&#22312;&#27169;&#25311;&#26679;&#26412;&#19978;&#33719;&#24471;&#26368;&#22823;&#24179;&#22343;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#23454;&#38469;&#65288;e&#65289;NMPC&#30340;&#19968;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#20339;&#65288;e&#65289;NMPC&#24615;&#33021;&#30340;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#33391;&#22909;&#24179;&#34913;&#30340;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#25605;&#25292;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
&lt;/p&gt;</description></item></channel></rss>