<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#30456;&#32467;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#20351;&#29992;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04080</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#30456;&#32467;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#20351;&#29992;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35757;&#32451;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#31574;&#30053;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#26680;&#24515;&#26159;&#19968;&#20010;&#22343;&#20540;&#22238;&#24402;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#23427;&#23558;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#22312;&#29615;&#22659;&#29366;&#24577;&#26465;&#20214;&#19979;&#20351;&#29992;&#30456;&#24212;&#30340;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#31867;&#20284;&#20110;&#20856;&#22411;&#30340;&#25193;&#25955;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;SDE&#26377;&#35299;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#23427;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#23545;&#25968;&#27010;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#29109;&#27491;&#21017;&#39033;&#65292;&#25913;&#36827;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#26469;&#33258;&#20998;&#24067;&#22806;&#25968;&#25454;&#28857;&#30340;&#19981;&#20934;&#30830;&#20540;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#20197;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#36890;&#36807;&#23558;&#29109;&#27491;&#21017;&#21270;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#32467;&#21512;&#24212;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;\href{https://github.com/ruoqizzz/Entro}{https://github.com/ruoqizzz/Entro}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entro
&lt;/p&gt;</description></item></channel></rss>