<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01467</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20869;&#20986;&#29616;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21306;&#22495;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#22238;&#25918;&#29616;&#35937;&#26159;&#21542;&#33021;&#22815;&#22312;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#20013;&#33258;&#28982;&#20135;&#29983;&#65311;&#22914;&#26524;&#26159;&#30340;&#35805;&#65292;&#23427;&#26159;&#21542;&#23545;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20219;&#21153;&#20248;&#21270;&#30340;&#33539;&#24335;&#19979;&#21457;&#29616;&#20102;&#22238;&#25918;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#27169;&#22411;&#27169;&#25311;&#20102;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#23618;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#27807;&#36890;&#21644;&#24863;&#35273;&#30382;&#23618;&#30340;&#36755;&#20837;&#12290;&#28023;&#39532;&#20307;&#20013;&#30340;&#22238;&#25918;&#26159;&#30001;&#20110;&#24773;&#26223;&#35760;&#24518;&#12289;&#35748;&#30693;&#22320;&#22270;&#20197;&#21450;&#29615;&#22659;&#35266;&#23519;&#32780;&#20135;&#29983;&#30340;&#65292;&#19982;&#21160;&#29289;&#23454;&#39564;&#25968;&#25454;&#30456;&#20284;&#65292;&#24182;&#19988;&#26159;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#36824;&#25104;&#21151;&#22320;&#37325;&#29616;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#22238;&#25918;&#65292;&#19982;&#20154;&#31867;&#23454;&#39564;&#25968;&#25454;&#30456;&#31526;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17364</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;LQR&#20803;&#31574;&#30053;&#20272;&#35745;&#30340;Moreau&#21253;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Moreau Envelope Approach for LQR Meta-Policy Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#26102;&#19981;&#21464;&#31163;&#25955;&#26102;&#38388;&#19981;&#30830;&#23450;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#31574;&#30053;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#30001;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#26377;&#38480;&#23454;&#29616;&#26500;&#24314;&#65292;&#20197;&#23450;&#20041;&#19968;&#20010;&#23545;&#26032;&#23454;&#29616;&#26377;&#25928;&#35843;&#25972;&#30340;&#20803;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25214;&#21040;&#20803;LQR&#25104;&#26412;&#20989;&#25968;&#30340;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#23454;&#29616;&#30340;&#32447;&#24615;&#31995;&#32479;&#19978;&#32988;&#36807;&#20102;&#25511;&#21046;&#22120;&#30340;&#26420;&#32032;&#24179;&#22343;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17364v1 Announce Type: cross  Abstract: We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item></channel></rss>