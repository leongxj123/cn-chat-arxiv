<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2111.10933</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#21487;&#20197;&#36229;&#36234;&#38598;&#20013;&#24335;&#19978;&#32622;&#20449;&#30028;&#38480;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20551;&#35774;N&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#38754;&#23545;&#30528;&#19968;&#32452;&#20849;&#21516;&#30340;M&#20010;&#33218;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#33218;&#22870;&#21169;&#20998;&#24067;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#20174;&#37051;&#23621;&#22788;&#25509;&#25910;&#20449;&#24687;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#30001;&#19968;&#20010;&#26080;&#21521;&#22270;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;KL-UCB&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#23454;&#29616;&#27604;&#20854;&#21333;&#19968;&#26234;&#33021;&#20307;&#30456;&#24212;&#31639;&#27861;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#33267;&#23569;&#26377;&#19968;&#20010;&#37051;&#23621;&#65292;&#32780;&#19988;&#26234;&#33021;&#20307;&#26377;&#36234;&#22810;&#30340;&#37051;&#23621;&#65292;&#21518;&#24724;&#20540;&#20250;&#36234;&#22909;&#65292;&#36825;&#24847;&#21619;&#30528;&#25972;&#20307;&#30340;&#21644;&#22823;&#20110;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
&lt;/p&gt;</description></item></channel></rss>