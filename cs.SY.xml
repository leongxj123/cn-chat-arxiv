<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05858</link><description>&lt;p&gt;
DSAC-T: &#24102;&#26377;&#19977;&#20010;&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#8212;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#25152;&#24341;&#36215;&#30340;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#35780;&#35770;&#32773;&#65288;DSAC&#25110;DSAC-v1&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#30340;&#39640;&#26031;&#20540;&#20998;&#24067;&#26469;&#26377;&#25928;&#25552;&#39640;&#20540;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;DSAC&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#32780;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#32553;&#25918;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#19968;&#20123;&#29305;&#27530;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19977;&#20010;&#23545;&#26631;&#20934;DSAC&#30340;&#37325;&#35201;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#12290;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31216;&#20026;DSAC-T&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
&lt;/p&gt;</description></item></channel></rss>