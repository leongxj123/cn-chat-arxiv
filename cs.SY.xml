<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2404.02325</link><description>&lt;p&gt;
&#38381;&#29615;&#23398;&#20064;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#28909;&#21147;&#23398;&#27515;&#20129;
&lt;/p&gt;
&lt;p&gt;
Heat Death of Generative Models in Closed-Loop Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02325
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25913;&#36827;&#21644;&#37319;&#32435;&#27491;&#22312;&#36805;&#36895;&#21152;&#36895;&#65292;&#20363;&#22914;&#25991;&#26412;&#20013;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#25972;&#21512;&#21040;&#20844;&#20849;&#32593;&#32476;&#20013;&#30340;&#20849;&#20139;&#20869;&#23481;&#20013;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#36865;&#22238;&#21040;&#27169;&#22411;&#36827;&#34892;&#21518;&#32493;&#35757;&#32451;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35757;&#32451;&#36807;&#31243;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#20844;&#20849;&#21487;&#35775;&#38382;&#20869;&#23481;&#30340;&#20998;&#24067;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30693;&#35782;&#8221;&#65289;&#26159;&#21542;&#20445;&#25345;&#31283;&#23450;&#36824;&#26159;&#23849;&#28291;&#12290;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#23567;&#35268;&#27169;&#23454;&#35777;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#31181;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#36864;&#21270;&#12290;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#65288;&#31216;&#20026;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2206.02346</link><description>&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#32422;&#26463;MDP&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. (arXiv:2206.02346v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#22870;&#21169;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#39044;&#26399;&#24635;&#25928;&#29992;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#32422;&#26463;MDP&#65289;&#30340;&#25240;&#25187;&#26080;&#38480;&#26102;&#24207;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#65288;NPG-PD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#65292;&#36890;&#36807;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#23613;&#31649;&#24213;&#23618;&#26368;&#22823;&#21270;&#28041;&#21450;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38750;&#20984;&#32422;&#26463;&#38598;&#65292;&#20294;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#35268;&#26041;&#38754;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#27492;&#31867;&#25910;&#25947;&#19982;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#21363;&#26080;&#32500;&#24230;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#25968;&#32447;&#24615;&#21644;&#19968;&#33324;&#24179;&#28369;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30830;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we esta
&lt;/p&gt;</description></item></channel></rss>