<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.10945</link><description>&lt;p&gt;
Twin-in-the-Loop Observers&#30340;&#33258;&#21160;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Automatic dimensionality reduction of Twin-in-the-Loop Observers. (arXiv:2401.10945v1 [cs.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#27599;&#20010;&#35201;&#20272;&#35745;&#30340;&#21464;&#37327;&#37117;&#26159;&#29992;&#29420;&#31435;&#30340;&#31616;&#21270;&#28388;&#27874;&#27169;&#22359;&#35745;&#31639;&#30340;&#12290;&#36825;&#20123;&#27169;&#22359;&#24182;&#34892;&#36816;&#34892;&#24182;&#38656;&#35201;&#21333;&#29420;&#26657;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;Twin-in-the-Loop(TiL)&#35266;&#27979;&#22120;&#26550;&#26500;&#65306;&#20272;&#35745;&#22120;&#20013;&#30340;&#32463;&#20856;&#31616;&#21270;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#34987;&#19968;&#20010;&#23436;&#25972;&#30340;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26367;&#20195;&#12290;DT&#30340;&#29366;&#24577;&#36890;&#36807;&#32447;&#24615;&#26102;&#19981;&#21464;&#30340;&#36755;&#20986;&#35823;&#24046;&#23450;&#24459;&#23454;&#26102;&#26657;&#27491;&#12290;&#30001;&#20110;&#27169;&#25311;&#22120;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#20998;&#26512;&#20844;&#24335;&#21487;&#29992;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;&#32463;&#20856;&#30340;&#28388;&#27874;&#22120;&#35843;&#33410;&#25216;&#26415;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#23558;&#29992;&#20110;&#35299;&#20915;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;&#30001;&#20110;DT&#30340;&#22797;&#26434;&#24615;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#39640;&#32500;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#35843;&#33410;&#39640;&#22797;&#26434;&#24230;&#35266;&#27979;&#22120;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art vehicle dynamics estimation techniques usually share one common drawback: each variable to estimate is computed with an independent, simplified filtering module. These modules run in parallel and need to be calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL) Observer architecture has recently been proposed: the classical simplified control-oriented vehicle model in the estimators is replaced by a full-fledged vehicle simulator, or digital twin (DT). The states of the DT are corrected in real time with a linear time invariant output error law. Since the simulator is a black-box, no explicit analytical formulation is available, hence classical filter tuning techniques cannot be used. Due to this reason, Bayesian Optimization will be used to solve a data-driven optimization problem to tune the filter. Due to the complexity of the DT, the optimization problem is high-dimensional. This paper aims to find a procedure to tune the high-complexity obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06566</link><description>&lt;p&gt;
&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#25240;&#25187;&#22238;&#25253;&#26368;&#20248;&#24615;&#20934;&#21017;&#19979;&#65292;&#38024;&#23545;&#31163;&#25955;&#26102;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#38382;&#39064;&#12290;&#20856;&#22411;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#31354;&#38388;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#24773;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f
&lt;/p&gt;</description></item></channel></rss>