<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20151</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#19968;&#33268;&#24615;&#23547;&#27714;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21327;&#20316;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#19968;&#33268;&#24615;&#23547;&#27714;&#12290;&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#19968;&#36215;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#26234;&#33021;&#20307;&#38388;&#30340;&#21327;&#21830;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33268;&#24615;&#23547;&#27714;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#26159;&#19968;&#20010;&#25968;&#20540;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#36807;&#30456;&#20114;&#21327;&#21830;&#26469;&#36798;&#25104;&#19968;&#33268;&#20540;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#24212;&#37319;&#29992;&#21738;&#31181;&#31574;&#30053;&#26102;&#65292;LLM&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#20598;&#23572;&#20250;&#20351;&#29992;&#20854;&#20182;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#26395;&#20026;&#29702;&#35299;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-
&lt;/p&gt;</description></item></channel></rss>