<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08017</link><description>&lt;p&gt;
Stackelberg&#36712;&#36857;&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#30340;&#36870;&#21521;&#23398;&#20064;&#26159;&#20174;&#29609;&#23478;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Stackelberg&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#27599;&#20010;&#29609;&#23478;&#30340;&#21160;&#24577;&#31995;&#32479;&#36712;&#36857;&#26469;&#23450;&#20041;&#19968;&#20010;&#36870;&#21521;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39046;&#23548;&#32773;&#21644;&#19968;&#20010;&#36319;&#38543;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35753;&#39046;&#23548;&#32773;&#25512;&#26029;&#20986;&#19968;&#20010;&#26377;&#38480;&#20505;&#36873;&#38598;&#20013;&#25551;&#36848;&#36319;&#38543;&#32773;&#30446;&#26631;&#20989;&#25968;&#30340;&#20551;&#35774;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#34987;&#21160;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#21160;&#22320;&#26368;&#22823;&#21270;&#19981;&#21516;&#20551;&#35774;&#19979;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#24046;&#24322;&#65292;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36882;&#36827;&#30340;&#37325;&#22797;&#36712;&#36857;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#19982;&#22343;&#21248;&#38543;&#26426;&#36755;&#20837;&#30456;&#27604;&#65292;&#25152;&#25552;&#20379;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#27010;&#29575;&#25910;&#25947;&#21040;&#26465;&#20214;&#20110;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#19981;&#21516;&#20551;&#35774;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.17842</link><description>&lt;p&gt;
RL+&#27169;&#22411;&#25511;&#21046;&#65306;&#20351;&#29992;&#25353;&#38656;&#26368;&#20248;&#25511;&#21046;&#23398;&#20064;&#22810;&#21151;&#33021;&#36275;&#24335; locomotion
&lt;/p&gt;
&lt;p&gt;
RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#30340;&#36275;&#24335; locomotion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#26469;&#22686;&#24378; RL &#35757;&#32451;&#36807;&#31243;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#36895;&#24230;&#21644;&#27493;&#24577;&#12290;&#36825;&#20123;&#21442;&#32771;&#36816;&#21160;&#20316;&#20026; RL &#31574;&#30053;&#27169;&#20223;&#30340;&#30446;&#26631;&#65292;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#36523;&#21160;&#21147;&#23398;&#65292;RL &#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30828;&#20214;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RL &#35757;&#32451;&#36807;&#31243;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#30340;&#24378;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#21644;&#22788;&#29702;&#21487;&#33021;&#23545;&#31616;&#21270;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#30340;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#20102; RL &#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
&lt;/p&gt;</description></item></channel></rss>