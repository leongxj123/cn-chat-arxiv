<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#23558;Koopman&#31639;&#23376;&#26694;&#26550;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;Nystro&#776;m&#36924;&#36817;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#20854;&#29702;&#35770;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.02811</link><description>&lt;p&gt;
&#20855;&#26377;Koopman&#31639;&#23376;&#23398;&#20064;&#21644;Nystro&#776;m&#26041;&#27861;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\"om method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Koopman&#31639;&#23376;&#26694;&#26550;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;Nystro&#776;m&#36924;&#36817;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#20854;&#29702;&#35770;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Koopman&#31639;&#23376;&#26694;&#26550;&#22914;&#20309;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#26377;&#25928;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#34429;&#28982;&#26680;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#24456;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#65288;Nystro&#776;m&#36924;&#36817;&#65289;&#22914;&#20309;&#23454;&#29616;&#24040;&#22823;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;&#20851;&#20110;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#30456;&#20851;&#35299;&#30340;&#36817;&#20284;Riccati&#31639;&#23376;&#21644;&#35843;&#33410;&#22120;&#30446;&#26631;&#37117;&#20197;$ m^{-1/2} $&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20854;&#20013;$ m $&#26159;&#38543;&#26426;&#23376;&#31354;&#38388;&#22823;&#23567;&#12290;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#25968;&#20540;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02811v1 Announce Type: cross  Abstract: In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\"om approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\"om approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12546</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Building Myopic MPC Policies using Supervised Learning. (arXiv:2401.12546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20013;&#65292;&#32467;&#21512;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#36817;&#20284;&#26174;&#24335;MPC&#39046;&#22495;&#65292;&#20854;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#31163;&#32447;&#29983;&#25104;&#30340;&#26368;&#20339;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;MPC&#31574;&#30053;&#12290;&#34429;&#28982;&#36817;&#20284;&#26174;&#24335;MPC&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#22797;&#21046;MPC&#31574;&#30053;&#65292;&#29992;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#22312;&#32447;&#20248;&#21270;&#65292;&#20294;&#36890;&#24120;&#20250;&#22833;&#21435;&#35299;&#20915;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21363;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#32780;&#19981;&#26159;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#22312;&#19968;&#20010;&#38750;&#24120;&#30701;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#23558;&#20854;&#20316;&#20026;&#36817;&#35270;MPC&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of supervised learning techniques in combination with model predictive control (MPC) has recently generated significant interest, particularly in the area of approximate explicit MPC, where function approximators like deep neural networks are used to learn the MPC policy via optimal state-action pairs generated offline. While the aim of approximate explicit MPC is to closely replicate the MPC policy, substituting online optimization with a trained neural network, the performance guarantees that come with solving the online optimization problem are typically lost. This paper considers an alternative strategy, where supervised learning is used to learn the optimal value function offline instead of learning the optimal policy. This can then be used as the cost-to-go function in a myopic MPC with a very short prediction horizon, such that the online computation burden reduces significantly without affecting the controller performance. This approach differs from existing wor
&lt;/p&gt;</description></item></channel></rss>