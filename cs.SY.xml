<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.13654</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#27604;&#20363;&#31215;&#20998;&#25511;&#21046;&#22120;&#22312;&#33410;&#27969;&#38400;&#22522;&#20934;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#65292;&#35813;&#33410;&#27969;&#38400;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#30913;&#28382;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#30340;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#24320;&#22987;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#24341;&#23548;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20174;&#19982;&#38400;&#38376;&#30340;&#39069;&#22806;&#20132;&#20114;&#20013;&#23398;&#20064;&#26469;&#25913;&#36827;&#38381;&#29615;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38400;&#38376;&#19978;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#31361;&#26174;&#20102;&#23558;PI&#21644;RL&#26694;&#26550;&#32467;&#21512;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#25511;&#21046;&#24615;&#33021;&#30340;&#22909;&#22788;&#12290;&#22312;&#25152;&#26377;&#23454;&#39564;&#27979;&#35797;&#26696;&#20363;&#20013;&#65292;&#32467;&#26524;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#37117;&#20248;&#20110;&#20256;&#32479;RL&#20195;&#29702;&#65292;&#24182;&#19988;&#20248;&#20110;PI&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13654v1 Announce Type: cross  Abstract: This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#21452;&#20195;&#29702;&#30340;&#26368;&#20248;&#25237;&#36164;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24179;&#22343;&#20559;&#24046;&#26469;&#34913;&#37327;&#20195;&#29702;&#20915;&#31574;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#36890;&#36807;&#29702;&#24615;&#20915;&#31574;&#20998;&#35299;&#65292;&#20998;&#26512;&#20102;&#32676;&#20307;&#34892;&#20026;&#23545;&#26368;&#20248;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07183</link><description>&lt;p&gt;
&#26368;&#20248;&#25237;&#36164;&#20013;&#30340;&#32676;&#20307;&#34892;&#20026;: &#24102;&#26377;&#25237;&#36164;&#24847;&#35265;&#21644;&#29702;&#24615;&#20915;&#31574;&#20998;&#35299;&#30340;&#21452;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Herd Behavior in Optimal Investment: A Dual-Agent Approach with Investment Opinion and Rational Decision Decomposition. (arXiv:2401.07183v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#21452;&#20195;&#29702;&#30340;&#26368;&#20248;&#25237;&#36164;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24179;&#22343;&#20559;&#24046;&#26469;&#34913;&#37327;&#20195;&#29702;&#20915;&#31574;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#36890;&#36807;&#29702;&#24615;&#20915;&#31574;&#20998;&#35299;&#65292;&#20998;&#26512;&#20102;&#32676;&#20307;&#34892;&#20026;&#23545;&#26368;&#20248;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#20004;&#20010;&#20195;&#29702;&#30340;&#26368;&#20248;&#25237;&#36164;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#30340;&#20915;&#31574;&#21463;&#21040;&#21478;&#19968;&#20010;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#34913;&#37327;&#20004;&#20010;&#20195;&#29702;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#31243;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#22343;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#25512;&#23548;&#20986;&#20102;&#32771;&#34385;&#32676;&#20307;&#34892;&#20026;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#29992;&#25143;&#32676;&#20307;&#34892;&#20026;&#23545;&#26368;&#20248;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#25104;&#29702;&#24615;&#20915;&#31574;&#65292;&#36825;&#34987;&#31216;&#20026;&#29702;&#24615;&#20915;&#31574;&#20998;&#35299;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#37327;&#21270;&#20195;&#29702;&#23545;&#33258;&#24049;&#30340;&#29702;&#24615;&#20915;&#31574;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#20195;&#29702;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20195;&#29702;&#30340;&#25237;&#36164;&#24847;&#35265;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#30340;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the optimal investment problem involving two agents, where the decision of one agent is influenced by the other. To measure the distance between two agents' decisions, we introduce the average deviation. We formulate the stochastic optimal control problem considering herd behavior and derive the analytical solution through the variational method. We theoretically analyze the impact of users' herd behavior on the optimal decision by decomposing it into their rational decisions, which is called the rational decision decomposition. Furthermore, to quantify the preference for their rational decision over that of the other agent, we introduce the agent's investment opinion. Our study is validated through simulations on real stock data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11694</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#36319;&#36394;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#36319;&#36394;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#26469;&#33258;&#27668;&#21160;&#21147;&#30340;&#38459;&#21147;&#21644;&#21147;&#30697;&#21464;&#21270;&#26159;&#28151;&#27788;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#31934;&#30830;&#35782;&#21035;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22235;&#26059;&#32764;&#36319;&#36394;&#31995;&#32479;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#31616;&#21333;&#8220;&#24178;&#25200;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#36712;&#36857;&#36319;&#36394;&#22120;&#65292;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#19982;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;SMPC&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#27668;&#21160;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#8220;&#21463;&#38480;&#20998;&#24067;&#24335;&#24378;&#21270;&#24178;&#25200;&#20272;&#35745;&#22120;&#8221;&#65288;ConsDRED&#65289;&#20934;&#30830;&#22320;&#35782;&#21035;&#30495;&#23454;&#27668;&#21160;&#25928;&#24212;&#19982;&#20272;&#35745;&#20540;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#31616;&#21270;&#20223;&#23556;&#24178;&#25200;&#21453;&#39304;&#36827;&#34892;&#25511;&#21046;&#21442;&#25968;&#21270;&#65292;&#20197;&#20445;&#35777;&#20984;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;SMPC&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;ConsDRED&#33267;&#23569;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
&lt;/p&gt;</description></item></channel></rss>