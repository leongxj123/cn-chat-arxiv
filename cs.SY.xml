<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#65292;&#38477;&#20302;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#38382;&#39064;&#26469;&#39564;&#35777;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.03906</link><description>&lt;p&gt;
&#22270;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Graph Reinforcement Learning for Radio Resource Allocation. (arXiv:2203.03906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03906
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#65292;&#38477;&#20302;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#38382;&#39064;&#26469;&#39564;&#35777;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22788;&#29702;&#26080;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;DRL&#30340;&#39640;&#35757;&#32451;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#21160;&#24577;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#24378;&#21270;&#23398;&#20064;&#26469;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#20013;&#35768;&#22810;&#38382;&#39064;&#22266;&#26377;&#30340;&#20004;&#31181;&#20851;&#31995;&#20808;&#39564;&#65306;&#25299;&#25169;&#20449;&#24687;&#21644;&#25490;&#21015;&#29305;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35774;&#35745;&#22270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#21033;&#29992;&#36825;&#20004;&#20010;&#20808;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24605;&#20102;&#19968;&#31181;&#23558;&#29366;&#24577;&#30697;&#38453;&#36716;&#25442;&#20026;&#29366;&#24577;&#22270;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#28385;&#36275;&#29702;&#24819;&#30340;&#25490;&#21015;&#29305;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#22914;&#20309;&#24212;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20197;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;(DDPG)&#20026;&#20363;&#65292;&#20248;&#21270;&#20102;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#19968;&#20010;&#26159;&#39044;&#27979;&#21151;&#29575;&#20998;&#37197;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) for resource allocation has been investigated extensively owing to its ability of handling model-free and end-to-end problems. Yet the high training complexity of DRL hinders its practical use in dynamic wireless systems. To reduce the training cost, we resort to graph reinforcement learning for exploiting two kinds of relational priors inherent in many problems in wireless communications: topology information and permutation properties. To design graph reinforcement learning framework systematically for harnessing the two priors, we first conceive a method to transform state matrix into state graph, and then propose a general method for graph neural networks to satisfy desirable permutation properties. To demonstrate how to apply the proposed methods, we take deep deterministic policy gradient (DDPG) as an example for optimizing two representative resource allocation problems. One is predictive power allocation that minimizes the energy consumed for e
&lt;/p&gt;</description></item></channel></rss>