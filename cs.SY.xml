<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05737</link><description>&lt;p&gt;
HVAC&#25511;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#26159;&#21830;&#19994;&#21644;&#23621;&#20303;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#21453;&#24212;&#24335;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#27604;&#24615;&#30340;&#26631;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;Sinergym&#26694;&#26550;&#65292;&#20197;&#33298;&#36866;&#24230;&#21644;&#33021;&#28304;&#28040;&#32791;&#20026;&#35780;&#21028;&#26631;&#20934;&#65292;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HVAC&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#21644;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#26816;&#26597;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;SAC&#21644;TD3&#31561;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2307.15438</link><description>&lt;p&gt;
&#33258;&#20027;&#36733;&#33655;&#28909;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#22411;&#21355;&#26143;&#20013;&#65292;&#28909;&#25511;&#21046;&#35774;&#22791;&#12289;&#31185;&#23398;&#20202;&#22120;&#21644;&#30005;&#23376;&#37096;&#20214;&#30340;&#31354;&#38388;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#30005;&#23376;&#35774;&#22791;&#30340;&#36817;&#36317;&#31163;&#20351;&#24471;&#21151;&#32791;&#25955;&#28909;&#22256;&#38590;&#65292;&#23384;&#22312;&#26080;&#27861;&#36866;&#24403;&#25511;&#21046;&#28201;&#24230;&#12289;&#38477;&#20302;&#37096;&#20214;&#23551;&#21629;&#21644;&#20219;&#21153;&#24615;&#33021;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#21033;&#29992;&#21355;&#26143;&#19978;&#36880;&#28176;&#22686;&#21152;&#30340;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#23398;&#20064;&#26426;&#36733;&#28909;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#26410;&#26469;&#23558;&#36816;&#24448;ISS&#24182;&#22312;IMAGIN-e&#20219;&#21153;&#20013;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#30340;&#30495;&#23454;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#25511;&#21046;&#36733;&#33655;&#22788;&#29702;&#21151;&#29575;&#65292;&#20197;&#20445;&#25345;&#28201;&#24230;&#22312;&#25805;&#20316;&#33539;&#22260;&#20869;&#65292;&#34917;&#20805;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2210.12583</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control. (arXiv:2210.12583v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#31934;&#30830;&#19988;&#23433;&#20840;&#22320;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#22312;&#25805;&#20316;&#26465;&#20214;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#19981;&#26029;&#35843;&#25972;&#20197;&#24357;&#34917;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#20027;&#21160;&#24314;&#27169;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#31163;&#32447;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#21644;&#22312;&#32447;&#23398;&#20064;&#24403;&#21069;&#26426;&#22120;&#20154;&#19982;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#22823;&#22823;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#25805;&#20316;&#33539;&#22260;&#20869;&#20063;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#30340;aleatoric&#65288;&#25968;&#25454;&#65289;&#19981;&#30830;&#23450;&#24615;&#21551;&#21457;&#24335;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#26368;&#20248;&#30340;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in the presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control act
&lt;/p&gt;</description></item></channel></rss>