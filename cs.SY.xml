<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#23558;&#31232;&#30095;&#35782;&#21035;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;SINDy-RL&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#35299;&#37322;&#24615;&#36824;&#26377;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09110</link><description>&lt;p&gt;
SINDy-RL: &#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09110
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31232;&#30095;&#35782;&#21035;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;SINDy-RL&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#35299;&#37322;&#24615;&#36824;&#26377;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#26174;&#31034;&#20986;&#22312;&#19982;&#22797;&#26434;&#21160;&#24577;&#29615;&#22659;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#25511;&#21046;&#31574;&#30053;&#20013;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#65292;&#20363;&#22914;&#31283;&#23450;&#25176;&#21345;&#39532;&#20811;&#32858;&#21464;&#21453;&#24212;&#22534;&#30340;&#30913;&#27969;&#20307;&#21160;&#21147;&#23398;&#25110;&#20351;&#29289;&#20307;&#22312;&#27969;&#20307;&#27969;&#21160;&#20013;&#21463;&#21040;&#30340;&#38459;&#21147;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#23545;&#35768;&#22810;&#24212;&#29992;&#32780;&#35328;&#25104;&#26412;&#21487;&#33021;&#36807;&#39640;&#12290;&#21478;&#22806;&#65292;&#20381;&#36182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#30340;&#40657;&#30418;&#31574;&#30053;&#65292;&#21487;&#33021;&#22312;&#26576;&#20123;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20351;&#29992;&#26102;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31232;&#30095;&#35782;&#21035;&#65288;SINDy&#65289;&#65292;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#21046;&#24230;&#19979;&#21019;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#21069;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;SINDy-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;SINDy&#21644;DRL&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09110v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpret
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;</title><link>http://arxiv.org/abs/2401.10266</link><description>&lt;p&gt;
&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;: &#26041;&#27861;&#35770;&#21644;&#19981;&#30830;&#23450;&#24615;&#31649;&#29702;&#31574;&#30053;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#22312;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20851;&#27880;&#30340;&#22686;&#38271;&#20027;&#39064;&#21644;&#19968;&#31181;&#24378;&#22823;&#30340;&#25925;&#38556;&#35782;&#21035;&#26041;&#24335;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#28304;&#22522;&#20934;Tennessee Eastman Process&#65288;TEP&#65289;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#24635;&#32467;&#20102;&#29992;&#20110;&#24037;&#19994;&#21378;&#25151;&#29366;&#24577;&#30417;&#27979;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#36824;&#28085;&#30422;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#12289;&#26080;&#26631;&#35760;&#26679;&#26412;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#27604;&#36739;&#20102;&#21033;&#29992;Tennessee Eastman Process&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#20915;&#31574;&#32773;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#21644;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.09018</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#20915;&#31574;&#32773;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#21644;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#20110;&#23545;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#29615;&#22659;&#21464;&#21270;&#26102;&#40065;&#26834;&#31574;&#30053;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20026;&#20998;&#24067;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36890;&#36807;&#19968;&#20010;&#20197;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DRMDPs&#65289;&#20026;&#20013;&#24515;&#30340;&#32508;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#20915;&#31574;&#32773;&#22312;&#19968;&#20010;&#30001;&#23545;&#25163;&#25805;&#32437;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#36716;&#21464;&#19979;&#36873;&#25321;&#26368;&#20248;&#31574;&#30053;&#12290;&#36890;&#36807;&#32479;&#19968;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#34920;&#36848;&#65292;&#25105;&#20204;&#20005;&#26684;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20915;&#31574;&#32773;&#21644;&#23545;&#25163;&#30340;&#21508;&#31181;&#24314;&#27169;&#23646;&#24615;&#30340;DRMDPs&#65292;&#21253;&#25324;&#36866;&#24212;&#24615;&#31890;&#24230;&#12289;&#25506;&#32034;&#21382;&#21490;&#20381;&#36182;&#24615;&#12289;&#39532;&#23572;&#31185;&#22827;&#21644;&#39532;&#23572;&#31185;&#22827;&#26102;&#38388;&#40784;&#27425;&#30340;&#20915;&#31574;&#32773;&#21644;&#23545;&#25163;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23545;&#25163;&#24341;&#36215;&#30340;&#36716;&#21464;&#30340;&#28789;&#27963;&#24615;&#65292;&#30740;&#31350;&#20102;SA&#21644;S-&#30697;&#24418;&#24615;&#12290;&#22312;&#36825;&#20010;DRMDP&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#25152;&#38656;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the e
&lt;/p&gt;</description></item></channel></rss>