<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09454</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#26465;&#20214;&#19979;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data. (arXiv:2309.09454v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#20173;&#32570;&#20047;&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#25928;&#29575;&#30340;&#20840;&#38754;&#29702;&#35770;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#31532;&#19968;&#27493;&#19987;&#27880;&#20110;&#23454;&#29616;&#31639;&#27861;&#25910;&#25947;&#24615;&#65292;&#31532;&#20108;&#27493;&#29992;&#20110;&#25913;&#21892;&#20272;&#35745;&#24615;&#33021;&#12290;&#22312;&#25968;&#25454;&#30340;&#19968;&#33324;&#28608;&#21169;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26041;&#27861;&#21644;&#23545;&#38789;&#30340;&#26497;&#38480;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#24378;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24577;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20272;&#35745;&#20540;&#30340;&#21327;&#26041;&#24046;&#22312;&#28176;&#36817;&#19978;&#21487;&#20197;&#36798;&#21040;&#20811;&#25289;&#32654;&#27931;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#21487;&#20197;&#26399;&#26395;&#30340;&#26368;&#22909;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#19981;&#20381;&#36182;&#20256;&#32479;&#26041;&#27861;&#32780;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionall
&lt;/p&gt;</description></item></channel></rss>