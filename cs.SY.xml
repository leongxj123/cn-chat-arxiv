<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08017</link><description>&lt;p&gt;
Stackelberg&#36712;&#36857;&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#30340;&#36870;&#21521;&#23398;&#20064;&#26159;&#20174;&#29609;&#23478;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Stackelberg&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#27599;&#20010;&#29609;&#23478;&#30340;&#21160;&#24577;&#31995;&#32479;&#36712;&#36857;&#26469;&#23450;&#20041;&#19968;&#20010;&#36870;&#21521;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39046;&#23548;&#32773;&#21644;&#19968;&#20010;&#36319;&#38543;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35753;&#39046;&#23548;&#32773;&#25512;&#26029;&#20986;&#19968;&#20010;&#26377;&#38480;&#20505;&#36873;&#38598;&#20013;&#25551;&#36848;&#36319;&#38543;&#32773;&#30446;&#26631;&#20989;&#25968;&#30340;&#20551;&#35774;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#34987;&#21160;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#21160;&#22320;&#26368;&#22823;&#21270;&#19981;&#21516;&#20551;&#35774;&#19979;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#24046;&#24322;&#65292;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36882;&#36827;&#30340;&#37325;&#22797;&#36712;&#36857;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#19982;&#22343;&#21248;&#38543;&#26426;&#36755;&#20837;&#30456;&#27604;&#65292;&#25152;&#25552;&#20379;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#27010;&#29575;&#25910;&#25947;&#21040;&#26465;&#20214;&#20110;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#19981;&#21516;&#20551;&#35774;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04428</link><description>&lt;p&gt;
&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#20803;&#23398;&#20064;&#25805;&#20316;&#31526;&#21040;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Operators to Optimality from Multi-Task Non-IID Data. (arXiv:2308.04428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#20010;&#24378;&#22823;&#27010;&#24565;&#26159;&#20174;&#24322;&#26500;&#26469;&#28304;&#25110;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#21516;&#29305;&#24449;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#20989;&#25968;&#65292;&#26082;&#26377;&#21161;&#20110;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#26377;&#21161;&#20110;&#32479;&#35745;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#23569;&#35201;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#22312;&#29702;&#35770;&#19978;&#20570;&#20986;&#36825;&#20123;&#20248;&#28857;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22122;&#22768;&#21521;&#37327;&#27979;&#37327;$y = Mx + w$&#20013;&#22238;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;$M$&#30340;&#19968;&#33324;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#21327;&#21464;&#37327;$x$&#26082;&#21487;&#20197;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#36825;&#23548;&#33268;&#22122;&#22768;&#39033;&#30340;&#32553;&#25918;&#19981;&#20877;&#26377;&#21033;&#20110;&#28304;&#20219;&#21153;&#25968;&#37327;&#12290;&#36825;&#21453;&#36807;&#26469;&#20250;&#23548;&#33268;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21463;&#21040;&#21333;&#20219;&#21153;&#25968;&#25454;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias &amp; Feature-Whiten}
&lt;/p&gt;</description></item></channel></rss>