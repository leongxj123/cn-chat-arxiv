<rss version="2.0"><channel><title>Chat Arxiv cs.SY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SY</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17639</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Point-based Value Iteration for Neuro-Symbolic POMDPs. (arXiv:2306.17639v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#32467;&#21512;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26412;&#25991;&#32771;&#34385;&#20854;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NS-POMDPs&#65289;&#65292;&#35813;&#27169;&#22411;&#25551;&#36848;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#24182;&#36827;&#34892;&#31526;&#21495;&#20915;&#31574;&#30340;&#20195;&#29702;&#65292;&#24182;&#30740;&#31350;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#27573;&#32447;&#24615;&#21644;&#20984;&#34920;&#31034;&#65288;P-PWLC&#65289;&#65292;&#36890;&#36807;&#35206;&#30422;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#22810;&#38754;&#20307;&#21644;&#20540;&#21521;&#37327;&#23454;&#29616;&#65292;&#24182;&#23558;Bellman backups&#25193;&#23637;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20540;&#20989;&#25968;&#30340;&#20984;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#29366;&#24577;&#27169;&#22411;&#21644;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#24213;&#23618;&#32467;&#26500;&#26469;&#20445;&#35777;&#26377;&#38480;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic artificial intelligence is an emerging area that combines traditional symbolic techniques with neural networks. In this paper, we consider its application to sequential decision making under uncertainty. We introduce neuro-symbolic partially observable Markov decision processes (NS-POMDPs), which model an agent that perceives a continuous-state environment using a neural network and makes decisions symbolically, and study the problem of optimising discounted cumulative rewards. This requires functions over continuous-state beliefs, for which we propose a novel piecewise linear and convex representation (P-PWLC) in terms of polyhedra covering the continuous-state space and value vectors, and extend Bellman backups to this representation. We prove the convexity and continuity of value functions and present two value iteration algorithms that ensure finite representability by exploiting the underlying structure of the continuous-state model and the neural perception mechani
&lt;/p&gt;</description></item></channel></rss>