<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.10020</link><description>&lt;p&gt;
&#22312;&#37325;&#21472;&#20013;&#36855;&#22833;&#65306;&#25506;&#32034;LLMs&#20013;&#30340;&#27700;&#21360;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Lost in Overlap: Exploring Watermark Collision in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#25991;&#26412;&#29256;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;logit&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21487;&#23519;&#35273;&#30340;&#26631;&#35782;&#23884;&#20837;&#25991;&#26412;&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#27700;&#21360;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#24120;&#35265;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#21644;&#25913;&#20889;&#65289;&#20013;&#21457;&#29983;&#30340;&#27700;&#21360;&#20914;&#31361;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21452;&#27700;&#21360;&#20914;&#31361;&#65292;&#21363;&#21516;&#19968;&#25991;&#26412;&#20013;&#21516;&#26102;&#23384;&#22312;&#20004;&#20010;&#27700;&#21360;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27700;&#21360;&#20914;&#31361;&#23545;&#19978;&#28216;&#21644;&#19979;&#28216;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#24615;&#33021;&#26500;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10020v1 Announce Type: new  Abstract: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
&lt;/p&gt;</description></item></channel></rss>