<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item></channel></rss>