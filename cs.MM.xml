<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19723</link><description>&lt;p&gt;
HGT&#65306;&#21033;&#29992;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19723
&lt;/p&gt;
&lt;p&gt;
HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#29702;&#35299; (TU) &#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38754;&#20020;&#25163;&#21160;&#26631;&#35760;&#34920;&#26684;&#30340;&#31232;&#32570;&#24615;&#21644;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HGT &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#24322;&#36136;&#22270; (HG) &#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412; TU &#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#25351;&#23548;&#36716;&#25442;&#23558;&#34920;&#26684;&#35821;&#20041;&#19982;LLM&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#19977;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#26696;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;HGT&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#23569;&#26679;&#26412;&#22797;&#26434;TU&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;</title><link>https://arxiv.org/abs/2403.05050</link><description>&lt;p&gt;
DyRoNet&#65306;&#19968;&#31181;&#20302;&#31209;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65292;&#29992;&#20110;&#27969;&#23186;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05050
&lt;/p&gt;
&lt;p&gt;
DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#24863;&#30693;&#26469;&#24212;&#23545;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65288;DyRoNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#20197;&#22686;&#24378;&#27969;&#23186;&#20307;&#24863;&#30693;&#12290;&#36890;&#36807;&#38598;&#25104;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#38024;&#23545;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#24494;&#35843;&#65292;DyRoNet&#22312;&#24310;&#36831;&#21644;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#20854;&#26680;&#24515;&#29305;&#24449;&#26159;&#36895;&#24230;&#36335;&#30001;&#27169;&#22359;&#65292;&#26234;&#33021;&#22320;&#23558;&#36755;&#20837;&#25968;&#25454;&#24341;&#23548;&#21040;&#26368;&#36866;&#21512;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;DyRoNet&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#65292;&#20026;&#21508;&#31181;&#22330;&#26223;&#24615;&#33021;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#26438;&#12290;DyRoNet&#19981;&#20165;&#20026;&#27969;&#23186;&#20307;&#24863;&#30693;&#24314;&#31435;&#20102;&#26032;&#30340;&#26631;&#26438;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24037;&#31243;&#27934;&#35265;&#12290;&#26377;&#20851;&#26356;&#22810;&#39033;&#30446;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382; https://tastevision.github.io/DyRoNet/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12121</link><description>&lt;p&gt;
&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Image Review Ability of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26159;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LVLM&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;LVLM&#23545;&#22270;&#20687;&#30340;&#35780;&#20215;&#33021;&#21147;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#31361;&#26174;&#20102;&#23545;&#20854;&#35780;&#20215;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#19982;&#22270;&#20687;&#26631;&#39064;&#19981;&#21516;&#65292;&#35780;&#20215;&#25991;&#26412;&#21487;&#20197;&#20174;&#22270;&#20687;&#26500;&#22270;&#21644;&#26333;&#20809;&#31561;&#19981;&#21516;&#35270;&#35282;&#25776;&#20889;&#12290;&#36825;&#31181;&#35780;&#20215;&#35282;&#24230;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21807;&#19968;&#30830;&#23450;&#22270;&#20687;&#30340;&#27491;&#30830;&#35780;&#20215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;LVLM&#23545;&#35780;&#20215;&#25991;&#26412;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#27979;&#37327;&#36825;&#20123;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#26368;&#26032;LVLM&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
&lt;/p&gt;</description></item><item><title>JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.19180</link><description>&lt;p&gt;
JEN-1 Composer: &#19968;&#20010;&#29992;&#20110;&#39640;&#20445;&#30495;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19180
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#21512;&#25104;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#38899;&#36712;&#29983;&#25104;&#30340;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#21407;&#22987;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#21333;&#29420;&#32452;&#25104;&#21644;&#32452;&#21512;&#22810;&#38899;&#36712;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#20316;&#26354;&#23478;&#30340;&#20856;&#22411;&#24037;&#20316;&#27969;&#31243;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEN-1 Composer&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#39640;&#25928;&#22320;&#24314;&#27169;&#22810;&#38899;&#36712;&#38899;&#20048;&#30340;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#12290;JEN-1 Composer&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#22320;&#25972;&#21512;&#20219;&#20309;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#20363;&#22914;Jen-1&#65292;&#22686;&#24378;&#20854;&#22810;&#21151;&#33021;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35838;&#31243;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#36880;&#27493;&#25351;&#23548;&#27169;&#22411;&#20174;&#21333;&#38899;&#36712;&#29983;&#25104;&#21040;&#28789;&#27963;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
&lt;/p&gt;</description></item></channel></rss>