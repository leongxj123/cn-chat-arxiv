<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07640</link><description>&lt;p&gt;
&#21512;&#25104;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#29255;&#25968;&#25454;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#22810;&#27169;&#24577;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;&#33021;&#22815;&#24357;&#34917;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#21516;&#29702;&#24515;&#12289;&#20934;&#30830;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#26377;&#30528;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#25511;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#65288;CMFeed&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#25511;&#21046;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#23427;&#20351;&#29992;Transformer&#21644;Faster R-CNN&#32593;&#32476;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#21453;&#39304;&#12290;CMFeed&#25968;&#25454;&#38598;&#21253;&#21547;&#22270;&#29255;&#12289;&#25991;&#26412;&#12289;&#23545;&#24086;&#23376;&#30340;&#21453;&#24212;&#12289;&#24102;&#26377;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35780;&#35770;&#20197;&#21450;&#23545;&#35780;&#35770;&#30340;&#21453;&#24212;&#12290;&#23545;&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#21453;&#24212;&#34987;&#29992;&#26469;&#35757;&#32451;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#24773;&#24863;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
&lt;/p&gt;</description></item></channel></rss>