<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#25110;&#19981;&#21305;&#37197;&#20063;&#33021;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#21644;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2305.07216</link><description>&lt;p&gt;
&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#25110;&#19981;&#21305;&#37197;&#20063;&#33021;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#21644;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#32570;&#20047;&#23454;&#38469;&#24212;&#29992;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#27169;&#24577;&#21487;&#29992;&#65292;&#20063;&#21487;&#20197;&#20114;&#25442;&#22320;&#23454;&#29616;&#39044;&#27979;&#24773;&#24863;&#23646;&#24615;&#25110;&#35782;&#21035;&#20998;&#31867;&#24773;&#24863;&#12290;&#22312;&#19968;&#20010;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#20013;&#23454;&#29616;&#36825;&#26679;&#30340;&#28789;&#27963;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#20934;&#30830;&#35299;&#37322;&#21644;&#25972;&#21512;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#26159;&#22256;&#38590;&#30340;&#12290;&#21516;&#26102;&#65292;&#20801;&#35768;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30452;&#25509;&#20999;&#25442;&#65292;&#21516;&#26102;&#22788;&#29702;&#32570;&#22833;&#25110;&#37096;&#20998;&#20449;&#24687;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#24773;&#24863;&#22238;&#24402;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#36890;&#29992;&#35270;&#21548;&#23398;&#20064;&#65288;VAVL&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22788;&#29702;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#38899;&#35270;&#39057;&#26694;&#26550;&#65292;&#21363;&#20351;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#19981;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only
&lt;/p&gt;</description></item></channel></rss>