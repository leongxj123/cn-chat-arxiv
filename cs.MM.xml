<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10107</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;LLM&#21512;&#20316;&#25512;&#29702;&#25552;&#21319;&#20154;&#31867;&#20013;&#24515;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#30340;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#22312;&#22686;&#24378;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#35270;&#39057;&#20154;-&#29289;&#20132;&#20114;&#65288;V-HOI&#65289;&#26816;&#27979;&#26159;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;HOI&#20851;&#31995;&#65292;&#20197;&#20351;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#34892;&#20026;&#20915;&#31574;&#21463;&#30410;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;V-HOI&#26816;&#27979;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;HOI&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-HOI&#22810;LLM&#21327;&#21516;&#25512;&#29702;&#65288;V-HOI MLCR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#30001;&#19968;&#31995;&#21015;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#29616;&#25104;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20419;&#36827;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10107v1 Announce Type: cross  Abstract: Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.17395</link><description>&lt;p&gt;
WavCaps: &#19968;&#31181;ChatGPT&#36741;&#21161;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35821;&#35328;&#22810;&#27169;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. (arXiv:2303.17395v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#39057;-&#35821;&#35328;&#65288;AL&#65289;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21457;&#23637;&#38750;&#24120;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AL&#25968;&#25454;&#38598;&#25910;&#38598;&#36807;&#31243;&#26114;&#36149;&#36153;&#26102;&#65292;&#35268;&#27169;&#26377;&#38480;&#65292;&#32473;&#30740;&#31350;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WavCaps&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22823;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20174;Web&#36164;&#28304;&#21644;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#38899;&#39057;&#21098;&#36753;&#21450;&#21407;&#22987;&#25551;&#36848;&#12290;&#20294;&#26159;&#65292;&#22312;&#32447;&#25910;&#38598;&#21040;&#30340;&#21407;&#22987;&#25551;&#36848;&#38750;&#24120;&#22024;&#26434;&#65292;&#19981;&#36866;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#36807;&#28388;&#22024;&#26434;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#23383;&#24149;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#33258;&#21160;&#36807;&#28388;&#21644;&#36716;&#25442;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#23545;WavCaps&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps 
&lt;/p&gt;</description></item></channel></rss>