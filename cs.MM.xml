<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.09502</link><description>&lt;p&gt;
EquiAV: &#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#38899;&#39057;-&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#25429;&#25417;&#20016;&#23500;&#32508;&#21512;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#23398;&#20064;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#20173;&#28982;&#24456;&#38590;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#65292;&#22240;&#20026;&#22686;&#24378;&#21487;&#33021;&#20250;&#36731;&#26131;&#30772;&#22351;&#36755;&#20837;&#23545;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EquiAV&#65292;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25193;&#23637;&#31561;&#21464;&#24615;&#24320;&#22987;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#12290;&#23427;&#20351;&#24471;&#26469;&#33258;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#33021;&#22815;&#32858;&#21512;&#21040;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#20013;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item></channel></rss>