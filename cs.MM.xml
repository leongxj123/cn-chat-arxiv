<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;</title><link>https://arxiv.org/abs/2403.13501</link><description>&lt;p&gt;
VSTAR&#65306;&#29992;&#20110;&#29983;&#25104;&#38271;&#21160;&#24577;&#35270;&#39057;&#21512;&#25104;&#30340;&#26102;&#38388;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24320;&#28304;&#30340;T2V&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#21644;&#19981;&#26029;&#36827;&#21270;&#20869;&#23481;&#30340;&#36739;&#38271;&#35270;&#39057;&#12290;&#23427;&#20204;&#24448;&#24448;&#21512;&#25104;&#20934;&#38745;&#24577;&#35270;&#39057;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#25552;&#31034;&#20013;&#28041;&#21450;&#30340;&#24517;&#35201;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#23454;&#29616;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;&#21512;&#25104;&#24448;&#24448;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21363;&#26102;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GTN&#26041;&#27861;&#65292;&#21517;&#20026;VSTAR&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;1&#65289;&#35270;&#39057;&#26775;&#27010;&#25552;&#31034;&#65288;VSP&#65289;-&#22522;&#20110;&#21407;&#22987;&#21333;&#20010;&#25552;&#31034;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#65292;&#21033;&#29992;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#25991;&#26412;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13501v1 Announce Type: cross  Abstract: Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to differe
&lt;/p&gt;</description></item></channel></rss>