<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item></channel></rss>