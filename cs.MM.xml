<rss version="2.0"><channel><title>Chat Arxiv cs.MM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MM</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.14263</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions. (arXiv:2308.14263v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#21333;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#35775;&#38382;&#30340;&#38656;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#24212;&#36816;&#32780;&#29983;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#20419;&#36827;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#25991;&#29486;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20294;&#23384;&#22312;&#30528;&#20851;&#20110;&#21450;&#26102;&#24615;&#12289;&#20998;&#31867;&#20307;&#31995;&#21644;&#20840;&#38754;&#24615;&#31561;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#25216;&#26415;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28436;&#36827;&#12290;&#25991;&#31456;&#39318;&#20808;&#20174;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12289;&#26426;&#21046;&#21644;&#27169;&#22411;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#27010;&#36848;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12289;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#21644;&#24120;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users demanding access to data from various modalities. To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. Although prior literature undertook a review of the cross-modal retrieval field, it exhibits numerous deficiencies pertaining to timeliness, taxonomy, and comprehensiveness. This paper conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the paper then delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. Furthermore, it offers an overview of widel
&lt;/p&gt;</description></item></channel></rss>