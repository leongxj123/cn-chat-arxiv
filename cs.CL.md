# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large Language Models on Graphs: A Comprehensive Survey](https://rss.arxiv.org/abs/2312.02783) | 这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。 |
| [^2] | [CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks](https://arxiv.org/abs/2404.00566) | 提出了CodeBenchGen框架，通过利用大型语言模型将任意代码转化为评估示例，创造了一个包含大量代码示例的数据集Exec-CSN，展示了其可扩展性和实用性。 |
| [^3] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^4] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^5] | [Exploring language relations through syntactic distances and geographic proximity](https://arxiv.org/abs/2403.18430) | 通过句法距离和地理邻近性探索语言关系，使用POS trigrams最大化捕捉句法变化，建立语言连接并揭示语言家族和群体的簇。 |
| [^6] | [PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents](https://arxiv.org/abs/2403.13681) | PARAMANU-AYN是一种基于印度法律案例文件的高效生成式语言模型，采用自回归解码器进行预训练，并经过面向指令的微调，在各种法律任务上取得了良好表现。 |
| [^7] | [3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs](https://arxiv.org/abs/2403.07179) | 提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。 |
| [^8] | [Human vs. Machine: Language Models and Wargames](https://arxiv.org/abs/2403.03407) | 人工智能大型语言模型在战争游戏中与人类响应存在一致性，但也存在显著的差异，这表明在政策制定者交出自主权或听从基于AI的战略建议之前应谨慎对待。 |
| [^9] | [MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets](https://arxiv.org/abs/2403.03194) | MAGID是一个用于将仅文本对话增强为多样性和高质量图像的框架，通过创新的反馈循环生成高质量和多模态对话。 |
| [^10] | [Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore](https://arxiv.org/abs/2402.18045) | 本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。 |
| [^11] | [Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps](https://arxiv.org/abs/2402.17954) | 多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。 |
| [^12] | [Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402.17512) | 提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。 |
| [^13] | [Immunization against harmful fine-tuning attacks](https://arxiv.org/abs/2402.16382) | 本文提出了一种用于防范大型语言模型中有害微调攻击的免疫条件集，以帮助理解如何构建和衡量未来的防御措施。 |
| [^14] | [LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding](https://arxiv.org/abs/2402.16050) | LSTP提出了语言引导的时空提示学习方法，通过整合时间提示采样器（TPS）和空间提示求解器（SPS）以及一致的训练策略，显著提升了计算效率、时间理解和空间-时间对齐。 |
| [^15] | [SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization](https://arxiv.org/abs/2402.13919) | 该研究提出了一种创新流程，利用GPT-3.5和GPT-4生成高质量反馈，以增强临床笔记摘要中的事实一致性，弥补了专家注释数据的高成本和有限可用性问题。 |
| [^16] | [On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices](https://arxiv.org/abs/2402.12817) | 有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。 |
| [^17] | [PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment](https://arxiv.org/abs/2402.08702) | 该论文介绍了一种在多步任务中集成人类反馈和偏好对齐的PRompt优化方法。它使用遗传算法框架，结合人类反馈自动提出优化建议并解决了复杂的提示内容分析、单步评估和任务执行偏好的挑战。 |
| [^18] | [The Effect of Sampling Temperature on Problem Solving in Large Language Models](https://arxiv.org/abs/2402.05201) | 这项研究实证研究了采样温度对大型语言模型在解题中的影响，结果显示在0.0至1.0的温度范围内，LLM性能对解题任务没有显著影响。 |
| [^19] | [LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K](https://arxiv.org/abs/2402.05136) | LV-Eval是一个具有五个长度级别的长上下文基准测试，支持256k上下文长度，并具有混淆事实、关键词和短语替换以及基于关键词回忆的度量设计等关键技术，旨在减少知识泄漏和提供更客观的评估。 |
| [^20] | [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757) | 本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。 |
| [^21] | [Tradeoffs Between Alignment and Helpfulness in Language Models](https://arxiv.org/abs/2401.16332) | 本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。 |
| [^22] | [DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation](https://arxiv.org/abs/2311.09581) | 本文提出了一个名为DocLens的框架，通过一组新的度量标准，在多个任务中展示其对医学文本生成的有效性，并且通过人类研究表明其在与医学专家判断的一致性上优于现有指标，同时指出了改进开源评估者的必要性。 |
| [^23] | [LongForm: Effective Instruction Tuning with Reverse Instructions](https://arxiv.org/abs/2304.08460) | 使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。 |
| [^24] | [Generative Deduplication For Socia Media Data Selection.](http://arxiv.org/abs/2401.05883) | 提出了一种名为生成去重的方法，用于解决社交媒体数据中的冗余问题和模型偏差。通过删除重复的文本，可以提高语言理解性能并节省训练时间。 |
| [^25] | [The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities.](http://arxiv.org/abs/2311.00237) | 该论文对LLMs的新兴能力的解释和分析进行了全面调查，旨在理解这些能力的机制和实际应用，并解决可能出现的潜在风险和担忧。 |
| [^26] | [Ada-Instruct: Adapting Instruction Generators for Complex Reasoning.](http://arxiv.org/abs/2310.04484) | Ada-Instruct是一种自适应指令生成器，通过对开源LLMs进行微调，能够生成复杂推理任务中长度大于等于100的指令。在代码补全、数学推理和常识推理等任务中，Ada-Instruct显示出优于基本模型和当前自我指导方法的改进效果。 |
| [^27] | [Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis.](http://arxiv.org/abs/2309.15656) | 本文通过对英语、法语、德语、匈牙利语、意大利语、日语、挪威语和中文的脚本对话和自发对话数据进行量化分析，研究了交流反馈现象。研究发现这些对话类型在交流反馈和落地现象方面存在明显差异。 |
| [^28] | [PharmacyGPT: The AI Pharmacist.](http://arxiv.org/abs/2307.10432) | PharmacyGPT是一个新颖的框架，利用大型语言模型（LLM）来仿真临床药师的角色。通过生成患者群集、制定用药计划和预测患者结果，PharmacyGPT在临床药学中具有潜在应用和限制，为促进负责任和有效使用人工智能技术做出贡献。 |

# 详细

[^1]: 在图上的大型语言模型：一项全面调查

    Large Language Models on Graphs: A Comprehensive Survey

    [https://rss.arxiv.org/abs/2312.02783](https://rss.arxiv.org/abs/2312.02783)

    这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。

    

    大型语言模型（LLMs），如GPT4和LLaMA，由于其强大的文本编码/解码能力和新发现的紧急能力（例如推理）在自然语言处理方面取得了显著的进展。虽然LLMs主要设计用于处理纯文本，但在许多现实场景中，文本数据与图形形式的丰富结构信息相关联（例如学术网络和电子商务网络），或者图形数据与丰富的文本信息配对（例如带有描述的分子）。此外，尽管LLMs已经展示了其基于纯文本的推理能力，但尚未探索此类能力是否可以推广到图形上（即基于图形的推理）。在本文中，我们对在图上的大型语言模型相关场景和技术进行了系统回顾。我们首先总结了采用LLMs在图形上的潜在场景，分为纯图形、文本属性图形和文本配对图形三个类别。

    Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
    
[^2]: CodeBenchGen: 创建可扩展的基于执行的代码生成基准

    CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks

    [https://arxiv.org/abs/2404.00566](https://arxiv.org/abs/2404.00566)

    提出了CodeBenchGen框架，通过利用大型语言模型将任意代码转化为评估示例，创造了一个包含大量代码示例的数据集Exec-CSN，展示了其可扩展性和实用性。

    

    为了促进在不同场景下评估代码生成系统，我们提出了CodeBenchGen，这是一个框架，可以创建可扩展的基于执行的基准，仅需要轻微的人类指导。具体来说，我们利用一个大型语言模型（LLM）将任意代码片段转化为评估示例，包括用于执行评估的测试用例。我们通过创建包含来自CodeSearchNet数据集的367个GitHub存储库中的代码修改的293个库的1,931个例子的数据集Exec-CSN，展示了我们框架的实用性。为了展示Exec-CSN中示例的复杂性和可解性，我们进行了一个人类研究，结果显示81.3%的例子可以被人类解决，61%被评为“需要努力解决”。我们对开源和专有模型进行了代码生成实验，并分析了人类和模型的性能。

    arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
    
[^3]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^4]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^5]: 通过句法距离和地理邻近性探索语言关系

    Exploring language relations through syntactic distances and geographic proximity

    [https://arxiv.org/abs/2403.18430](https://arxiv.org/abs/2403.18430)

    通过句法距离和地理邻近性探索语言关系，使用POS trigrams最大化捕捉句法变化，建立语言连接并揭示语言家族和群体的簇。

    

    语言被分为共享相同语言特征的语系。虽然这种方法在理解不同语言之间的基因关系方面取得了成功，但需要更多的分析来准确量化它们的关联性，特别是在较少研究的语言层面，比如句法。本文使用从Universal Dependencies数据集中提取的一系列词性（POS）来探索语言距离。在信息论框架内，我们展示了采用POS三元组最大化捕捉句法变化的可能性，同时与可用数据量兼容。通过基于POS分布的成对距离评估建立语言连接。有趣的是，我们的分析揭示了明确对应于众所周知的语言家族和群体的簇，异常情况被解释为独特的形态类型学。此外，我们获得

    arXiv:2403.18430v1 Announce Type: new  Abstract: Languages are grouped into families that share common linguistic traits. While this approach has been successful in understanding genetic relations between diverse languages, more analyses are needed to accurately quantify their relatedness, especially in less studied linguistic levels such as syntax. Here, we explore linguistic distances using series of parts of speech (POS) extracted from the Universal Dependencies dataset. Within an information-theoretic framework, we show that employing POS trigrams maximizes the possibility of capturing syntactic variations while being at the same time compatible with the amount of available data. Linguistic connections are then established by assessing pairwise distances based on the POS distributions. Intriguingly, our analysis reveals definite clusters that correspond to well known language families and groups, with exceptions explained by distinct morphological typologies. Furthermore, we obtain
    
[^6]: PARAMANU-AYN：一种有效的新型生成式、面向印度法律案例文件的语言模型

    PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents

    [https://arxiv.org/abs/2403.13681](https://arxiv.org/abs/2403.13681)

    PARAMANU-AYN是一种基于印度法律案例文件的高效生成式语言模型，采用自回归解码器进行预训练，并经过面向指令的微调，在各种法律任务上取得了良好表现。

    

    在这篇论文中，我们介绍了PARAMANU-AYN，这是一个仅基于印度最高法院案例文件、印度宪法和印度刑法的语言模型。这种新颖的基于自回归（AR）解码器的模型是从头开始在上下文大小为8192的情况下进行预训练的。我们在困惑度指标上评估了我们的预训练法律模型。我们还对一组包括各种法律任务（如法律推理、判决解释、法律条款生成、法律草拟、法律合同草拟、案件摘要、宪法问题回答等）的10,763条指令进行了针对性训练。我们还通过GPT-3.5-Turbo对面向指令的模型的提示响应进行了在10分制度上的清晰度、相关性、完整性和法律推理指标的评估。我们的模型可以在CPU上运行，并实现每秒42.46个令牌的CPU推理速度。

    arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
    
[^7]: 3M-Diffusion：用于文本引导生成分子图的潜在多模态扩散

    3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs

    [https://arxiv.org/abs/2403.07179](https://arxiv.org/abs/2403.07179)

    提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。

    

    生成具有所需属性的分子是一项关键任务，在药物发现和材料设计中具有广泛应用。受到大型语言模型的最新进展的启发，越来越多的人对使用分子的自然语言描述来生成具有所需属性的分子产生了兴趣。大多数现有方法侧重于生成与文本描述精确匹配的分子。然而，实际应用需要能够生成具有所需属性的多样化，理想情况下是新颖的分子的方法。我们提出了一种新颖的多模态分子图生成方法3M-Diffusion，以解决这一挑战。

    arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
    
[^8]: 人类对抗机器：语言模型与战争游戏

    Human vs. Machine: Language Models and Wargames

    [https://arxiv.org/abs/2403.03407](https://arxiv.org/abs/2403.03407)

    人工智能大型语言模型在战争游戏中与人类响应存在一致性，但也存在显著的差异，这表明在政策制定者交出自主权或听从基于AI的战略建议之前应谨慎对待。

    

    战争游戏在军事战略的发展和国家对威胁或攻击的响应中有着悠久的历史。人工智能（AI）的出现承诺了更好的决策制定和增强的军事效果。然而，关于AI系统，尤其是大型语言模型（LLMs），与人类的行为有何不同仍存在争议。为此，我们进行了一项战争游戏实验，共有107位国家安全专家人类参与者参与，旨在研究在一个虚构的美中情景中的危机升级，并比较人类参与者与LLM模拟响应之间的差异。我们发现LLM和人类响应存在显著一致性，但在战争游戏中模拟和人类参与者之间也存在显著的定量和定性差异，这促使决策者在交出自主权或遵循基于AI的战略建议之前谨慎对待。

    arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
    
[^9]: MAGID：用于生成合成多模态数据集的自动化流水线

    MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets

    [https://arxiv.org/abs/2403.03194](https://arxiv.org/abs/2403.03194)

    MAGID是一个用于将仅文本对话增强为多样性和高质量图像的框架，通过创新的反馈循环生成高质量和多模态对话。

    

    多模态交互系统的发展受限于缺乏丰富的多模态（文本、图像）对话数据，这些数据对LLMs而言需要大量。先前的方法通过检索图像来增强文本对话，存在隐私、多样性和质量等约束。在这项工作中，我们介绍了MAGID（\textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues）, 一个框架，用于用各种多样性和高质量图像增强仅限于文本的对话。随后，应用扩散模型来创建相应的图像，确保与确定的文本保持一致。最后，MAGID包含了一个创新性的反馈循环，介于图像描述生成模块（文本LLM）和图像质量模块（解决美学、图像文本匹配和安全性），二者协作生成高质量和多模态对话。我们将MAGID与其他SOTA基线在三个对话方面进行了比较。

    arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo
    
[^10]: Multi-FAct: 使用FActScore评估多语言LLM的多区域知识

    Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore

    [https://arxiv.org/abs/2402.18045](https://arxiv.org/abs/2402.18045)

    本文系统评估了多语言LLMs跨语言和地理区域的事实准确性，并发现英语在事实准确性和生成事实数量方面优于其他语言，同时多语言模型存在对来自西方大陆事实信息的偏见。

    

    大型语言模型（LLMs）容易出现事实上的幻觉，生成与已知知识相矛盾的文本。尽管广泛研究了英语中的这一问题，但对于多语言LLMs知之甚少。本文系统评估了多语言LLMs跨语言和地理区域的事实准确性。我们引入了一个新颖的多语言事实评估流程，将FActScore（Min等，2023）改编为多样化语言。我们在九种语言上的分析显示，英语在事实准确性和生成事实数量方面始终表现优异。此外，多语言模型展现出对来自西方大陆的事实信息的偏见。这些发现突显了对改进多语言事实性评估的需求，并强调了LLMs的事实生成中的地理偏见。

    arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
    
[^11]: 自动语音识别的多语言语音模型存在性别差距

    Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps

    [https://arxiv.org/abs/2402.17954](https://arxiv.org/abs/2402.17954)

    多语言语音模型在自动语音识别中表现出性别差距，且在不同语言中受益的性别群体各不相同。

    

    当前的语音识别方法使用多任务、多语言模型来进行诸如自动语音识别（ASR）的语音任务，使其适用于许多语言而不需要实质性的更改。然而，广泛的语言覆盖仍然可能掩盖语言内部存在的性别差距。本文系统评估多语言ASR系统在性别表现差距上的情况。在19种语言的三个数据集上使用两种流行模型，跨越七个语言家族，我们发现明显的性别差异。不过，不同语言中受益的群体各不相同。尽管在语音学变量（音高、说话速度等）上各群体间没有显著差异，但探索模型内部状态却揭示了探查性能和性别表现差距之间的负相关关系。即，在某种语言中更容易区分说话者性别，模型就更偏向于女性说话者。我们的结果显示组别差异

    arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
    
[^12]: Latent Attention for Linear Time Transformers

    Latent Attention for Linear Time Transformers

    [https://arxiv.org/abs/2402.17512](https://arxiv.org/abs/2402.17512)

    提出了一种基于潜在向量定义注意力的方法，将标准transformer中的注意力机制的时间复杂度从二次方降低到与时间线性相关，表现与标准注意力媲美，但允许上下文窗口扩展到远远超出标准的范围。

    

    标准transformer中的注意力机制的时间复杂度随着序列长度的增加呈二次方增长。我们引入一种通过定义潜在向量的注意力来将其降低到与时间线性相关的方法。该方法可以轻松作为标准注意力机制的替代品。我们的“Latte Transformer”模型可用于双向和单向任务，因果版本允许一种在推理语言生成任务中内存和时间高效的递归实现。标准transformer的下一个标记预测随着序列长度线性增长，而Latte Transformer计算下一个标记所需的时间是恒定的。我们的方法的实证表现可与标准注意力媲美，但允许将上下文窗口扩展到远远超出标准注意力实际可行的范围。

    arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
    
[^13]: 防范有害微调攻击

    Immunization against harmful fine-tuning attacks

    [https://arxiv.org/abs/2402.16382](https://arxiv.org/abs/2402.16382)

    本文提出了一种用于防范大型语言模型中有害微调攻击的免疫条件集，以帮助理解如何构建和衡量未来的防御措施。

    

    大型语言模型（LLMs）与人类价值观的调整方法主要集中在纠正预训练中出现的不一致。然而，这种关注忽略了另一种不一致的来源：恶意行为者可能有意对LLMs进行微调以实现有害目标。本文提出了一种新兴的威胁模型，该模型源于对齐规避和微调攻击。然而，以前的作品缺乏有效防御条件的清晰呈现。我们提出了对抗LLMs中有害微调的有效防御条件集，称为“免疫条件”，这有助于我们了解如何构建和衡量未来的防御措施。利用这种防御的形式框架，我们提供了不同研究方向的综合，以防止有害微调攻击，并展示了如何在实验中使用这些条件的早期结果。

    arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called "Immunization conditions," which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using
    
[^14]: LSTP: 语言引导的时空提示学习用于长篇视频文本理解

    LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding

    [https://arxiv.org/abs/2402.16050](https://arxiv.org/abs/2402.16050)

    LSTP提出了语言引导的时空提示学习方法，通过整合时间提示采样器（TPS）和空间提示求解器（SPS）以及一致的训练策略，显著提升了计算效率、时间理解和空间-时间对齐。

    

    尽管视频语言建模取得了进展，但在回应特定任务的语言查询时解释长篇视频的计算挑战仍然存在，这主要是由于高维视频数据的复杂性和语言与空间和时间上视觉线索之间的不一致性。为解决这一问题，我们引入了一种名为语言引导的时空提示学习（LSTP）的新方法。该方法具有两个关键组件：利用光流先验的时间提示采样器（TPS），可利用时间信息有效提取相关视频内容；以及灵巧地捕捉视觉和文本元素之间复杂空间关系的空间提示求解器（SPS）。通过将TPS和SPS与一致的训练策略相协调，我们的框架显著提升了计算效率、时间理解和空间-时间对齐。在两个挑战中的实证评估显示，我们的方法优于现有技术。

    arXiv:2402.16050v1 Announce Type: cross  Abstract: Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challeng
    
[^15]: SYNFAC-EDIT: 用于临床摘要中的事实对齐的合成模仿编辑反馈

    SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization

    [https://arxiv.org/abs/2402.13919](https://arxiv.org/abs/2402.13919)

    该研究提出了一种创新流程，利用GPT-3.5和GPT-4生成高质量反馈，以增强临床笔记摘要中的事实一致性，弥补了专家注释数据的高成本和有限可用性问题。

    

    大型语言模型（LLMs）如GPT和Llama在摘要任务上取得了重大进展，但在事实不准确方面存在困难，这是临床NLP应用中的关键问题，错误可能导致严重后果。为了解决事实对齐的专家注释数据成本高昂且有限的问题，本研究引入了一种创新的流程，利用GPT-3.5和GPT-4生成高质量反馈，旨在增强临床笔记摘要中的事实一致性。我们的研究主要关注编辑反馈，在没有额外注释的情况下，模拟了医疗专业人员改善AI系统输出的实际场景。尽管GPT在各种临床NLP任务中都表现出了专业水平，比如医学执照考试，但对其提供改善较弱LM或LLM生成质量的专业级编辑反馈的研究很少。

    arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
    
[^16]: 有限标注数据学习对随机性的敏感性：相互作用和系统选择的影响

    On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices

    [https://arxiv.org/abs/2402.12817](https://arxiv.org/abs/2402.12817)

    有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。

    

    有限标注数据学习可以在标签不足时提高性能，但也对所谓的随机因素（例如数据的变化顺序）引入的无法控制的随机性敏感。我们提出了一种方法，系统地调查随机因素的影响，同时考虑它们之间的相互作用。为了测量单个随机因素的真实影响，我们的方法减轻了其他因素的影响，并观察了性能在多次运行中的变化。将我们的方法应用于7个代表性文本分类任务的上下文学习和微调方法以及3个任务的元学习，我们发现：1）现有作品中忽略随机因素之间的相互作用导致了不一致的研究结果，因为错误地归因于随机因素的影响，比如否定了一些一

    arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
    
[^17]: PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment

    PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment

    [https://arxiv.org/abs/2402.08702](https://arxiv.org/abs/2402.08702)

    该论文介绍了一种在多步任务中集成人类反馈和偏好对齐的PRompt优化方法。它使用遗传算法框架，结合人类反馈自动提出优化建议并解决了复杂的提示内容分析、单步评估和任务执行偏好的挑战。

    

    Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.

    arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
    
[^18]: 采样温度对大型语言模型在解题中的影响

    The Effect of Sampling Temperature on Problem Solving in Large Language Models

    [https://arxiv.org/abs/2402.05201](https://arxiv.org/abs/2402.05201)

    这项研究实证研究了采样温度对大型语言模型在解题中的影响，结果显示在0.0至1.0的温度范围内，LLM性能对解题任务没有显著影响。

    

    在这项研究中，我们通过实证研究调查了采样温度对大型语言模型（LLMs）在各种解题任务中的性能影响。我们通过从标准LLM基准中随机抽取问题，创建了一个多项选择问题（MCQA）考试。然后，我们使用了四种常见的LLM以及五种提示引擎技术来解决MCQA问题，同时将采样温度从0.0增加到1.0。尽管有关的报道与之相反，我们的实证结果表明，在0.0至1.0的温度范围内，LLM性能在解题任务中的变化没有统计学上显著的影响。此外，这些结果似乎不受LLM、提示引擎技术或问题领域的影响。所有代码、数据和补充资料都可以在GitHub上找到：https://github.com/matthewrenze/jhu-llm-temperature。

    In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
    
[^19]: LV-Eval:一个平衡的长上下文基准测试，具有5个长度级别，最多可达256K

    LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K

    [https://arxiv.org/abs/2402.05136](https://arxiv.org/abs/2402.05136)

    LV-Eval是一个具有五个长度级别的长上下文基准测试，支持256k上下文长度，并具有混淆事实、关键词和短语替换以及基于关键词回忆的度量设计等关键技术，旨在减少知识泄漏和提供更客观的评估。

    

    最先进的大型语言模型（LLMs）现在声称支持的上下文长度可以达到256k甚至更多。相比之下，主流基准测试的平均上下文长度不足（5k-21k），并且它们容易出现知识泄漏和不准确的评估指标，导致评估结果偏见。本文介绍了LV-Eval，一个具有五个长度级别（16k，32k，64k，128k和256k）的具有挑战性的长上下文基准测试，最多可达256k个单词。LV-Eval包含两个主要任务，单跳问答和多跳问答，包含11个双语数据集。LV-Eval的设计融合了三个关键技术，即混淆事实插入、关键词和短语替换以及基于关键词回忆的度量设计。LV-Eval的优点包括对不同上下文长度的可控评估、具有混淆事实的具有挑战性的测试实例、减少的知识泄漏以及更客观的评估。我们在LV-Eval上评估了10个LLMs，并进行了消融研究

    State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies o
    
[^20]: 本能偏见：虚假图像导致MLLMs产生幻觉

    The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs

    [https://arxiv.org/abs/2402.03757](https://arxiv.org/abs/2402.03757)

    本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。

    

    大型语言模型（LLMs）近年来取得了显著进展，多模态大型语言模型（MLLMs）的出现使LLMs具备了视觉能力，在各种多模态任务中表现出色。然而，像GPT-4V这样强大的MLLMs在面对某些图像和文本输入时仍然以惊人的方式失败了。本文中，我们确定了一类典型输入，这些输入令MLLMs困惑，它们由高度相关但与答案不一致的图像组成，导致MLLMs产生幻觉。为了量化这种影响，我们提出了CorrelationQA，这是首个评估给定虚假图像的幻觉程度的基准。该基准包含13个类别的7,308个文本-图像对。基于提出的CorrelationQA，我们对9个主流MLLMs进行了深入分析，表明它们普遍受到这种本能偏见的不同程度的影响。我们希望我们精选的基准和评估结果能有所帮助。

    Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
    
[^21]: 对齐和有用性之间的权衡：语言模型的研究

    Tradeoffs Between Alignment and Helpfulness in Language Models

    [https://arxiv.org/abs/2401.16332](https://arxiv.org/abs/2401.16332)

    本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。

    

    语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望行为和抑制非期望行为，实现人类与语言模型之间的安全交互。通常通过调整模型或插入预设的对齐提示来实现。最近，通过改变训练后的表示来改变模型行为的表示工程方法在对齐语言模型方面表现出了有效性。表示工程在面对对抗攻击和降低社会偏见等对齐导向任务方面取得了增益，但也导致了模型执行基本任务能力的降低。本文研究了增加对齐度和减少模型有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。有趣的是，我们发现，尽管模型的有用性通常会减少

    Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
    
[^22]: DocLens: 多方面细粒度评估医学文本生成

    DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation

    [https://arxiv.org/abs/2311.09581](https://arxiv.org/abs/2311.09581)

    本文提出了一个名为DocLens的框架，通过一组新的度量标准，在多个任务中展示其对医学文本生成的有效性，并且通过人类研究表明其在与医学专家判断的一致性上优于现有指标，同时指出了改进开源评估者的必要性。

    

    医学文本生成旨在协助行政工作并突出要点信息以支持决策制定。为了反映医学文本的特定要求，本文提出了一组度量标准，用于在细粒度水平上评估生成文本的完整性、简明性和归因性。这些度量标准可以由各种类型的评估者计算，包括遵循说明（专有和开源）和监督蕴涵模型。我们通过三个评估者在三个任务上展示了所得出的框架DocLens的有效性：临床记录生成、放射学报告摘要和患者问题摘要。一项全面的人类研究显示，DocLens与医学专家的判断之间存在相当高的一致性，高于现有指标。结果还突出显示了改进开源评估者的必要性，并提出了潜在的改进方向。

    arXiv:2311.09581v2 Announce Type: replace  Abstract: Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.
    
[^23]: LongForm: 使用反向指令进行有效的指令调优

    LongForm: Effective Instruction Tuning with Reverse Instructions

    [https://arxiv.org/abs/2304.08460](https://arxiv.org/abs/2304.08460)

    使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    

    Instruction tuning使得语言模型能够更有效地泛化，并更好地遵循用户意图。然而，获取指令数据成本高且具有挑战性。先前的工作采用了诸如昂贵的人工注释、存在对齐问题的众包数据集、以及通过LLMs生成噪声示例的方法。我们引入了LongForm-C数据集，该数据集由反向指令创建。我们使用LLMs为人类写作语料库示例使用反向指令生成指令。首先，我们从诸如C4和Wikipedia的语料库中选择了一组多样性的人类撰写文档；然后，我们使用LLMs为这些文档生成指令。这种方法提供了一个更便宜、更干净、输出自然以及适用于长文本生成的指令调优数据集。我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
    
[^24]: 社交媒体数据选择的生成去重方法

    Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])

    [http://arxiv.org/abs/2401.05883](http://arxiv.org/abs/2401.05883)

    提出了一种名为生成去重的方法，用于解决社交媒体数据中的冗余问题和模型偏差。通过删除重复的文本，可以提高语言理解性能并节省训练时间。

    

    社交媒体数据受其噪声特性的影响，存在冗余问题，导致训练时间增加和模型偏差。为了解决这个问题，我们提出了一种新颖的方法，称为生成去重。它旨在从嘈杂的社交媒体数据中删除重复的文本，并减轻模型偏差。通过这样做，它可以提高社交媒体语言理解性能并节省训练时间。大量实验证明，提出的生成去重方法可以有效减少训练样本的同时提高性能。这一证据表明生成去重的有效性及其在社交媒体语言理解中的重要性。

    Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.
    
[^25]: LLMs的神秘和迷人之处：紧密调查对新兴能力的解释和分析

    The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])

    [http://arxiv.org/abs/2311.00237](http://arxiv.org/abs/2311.00237)

    该论文对LLMs的新兴能力的解释和分析进行了全面调查，旨在理解这些能力的机制和实际应用，并解决可能出现的潜在风险和担忧。

    

    理解新兴能力，如在大型语言模型（LLMs）中的上下文学习(ICL)和思维链(CoT)触发，至关重要。这种重要性不仅来自于在各种任务中更好地利用这些能力，还包括主动识别和缓解可能出现的潜在风险，包括真实性、偏见和有害性的担忧。本文在LLMs的新兴能力解释和分析方面提出了一项深入调查。首先，我们简要介绍了新兴能力的背景和定义。然后，我们从两个角度概述了研究的进展：1)宏观角度，强调对机制可解释性的研究，并深入探讨新兴能力背后的数学基础；2)微观角度，关注通过考察与这些能力相关的因素来实证可解释性的研究。

    Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these
    
[^26]: Ada-Instruct: 为复杂推理调整指令生成器

    Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04484](http://arxiv.org/abs/2310.04484)

    Ada-Instruct是一种自适应指令生成器，通过对开源LLMs进行微调，能够生成复杂推理任务中长度大于等于100的指令。在代码补全、数学推理和常识推理等任务中，Ada-Instruct显示出优于基本模型和当前自我指导方法的改进效果。

    

    通过大型语言模型（LLM）生成多样且复杂的指令对于推进效果至关重要。当前的方法利用闭源的LLMs，通过上下文提示进行指令生成。然而，本文发现对于诸如代码补全等任务，上下文提示无法生成长度大于等于100的复杂指令。为解决这个问题，我们引入Ada-Instruct，一种通过对开源LLMs进行微调的自适应指令生成器。我们的关键发现表明，仅使用十个样本对开源LLMs进行微调即可生成保持分布一致性的长指令，适用于复杂推理任务。我们在代码补全、数学推理和常识推理等不同应用中对Ada-Instruct的有效性进行了实证验证。结果显示Ada-Instruct优于其基本模型和当前的自我指导方法。

    Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
    
[^27]: 脚本对话与自发对话中的会话反馈：一种比较分析

    Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis. (arXiv:2309.15656v1 [cs.CL])

    [http://arxiv.org/abs/2309.15656](http://arxiv.org/abs/2309.15656)

    本文通过对英语、法语、德语、匈牙利语、意大利语、日语、挪威语和中文的脚本对话和自发对话数据进行量化分析，研究了交流反馈现象。研究发现这些对话类型在交流反馈和落地现象方面存在明显差异。

    

    脚本对话，如电影和电视字幕，构成了会话自然语言处理模型的广泛训练数据源。然而，这些对话的语言特点与自发交互的语料库中观察到的语言特点明显不同。特别是在交流反馈和落地现象（如回应、确认或澄清要求）方面，这种差异尤为明显。这些信号被认为是会话流程的重要组成部分，并由对话参与者用于对彼此之间正在进行的交互的感知提供反馈。本文在英语、法语、德语、匈牙利语、意大利语、日语、挪威语和中文的对话数据基础上，进行了这类交流反馈现象的定量分析。我们提取了词汇统计和使用神经对话行为标记器获得的分类输出。本文的主要发现有两个方面。

    Scripted dialogues such as movie and TV subtitles constitute a widespread source of training data for conversational NLP models. However, the linguistic characteristics of those dialogues are notably different from those observed in corpora of spontaneous interactions. This difference is particularly marked for communicative feedback and grounding phenomena such as backchannels, acknowledgments, or clarification requests. Such signals are known to constitute a key part of the conversation flow and are used by the dialogue participants to provide feedback to one another on their perception of the ongoing interaction. This paper presents a quantitative analysis of such communicative feedback phenomena in both subtitles and spontaneous conversations. Based on dialogue data in English, French, German, Hungarian, Italian, Japanese, Norwegian and Chinese, we extract both lexical statistics and classification outputs obtained with a neural dialogue act tagger. Two main findings of this empiri
    
[^28]: PharmacyGPT：AI药师

    PharmacyGPT: The AI Pharmacist. (arXiv:2307.10432v1 [cs.CL])

    [http://arxiv.org/abs/2307.10432](http://arxiv.org/abs/2307.10432)

    PharmacyGPT是一个新颖的框架，利用大型语言模型（LLM）来仿真临床药师的角色。通过生成患者群集、制定用药计划和预测患者结果，PharmacyGPT在临床药学中具有潜在应用和限制，为促进负责任和有效使用人工智能技术做出贡献。

    

    本研究介绍了PharmacyGPT，这是一个新颖的框架，用于评估大型语言模型（LLM）（如ChatGPT和GPT-4）在仿真临床药师角色方面的能力。我们的方法包括利用LLM生成可理解的患者群集、制定用药计划和预测患者结果。我们使用从北卡罗来纳大学教堂山医院（UNC）重症监护病房（ICU）获取的真实数据进行调查。我们的分析提供了对LLM在临床药学领域潜在应用和限制的有价值见解，对患者护理和未来基于AI的医疗解决方案的开发具有重要意义。通过评估PharmacyGPT的性能，我们旨在为有关在医疗保健环境中整合人工智能的持续讨论做出贡献，最终促进负责任和有效使用此类技术。

    In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.
    

