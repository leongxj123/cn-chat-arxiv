# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [RoleInteract: Evaluating the Social Interaction of Role-Playing Agents](https://arxiv.org/abs/2403.13679) | 该论文介绍了RoleInteract，一个旨在评估角色扮演对话代理社交性的基准，覆盖了500个角色、6000多个问题提示和30800个对话话语。 |
| [^2] | [Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model](https://arxiv.org/abs/2403.13244) | 介绍了一个多约束分子生成大型语言模型TSMMG，通过整合多个小模型和工具来帮助生成符合描述的新分子，在各种约束任务中表现优秀。 |
| [^3] | [Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples](https://arxiv.org/abs/2403.02875) | 提出了一种通过硬负样本改进多模态对比学习中概念理解的方法，并引入了一个评估视觉-语言模型中颜色、对象和大小细粒度对齐的新数据集。 |
| [^4] | [InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691) | 本研究引入了InjecAgent基准测试，用于评估工具集成的大型语言模型代理对间接提示注入攻击的脆弱性，通过评估30种LLM代理，发现这些代理存在漏洞 |
| [^5] | [Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions](https://arxiv.org/abs/2402.16364) | 论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。 |
| [^6] | [What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents](https://arxiv.org/abs/2402.13184) | 这项研究引入了“CosmoAgent”，利用LLM模拟人类和外星文明之间的复杂互动，评估和平共存的可行性，并量化评估文明的发展轨迹，同时考虑不同文明之间的巨大多样性。 |
| [^7] | [Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198) | 本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性 |
| [^8] | [A Chinese Dataset for Evaluating the Safeguards in Large Language Models](https://arxiv.org/abs/2402.12193) | 该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。 |
| [^9] | [eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data](https://arxiv.org/abs/2402.08831) | 本文利用开源的大规模高质量指导数据集ECInstruct，通过指导调优通用语言模型，开发了一系列电子商务LLMs（eCeLLM），在电子商务中表现出了显著的优势。 |
| [^10] | [Where Do People Tell Stories Online? Story Detection Across Online Communities](https://arxiv.org/abs/2311.09675) | 介绍了一个解决在线社区中故事检测困难的挑战，提出了StorySeeker工具包，包括详细注释的Reddit数据集和模型，突出了在线叙事的文本特征，引入了叙事跨度检测作为一个新任务。 |
| [^11] | [Empowering Private Tutoring by Chaining Large Language Models](https://arxiv.org/abs/2309.08112) | 通过链式连接大型语言模型，开发了一种全面智能辅导系统，实现了自动课程规划、个性化指导和灵活测验评估。 |
| [^12] | [Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending.](http://arxiv.org/abs/2401.16458) | 本文研究了如何利用P2P借贷平台上借款人提供的文本描述来构建风险指标。结果显示，利用大型语言模型生成的风险评分可以明显提高信用风险分类器的性能。 |
| [^13] | [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models.](http://arxiv.org/abs/2401.09002) | 本研究提出一种新方法评估大型语言模型上越狱攻击效果，引入粗粒度和细粒度评估框架，提供了更全面和细致的评估角度，并开发了专门的真实数据集作为基准，为未来研究建立了基础资源。 |
| [^14] | [Mission: Impossible Language Models.](http://arxiv.org/abs/2401.06416) | 本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。 |
| [^15] | [Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback.](http://arxiv.org/abs/2401.05695) | 这项研究整合了医生的诊断逻辑到大型语言模型中，提出了一种称为偏好学习从过程反馈（PLPF）的方法，并通过实验结果证明了其在医疗对话中的有效性和优越性。 |
| [^16] | [DrawTalking: Building Interactive Worlds by Sketching and Speaking.](http://arxiv.org/abs/2401.05631) | 用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。 |
| [^17] | [Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation.](http://arxiv.org/abs/2311.08640) | 将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。 |
| [^18] | [Reliable Academic Conference Question Answering: A Study Based on Large Language Model.](http://arxiv.org/abs/2310.13028) | 本研究以大型语言模型为基础，开发了可靠的学术会议问答系统，通过组织半结构化的会议数据并进行人工标注，解决了研究人员在获取准确、最新信息时的需求。 |
| [^19] | [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples.](http://arxiv.org/abs/2310.01469) | LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。 |
| [^20] | [Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data.](http://arxiv.org/abs/2309.16804) | 本论文提出了一种框架，通过综合对话数据开发语言学习聊天机器人。该机器人结合聊天机器人互动特征和英语教科书的系统材料，帮助学生提高口语技巧。该方法通过提取教科书主题并使用大型语言模型生成相关对话，通过微调机器人的对话数据创建一个课程驱动型聊天机器人。用户研究表明，该聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。该方法将传统教科书方法与对话式人工智能相结合，为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。 |
| [^21] | [KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation.](http://arxiv.org/abs/2309.14770) | 本研究提出了一种增强关系建模的知识图谱补全方法，通过利用外部知识库生成连贯的描述，并通过反向关系创建对称图来提供额外的标签和补充信息。实验证明这种方法在知识图谱补全方面取得了显著的改进。 |
| [^22] | [Affect Recognition in Conversations Using Large Language Models.](http://arxiv.org/abs/2309.12881) | 本研究探讨了使用大型语言模型（LLMs）识别对话中人类情感的能力，并对开放领域闲聊对话和任务导向对话进行了评估。研究结果表明，LLMs具有零样本和少样本能力，并且通过上下文学习和任务特定微调可以提高模型性能。 |
| [^23] | [Open Sesame! Universal Black Box Jailbreaking of Large Language Models.](http://arxiv.org/abs/2309.01446) | 本文提出了一种使用遗传算法的新颖方法，可以在无法访问模型架构和参数的情况下操纵大规模语言模型 (LLMs)。通过优化通用对抗提示与用户查询结合，可以扰乱被攻击模型的对齐，导致意外和潜在有害的输出。该方法可以揭示模型的局限性和漏洞，为负责任的AI开发提供了一种诊断工具。 |
| [^24] | [Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications.](http://arxiv.org/abs/2306.16143) | 本论文提出了一种在开发领域特定自然语言处理应用中整合生成式用户体验研究的方法。该方法将领域用户纳入原型开发的不同阶段，以更好地了解用户需求和评估用户价值的变化。 |
| [^25] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^26] | [Evidence of Meaning in Language Models Trained on Programs.](http://arxiv.org/abs/2305.11169) | 该论文证明了，通过在程序语料库上训练语言模型，即使没有针对学习语言语义提供归纳偏差，语言模型仍然能够学习含义。线性探测器能够从模型状态中提取程序状态的抽象，准确性与模型泛化到新程序的能力显著相关。 |
| [^27] | [Context-dependent communication under environmental constraints.](http://arxiv.org/abs/2305.05821) | 本文研究了在压缩词汇量的情况下，如何利用环境压力促进情境依赖性沟通的出现，并研究了在接收者无法处理歧义的情况下，发送者如何利用环境的制约因素实现沟通。 |
| [^28] | [From Retrieval to Generation: Efficient and Effective Entity Set Expansion.](http://arxiv.org/abs/2304.03531) | 本文提出了GenExpan，一种基于生成式预训练语言模型的实体集扩展框架，利用前缀树保证实体生成的有效性，采用自动生成的类名来引导模型生成同一类实体，从而提高了效率和可扩展性。 |
| [^29] | [Vision Learners Meet Web Image-Text Pairs.](http://arxiv.org/abs/2301.07088) | 本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。 |

# 详细

[^1]: RoleInteract：评估角色扮演代理的社交互动

    RoleInteract: Evaluating the Social Interaction of Role-Playing Agents

    [https://arxiv.org/abs/2403.13679](https://arxiv.org/abs/2403.13679)

    该论文介绍了RoleInteract，一个旨在评估角色扮演对话代理社交性的基准，覆盖了500个角色、6000多个问题提示和30800个对话话语。

    

    大型语言模型（LLMs）推动了各种AI对话代理的发展，包括模仿不同角色和人类行为的角色扮演对话代理。本文引入了RoleInteract，这是第一个旨在系统评估角色扮演对话代理在社交方面表现的基准。该基准从各种来源构建，涵盖了超过500个角色、6000多个问题提示和30800个多轮角色扮演话语。

    arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
    
[^2]: 使用师生大型语言模型进行多约束分子生成

    Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model

    [https://arxiv.org/abs/2403.13244](https://arxiv.org/abs/2403.13244)

    介绍了一个多约束分子生成大型语言模型TSMMG，通过整合多个小模型和工具来帮助生成符合描述的新分子，在各种约束任务中表现优秀。

    

    尽管已经提出了各种模型和计算工具用于分子的结构和性质分析，但生成符合所有期望结构和性质的分子仍然是一个挑战。在这里，我们介绍了一个多约束分子生成大型语言模型TSMMG，类似于学生，该模型整合了来自各种小模型和工具（即“老师”）的知识。为了训练TSMMG，我们通过从这些‘老师’中提取的分子知识构建了大量文本-分子对，使其能够通过各种文本提示生成符合描述的新分子。我们通过实验证明，TSMMG在生成符合复杂、自然语言描述的两、三和四约束任务的分子方面表现出色，平均分子有效性超过99％，成功率分别为88.08％、65.27％和61.44％。该模型还ex

    arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
    
[^3]: 通过硬负样本增强多模态对比学习中的概念理解

    Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples

    [https://arxiv.org/abs/2403.02875](https://arxiv.org/abs/2403.02875)

    提出了一种通过硬负样本改进多模态对比学习中概念理解的方法，并引入了一个评估视觉-语言模型中颜色、对象和大小细粒度对齐的新数据集。

    

    当前利用对比学习的多模态模型在发展精细的概念理解方面通常存在一些限制。在预训练过程中，由于随机负样本，导致几乎只有非常不同的概念进行损失函数比较。因此，模型在处理细粒度语义差异时遇到困难。为了解决这个问题，我们引入了一种新颖的预训练方法，结合了合成的硬负文本示例。这些硬负样本对应于视觉概念的排列，导致更精细的视觉和文本概念对齐。此外，我们引入了InpaintCOCO，一个用于评估视觉-语言模型中颜色、对象和大小细粒度对齐的新挑战性数据集。我们使用从COCO图像生成的信息填充来创建数据集，通过改变视觉概念，使图像不再与其原始标题匹配。我们的结果显示...

    arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
    
[^4]: InjecAgent：基于工具集成的大型语言模型Agent中的间接提示注入基准测试

    InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents

    [https://arxiv.org/abs/2403.02691](https://arxiv.org/abs/2403.02691)

    本研究引入了InjecAgent基准测试，用于评估工具集成的大型语言模型代理对间接提示注入攻击的脆弱性，通过评估30种LLM代理，发现这些代理存在漏洞

    

    最近的工作将LLMs作为代理体现出来，使它们能够访问工具，执行操作，并与外部内容（例如，电子邮件或网站）进行交互。然而，外部内容引入了间接提示注入（IPI）攻击的风险，恶意指令被嵌入LLMs处理的内容中，旨在操纵这些代理执行对用户有害的操作。考虑到这类攻击的潜在严重后果，建立用于评估和减轻这些风险的基准测试至关重要。在这项工作中，我们介绍了InjecAgent，这是一个旨在评估工具集成的LLM代理对IPI攻击的脆弱性的基准测试。InjecAgent包括1,054个测试用例，涵盖17种不同的用户工具和62种攻击者工具。我们将攻击意图分为两种主要类型：对用户造成直接伤害和窃取私人数据。我们评估了30种不同的LLM代理，并表明这些代理是脆弱的。

    arXiv:2403.02691v1 Announce Type: new  Abstract: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable
    
[^5]: 从哪里出发？来自自然空间描述中的多尺度空间关系推理

    Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions

    [https://arxiv.org/abs/2402.16364](https://arxiv.org/abs/2402.16364)

    论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。

    

    当用自然语言传达路线时，“获得的空间知识”概念对地理信息检索（GIR）和空间认知研究至关重要。然而，自然语言处理导航研究经常忽视这种获得知识对文本描述的影响。当前导航研究集中在以自我为中心的本地描述（例如，“它将在您的右边”），这些描述需要对代理人的本地知觉进行推理。在地图获得的知识基础上的描述提供了环境的整体视图，并捕捉了其总体结构。

    arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
    
[^6]: 如果LLM具有不同的世界观：使用基于LLM的代理模拟外星文明

    What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents

    [https://arxiv.org/abs/2402.13184](https://arxiv.org/abs/2402.13184)

    这项研究引入了“CosmoAgent”，利用LLM模拟人类和外星文明之间的复杂互动，评估和平共存的可行性，并量化评估文明的发展轨迹，同时考虑不同文明之间的巨大多样性。

    

    在这项研究中，我们介绍了“CosmoAgent”，这是一个创新的人工智能框架，利用大型语言模型（LLMs）来模拟人类与外星文明之间复杂的交互，特别强调史蒂芬·霍金关于不要随意向宇宙发送无线电信号的谨慎建议。该研究的目标是评估和平共存的可行性，同时考虑可能威胁善意文明的潜在风险。通过采用数学模型和状态转换矩阵，我们的方法定量评估文明的发展轨迹，为在关键增长和饱和点做出未来决策提供见解。此外，本文承认宇宙中潜在生活条件的巨大多样性可能会促进不同文明之间独特的宇宙观、道德准则和世界观。认识到地球上--

    arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
    
[^7]: 零-shot 可见语言模型用于仇恨模因检测：我们已经到达目标了吗？

    Zero shot VLMs for hate meme detection: Are we there yet?

    [https://arxiv.org/abs/2402.12198](https://arxiv.org/abs/2402.12198)

    本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性

    

    社交媒体上的多媒体内容正在迅速发展，其中模因作为一种独特形式变得日益重要。不幸的是，一些恶意用户利用模因针对个人或易受攻击的社区，因此有必要识别和解决此类恶意模因。已经进行了大量研究来解决这个问题，通过开发仇恨模因检测模型。然而，传统的机器学习/深度学习模型的一个显著局限性是需要带标签的数据集才能进行准确分类。最近，研究界见证了几种可见语言模型的出现，在各种任务中展现出卓越的性能。在这项研究中，我们旨在调查这些可见语言模型在处理诸如仇恨模因检测等复杂任务中的有效性。我们使用各种提示设置来专注于对恶意/有害模因的零-shot 分类。通过我们的分析，我们o

    arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
    
[^8]: 用于评估大型语言模型中安全机制的中文数据集

    A Chinese Dataset for Evaluating the Safeguards in Large Language Models

    [https://arxiv.org/abs/2402.12193](https://arxiv.org/abs/2402.12193)

    该研究介绍了一个用于评估中文LLMs安全性的数据集，提出了细粒度的安全评估标准，以及扩展了两种场景用于识别有风险提示拒绝的虚阔负面和错误肯定示例。

    

    许多研究已经证明大型语言模型（LLMs）可能产生有害响应，在LLMs部署时使用户面临意外风险。先前的研究提出了关于LLMs引发风险的综合分类法，以及相应的提示，可用于检查LLMs的安全机制。然而，这些研究几乎完全集中在英语上，其他语言的研究较少。在本文中，我们旨在弥补这一空白。我们首先介绍了一个用于评估中文LLMs安全性的数据集，然后将其扩展到另外两种情景，可用于更好地识别关于有风险提示拒绝的虚阔负面和错误肯定示例。我们进一步针对每种风险类型提出一组细粒度的安全评估标准，促进人工标注和自动评估LLM响应有害性。我们对五个LLM的实验表明，特定于地区的风险...

    arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
    
[^9]: eCeLLM：从大规模高质量指导数据中将大型语言模型推广到电子商务中

    eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data

    [https://arxiv.org/abs/2402.08831](https://arxiv.org/abs/2402.08831)

    本文利用开源的大规模高质量指导数据集ECInstruct，通过指导调优通用语言模型，开发了一系列电子商务LLMs（eCeLLM），在电子商务中表现出了显著的优势。

    

    通过在开发有效的电子商务模型方面做出巨大努力，传统的电子商务模型在通用电子商务建模上取得了有限的成功，并且在新用户和新产品上的表现不佳——这是一个典型的领域外泛化挑战。与此同时，大型语言模型(LLMs)在许多领域展示出了出色的通用建模和领域外泛化能力。为了充分发挥它们在电子商务中的作用，本文构建了ECInstruct，这是第一个面向电子商务的开源、大规模和高质量的指导数据集。利用ECInstruct，我们通过指导调优通用语言模型开发了一系列电子商务LLMs，称为eCeLLM。我们的综合实验和评估表明，eCeLLM模型在内部环境中明显优于基准模型，包括最先进的GPT-4和最先进的特定任务模型。

    arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
    
[^10]: 人们在哪里在线讲故事？跨在线社区的故事检测

    Where Do People Tell Stories Online? Story Detection Across Online Communities

    [https://arxiv.org/abs/2311.09675](https://arxiv.org/abs/2311.09675)

    介绍了一个解决在线社区中故事检测困难的挑战，提出了StorySeeker工具包，包括详细注释的Reddit数据集和模型，突出了在线叙事的文本特征，引入了叙事跨度检测作为一个新任务。

    

    在线社区中的故事检测是一项具有挑战性的任务，因为故事分散在社区中，并且与单个文本中的非叙事部分交织在一起。我们通过构建和发布StorySeeker工具包来解决这一挑战，其中包括一个包含502个Reddit帖子和评论的丰富注释数据集，一个适应社交媒体背景的详细的代码书，以及用于在文档和跨度级别预测叙事的模型。我们的数据集是从数百个流行的英语Reddit社区中抽样而来，涵盖了33个主题类别，它包含了细粒度的专家注释，包括二元故事标签，故事跨度和事件跨度。我们使用我们的数据评估了一系列检测方法，并确定了在线叙事的独特文本特征，重点关注叙事跨度检测，这是我们引入的一个新任务。我们阐明了大规模叙事的分布特征。

    arXiv:2311.09675v2 Announce Type: replace  Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large
    
[^11]: 通过链接大型语言模型增强私人辅导

    Empowering Private Tutoring by Chaining Large Language Models

    [https://arxiv.org/abs/2309.08112](https://arxiv.org/abs/2309.08112)

    通过链式连接大型语言模型，开发了一种全面智能辅导系统，实现了自动课程规划、个性化指导和灵活测验评估。

    

    人工智能已被应用于在线教育的各个方面，以促进教学和学习。然而，很少有方法致力于完整的AI辅导系统。在这项工作中，我们探讨了一个由最先进的大型语言模型（LLMs）驱动的全面智能辅导系统的开发，涵盖自动课程规划和调整、定制指导以及灵活的测验评估。为了使系统能够经受住长时间交互并满足个性化教育的需求，系统被分解为三个相互连接的核心流程-交互、反思和反应。每个流程都通过链接LLM驱动的工具以及动态更新的记忆模块来实现。工具是LLMs，被提示执行一项特定任务，而记忆是在教育过程中更新的数据存储。学习日志中的统计结果证明了这种方法的有效性。

    arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
    
[^12]: 信用风险与大型语言模型相结合：从P2P借贷的贷款描述中构建风险指标。

    Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])

    [http://arxiv.org/abs/2401.16458](http://arxiv.org/abs/2401.16458)

    本文研究了如何利用P2P借贷平台上借款人提供的文本描述来构建风险指标。结果显示，利用大型语言模型生成的风险评分可以明显提高信用风险分类器的性能。

    

    P2P借贷作为一种独特的融资机制，通过在线平台将借款人与放款人联系起来。然而，P2P借贷面临信息不对称的挑战，因为放款人往往缺乏足够的数据来评估借款人的信用价值。本文提出了一种新颖的方法来解决这个问题，即利用借款人在贷款申请过程中提供的文本描述。我们的方法涉及使用大型语言模型（LLM）处理这些文本描述，LLM是一种能够识别文本中的模式和语义的强大工具。将迁移学习应用于将LLM适应特定任务。我们从Lending Club数据集的分析结果显示，BERT生成的风险评分显著提高了信用风险分类器的性能。然而，基于LLM的系统固有的不透明性，以及潜在偏差的不确定性，限制了其应用。

    Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
    
[^13]: 评估大型语言模型上越狱攻击效果的方法研究

    AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])

    [http://arxiv.org/abs/2401.09002](http://arxiv.org/abs/2401.09002)

    本研究提出一种新方法评估大型语言模型上越狱攻击效果，引入粗粒度和细粒度评估框架，提供了更全面和细致的评估角度，并开发了专门的真实数据集作为基准，为未来研究建立了基础资源。

    

    在我们的研究中，我们开创性地提出了一种评估大型语言模型（LLMs）上越狱攻击效果的新方法，与传统的健壮性评估方法不同。我们的研究引入了两个不同的评估框架：粗粒度评估和细粒度评估。每个框架都使用从0到1的评分范围，提供了独特的视角，能够更全面和细致地评估攻击效果，并帮助攻击者更好地优化攻击提示。此外，我们还开发了一个专门用于越狱任务的全面的真实数据集。这个数据集不仅是我们当前研究的关键基准，也为未来研究建立了一个基础资源，可以在这个不断发展的领域中进行一致和比较的分析。通过与传统评估方法的精心比较，我们发现我们的评估方法与之相一致。

    In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation alig
    
[^14]: 不可能任务：语言模型

    Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])

    [http://arxiv.org/abs/2401.06416](http://arxiv.org/abs/2401.06416)

    本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。

    

    Chomsky和其他人直接声称，大型语言模型(LLMs)能够学习人类无法学习的可能和不可能的语言。然而，很少有发表的实验证据支持这样的说法。在这里，我们通过系统地改变英文数据的词序和语法规则，开发了一组不可能的合成语言，每种语言的复杂程度不同。这些语言位于一个不可能的连续体上：一端是本质上不可能的语言，例如英文单词的随机和不可逆的洗牌，而另一端是在语言学上常被认为是不可能的语言，特别是基于计算词位置的规则。我们报告了广泛的评估来评估GPT-2小型模型学习这些无可争议的不可能语言的能力，并且至关重要的是，在整个过程中进行了这些评估。

    Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
    
[^15]: 将医生的诊断逻辑整合到大型语言模型中：从过程反馈进行偏好学习

    Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])

    [http://arxiv.org/abs/2401.05695](http://arxiv.org/abs/2401.05695)

    这项研究整合了医生的诊断逻辑到大型语言模型中，提出了一种称为偏好学习从过程反馈（PLPF）的方法，并通过实验结果证明了其在医疗对话中的有效性和优越性。

    

    大型语言模型在医疗对话生成中的应用引起了重视，致力于改善响应质量和流畅性。虽然先前的研究在单轮医疗问答任务的模型性能优化方面取得了进展，但有必要增强模型在多轮对话中避免逻辑不一致的能力。为了解决这个问题，我们提出了一种称为从过程反馈进行偏好学习的方法（PLPF），将医生的诊断逻辑整合到LLM中。PLPF包括规则建模、偏好数据生成和偏好对齐，以训练模型遵循诊断过程。使用标准化患者测试的实验结果表明，PLPF将医疗对话中基准模型的诊断准确性提高了17.6％，优于传统的人类反馈强化学习。此外，PLPF在多轮和单轮对话任务中均表现出有效性。

    The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue task
    
[^16]: DrawTalking：通过草图和语言建立互动世界

    DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])

    [http://arxiv.org/abs/2401.05631](http://arxiv.org/abs/2401.05631)

    用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。

    

    我们引入了一种交互式方法，DrawTalking，用户可以通过草图和语言建立互动世界。它强调用户控制和灵活性，并且在没有编程的情况下提供了类似编程的能力。我们在iPad上实现了它。一项开放式研究表明，这种机制与许多创造性探索性用例相契合和适用。我们希望能够激发和指导未来自然用户中心界面的研究。

    We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
    
[^17]: 大型语言模型的多阶段协作知识蒸馏在半监督序列生成任务中的应用

    Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08640](http://arxiv.org/abs/2311.08640)

    将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。

    

    我们研究了半监督序列生成任务，在这种任务中，标记数据太少以至于无法有效地微调模型，同时在大型语言模型 (LLM) 中进行少样本提示的性能也不够理想，尤其是对于一些昂贵且对预训练的 LLM 不熟悉的任务，如解析。本文发现，从上下文学习的 LLM 蒸馏出的学生模型在这些任务上通常比其教师模型具有更好的泛化能力。基于这一发现，我们提出了一种新的方法 - 大型语言模型的多阶段协作知识蒸馏 (MCKD) - 用于这些任务。MCKD 首先进行少样本提示，让LLM为无标签数据生成伪标签。在每个中间知识蒸馏 (KD) 阶段，使用伪标签数据的不重叠分区来训练一对新的学生模型。然后，每个学生模型为其未见分区生成新的和改进的伪标签，在下一个蒸馏阶段中使用。我们展示了该方法的优势。

    We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
    
[^18]: 可靠的学术会议问答：基于大型语言模型的研究

    Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])

    [http://arxiv.org/abs/2310.13028](http://arxiv.org/abs/2310.13028)

    本研究以大型语言模型为基础，开发了可靠的学术会议问答系统，通过组织半结构化的会议数据并进行人工标注，解决了研究人员在获取准确、最新信息时的需求。

    

    计算机科学的快速发展导致学术会议上的研究大量增加，促进了全球学术交流。研究人员在各个阶段都持续寻求关于这些事件的准确、最新信息。这种数据爆发需要一个智能的问答系统来高效解决研究人员的问题，并确保对最新进展的了解。会议信息通常在官方网站上发布，以半结构化的方式组织，并包含大量的文本。为了满足这一需求，我们开发了ConferenceQA数据集，涵盖了7个不同学术会议，并进行了人工标注。首先，我们采用手动和自动方法的组合，以半结构化的JSON格式组织学术会议数据。随后，我们为每个会议注释了近100个问题-答案对。每个对应对应了四个不同的维度分类。为了确保数据的可靠性，我们手动进行了标注。

    The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
    
[^19]: LLM谎言: 幻觉不是漏洞，而是对抗样本的特征。

    LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])

    [http://arxiv.org/abs/2310.01469](http://arxiv.org/abs/2310.01469)

    LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。

    

    大型语言模型（LLM），包括GPT-3.5、LLaMA和PaLM，似乎具有丰富的知识和适应多种任务的能力。然而，我们仍然不能完全信任它们的回答，因为LLM会出现幻觉，即捏造不存在的事实以欺骗用户而不被察觉。幻觉存在的原因和普遍性仍然不清楚。在本文中，我们证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应。这个现象迫使我们重新审视幻觉可能是对抗样本的另一种视角，并且它与常规的对抗样本具有类似的特征，作为LLM的基本特征。因此，我们以对抗的方式将自动幻觉触发方法形式化为幻觉攻击。最后，我们研究了被攻击的对抗提示的基本特征，并提出了一种简单而有效的防御策略。我们的代码已在GitHub上发布。

    Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
    
[^20]: 课程驱动的教育机器人：通过综合对话数据开发语言学习聊天机器人的框架

    Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data. (arXiv:2309.16804v1 [cs.CL])

    [http://arxiv.org/abs/2309.16804](http://arxiv.org/abs/2309.16804)

    本论文提出了一种框架，通过综合对话数据开发语言学习聊天机器人。该机器人结合聊天机器人互动特征和英语教科书的系统材料，帮助学生提高口语技巧。该方法通过提取教科书主题并使用大型语言模型生成相关对话，通过微调机器人的对话数据创建一个课程驱动型聊天机器人。用户研究表明，该聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。该方法将传统教科书方法与对话式人工智能相结合，为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。

    

    在教育场景中，聊天机器人已经变得流行起来，改变了学生与课程互动和教师授课的方式。我们提出了一种名为课程驱动的教育机器人的框架，该框架将聊天机器人的互动特征与英语教科书的系统材料相结合，帮助学生提高口语技巧。我们首先从教科书中提取相关主题，然后使用大型语言模型生成与这些主题相关的对话。接着，我们使用我们生成的对话数据对开源语言模型进行微调，从而创建我们的课程驱动型聊天机器人。用户研究表明，我们的聊天机器人在引领基于课程的对话和适应用户英语水平方面优于ChatGPT。通过将传统教科书方法与对话式人工智能相结合，我们的方法为学习者提供了一个与课程相匹配并提供用户个性化对话练习的互动工具。

    Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice.
    
[^21]: KERMIT: 带有反转变换的增强关系建模的知识图谱补全

    KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation. (arXiv:2309.14770v1 [cs.CL])

    [http://arxiv.org/abs/2309.14770](http://arxiv.org/abs/2309.14770)

    本研究提出了一种增强关系建模的知识图谱补全方法，通过利用外部知识库生成连贯的描述，并通过反向关系创建对称图来提供额外的标签和补充信息。实验证明这种方法在知识图谱补全方面取得了显著的改进。

    

    知识图谱补全是一项基于知识图谱中可用信息填充缺失三元组的任务。在当前的研究中，基于文本的方法通过利用三元组的文本描述来完成任务。然而，这种建模方法可能遇到一些限制，尤其是当描述不能准确充分地表达预期含义时。为了克服这些挑战，我们提出了通过两个额外机制来增加数据的方法。首先，我们使用ChatGPT作为外部知识库，生成连贯的描述以弥补查询和答案之间的语义差距。其次，我们利用反向关系创建对称图，从而为链接预测提供额外的标签和补充信息。这种方法提供了关系实体之间额外的洞察力。通过这些努力，我们观察到了知识图谱补全方面的显著改进。

    Knowledge graph completion is a task that revolves around filling in missing triples based on the information available in a knowledge graph. Among the current studies, text-based methods complete the task by utilizing textual descriptions of triples. However, this modeling approach may encounter limitations, particularly when the description fails to accurately and adequately express the intended meaning. To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers. Secondly, we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction. This approach offers additional insights into the relationships between entities. Through these efforts, we have observed significant improvements in knowledge graph completion
    
[^22]: 使用大型语言模型进行对话中的情感识别

    Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])

    [http://arxiv.org/abs/2309.12881](http://arxiv.org/abs/2309.12881)

    本研究探讨了使用大型语言模型（LLMs）识别对话中人类情感的能力，并对开放领域闲聊对话和任务导向对话进行了评估。研究结果表明，LLMs具有零样本和少样本能力，并且通过上下文学习和任务特定微调可以提高模型性能。

    

    情感识别在人类交流中起着关键作用，涵盖情绪、心情和感受。在会话型人工智能领域，识别和回应人类情感线索的能力对于创建引人入胜且富有同理心的互动至关重要。本研究深入探讨了大型语言模型（LLMs）在对话中识别人类情感的能力，重点关注开放领域闲聊对话和任务导向对话。利用三个不同的数据集，包括IEMOCAP、EmoWOZ和DAIC-WOZ，涵盖了从日常对话到临床面试的不同类型对话，我们评估并比较了LLMs在情感识别方面的性能。我们的研究探讨了LLMs的零样本和少样本能力，通过上下文学习（ICL）以及任务特定微调来提高模型能力。此外，本研究还考虑了自动语音识别（ASR）的潜在影响。

    Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence (AI), the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study delves into the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluated and compared LLMs' performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning (ICL) as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition (ASR) e
    
[^23]: 开门吧！大规模语言模型的通用黑盒破解

    Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01446](http://arxiv.org/abs/2309.01446)

    本文提出了一种使用遗传算法的新颖方法，可以在无法访问模型架构和参数的情况下操纵大规模语言模型 (LLMs)。通过优化通用对抗提示与用户查询结合，可以扰乱被攻击模型的对齐，导致意外和潜在有害的输出。该方法可以揭示模型的局限性和漏洞，为负责任的AI开发提供了一种诊断工具。

    

    大规模语言模型（LLMs）旨在提供有帮助和安全的回复，通常依赖于对齐技术与用户意图和社会指南保持一致。然而，这种对齐可能会被恶意行为者利用，以用于意想不到的目的。在本文中，我们引入了一种新颖的方法，利用遗传算法（GA）在模型架构和参数不可访问时操纵LLMs。GA攻击通过优化通用对抗提示与用户查询结合，扰乱被攻击模型的对齐，导致意外和潜在有害的输出。我们的新颖方法通过揭示模型的局限性和漏洞，系统地揭示了其响应与预期行为不符的情况。通过广泛的实验，我们证明了我们的技术的有效性，从而为关于负责任的AI开发的讨论提供了一种诊断工具。

    Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool 
    
[^24]: 为开发领域特定自然语言处理应用而进行的生成式用户体验研究

    Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])

    [http://arxiv.org/abs/2306.16143](http://arxiv.org/abs/2306.16143)

    本论文提出了一种在开发领域特定自然语言处理应用中整合生成式用户体验研究的方法。该方法将领域用户纳入原型开发的不同阶段，以更好地了解用户需求和评估用户价值的变化。

    

    用户体验（UX）是人机交互（HCI）研究的一部分，专注于提高系统用户的直观性、透明度、简洁性和信任度。大多数针对机器学习（ML）或自然语言处理（NLP）的UX研究都采用数据驱动的方法，即没有关注用户需求，并仅仅将领域用户用于可用性评估。此外，更典型的UX方法是先针对用户的可用性进行定制，而不是首先了解用户需求。本文提出了一种将生成式UX研究整合到开发领域NLP应用中的方法。生成式UX研究将领域用户纳入原型开发的初始阶段，即构思和概念评估阶段，以及最后一阶段评估用户价值的变化。案例研究中，我们报道了一个针对过程工业中日常操作的领域特定语义搜索的完整原型开发过程。

    User experience (UX) is a part of human-computer interaction (HCI) research and focuses on increasing intuitiveness, transparency, simplicity, and trust for system users. Most of the UX research for machine learning (ML) or natural language processing (NLP) focuses on a data-driven methodology, i.e., it fails to focus on users' requirements, and engages domain users mainly for usability evaluation. Moreover, more typical UX methods tailor the systems towards user usability, unlike learning about the user needs first. The paper proposes a methodology for integrating generative UX research into developing domain NLP applications. Generative UX research employs domain users at the initial stages of prototype development, i.e., ideation and concept evaluation, and the last stage for evaluating the change in user value. In the case study, we report the full-cycle prototype development of a domain-specific semantic search for daily operations in the process industry. Our case study shows tha
    
[^25]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^26]: 在编程语言模型中发现语义的证据

    Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11169](http://arxiv.org/abs/2305.11169)

    该论文证明了，通过在程序语料库上训练语言模型，即使没有针对学习语言语义提供归纳偏差，语言模型仍然能够学习含义。线性探测器能够从模型状态中提取程序状态的抽象，准确性与模型泛化到新程序的能力显著相关。

    

    我们提供证据表明，尽管被训练只是执行文本上的下一个标记预测，特别是一个程序语料库，语言模型仍然能够学习含义。每个程序都以（文本）输入输出示例的形式作为规范。与程序一起工作使我们能够精确地定义与语言中有关含义的概念（例如，正确性和语义），使得程序综合成为一个中间测试平台，用于表征语言模型中是否存在含义的存在（或不存在）。我们首先在程序语料库上训练了一个Transformer模型，然后探查了已经完成规范的程序时，经过训练的模型的隐藏状态。尽管没有针对学习语言语义提供归纳偏差，但我们发现，线性探测器能够从模型状态中提取当前和未来程序状态的抽象。此外，线性探测器的准确性与模型泛化到新程序的能力强有力、统计学显著地相关。

    We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
    
[^27]: 环境约束下的情境依赖性沟通

    Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])

    [http://arxiv.org/abs/2305.05821](http://arxiv.org/abs/2305.05821)

    本文研究了在压缩词汇量的情况下，如何利用环境压力促进情境依赖性沟通的出现，并研究了在接收者无法处理歧义的情况下，发送者如何利用环境的制约因素实现沟通。

    

    存在大量的证据表明，现实世界中的沟通不能简单地通过发送具有独立于情境意义的信号来实现。本文以经典的Lewis(1969)信号模型的变体为基础，探讨在情境化场景下产生情境依赖性沟通的条件。具体而言，我们证明了在最小化词汇量的压力下，这种沟通的出现是足够的。同时，我们研究了可能使符号含义得到情境区分的环境条件和认知能力。我们展示了在接受者的指代选择受到环境限制的情况下，发送者可以单方面地利用这些限制，而无需接收者具有澄清歧义的能力。与常见的假设一致，发送者对情境的意识似乎是需要的。我们认为，情境依赖性沟通是一种多层次的情境化现象，其受环境特性的影响至关重要。

    There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properti
    
[^28]: 从检索到生成：高效且有效的实体集扩展方法

    From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])

    [http://arxiv.org/abs/2304.03531](http://arxiv.org/abs/2304.03531)

    本文提出了GenExpan，一种基于生成式预训练语言模型的实体集扩展框架，利用前缀树保证实体生成的有效性，采用自动生成的类名来引导模型生成同一类实体，从而提高了效率和可扩展性。

    

    实体集扩展（ESE）是一项至关重要的任务，旨在扩展由小的种子实体集描述的目标语义类的实体。大多数现有的ESE方法是基于检索的框架，需要提取实体的上下文特征，并计算种子实体和候选实体之间的相似性。为了实现这两个目的，它们必须迭代地遍历语料库和数据集中提供的实体词汇，导致效率和可扩展性较差。实验结果表明，基于检索的ESE方法消耗的时间与实体词汇和语料库的大小成线性增长。本文首先提出了一种生成式ESE框架，Generative Entity Set Expansion (GenExpan)，它利用生成式预训练语言模型来完成ESE任务。具体而言，采用前缀树来保证实体生成的有效性，并采用自动生成的类名来引导模型生成同一类实体。

    Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
    
[^29]: 视觉学习者遇见Web图像-文本对

    Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07088](http://arxiv.org/abs/2301.07088)

    本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。

    

    大多数最新的自监督学习方法都是在维护良好的ImageNet-1K数据集上进行预训练的。在本研究中，考虑到网络数据的出色可伸缩性，我们认为自我监督预训练应该基于嘈杂的网络源图文配对数据。首先，我们在如此设置下，对大规模网络数据上的代表性自监督预训练方法进行了基准研究。我们比较了一系列方法，包括使用被屏蔽的训练目标的单模式方法和使用图像-文本对比训练的多模式方法。我们发现，现有的多模态方法在视觉转移学习任务上并不比单模态方法表现更好。我们提出了一个信息论视角来解释这些基准结果，这提供了如何设计新型视觉学习者的见解。受到这些见解的启发，我们提出了一种新的视觉表示预训练方法——多模式生成器（MUG），它从可伸缩的网络源图文数据中学习。MUG在几个视觉数据集的转移学习任务上取得了最先进的性能，在CIFAR-10上优于之前最佳的结果3.4％，在STL-10上优于之前最佳的结果2.2％。

    Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
    

