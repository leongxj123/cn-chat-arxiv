# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Scalable Model Editing via Customized Expert Networks](https://arxiv.org/abs/2404.02699) | 通过引入SCEN方法，使用定制的专家网络实现了可扩展的模型编辑，解决了大型语言模型中的幻觉和过时知识问题，并在实验证实中取得了最先进的结果。 |
| [^2] | [Laying Anchors: Semantically Priming Numerals in Language Modeling](https://arxiv.org/abs/2404.01536) | 通过生成受数字分布规律控制的锚点，我们引入了一种在语义上引导数字的策略，在广泛范围的数字任务上实现了数学基础表示的显著改进。 |
| [^3] | [Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided](https://arxiv.org/abs/2404.01288) | 通过引入认知重评，这项工作将心理学原则融入大型语言模型中，为其提供先进的心理学能力。 |
| [^4] | [MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models](https://arxiv.org/abs/2403.19913) | 提出了用于评估大型语言模型执行文本映射和导航能力的MANGO基准，发现即使是迄今为止最好的语言模型GPT-4在回答涉及映射和导航的问题时表现不佳。 |
| [^5] | [STaR-GATE: Teaching Language Models to Ask Clarifying Questions](https://arxiv.org/abs/2403.19154) | 通过奖励语言模型生成有用问题来自我改进的方法，提问者通过询问角色扮演者来引出偏好，从而迭代微调以增加任务高质量响应的概率。 |
| [^6] | [Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349) | 这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。 |
| [^7] | [Duwak: Dual Watermarks in Large Language Models](https://arxiv.org/abs/2403.13000) | Duwak提出了一种在大型语言模型中嵌入双重秘密模式的水印技术，可以显著提高水印的效率和质量。 |
| [^8] | [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530) | Gemini 1.5 Pro是一种高效计算的多模态混合模型，能在数百万标记的上下文中回忆和推理信息，达到近乎完美的长上下文检索任务召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平。 |
| [^9] | [MediSwift: Efficient Sparse Pre-trained Biomedical Language Models](https://arxiv.org/abs/2403.00952) | MediSwift在生物医学领域引入了高效稀疏预训练模型，通过75%的权重稀疏性实现了2-2.5倍的训练FLOPs减少，从而显著提高了效率。 |
| [^10] | [Empowering Large Language Model Agents through Action Learning](https://arxiv.org/abs/2402.15809) | 学习新动作的能力对于大型语言模型代理的学习进步至关重要，本研究提出了开放式行为学习框架，通过迭代学习策略改进动作，增强代理的学习效果。 |
| [^11] | [Towards Privacy-Aware Sign Language Translation at Scale](https://arxiv.org/abs/2402.09611) | 本研究提出了一种两阶段框架，用于实现规模化隐私感知手语翻译。我们利用自监督视频预训练和有监督微调的方法，在数据稀缺和隐私风险的情况下实现了最先进的手语翻译性能。 |
| [^12] | [Retrieval-Augmented Thought Process as Sequential Decision Making](https://arxiv.org/abs/2402.07812) | 检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。 |
| [^13] | [Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction](https://arxiv.org/abs/2311.06555) | 通过启发式驱动的类比链接促进方法，该研究增强了大型语言模型用于文档级事件论证提取，使其能够从示例中学习任务特定启发式，并通过类比推理处理新情况以提高性能。 |
| [^14] | [TarGEN: Targeted Data Generation with Large Language Models.](http://arxiv.org/abs/2310.17876) | TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。 |
| [^15] | [CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias.](http://arxiv.org/abs/2308.12539) | CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。 |
| [^16] | [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success.](http://arxiv.org/abs/2307.06865) | 本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。 |
| [^17] | [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts.](http://arxiv.org/abs/2306.04845) | 该论文提出了一种混合超网络方法，通过基于结构路由的专家混合来增强超级网络模型的表达能力，改善了子网络的质量问题和性能差异。 |

# 详细

[^1]: 通过定制化专家网络实现可扩展的模型编辑

    Scalable Model Editing via Customized Expert Networks

    [https://arxiv.org/abs/2404.02699](https://arxiv.org/abs/2404.02699)

    通过引入SCEN方法，使用定制的专家网络实现了可扩展的模型编辑，解决了大型语言模型中的幻觉和过时知识问题，并在实验证实中取得了最先进的结果。

    

    大型语言模型中存在幻觉和过时知识的问题对于其可靠应用至关重要。模型编辑是一个有前途的途径，可以以成本效益的方式减轻这些挑战。然而，现有方法经常受到不令人满意的泛化和对不相关样本的意外影响。为了克服这些限制，我们引入了一种新方法：通过定制化专家网络实现可扩展的模型编辑（SCEN），这是一个有两个阶段的连续训练范式。具体地，在第一个阶段，我们为每个需要更新的知识片段单独训练轻量级专家网络。随后，我们训练每个专家对应的神经元来控制该专家的激活状态。我们在两种不同规模的开源大型语言模型，Llama2 7B 和 13B 上的实验证实，相比现有主流的模型编辑方法取得了最先进的结果。

    arXiv:2404.02699v1 Announce Type: new  Abstract: Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editi
    
[^2]: 放置锚点：在语言建模中给数字语义上的引导

    Laying Anchors: Semantically Priming Numerals in Language Modeling

    [https://arxiv.org/abs/2404.01536](https://arxiv.org/abs/2404.01536)

    通过生成受数字分布规律控制的锚点，我们引入了一种在语义上引导数字的策略，在广泛范围的数字任务上实现了数学基础表示的显著改进。

    

    现有大量预训练语言模型已成为自然语言处理管线中的事实标准，然而这些模型未能正确编码数字，限制了它们在需要数字理解的任务上的性能。我们引入了一种策略，通过在任何语料库中生成受数字分布规律控制的锚点来在语义上引导数字，从而实现这些数字标记的数学基础表示。我们通过对一系列数值任务进行评估，证明了我们提出的技术的优越性，对领域内（已见）和领域外（未见）的数字都适用。此外，我们将实证评估扩展到从1到10亿的数字范围，比以往相同类型研究的范围广得多，展示了我们学得的嵌入向数学上的显著改进。

    arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
    
[^3]: 大型语言模型能够提供认知重评，如得到指导

    Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided

    [https://arxiv.org/abs/2404.01288](https://arxiv.org/abs/2404.01288)

    通过引入认知重评，这项工作将心理学原则融入大型语言模型中，为其提供先进的心理学能力。

    

    大型语言模型（LLMs）为情感支持提供了新的机会，最近的研究表明，它们可以对处于困境中的人产生共情回应。然而，长期的心理健康需要情绪自我调节，而一次性的共情回应则显得力不从心。本文通过参与认知重评迈出了第一步，认知重评是心理学从业者使用语言有针对性地改变个体对情境的负面评价的策略；这种评价被认为是人类情感体验的根源。我们假设心理学上的原则可以使LLMs具备这种先进的心理学能力，并设计了RESORT，其中包括一系列跨多个维度的重评构成，可用作LLM的指导。我们对LLM进行了首次专家评估（由持有硕士或博士学位的临床心理学家进行）。

    arXiv:2404.01288v1 Announce Type: new  Abstract: Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM
    
[^4]: MANGO：用于评估大型语言模型映射和导航能力的基准

    MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models

    [https://arxiv.org/abs/2403.19913](https://arxiv.org/abs/2403.19913)

    提出了用于评估大型语言模型执行文本映射和导航能力的MANGO基准，发现即使是迄今为止最好的语言模型GPT-4在回答涉及映射和导航的问题时表现不佳。

    

    如ChatGPT和GPT-4等大型语言模型最近在各种自然语言处理任务上取得了惊人的性能。本文提出了MANGO，这是一个用于评估它们执行基于文本映射和导航能力的基准。我们的基准包括来自一套文本游戏的53个迷宫：每个迷宫都与一个游览说明配对，其中包含每个位置的访问但不涵盖所有可能的路径。任务是问答：对于每个迷宫，大型语言模型读取游览说明并回答数百个映射和导航问题，例如“你应该从房子西部如何去阁楼？”和“如果我们从地下室向北和东走，我们会在哪里？”。尽管这些问题对人类来说很容易，但事实证明，迄今为止最好的语言模型GPT-4甚至在回答这些问题时表现不佳。此外，我们的实验表明，强大的映射和导航能力将有利于大型语言模型。

    arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
    
[^5]: STaR-GATE: 教授语言模型询问澄清问题

    STaR-GATE: Teaching Language Models to Ask Clarifying Questions

    [https://arxiv.org/abs/2403.19154](https://arxiv.org/abs/2403.19154)

    通过奖励语言模型生成有用问题来自我改进的方法，提问者通过询问角色扮演者来引出偏好，从而迭代微调以增加任务高质量响应的概率。

    

    当提示语言模型完成任务时，用户通常会遗漏重要的细节。虽然提问可以解决这种歧义，但模型往往很难提出好问题。我们探讨了语言模型通过奖励模型生成有用问题来自我改进的能力，这是一种简单方法，我们称之为STaR-GATE。我们生成了一个包含25,500个独特人物-任务提示的合成数据集，以模拟预训练语言模型--提问者--与一个其偏好未知的角色扮演者之间的对话。通过提问，提问者从角色扮演者那里引出偏好。提问者在那些增加高质量响应概率的问题上进行迭代微调，这些问题是由具有对角色扮演者访问权限的预言者生成的。

    arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
    
[^6]: 拒绝提高可靠性：使用强化学习从知识反馈训练LLMs拒绝未知问题

    Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback

    [https://arxiv.org/abs/2403.18349](https://arxiv.org/abs/2403.18349)

    这里是中文总结出的一句话要点: 该论文研究了拒绝机制在提高大型语言模型可靠性中的作用，提出了一种基于知识反馈的强化学习框架RLKF。

    

    大型语言模型（LLMs）经常生成错误输出，被称为幻想，这是由于它们在辨别超出其知识范围的问题时的局限性。虽然解决幻想一直是研究的焦点，以往的努力主要集中在提高正确性而未充分考虑拒绝机制的重要性。本文全面研究了拒绝的作用，引入了模型可靠性的概念以及相应的度量标准。这些度量标准衡量了模型在提供准确响应的同时，灵活拒绝超出其知识边界的问题，从而最小化幻想。为了提高LLMs固有的可靠性，我们提出了一种名为知识反馈强化学习（RLKF）的新对齐框架。

    arXiv:2403.18349v1 Announce Type: new  Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and 
    
[^7]: Duwak: 大型语言模型中的双重水印

    Duwak: Dual Watermarks in Large Language Models

    [https://arxiv.org/abs/2403.13000](https://arxiv.org/abs/2403.13000)

    Duwak提出了一种在大型语言模型中嵌入双重秘密模式的水印技术，可以显著提高水印的效率和质量。

    

    随着大型语言模型（LLM）在文本生成任务中的日益使用，审计它们的用途、管理它们的应用并减轻其潜在危害至关重要。本文提出了Duwak，通过在令牌概率分布和抽样方案中嵌入双重秘密模式，从根本上提高了水印的效率和质量。

    arXiv:2403.13000v1 Announce Type: cross  Abstract: As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances 
    
[^8]: Gemini 1.5：解锁跨数百万标记上下文的多模态理解

    Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

    [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)

    Gemini 1.5 Pro是一种高效计算的多模态混合模型，能在数百万标记的上下文中回忆和推理信息，达到近乎完美的长上下文检索任务召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平。

    

    在这份报告中，我们介绍了Gemini家族的最新模型Gemini 1.5 Pro，这是一个高效计算的多模态专家混合模型，能够回忆和推理数百万标记上下文中的细粒度信息，包括多个长文档和几小时的视频和音频。Gemini 1.5 Pro在各种形式的长上下文检索任务中实现了近乎完美的召回率，改进了长文档问答、长视频问答和长上下文ASR的最新技术水平，并在广泛一系列基准测试中与Gemini 1.0 Ultra的最新技术水平相匹敌甚至超过。在研究Gemini 1.5 Pro长上下文能力的极限时，我们发现在至少10M标记的范围内继续改进下一个标记的预测，并且几乎完美地达到了超过99%的检索率，这是对现有模型如Claude 2.1（200k）和GPT-4 Turbo（128k）的世代性飞跃。最后，我们突出了大型语言模型在新领域的令人惊讶的新能力。

    arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
    
[^9]: MediSwift：高效稀疏预训练生物医学语言模型

    MediSwift: Efficient Sparse Pre-trained Biomedical Language Models

    [https://arxiv.org/abs/2403.00952](https://arxiv.org/abs/2403.00952)

    MediSwift在生物医学领域引入了高效稀疏预训练模型，通过75%的权重稀疏性实现了2-2.5倍的训练FLOPs减少，从而显著提高了效率。

    

    大型语言模型（LLMs）通常在通用源数据上进行训练，用于各种领域，但最近领域特定的LLMs激增表明它们在领域特定任务（例如生物医学）中的潜力超过了通用型模型。虽然领域特定的预训练提高了效率并导致模型更小，但这些LLMs的训练计算成本仍然很高，构成了预算挑战。我们引入了MediSwift，一套利用领域特定生物医学文本数据上的稀疏预训练的生物医学LM。通过在预训练阶段引入高达75％的权重稀疏性，MediSwift在训练FLOPs方面实现了2-2.5倍的减少。值得注意的是，所有的稀疏预训练均在专门设计用于实现来自非结构化权重稀疏性的加速好处的Cerebras CS-2系统上进行，从而显着提高了MediSwift模型的效率。

    arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f
    
[^10]: 通过行为学习增强大型语言模型代理

    Empowering Large Language Model Agents through Action Learning

    [https://arxiv.org/abs/2402.15809](https://arxiv.org/abs/2402.15809)

    学习新动作的能力对于大型语言模型代理的学习进步至关重要，本研究提出了开放式行为学习框架，通过迭代学习策略改进动作，增强代理的学习效果。

    

    大型语言模型（LLM）代理近来引起越来越多的关注，然而它们在从试错中学习的能力方面存在限制，这是智能行为的关键因素。本研究认为，从经验中学习新动作的能力对于LLM代理的学习进步至关重要。虽然人类自然地扩展他们的动作空间并通过经验学习发展技能，但LLM代理通常在固定的动作空间内操作，限制了它们的成长潜力。为解决这些挑战，我们的研究探讨了语言代理的开放式行为学习。我们提出了一个名为LearnAct的框架，采用迭代学习策略来创建和改进Python函数形式的动作。在每次迭代中，LLM根据在失败的训练任务中识别出的错误，修订和更新当前可用的动作，从而增强动作的有效性。我们的实验评估是...

    arXiv:2402.15809v1 Announce Type: new  Abstract: Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations acr
    
[^11]: 实现规模化隐私感知手语翻译

    Towards Privacy-Aware Sign Language Translation at Scale

    [https://arxiv.org/abs/2402.09611](https://arxiv.org/abs/2402.09611)

    本研究提出了一种两阶段框架，用于实现规模化隐私感知手语翻译。我们利用自监督视频预训练和有监督微调的方法，在数据稀缺和隐私风险的情况下实现了最先进的手语翻译性能。

    

    手语翻译的一个主要障碍是数据稀缺。目前在网络上可用的大部分手语数据由于缺乏对齐的字幕而无法用于训练监督模型。此外，使用大规模网络抓取的数据集来扩展手语翻译存在隐私风险，因为其中包含生物特征信息，负责任地开发手语翻译技术应该考虑这一点。在这项工作中，我们提出了一种针对规模化隐私感知手语翻译的两阶段框架，解决了这两个问题。我们引入了SSVP-SLT，它利用匿名和未注释的视频进行自监督视频预训练，然后利用经过筛选的平行数据集进行有监督的手语翻译微调。 SSVP-SLT在How2Sign数据集上实现了最新的微调和零次gloss-free手语翻译性能，比最强的基线模型提高了3个BLEU-4。通过受控实验，我们证明了我们的方法在多个语言和手语词汇上都具有较好的泛化能力。

    arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
    
[^12]: 检索增强的思维过程作为序列决策制定

    Retrieval-Augmented Thought Process as Sequential Decision Making

    [https://arxiv.org/abs/2402.07812](https://arxiv.org/abs/2402.07812)

    检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。

    

    大型语言模型(LLM)展示了其强大的辅助人类并展现出"智能的火花"的能力。然而，几个开放挑战阻碍了它们的广泛应用：如对隐私的关注、倾向于产生幻觉、难以处理长文本。在本研究中，我们通过引入检索增强思维过程(RATP)来解决这些挑战。通过获取外部知识，RATP将LLM的思考生成过程定式为多步决策过程。为了优化这种思考过程，RATP利用蒙特卡洛树搜索，并学习了一个Q值估计器，实现了高效的推理。在处理具有私人数据的问答任务时，LLM训练方法受到伦理和安全问题的限制。RATP在上下文检索增强语言模型的基础上实现了50%的性能提升。

    Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
    
[^13]: 启发驱动的类比链接促进：增强大型语言模型用于文档级事件论证提取

    Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction

    [https://arxiv.org/abs/2311.06555](https://arxiv.org/abs/2311.06555)

    通过启发式驱动的类比链接促进方法，该研究增强了大型语言模型用于文档级事件论证提取，使其能够从示例中学习任务特定启发式，并通过类比推理处理新情况以提高性能。

    

    在这项研究中，我们调查了文档级事件论证提取中的上下文学习（ICL），以减轻这一任务对大规模标记数据的依赖。我们引入了启发驱动的类比链接（HD-LoA）提示，以解决示例选择的挑战，并开发了一种为EAE量身定制的提示策略。具体而言，我们假设并验证了LLMs通过ICL从示范中学习任务特定启发式。基于这一假设，我们引入了一种显式的启发式驱动示范构建方法，将杂乱的示例选择过程转化为强调任务启发式的有条不紊方法。此外，受人类类比推理的启发，我们提出了类比链接提示，使LLMs能够通过将新情况类比于已知情况来处理新情况，从而提高它们在有限ICL示例以外的未见类别上的性能。

    arXiv:2311.06555v2 Announce Type: replace-cross  Abstract: In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL exa
    
[^14]: TarGEN: 基于大型语言模型的目标数据生成技术

    TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])

    [http://arxiv.org/abs/2310.17876](http://arxiv.org/abs/2310.17876)

    TarGEN是一种利用大型语言模型生成高质量合成数据集的多步提示策略，通过自我修正方法确保可靠的标签。在SuperGLUE基准测试中，模型在合成数据集上的训练效果与原始数据集相当。

    

    大型语言模型（LLM）的快速发展引发了对数据合成技术的兴趣，旨在生成多样且高质量的合成数据集。然而，这些合成数据集往往缺乏多样性并且存在噪声。在本文中，我们提出了TarGEN，一种利用LLM生成高质量的合成数据集的多步提示策略。TarGEN的一个优点是无需种子；它不需要特定的任务实例，扩大了其适用性。我们还通过一种称为自我修正的方法，使LLM能够在创建数据集过程中纠正标记错误的实例，确保可靠的标签。为了评估我们技术的有效性，我们模拟了SuperGLUE基准测试中的8个任务，并在合成和原始训练集上微调了各种语言模型，包括仅编码器、编码器-解码器和仅解码器模型。在原始测试集上的评估结果显示，模型在合成数据集上训练的效果与原始数据集相当。

    The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine
    
[^15]: CALM: 一种用于全面评估语言模型偏见的多任务基准数据集

    CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])

    [http://arxiv.org/abs/2308.12539](http://arxiv.org/abs/2308.12539)

    CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。

    

    随着语言模型（LMs）的不断增强，量化和比较它们在社会和人口学偏见方面的能力以及潜在的危害变得越来越重要。先前的偏见测量数据集对于人工设计模板的扰动敏感，因此不可靠。为了保证可靠性，我们引入了全面评估语言模型偏见（CALM）的基准数据集，用于量化LMs在三个任务上的偏见。我们整合了来自不同领域（如维基百科和新闻文章）的16个现有数据集，过滤出224个模板，并构建了一个包含78,400个示例的数据集。我们通过平均语义相似性和模板长度的变异程度等指标，比较CALM与先前数据集的多样性，并测试其对细微扰动的敏感性。我们展示了我们的数据集相对于先前数据集更加多样和可靠，因此能更好地捕捉评估模型偏见所需的语言变化的广度。我们评估了20个大型语言模型的偏见。

    As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
    
[^16]: 提示不应被视为秘密：系统地衡量提示提取攻击的成功性

    Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])

    [http://arxiv.org/abs/2307.06865](http://arxiv.org/abs/2307.06865)

    本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。

    

    大型语言模型的生成通常通过提示技术来控制，其中用户对模型的查询以旨在指导模型在该查询上的行为的提示作为前缀。公司用于指导其模型的提示通常被视为秘密，隐藏在查询的用户之外。它们甚至被视为可以买卖的商品。然而，有经验性的证据显示，即使提示被保密，用户仍然可以提取它们。在本文中，我们提出了一个系统地衡量提示提取攻击成功的框架。在使用多个提示源和多个基础语言模型的实验中，我们发现简单的基于文本的攻击实际上可以高概率地揭示提示。

    The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
    
[^17]: 混合超网络：通过基于结构路由的专家混合改进共享权重超网络训练

    Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])

    [http://arxiv.org/abs/2306.04845](http://arxiv.org/abs/2306.04845)

    该论文提出了一种混合超网络方法，通过基于结构路由的专家混合来增强超级网络模型的表达能力，改善了子网络的质量问题和性能差异。

    

    共享权重的超级网络已经成为当前最先进的神经体系结构搜索（NAS）框架中性能评估的关键组成部分。然而，由于权重共享，超级网络直接生成的不同子网络的质量无法保证。在机器翻译和预训练语言建模等NLP任务中，我们观察到，在相同的模型架构下，超级网络与从头开始训练之间存在较大的性能差距。因此，在找到最佳架构后，不能直接使用超级网络，必须进行重新训练。我们提出了混合超网络，这是一种广义的超级网络公式，其中采用了专家混合（MoE）来增强超级网络模型的表达能力，训练开销可以忽略不计。通过这种方式，不同的子网络不是直接共享模型权重，而是通过基于结构的路由机制共享。因此，模型性能得到了改善。

    Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures.  In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but through an architecture-based routing mechanism. As a result, model 
    

