# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models](https://arxiv.org/abs/2403.13513) | 本文引入了反事实启示（Counterfactual Inception）方法，通过将反事实思想植入到大型多模态模型（LMMs）中，可以减轻幻觉效应并提高模型的可信度。 |
| [^2] | [Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models](https://arxiv.org/abs/2403.00794) | 利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。 |
| [^3] | [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840) | 研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。 |
| [^4] | [Improving LLM-based Machine Translation with Systematic Self-Correction](https://arxiv.org/abs/2402.16379) | 引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。 |
| [^5] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^6] | [LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation](https://arxiv.org/abs/2402.11943) | LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models. |
| [^7] | [Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs](https://arxiv.org/abs/2402.11442) | 提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。 |
| [^8] | [Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models](https://arxiv.org/abs/2402.10552) | 通过对话式SimulMT框架，本文提高了基于LLM的SimulMT推理效率，在保持翻译质量的同时实现与专门的SimulMT模型相近的计算延迟。 |
| [^9] | [ApiQ: Finetuning of 2-Bit Quantized Large Language Model](https://arxiv.org/abs/2402.05147) | 这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。 |
| [^10] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^11] | [Exploring transfer learning for pathological speech feature prediction: Impact of layer selection](https://arxiv.org/abs/2402.01796) | 本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。 |
| [^12] | [Jellyfish: A Large Language Model for Data Preprocessing](https://arxiv.org/abs/2312.01678) | 这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整 |
| [^13] | [AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters.](http://arxiv.org/abs/2401.06408) | 这项研究调查了十个质量和语言识别过滤器对不同社交维度变化的网页的影响。实验发现在数据筛选过程中存在隐含的偏好，一些质量分类器类似于主题过滤器，而语言识别可能会忽视某些地区的英语内容。我们的研究为促进更公正和全面的模型开发提供了洞察。 |
| [^14] | [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models.](http://arxiv.org/abs/2401.01301) | 大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。 |
| [^15] | [GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents.](http://arxiv.org/abs/2310.12821) | GestureGPT是一个零样本交互手势理解和对接框架，利用大语言模型代理解读手势描述并根据交互环境提供上下文信息，能够将用户意图对接到交互功能上。 |
| [^16] | [XNLP: An Interactive Demonstration System for Universal Structured NLP.](http://arxiv.org/abs/2308.01846) | XNLP演示系统是一个通用的、高性能的自然语言处理平台，通过提供通用的建模方法、解释性、可扩展性和互动性，实现了统一XNLP任务。 |
| [^17] | [CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models.](http://arxiv.org/abs/2305.14318) | CREATOR是一个新的框架，使得大型语言模型（LLMs）能够利用文档和代码实现创建自己的工具。通过将抽象工具创建和具体决策执行解耦，CREATOR提高了性能，并在不同基准测试中超越了现有的基线方法。此外，我们引入了Creation Challenge数据集，展示了LLMs的工具创建能力的必要性和优势。 |
| [^18] | [Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models.](http://arxiv.org/abs/2305.13712) | 本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。 |

# 详细

[^1]: 如果......会怎样？：反事实启示在大型多模态模型中减轻幻觉效应

    What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models

    [https://arxiv.org/abs/2403.13513](https://arxiv.org/abs/2403.13513)

    本文引入了反事实启示（Counterfactual Inception）方法，通过将反事实思想植入到大型多模态模型（LMMs）中，可以减轻幻觉效应并提高模型的可信度。

    

    本文介绍了提高大型多模态模型（LMMs）在处理幻觉效应方面可靠性的方法，其中模型会生成不正确或无关的响应。没有额外的指导调整范式，我们引入了反事实启示，这是一种新颖的方法，通过精心选择的、不对齐的反事实关键词将反事实思想植入到LMMs中。该方法根植于反事实思维概念，这是一种认知过程，人类在其中考虑替代现实和结果。通过将这种类似人类的推理机制应用到LMMs中，我们旨在减少幻觉效应并提高模型的可信度。我们还提出了双模态验证过程（DVP），这是一个严格的框架，用于选择触发LMMs中反事实思维的最佳反事实关键词，同时考虑视觉和语言上下文。我们在各种LMMs上进行了大量实验

    arXiv:2403.13513v1 Announce Type: cross  Abstract: This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords. This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, i
    
[^2]: 认真对待幽默：利用不风趣的大型语言模型构建幽默数据集

    Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models

    [https://arxiv.org/abs/2403.00794](https://arxiv.org/abs/2403.00794)

    利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。

    

    幽默是人类认知和互动的基本要素。然而，尽管自然语言处理方面取得了近期进展，幽默检测仍然是一项具有挑战性的任务，这是因为幽默文本与类似非幽默文本的数据集稀缺。在我们的研究中，我们探讨了大型语言模型（LLMs）能否通过编辑文本生成用于幽默检测的合成数据。我们在现有人类数据集上对LLMs进行基准测试，并展示当前LLMs在“取消风趣”笑话方面显示出令人印象深刻的能力，这是由人类判断和幽默检测的下游任务衡量而得。我们将我们的方法扩展到了一个混合编码的英语-印地语幽默数据集，在那里我们发现GPT-4的合成数据被双语注释员高度评价，并为幽默分类器提供了具有挑战性的对抗性例子。

    arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
    
[^3]: 遵循我的指示并说出真相：来自检索增强生成系统的可扩展数据提取

    Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2402.17840](https://arxiv.org/abs/2402.17840)

    研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。

    

    检索增强生成（RAG）通过在测试时将外部知识纳入预训练模型，从而实现定制适应，提升了模型性能。本研究探讨了Retrieval-In-Context RAG语言模型（LMs）中的数据泄露风险。我们展示了当对使用指令调整的LMs构建的RAG系统进行提示注入时，对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据。这种漏洞存在于覆盖Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5和Platypus2等多种现代LMs的广泛范围内，并且随着模型规模的扩大，利用能力加剧。将研究扩展到生产RAG模型GPTs，我们设计了一种攻击，可以在对25个随机选择的定制GPTs施加最多2个查询时以100%成功率导致数据存储泄漏，并且我们能够以77,000字的书籍中的文本数据的提取率为41%，以及在含有1,569,00词的语料库中的文本数据的提取率为3%。

    arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
    
[^4]: 用系统自校正改进基于LLM的机器翻译

    Improving LLM-based Machine Translation with Systematic Self-Correction

    [https://arxiv.org/abs/2402.16379](https://arxiv.org/abs/2402.16379)

    引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。

    

    大型语言模型（LLMs）在机器翻译（MT）领域取得了令人印象深刻的结果。然而，人工仔细评估发现，LLMs生成的翻译仍然包含多个错误。重要的是，将这种错误信息反馈到LLMs中可以实现自校正，并改善翻译性能。受到这些观点的启发，我们引入了一个名为TER的系统LLM自校正翻译框架，代表了在这一方向上的重要进展。我们的研究结果表明：1）我们的自校正框架成功地帮助LLMs提高了多种语言的翻译质量，不管是从高资源语言到低资源语言，还是以英语为中心还是围绕其他语言；2）TER相比先前的方法展示出更优越的系统性和可解释性；3）

    arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
    
[^5]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^6]: LEMMA: 支持外部知识增强的LVLM增强多模态虚假信息检测

    LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation

    [https://arxiv.org/abs/2402.11943](https://arxiv.org/abs/2402.11943)

    LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models.

    

    社交平台上多模态虚假信息的兴起对个人和社会都带来了重大挑战。与文本虚假信息相比，多模态虚假信息具有更高的可信度和更广泛的影响，使得检测变得复杂，需要跨越不同媒体类型进行强大的推理，并具备准确验证的深刻知识。大规模视觉语言模型（LVLM）的出现为解决这一问题提供了潜在方案。利用LVLM在处理视觉和文本信息方面的熟练能力，LVLM在识别复杂信息和展现强大推理能力方面展示出有希望的能力。在本文中，我们首先研究了LVLM在多模态虚假信息检测中的潜力。我们发现，尽管LVLM的性能优于LLMs，但其深刻推理可能由于缺乏证据而表现出有限的效力。基于这些观察，我们提出了LEMMA：LVLM增强多模态虚假信息检测。

    arXiv:2402.11943v1 Announce Type: new  Abstract: The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Det
    
[^7]: 能够与规则进行推理吗？逻辑支架用于压力测试和提升LLM

    Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs

    [https://arxiv.org/abs/2402.11442](https://arxiv.org/abs/2402.11442)

    提出了一个逻辑支架推理规则生成框架，通过构建包含基础和组合规则的推理规则库ULogic，揭示了LLMs在逻辑理解方面与人类表现的显著差距，并通过生成准确、复杂和抽象的推理结果来提升下游推理。

    

    大型语言模型(LLMs)在各种推理任务中取得了令人印象深刻的接近人类表现的成绩。然而，它们对于基础推理规则的掌握仍然不及人类能力。为了研究这一问题，我们提出了一个逻辑支架推理规则生成框架，构建了一个包含五个领域中基础和组合规则的推理规则库ULogic。我们对GPT系列模型在规则子集上的分析揭示出LLMs在逻辑理解方面与人类表现存在显著差距，特别是在具有某些偏见模式的组合和结构复杂规则中。我们进一步将这些规则提炼成一个更小规模的推理引擎，用于灵活地生成规则并增强下游推理。通过多评估人员评估，我们的推理引擎证明在生成准确、复杂和抽象的结论和前提方面表现出效果，可以改善各种常识推理。

    arXiv:2402.11442v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reas
    
[^8]: Conversational SimulMT: 基于大型语言模型的高效同时翻译

    Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models

    [https://arxiv.org/abs/2402.10552](https://arxiv.org/abs/2402.10552)

    通过对话式SimulMT框架，本文提高了基于LLM的SimulMT推理效率，在保持翻译质量的同时实现与专门的SimulMT模型相近的计算延迟。

    

    同声机器翻译（SimulMT）在翻译质量和延迟之间存在挑战性的权衡。最近的研究表明，大型语言模型（LLMs）在SimulMT任务中可以取得很好的表现。然而，这往往是以推理成本和延迟的增加为代价的。本文提出了一种对话式SimulMT框架，通过基于多轮对话的解码来提高基于LLM的SimulMT的推理效率。我们在两个SimulMT基准上使用Llama2-7b-chat进行实验，结果表明LLM在翻译质量上具有优势，同时实现与专门的SimulMT模型相当的计算延迟。

    arXiv:2402.10552v1 Announce Type: new  Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
    
[^9]: ApiQ：2位量化大型语言模型的微调

    ApiQ: Finetuning of 2-Bit Quantized Large Language Model

    [https://arxiv.org/abs/2402.05147](https://arxiv.org/abs/2402.05147)

    这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。

    

    随着大型语言模型的增大，内存高效的模型微调近年来备受关注，主要是由于GPU内存限制和这些方法与完全微调的可比结果所带来的约束。尽管有了进展，如QLoRA这样的内存高效微调策略在不同位宽的量化和多样化任务中表现不一致。这种不一致主要来自于量化过程对保留知识的有害影响，导致灾难性遗忘，削弱了预训练模型在微调中的利用。在这项工作中，我们引入了一种名为ApiQ的新型量化框架，旨在通过同时初始化LoRA组件和量化LLM的权重来恢复量化损失的信息。这种方法确保了原始LLM的激活精度的维持，同时减轻了误差的传播。

    Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
    
[^10]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^11]: 探索用于病理语音特征预测的迁移学习：层选择的影响

    Exploring transfer learning for pathological speech feature prediction: Impact of layer selection

    [https://arxiv.org/abs/2402.01796](https://arxiv.org/abs/2402.01796)

    本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。

    

    利用人工智能对临床语音进行自动客观评估，并促进语音障碍的诊断和治疗具有重要意义。本研究探索了迁移学习，在预测病理语音存在性的下游任务中，重点分析了层选择的影响。我们发现选择最佳层能显著提高性能（平均平衡准确率增加12.4%），尽管最佳层因预测特征而异，并且并不总是对未见数据泛化良好。学得的加权和在分布内与平均最佳层具有可比性的性能，并且在分布外数据上具有更好的泛化能力。

    There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
    
[^12]: Jellyfish：一个用于数据预处理的大型语言模型

    Jellyfish: A Large Language Model for Data Preprocessing

    [https://arxiv.org/abs/2312.01678](https://arxiv.org/abs/2312.01678)

    这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整

    

    这篇论文探讨了在数据挖掘管道中将原始数据转换为有利于简单处理的干净格式的数据预处理（DP）中LLMs的利用。与使用LLMs为DP设计通用解决方案引起了兴趣相比，最近在这一领域的倡议通常依赖于GPT API，引发了不可避免的数据泄霏担忧。与这些方法不同，我们考虑将指导调整本地LLMs（7-13B模型）作为通用DP问解器。我们选择了代表性DP任务的四组数据集，并利用针对DP定制的序列化和知识注入技术构建了指导调整数据。因此，指导调整的LLMs使用户能够为DP手动制定指导。同时，它们可以在本地、单一和价格低廉的GPU上运行，确保数据安全并实现进一步调整。我们的实验表明，我们为DP指导构建的数据集

    arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
    
[^13]: 关于我：使用自我描述的网页来记录英语预训练数据过滤器的影响

    AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v1 [cs.CL])

    [http://arxiv.org/abs/2401.06408](http://arxiv.org/abs/2401.06408)

    这项研究调查了十个质量和语言识别过滤器对不同社交维度变化的网页的影响。实验发现在数据筛选过程中存在隐含的偏好，一些质量分类器类似于主题过滤器，而语言识别可能会忽视某些地区的英语内容。我们的研究为促进更公正和全面的模型开发提供了洞察。

    

    大型语言模型（LLM）的能力来源于它们的预训练数据，模型的开发始于数据的筛选。然而，在这个初步阶段决定保留哪些数据或移除哪些数据的决策常常没有被充分审查。在我们的工作中，我们将网页文本与其社交和地理背景联系起来。我们创建了一个新的数据集，包含1030万个网页创建者的自我描述，并提取了关于他们的个人信息以及他们来自哪里的信息：他们的兴趣领域、社交角色和地理归属。然后，我们进行了第一项研究，调查了十个“质量”和英语语言识别（langID）过滤器对这些社交维度变化的网页的影响。我们的实验揭示了数据筛选中一系列隐含的偏好：我们展示出一些质量分类器的作用类似于主题领域过滤器，而langID可能会忽视世界某些地区的英语内容。总体而言，我们希望我们的工作能够提供对数据筛选中隐含偏好的洞察，以促进更公正和全面的模型开发。

    Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten "quality" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will enc
    
[^14]: 大型法律虚构：揭示大型语言模型中的法律幻觉

    Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])

    [http://arxiv.org/abs/2401.01301](http://arxiv.org/abs/2401.01301)

    大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。

    

    大型语言模型（LLMs）有可能改变法律实践，但其潜力受到法律幻觉的威胁，即这些模型产生与法律事实不一致的回答。我们使用一套原创的法律查询来调查这些幻觉的程度，将LLMs的回答与结构化的法律元数据进行对比，并检查其一致性。我们的工作有四个关键贡献：（1）我们建立了法律幻觉的分类体系，为今后在这一领域进行的研究提供了概念框架。（2）我们发现，法律幻觉的普遍性令人担忧，在对随机联邦法院案例进行具体、可验证的问题时，ChatGPT 3.5产生的幻觉发生率为69％，而Llama 2为88％。（3）我们展示了LLMs在逆向问题设置中往往无法纠正用户的错误法律假设。（4）我们提供了证据表明LLMs并不总能预测或并不总知道...

    Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, wh
    
[^15]: GestureGPT: 零样本交互手势理解与基于大语言模型代理的对接

    GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])

    [http://arxiv.org/abs/2310.12821](http://arxiv.org/abs/2310.12821)

    GestureGPT是一个零样本交互手势理解和对接框架，利用大语言模型代理解读手势描述并根据交互环境提供上下文信息，能够将用户意图对接到交互功能上。

    

    当前的手势识别系统主要关注识别预定义集合中的手势，未能将这些手势与交互式图形用户界面元素或系统功能相连接（例如，将“竖起大拇指”手势与“喜欢”按钮关联起来）。我们引入了GestureGPT，这是一个新颖的零样本手势理解和对接框架，利用大语言模型（LLM）。手势描述根据手势视频中的手部关键点坐标进行形式化，并输入到我们的双代理对话系统中。一个手势代理解读这些描述，并询问有关交互环境的信息（例如，界面、历史记录、凝视数据），一个上下文代理负责组织并提供这些信息。经过迭代的交流，手势代理能够理解用户意图，并将其对接到一个交互功能上。我们使用公开的第一视角和第三视角手势数据集验证了手势描述模块，并在视频流和智能家居物联网控制的两个真实场景中测试了整个系统。

    Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
    
[^16]: XNLP：通用结构化自然语言处理的交互演示系统

    XNLP: An Interactive Demonstration System for Universal Structured NLP. (arXiv:2308.01846v1 [cs.CL])

    [http://arxiv.org/abs/2308.01846](http://arxiv.org/abs/2308.01846)

    XNLP演示系统是一个通用的、高性能的自然语言处理平台，通过提供通用的建模方法、解释性、可扩展性和互动性，实现了统一XNLP任务。

    

    结构化自然语言处理（XNLP）是自然语言处理的一个重要子集，涉及理解文本的底层语义或句法结构，为许多下游应用程序提供了基本组件。尽管最近有一些努力探索特定类别的XNLP任务的通用解决方案，但统一所有XNLP任务的综合有效方法仍然不完善。与此同时，XNLP演示系统对于研究人员探索各种XNLP任务至关重要，现有平台可能受到限制，例如只支持少数XNLP任务，缺乏互动性和通用性。因此，我们提出了一个先进的XNLP演示平台，其中我们提出利用LLM实现通用XNLP，通过一个模型实现高通用性。总体而言，我们的系统在多个方面取得了进展，包括通用XNLP建模、高性能、可解释性、可扩展性和互动性。

    Structured Natural Language Processing (XNLP) is an important subset of NLP that entails understanding the underlying semantic or syntactic structure of texts, which serves as a foundational component for many downstream applications. Despite certain recent efforts to explore universal solutions for specific categories of XNLP tasks, a comprehensive and effective approach for unifying all XNLP tasks long remains underdeveloped. In the meanwhile, while XNLP demonstration systems are vital for researchers exploring various XNLP tasks, existing platforms can be limited to, e.g., supporting few XNLP tasks, lacking interactivity and universalness. To this end, we propose an advanced XNLP demonstration platform, where we propose leveraging LLM to achieve universal XNLP, with one model for all with high generalizability. Overall, our system advances in multiple aspects, including universal XNLP modeling, high performance, interpretability, scalability, and interactivity, providing a unified p
    
[^17]: CREATOR: 大型语言模型的抽象和具体推理解耦工具的创建

    CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models. (arXiv:2305.14318v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14318](http://arxiv.org/abs/2305.14318)

    CREATOR是一个新的框架，使得大型语言模型（LLMs）能够利用文档和代码实现创建自己的工具。通过将抽象工具创建和具体决策执行解耦，CREATOR提高了性能，并在不同基准测试中超越了现有的基线方法。此外，我们引入了Creation Challenge数据集，展示了LLMs的工具创建能力的必要性和优势。

    

    大型语言模型（LLMs）在利用工具方面取得了重要进展，但由于API可用性和隐式推理的不稳定性，它们的能力有限，特别是在涉及规划和执行的情况下。为了克服这些限制，我们提出了一个新的框架CREATOR，可以使LLMs利用文档和代码实现来创建自己的工具。CREATOR将抽象工具创建和具体决策执行解耦，从而提高了性能。我们在MATH和TabMWP基准上评估了CREATOR，分别包含具有挑战性的数学竞赛问题和多样的表格内容。值得注意的是，CREATOR优于现有的思维链、思维程序和使用工具的基线。此外，我们还引入了Creation Challenge数据集，包含2K个多样的问题，以强调LLMs创建工具的必要性和益处。进一步的研究表明，将LLMs用作工具创建者有助于提升知识的利用。

    Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowled
    
[^18]: 知识的知识：探索大型语言模型对未知-已知不确定性的理解

    Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])

    [http://arxiv.org/abs/2305.13712](http://arxiv.org/abs/2305.13712)

    本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。

    

    本文研究了大型语言模型（LLM）在理解自身知识和测量不确定性方面的能力，以缓解虚构现象。我们专门关注解决“已知-未知”问题，这种问题由于缺乏确定的答案而具有高度不确定性。为了促进我们的研究，我们收集了一个新的已知-未知问题（KUQ）数据集，并提出了一个新的分类方案来阐明不确定性的来源。随后，我们评估LLM区分已知和未知问题以及相应分类的能力。此外，我们在开放式QA环境中评估LLM的答案质量。为了量化答案中表达的不确定性，我们创建了一种语义评估方法，用于测量模型在表达已知vs未知问题的不确定性方面的准确性。

    This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
    

