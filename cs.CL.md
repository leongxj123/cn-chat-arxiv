# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Institutional Platform for Secure Self-Service Large Language Model Exploration](https://rss.arxiv.org/abs/2402.00913) | 这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。 |
| [^2] | [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656) | WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路 |
| [^3] | [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282) | 大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。 |
| [^4] | [Evaluating Large Language Models with Runtime Behavior of Program Execution](https://arxiv.org/abs/2403.16437) | 本文提出了一个名为REval的框架，用于评估代码LLMs的代码推理能力以及与程序执行的一致性。 |
| [^5] | [On the Fragility of Active Learners](https://arxiv.org/abs/2403.15744) | 本研究发现主动学习技术只在特定情境下有效，对文本分类从业者的建议是选择适当的文本表示和分类器同样重要。 |
| [^6] | [Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases](https://arxiv.org/abs/2403.13901) | 本文提出了一种从主题和释义生成基于音韵学的绕口令的新方法，生成了迄今为止最大的绕口令数据集TwistList 2.0，并进行了自动和人工评估。 |
| [^7] | [m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks](https://arxiv.org/abs/2403.11085) | m&m's引入了一个包含4K+多步骤多模态任务的基准，涉及33种工具，用于评估LLM作为规划器的设计决策。 |
| [^8] | [Deciphering Hate: Identifying Hateful Memes and Their Targets](https://arxiv.org/abs/2403.10829) | 介绍了一个为孟加拉语设计的新颖多模态数据集BHM，用于检测仇恨表情包以及它们所针对的社会实体。 |
| [^9] | [DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models](https://arxiv.org/abs/2403.10081) | 提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。 |
| [^10] | [ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs](https://arxiv.org/abs/2403.09724) | ClaimVer是一个人为中心的框架，通过知识图谱实现可解释的声明级验证和证据归因，致力于提高用户对文本验证方法的信任并强调细粒度证据的重要性。 |
| [^11] | [Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning](https://arxiv.org/abs/2403.06769) | 该论文提出了一个名为TRIP的机制，通过整合用户感知的战略规划模块和基于人口的训练范式，解决了非合作对话代理商面临的挑战，有效地满足多样化用户的需求。 |
| [^12] | [Membership Inference Attacks and Privacy in Topic Modeling](https://arxiv.org/abs/2403.04451) | 主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险 |
| [^13] | [SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents](https://arxiv.org/abs/2403.02959) | 提出了SimuCourt司法基准，包括真实世界的司法文件，并引入了司法决策任务和多代理框架，评估了代理的司法分析和决策能力 |
| [^14] | [Exploring the Limitations of Large Language Models in Compositional Relation Reasoning](https://arxiv.org/abs/2403.02615) | 评估了大型语言模型在组合关系推理中的能力，设计了涵盖六种不同类型组合关系的基准测试，并扩展到多语言环境下进行评估。 |
| [^15] | [Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models](https://arxiv.org/abs/2402.18099) | 提出了一种新型的Layer-wise Scalable Adapter策略MedLaSA，用于编辑医学大型语言模型，能精确修改医学知识并解释事实，解决了当前方法在医学知识特殊化和复杂性方面的困难。 |
| [^16] | [DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators](https://arxiv.org/abs/2402.15200) | DeMPT提出了解码增强的多阶段提示优化，使得LLMs更好地模拟和利用句间和句内上下文，从而更有效地适应上下文感知NMT。 |
| [^17] | [Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?](https://arxiv.org/abs/2402.13703) | 本研究是第一个对多语模型在不同印欧语言上的性能进行了广泛研究，发现在并行教学调整数据集上进行教学调整可以显著提升跨语言遵循能力，同时提出了对表面对齐假设的质疑 |
| [^18] | [WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment](https://arxiv.org/abs/2402.12275) | 通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。 |
| [^19] | [Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse](https://arxiv.org/abs/2402.09934) | 本研究挖掘了在线话语中的演绎细微差别，提出了一种新的方法来准确检测反问主义，并在Twitter和YouTube数据集中取得了显著的改进。 |
| [^20] | [EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2402.09801) | EFUF是一种高效精细化去学习框架，可以消除多模态大语言模型中的物体幻觉，并不需要人工注释配对数据。 |
| [^21] | [History, Development, and Principles of Large Language Models-An Introductory Survey](https://arxiv.org/abs/2402.06853) | 这项综述性调查介绍了大型语言模型（LLMs）的历史、发展和原理，旨在帮助广泛的读者群体理解这些模型的背景和原理。 |
| [^22] | [Conditional and Modal Reasoning in Large Language Models](https://arxiv.org/abs/2401.17169) | 本文研究了大型语言模型中的条件和情态推理能力，并发现除了GPT-4外，其他模型在条件句方面存在基本错误，并且即使是GPT-4在涉及认识情态的推理模式上也显示出逻辑上不一致的判断。 |
| [^23] | [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models](https://arxiv.org/abs/2311.08598) | DALA是一种基于分布感知的LoRA对抗攻击方法，旨在改善对抗性样本的数据分布，提高攻击效果，并引入了非可检测攻击成功率（NASR）评价指标。 |
| [^24] | [Learning New Tasks from a Few Examples with Soft-Label Prototypes](https://arxiv.org/abs/2210.17437) | 本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。 |
| [^25] | [Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment.](http://arxiv.org/abs/2401.10768) | 本文提出了一种称为知识一致性对齐（KCA）的方法，通过减少训练数据中外部知识和预训练语料库中内在知识之间的不一致性，从而缓解了大型语言模型产生幻觉的问题。实验结果表明，KCA方法在多个基准测试中取得了优异的性能。 |
| [^26] | [Asynchronous Local-SGD Training for Language Modeling.](http://arxiv.org/abs/2401.09135) | 本文通过异步Local-SGD训练语言模型，并进行了全面的实证研究。研究发现，尽管异步更新更频繁，但其收敛所需的迭代次数多于同步方法。作者还提出了一种利用延迟的Nesterov动量更新进行调整的新方法来解决异步更新的挑战。 |
| [^27] | [RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning.](http://arxiv.org/abs/2401.08326) | RoTBench是一个多级基准，用于评估大型语言模型在工具学习中的鲁棒性。研究发现，LLMs在真实世界的噪声下表现出的稳定性需得到提高。 |
| [^28] | [Re-Reading Improves Reasoning in Language Models.](http://arxiv.org/abs/2309.06275) | 许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。 |
| [^29] | [LLM in the Shell: Generative Honeypots.](http://arxiv.org/abs/2309.00155) | 本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。 |
| [^30] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^31] | [CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation.](http://arxiv.org/abs/2305.06294) | 本文提出了一种基于上下文感知的图注意力模型，可以将上下文增强的知识聚合过程与相关知识图的全局特征有效融合，将增强的图结构知识集成到基于上下文感知的对话生成模型中。实验证明，该模型在自动度量和人类评估方面均优于现有方法。 |
| [^32] | [CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population.](http://arxiv.org/abs/2304.10392) | 本文介绍了CKBP v2, 一个使用专家注释而囊括对抗样本的高质量通识知识库填充基准，以解决CKBP v1由于众包注释和随机抽样导致的问题。实验结果表明，通识知识库填充任务对于现有技术水平仍然具有挑战性。 |

# 详细

[^1]: 用于安全自助大型语言模型探索的机构平台

    Institutional Platform for Secure Self-Service Large Language Model Exploration

    [https://rss.arxiv.org/abs/2402.00913](https://rss.arxiv.org/abs/2402.00913)

    这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。

    

    本文介绍了由肯塔基大学应用人工智能中心开发的用户友好型平台，旨在使大型定制语言模型（LLM）更易于使用。通过利用最近在多LoRA推理方面的进展，系统有效地适应了各类用户和项目的定制适配器。论文概述了系统的架构和关键特性，包括数据集策划、模型训练、安全推理和基于文本的特征提取。我们通过使用基于代理的方法建立了一个基于租户意识的计算网络，在安全地利用孤立资源岛的基础上形成了一个统一的系统。该平台致力于提供安全的LLM服务，强调过程和数据隔离、端到端加密以及基于角色的资源身份验证。该贡献与实现简化访问先进的AI模型和技术以支持科学发现的总体目标一致。

    This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
    
[^2]: WavLLM：面向稳健和自适应语音大语言模型

    WavLLM: Towards Robust and Adaptive Speech Large Language Model

    [https://arxiv.org/abs/2404.00656](https://arxiv.org/abs/2404.00656)

    WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路

    

    近年来，大型语言模型(LLMs)的最新进展彻底改变了自然语言处理领域，逐渐拓宽了它们的范围到多模态感知和生成。然而，有效地将听觉能力整合到LLMs中会带来显著挑战，特别是在泛化跨不同语境和执行复杂听觉任务方面。在这项工作中，我们引入了WavLLM，一个具有双编码器和Prompt-aware LoRA权重适配器的稳健和自适应语音大语言模型，通过两阶段课程学习方法进行优化。利用双编码器，我们解耦不同类型的语音信息，利用Whisper编码器处理语音的语义内容，利用WavLM编码器捕捉说话者身份的独特特征。在课程学习框架内，WavLLM首先通过混合要素进行优化来建立其基础能力

    arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
    
[^3]: 基于大型语言模型增强强化学习的调查:概念、分类和方法

    Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

    [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)

    大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。

    

    随着大规模语言模型(LLMs)拥有广泛的预训练知识和高级通用能力，它们在增强学习方面如多任务学习、样本效率和任务规划等方面展现出潜力。本调查综述了现有$\textit{LLM增强RL}$文献，总结了其与传统RL方法的特征，旨在澄清研究范围和未来研究方向。利用经典的Agent-环境交互范例，我们提出了一个结构化的分类法，系统地将LLMs在RL中的功能分类，包括四种角色：信息处理器、奖励设计者、决策者和生成器。此外，针对每个角色，我们总结了方法论，分析了缓解的特定RL挑战，并提供了未来方向的见解。最后，潜在应用、前景

    arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
    
[^4]: 使用程序执行运行时行为评估大型语言模型

    Evaluating Large Language Models with Runtime Behavior of Program Execution

    [https://arxiv.org/abs/2403.16437](https://arxiv.org/abs/2403.16437)

    本文提出了一个名为REval的框架，用于评估代码LLMs的代码推理能力以及与程序执行的一致性。

    

    大型代码语言模型（即代码LLMs）展示了强大的代码理解和生成能力。为了评估代码LLMs在各个方面的能力，已经提出了许多基准（如HumanEval和ClassEval）。代码推理是代码LLMs最重要的能力之一，但现有的代码推理基准不足。通常，它们重点预测程序的输入和输出，忽略了程序执行过程中的中间行为评估，以及逻辑一致性（例如，如果执行路径预测错误，则模型不应该给出正确的输出）在执行推理时。为了解决这些问题，本文提出了一个名为REval的框架，用于评估代码LLMs的代码推理能力以及与程序执行的一致性。我们利用现有的代码基准，并将它们适应到我们的框架中的新基准中。

    arXiv:2403.16437v1 Announce Type: cross  Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framew
    
[^5]: 论主动学习者的脆弱性

    On the Fragility of Active Learners

    [https://arxiv.org/abs/2403.15744](https://arxiv.org/abs/2403.15744)

    本研究发现主动学习技术只在特定情境下有效，对文本分类从业者的建议是选择适当的文本表示和分类器同样重要。

    

    主动学习（AL）技术旨在通过迭代选择最有可能提高预测准确性的实例，最大程度地利用标注预算。然而，与随机抽样相比，在不同设置下（例如不同数据集，分类器），它们的益处并不一致。在这项实证研究中，我们研究了不同因素的组合如何可能掩盖主动学习技术的任何收益。专注于文本分类，我们在大约1000个实验中严格评估了进行分类，我们在大约1000个实验中严格评估了AL技术，这些实验在数据集、批大小、文本表示和分类器方面变化。我们表明，AL只在一组有限的情境中有效。我们还解决了使用与现实世界期望更好对齐的度量的问题。这项研究的影响在于对从业者的洞察：(a) 文本表示和分类器的选择与AL技术的选择一样重要，(b) 选择的

    arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
    
[^6]: 训练与限制：从主题和释义生成基于音韵学的绕口令

    Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases

    [https://arxiv.org/abs/2403.13901](https://arxiv.org/abs/2403.13901)

    本文提出了一种从主题和释义生成基于音韵学的绕口令的新方法，生成了迄今为止最大的绕口令数据集TwistList 2.0，并进行了自动和人工评估。

    

    过去在音韵和语音基础的语言生成方面的工作主要集中在领域，如双关语和诗歌。在本文中，我们提出了产生绕口令的新工作-这种语言形式需要在音素级别上进行条件约束，以最大程度地实现声音重叠，同时与输入主题保持语义一致，仍然保持语法正确。我们提出了TwisterLister，这是一个从大型语言模型（LLMs）中生成基于音韵学的绕口令的流程，我们用它来生成TwistList 2.0，到目前为止最大的一个已标记数据集，包含来自人类和LLM作者合作的超过17K个例子。我们的生成流程涉及使用音韵受限词汇以及LLM提示来生成新颖的、非衍生的绕口令实例。此外，我们还提出了对较小规模的自动和人工评估结果。

    arXiv:2403.13901v1 Announce Type: new  Abstract: Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller
    
[^7]: m&m's: 一个用于评估多步骤多模态任务工具使用的基准

    m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks

    [https://arxiv.org/abs/2403.11085](https://arxiv.org/abs/2403.11085)

    m&m's引入了一个包含4K+多步骤多模态任务的基准，涉及33种工具，用于评估LLM作为规划器的设计决策。

    

    现实世界中的多模态问题很少由单个机器学习模型解决，通常需要多步骤计算计划，涉及拼接多个模型。 工具增强型LLM极有可能自动化生成这种计算计划。然而，缺乏用于评估LLM作为多步骤多模态任务规划器的标准化基准，阻碍了对规划器设计决策的系统研究。LLM是否应一次性生成整个计划还是逐步生成？它们是否应该直接使用Python代码调用工具，还是通过类似JSON的结构化数据格式？反馈是否改善规划？为了回答这些问题以及更多问题，我们介绍了m&m's：一个基准，包含4K+个涉及33种工具的多步骤多模态任务，其中包括多模态模型、(免费)公共API和图像处理模块。对于每个任务查询，我们提供使用这种方法自动生成的计划。

    arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
    
[^8]: 辨析仇恨：识别仇恨表情包及其目标

    Deciphering Hate: Identifying Hateful Memes and Their Targets

    [https://arxiv.org/abs/2403.10829](https://arxiv.org/abs/2403.10829)

    介绍了一个为孟加拉语设计的新颖多模态数据集BHM，用于检测仇恨表情包以及它们所针对的社会实体。

    

    互联网表情包已经成为个人在社交媒体上表达情感、思想和观点的强大手段。虽然通常被视为一种幽默和娱乐来源，但表情包也可以传播针对个人或社区的仇恨内容。大多数现有研究侧重于高资源语言中表情包的负面方面，忽视了低资源语言（如孟加拉语）所面临的独特挑战。此外，尽管之前关于孟加拉语表情包的工作集中在检测仇恨表情包上，但并没有对它们的目标实体进行检测。为了弥补这一差距并促进这一领域的研究，我们引入了一个面向孟加拉语的新颖多模态数据集BHM（孟加拉仇恨表情包）。该数据集包含7,148个带有孟加拉语以及混合代码字幕的表情包，专为两项任务量身定制：（i）检测仇恨表情包，以及（ii）检测它们所针对的社会实体。

    arXiv:2403.10829v1 Announce Type: new  Abstract: Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target
    
[^9]: DRAGIN：基于大型语言模型实时信息需求的动态检索增强生成

    DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models

    [https://arxiv.org/abs/2403.10081](https://arxiv.org/abs/2403.10081)

    提出了一种新框架DRAGIN，旨在解决大型语言模型在文本生成过程中动态检索和生成中存在的问题。

    

    动态检索增强生成（RAG）范式在大型语言模型（LLMs）的文本生成过程中主动决定何时以及何时检索。该范式的两个关键元素是确定激活检索模块的最佳时机（决定何时检索）以及一旦触发检索，制定适当的查询（确定要检索什么）。然而，当前动态RAG方法在两个方面都存在不足。首先，决定何时进行检索的策略通常依赖于静态规则。此外，决定要检索什么的策略通常局限于LLM的最近一句或最后几个标记，而LLM的实时信息需求可能跨越整个上下文。为克服这些局限性，我们引入了一个新框架DRAGIN， 即基于LLMs实时信息需求的动态检索增强生成。

    arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
    
[^10]: ClaimVer：通过知识图谱实现可解释的声明级验证和证据归因

    ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs

    [https://arxiv.org/abs/2403.09724](https://arxiv.org/abs/2403.09724)

    ClaimVer是一个人为中心的框架，通过知识图谱实现可解释的声明级验证和证据归因，致力于提高用户对文本验证方法的信任并强调细粒度证据的重要性。

    

    在广泛传播的信息误导和社交媒体以及人工智能生成的文本的激增中，验证和信任所遇到的信息变得日益困难。许多事实核查方法和工具已被开发，但它们往往缺乏适当的可解释性或细粒度，无法在各种情境中发挥作用。一种易于使用、可访问且能够执行细粒度证据归因的文本验证方法变得至关重要。更重要的是，建立用户对这种方法的信任需要呈现每个预测背后的理由，因为研究表明这显著影响人们对自动化系统的信任。将用户关注重点放在具体的问题内容上，而不是提供简单的笼统标签也非常重要。在本文中，我们提出了$\textit{ClaimVer，一个以人为中心的框架}$，旨在满足用户的信息需求。

    arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
    
[^11]: 不同之处在于力量！通过定制策略规划实现有效的非合作对话

    Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning

    [https://arxiv.org/abs/2403.06769](https://arxiv.org/abs/2403.06769)

    该论文提出了一个名为TRIP的机制，通过整合用户感知的战略规划模块和基于人口的训练范式，解决了非合作对话代理商面临的挑战，有效地满足多样化用户的需求。

    

    我们研究了非合作对话代理商，他们必须进行定制的战略规划，以确保与多样化用户达成有利的协议。这对现有的对话代理商构成挑战，主要原因有两点：他们无法将用户特定特征整合到战略规划中，以及他们的训练范式未能产生可以泛化到多样化用户的战略规划者。为了解决这些挑战，我们提出了TRIP以增强定制战略规划的能力，包括用户感知的战略规划模块和基于人口的训练范式。通过对基准非合作对话任务的实验，我们展示了TRIP在迎合多样化用户方面的有效性。

    arXiv:2403.06769v1 Announce Type: new  Abstract: We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.
    
[^12]: 会员推理攻击与主题建模中的隐私

    Membership Inference Attacks and Privacy in Topic Modeling

    [https://arxiv.org/abs/2403.04451](https://arxiv.org/abs/2403.04451)

    主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险

    

    最近的研究表明，大型语言模型容易受到推理训练数据方面的隐私攻击。然而，目前还不清楚更简单的生成模型，例如主题模型，是否存在类似的漏洞。在这项工作中，我们提出了一种针对主题模型的攻击，可以自信地识别Latent Dirichlet Allocation中训练数据的成员。我们的结果表明，与大型神经模型相关联的隐私风险并不仅限于大型神经模型。此外，为了减轻这些漏洞，我们探讨了差分隐私（DP）主题建模。我们提出了一个私密主题建模框架，将DP词汇选择作为预处理步骤，并展示它不仅改善了隐私性，而且在实用性方面的影响有限。

    arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
    
[^13]: SimuCourt: 利用真实司法判决文件构建司法决策代理

    SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents

    [https://arxiv.org/abs/2403.02959](https://arxiv.org/abs/2403.02959)

    提出了SimuCourt司法基准，包括真实世界的司法文件，并引入了司法决策任务和多代理框架，评估了代理的司法分析和决策能力

    

    随着深度学习、自然语言处理技术的发展，有效提高了传统司法行业各个方面的效率。然而，目前大多数工作主要集中在个别司法阶段，忽视了跨阶段的协作。随着由大型语言模型提供支持的自主代理在现实环境中变得越来越智能，并能做出复杂决策，为司法智能提供了新的见解。本文介绍了SimuCourt，一个司法基准，包括来自真实世界的420份判决文件，涵盖了三种最常见类型的司法案例，以及一个新颖任务司法决策，用于评估代理的司法分析和决策能力。为了支持这一任务，我们构建了一个大规模司法知识库，JudicialKB，其中包含多种法律知识。我们提出了一种新颖的多代理框架，AgentsCourt

    arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
    
[^14]: 探讨大型语言模型在组合关系推理中的局限性

    Exploring the Limitations of Large Language Models in Compositional Relation Reasoning

    [https://arxiv.org/abs/2403.02615](https://arxiv.org/abs/2403.02615)

    评估了大型语言模型在组合关系推理中的能力，设计了涵盖六种不同类型组合关系的基准测试，并扩展到多语言环境下进行评估。

    

    我们对大型语言模型(LLMs)在通过一个包含1,500个英文测试案例的基准测试中推理组合关系的能力进行了全面评估，旨在涵盖六种不同类型的组合关系：位置、比较、个人、数学、身份和其他。意识到多语言能力的重要性，我们将评估扩展到将这些案例翻译成中文、日文、法文和韩文。我们的多语言组合关系(MCR)基准旨在探讨大型语言模型在处理不同语言背景下的组合关系推理的稳健性和适应性。

    arXiv:2403.02615v1 Announce Type: new  Abstract: We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.
    
[^15]: 编辑医学大型语言模型的事实知识和解释能力

    Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models

    [https://arxiv.org/abs/2402.18099](https://arxiv.org/abs/2402.18099)

    提出了一种新型的Layer-wise Scalable Adapter策略MedLaSA，用于编辑医学大型语言模型，能精确修改医学知识并解释事实，解决了当前方法在医学知识特殊化和复杂性方面的困难。

    

    模型编辑旨在精确修改大型语言模型（LLMs）对特定知识的行为，同时保持不相关的知识不变。已经证明，这种方法在解决LLMs中的幻觉和过时问题方面是有效的。因此，它可以提高LLMs在许多关键领域（例如医学领域）中的应用，其中幻觉是不可容忍的。本文提出两项模型编辑研究，并在医学领域验证它们：（1）直接编辑医学事实知识和（2）编辑对事实的解释。同时，我们观察到当前的模型编辑方法在医学知识的特殊化和复杂性方面存在困难。因此，我们提出了MedLaSA，一种新型的适用于医学模型编辑的分层可扩展适配器策略。它采用因果追踪来识别神经元中知识的精确位置，然后将可扩展适配器引入到LLMs的密集层中。

    arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
    
[^16]: DeMPT：解码增强的多阶段提示优化，使LLMs成为更好的上下文感知翻译器

    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators

    [https://arxiv.org/abs/2402.15200](https://arxiv.org/abs/2402.15200)

    DeMPT提出了解码增强的多阶段提示优化，使得LLMs更好地模拟和利用句间和句内上下文，从而更有效地适应上下文感知NMT。

    

    通常，仅具有解码器的大型语言模型（LLMs）通过连接的方式适应上下文感知神经机器翻译（NMT），其中LLMs将源句（即句内上下文）和句间上下文的连接作为输入，然后顺序生成目标标记。本文提出了一种名为Decoding-enhanced Multi-phase Prompt Tuning（DeMPT）的替代适应方法，以使LLMs能够歧视性地对模组和利用句间和句内上下文，并更有效地将LLMs调整到上下文感知NMT。首先，DeMPT将上下文感知NMT过程分为三个单独阶段。在每个阶段，引入不同的连续提示，使LLMs能够区分地模型。

    arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel
    
[^17]: 调查多语言教学调整：多语模型是否需要多语教学？

    Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?

    [https://arxiv.org/abs/2402.13703](https://arxiv.org/abs/2402.13703)

    本研究是第一个对多语模型在不同印欧语言上的性能进行了广泛研究，发现在并行教学调整数据集上进行教学调整可以显著提升跨语言遵循能力，同时提出了对表面对齐假设的质疑

    

    arXiv:2402.13703v1 公告类型：新摘要：将多语言预训练大型语言模型（LLMs）转化为雄辩而有用的助手对促进它们在不同语言地区的使用至关重要。基于这一精神，我们是第一个对跨多种印欧语言进行大规模研究的研究者，旨在研究多语模型在选择的最常用的印欧语言上的并行、多轮教学调整基准测试的性能。我们系统地研究了语言和教学数据集大小对中型多语言LLM的影响，通过在并行教学调整数据集上进行教学调整。我们的结果表明，在并行教学调整而不是单语语料库上进行教学调整可以使跨语言遵循能力提高多达4.6%。此外，我们表明，表面对齐假设通常不成立，因为所调查的多语7B参数模型是一个反例，需要大规模的教学调整。

    arXiv:2402.13703v1 Announce Type: new  Abstract: The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning
    
[^18]: WorldCoder，一种基于模型的LLM代理：通过编写代码和与环境交互构建世界模型

    WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment

    [https://arxiv.org/abs/2402.12275](https://arxiv.org/abs/2402.12275)

    通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。

    

    我们提出了一种基于模型的代理，通过与环境的交互构建代表其对世界知识的Python程序。该世界模型试图解释其交互，同时对自己能够获得的奖励持乐观态度。我们通过扩展LLM的程序合成工作来实现这一点。我们在网格世界上研究了我们的代理，发现我们的方法在样本效率上比深度强化学习更高，并且在计算效率上比ReAct风格的代理更高效。

    arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
    
[^19]: 关注偏差：挖掘在线话语中的演绎细微差别，检测反问主义

    Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse

    [https://arxiv.org/abs/2402.09934](https://arxiv.org/abs/2402.09934)

    本研究挖掘了在线话语中的演绎细微差别，提出了一种新的方法来准确检测反问主义，并在Twitter和YouTube数据集中取得了显著的改进。

    

    反问主义在扰乱叙事和播种不信任方面具有强大的工具效用，但在定量自然语言处理研究中却未得到充分探索。此外，过去的研究未能区分反问主义作为误导和宣传策略的用途与其作为语用和语义框架工具的用途。我们介绍了新的来自Twitter和YouTube的数据集，揭示了反问主义、宣传和tu quoque谬误之间的重叠和区别。此外，结合最近在语言语义学领域的研究，我们将“what about”词汇结构与反问主义区分开来。我们的实验揭示了准确检测反问主义的独特挑战，促使我们引入了一种使用注意权重进行负样本挖掘的新方法。在Twitter和YouTube数据集中，我们的方法分别相对于之前最先进的方法提高了4%和10%。

    arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
    
[^20]: EFUF: 高效精细化去学习多模态大语言模型中减轻幻像的框架

    EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models

    [https://arxiv.org/abs/2402.09801](https://arxiv.org/abs/2402.09801)

    EFUF是一种高效精细化去学习框架，可以消除多模态大语言模型中的物体幻觉，并不需要人工注释配对数据。

    

    多模态大语言模型（MLLMs）近年来受到越来越多的关注，但它们可能仍会生成包含图像中不存在的物体的描述，这种现象称为物体幻觉。为了消除幻觉，现有方法手动注释包含和不包含幻觉的配对响应，并采用各种对齐算法来提高图像和文本之间的对齐能力。然而，它们不仅在微调阶段需要大量计算资源，还需要昂贵的人工注释来构建对齐算法所需的配对数据。为了解决这些问题，我们借鉴了去学习的思想，提出了一种高效精细化去学习框架（EFUF），可以在不需要配对数据的情况下消除幻觉。大量实验证明，我们的方法能够持续减少幻觉同时保留准确的描述。

    arXiv:2402.09801v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserv
    
[^21]: 大型语言模型的历史、发展和原理-一项综述性调查

    History, Development, and Principles of Large Language Models-An Introductory Survey

    [https://arxiv.org/abs/2402.06853](https://arxiv.org/abs/2402.06853)

    这项综述性调查介绍了大型语言模型（LLMs）的历史、发展和原理，旨在帮助广泛的读者群体理解这些模型的背景和原理。

    

    语言模型作为自然语言处理中的重要基石，利用数学方法来推广语言规律和知识，用于预测和生成。经过几十年的广泛研究，语言建模从最初的统计语言模型（SLMs）发展到当今的大型语言模型（LLMs）。值得注意的是，LLMs的快速演进已经达到了处理、理解和生成人类水平文本的能力。然而，尽管LLMs在改善工作和个人生活方面具有显著优势，但一般从业人员对这些模型的背景和原理了解有限，限制了它们的应用潜力。值得注意的是，大多数关于LLMs的综述都集中在特定方面，并使用了专门的语言，给缺乏相关背景知识的从业人员带来了困难。因此，本综述旨在提供一个简明扼要的LLMs概述，以帮助更广泛的读者群体。

    Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. 
    
[^22]: 大型语言模型中的条件和情态推理

    Conditional and Modal Reasoning in Large Language Models

    [https://arxiv.org/abs/2401.17169](https://arxiv.org/abs/2401.17169)

    本文研究了大型语言模型中的条件和情态推理能力，并发现除了GPT-4外，其他模型在条件句方面存在基本错误，并且即使是GPT-4在涉及认识情态的推理模式上也显示出逻辑上不一致的判断。

    

    关于大型语言模型（LLM）的推理能力的研究正在人工智能和认知科学领域不断增加。本文探讨了十几个LLM能否区分逻辑上正确的推论和逻辑上荒谬的推论。我们重点关注涉及条件句（例如，“如果安有一个皇后，那么鲍勃有一个J牌”）和认识情态（例如，“安可能有一个A牌”，“鲍勃必须有一个K牌”）的推理模式。这些推理模式对于逻辑学家、哲学家和语言学家来说具有特殊的兴趣，因为它们可能在人类推理中扮演一个核心角色。因此，评估LLM在这些推理模式上的表现与人类的推理能力是否相匹配是非常相关的。在我们测试的LLM中，除了GPT-4，其他都常常在条件句方面犯基本错误。此外，即使是GPT-4，在涉及认识情态的推理模式上也显示出逻辑上不一致的判断。

    The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
    
[^23]: DALA: 一种基于分布感知的面向语言模型的对抗攻击方法

    DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models

    [https://arxiv.org/abs/2311.08598](https://arxiv.org/abs/2311.08598)

    DALA是一种基于分布感知的LoRA对抗攻击方法，旨在改善对抗性样本的数据分布，提高攻击效果，并引入了非可检测攻击成功率（NASR）评价指标。

    

    语言模型（LMs）可以通过对抗性攻击进行操纵，这些攻击在输入数据中引入微妙的扰动。近期的攻击方法可以实现相对较高的攻击成功率（ASR），但我们观察到生成的对抗性样本与原始样本相比具有不同的数据分布。具体而言，这些对抗性样本表现出降低的置信水平和与训练数据分布的较大差异。因此，它们很容易被简单的检测方法检测出来，降低了此类攻击的有效性。为解决这一问题，我们提出了一种基于LoRA的分布感知的对抗攻击方法（DALA）。DALA考虑对抗性样本的分布变化，以提高在检测方法下的攻击效果。我们进一步设计了一种新颖的评价度量，非可检测攻击成功率（NASR），它融合了ASR和可检测性。

    arXiv:2311.08598v2 Announce Type: replace  Abstract: Language models (LMs) can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware LoRA-based Adversarial Attack (DALA) method. DALA considers distribution shifts of adversarial examples to improve the attack's effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for 
    
[^24]: 用软标签原型从少量示例中学习新任务

    Learning New Tasks from a Few Examples with Soft-Label Prototypes

    [https://arxiv.org/abs/2210.17437](https://arxiv.org/abs/2210.17437)

    本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。

    

    自然语言处理中的少样本学习现有方法依赖于大型语言模型和对其微调，以在分布外数据上进行泛化。在这项工作中，我们提出了一种简单但强大的“极端”少样本学习方法，其中模型只需接触每个类别至少4个示例，这些示例基于软标签原型，这些软标签原型共同捕获了输入域空间中不同类别的分布。受到先前关于一元或简单多元（合成）数据（Sucholutsky等人，2021）的工作的启发，我们提出了一种在大型、高维和现实世界数据集上有效的新方法。我们在神经框架（DeepSLP）中学习软标签原型，并在实验中展示，它在31/48个测试任务和少样本设置上表现优异，同时在其他任务上与强基线模型的性能相匹配。我们专注于从v中学习以前未见过的NLP任务

    arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
    
[^25]: 缓解大型语言模型的幻觉问题：通过知识一致性对齐

    Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])

    [http://arxiv.org/abs/2401.10768](http://arxiv.org/abs/2401.10768)

    本文提出了一种称为知识一致性对齐（KCA）的方法，通过减少训练数据中外部知识和预训练语料库中内在知识之间的不一致性，从而缓解了大型语言模型产生幻觉的问题。实验结果表明，KCA方法在多个基准测试中取得了优异的性能。

    

    虽然大型语言模型在对齐后在各种任务上表现出色，但它们仍可能产生与上下文或世界知识自信矛盾的响应，这被称为“幻觉”现象。本文展示了通过减少训练数据中的外部知识与预训练语料库中继承的内在知识之间的不一致性，可以缓解对齐中的幻觉问题。具体而言，我们引入了一种新颖的知识一致性对齐（KCA）方法，该方法通过根据外部知识自动制定考试来评估大型语言模型的理解能力。对于包含知识不一致性的数据，KCA实施了几种简单而高效的处理策略。我们通过使用不同背景和规模的大型语言模型在六个基准测试中展示了所提出的KCA方法在缓解幻觉方面的卓越性能。

    While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
    
[^26]: 异步Local-SGD训练语言建模

    Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])

    [http://arxiv.org/abs/2401.09135](http://arxiv.org/abs/2401.09135)

    本文通过异步Local-SGD训练语言模型，并进行了全面的实证研究。研究发现，尽管异步更新更频繁，但其收敛所需的迭代次数多于同步方法。作者还提出了一种利用延迟的Nesterov动量更新进行调整的新方法来解决异步更新的挑战。

    

    Local随机梯度下降(Local-SGD)，也称为联邦平均，是一种分布式优化方法，其中每个设备在通信中执行多个SGD更新。本文介绍了异步Local-SGD用于训练语言模型的经验证研究；即，每个工作节点在完成其SGD步骤后立即更新全局参数。我们通过考察工作节点硬件异构性、模型大小、工作节点数量和优化器等因素对学习性能的影响进行了全面调查。我们发现，尽管更频繁地更新（全局）模型参数，但异步Local-SGD比其同步对应物需要更多迭代才能收敛。我们确定了在工作节点梯度陈旧时全局参数的动量加速作为一个关键挑战。我们提出了一种利用延迟的Nesterov动量更新，根据工作节点的本地训练步骤进行调整的新方法。

    Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
    
[^27]: RoTBench: 评估大型语言模型在工具学习中的鲁棒性的多级基准

    RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08326](http://arxiv.org/abs/2401.08326)

    RoTBench是一个多级基准，用于评估大型语言模型在工具学习中的鲁棒性。研究发现，LLMs在真实世界的噪声下表现出的稳定性需得到提高。

    

    工具学习作为大型语言模型（LLMs）与物理世界之间互动的重要手段，引起了广泛的兴趣。当前的研究主要强调LLMs在结构良好的环境中利用工具的能力，但忽视了它们在面对真实世界中不可避免的噪声时的稳定性。为了弥合这一差距，我们引入了RoTBench，这是一个用于评估LLMs在工具学习中鲁棒性的多级基准。具体而言，我们建立了五个外部环境，每个环境都具有不同级别的噪声（即清洁、轻微、中等、重度和联合），对模型在工具选择、参数识别和内容填充三个关键阶段的抗干扰能力进行了深入分析。六个广泛使用的模型的实验表明，提高LLMs在工具学习中的鲁棒性迫在眉睫。例如，当没有实质性的噪声存在时，GPT-4的性能甚至从80.00下降到58.10。

    Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
    
[^28]: 重新阅读改善语言模型的推理能力

    Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])

    [http://arxiv.org/abs/2309.06275](http://arxiv.org/abs/2309.06275)

    许多研究关注于如何引导和结构化大型语言模型的推理过程，但很少有研究关注于输入问题本身。本研究引入了一种称为“重新阅读”的提示策略，通过深入阅读输入提示中的问题信息，提供了更深入的洞察、更准确的模式识别和更有效的推理能力。

    

    推理对于大型语言模型（LLM）是一个重要而具有挑战性的问题。目前的研究主要集中在开发多样化的提示策略，以引导和结构化LLM的推理过程。然而，这些基于仅解码的因果语言模型的方法通常在单个前向传递中操作输入问题，可能会忽略人类推理中丰富的前后交互。对于嵌入在提示中的输入问题这一关键维度，目前关注较少。为此，我们引入了一种简单但高效的提示策略，称为“重新阅读”。从人类学习和问题解决中汲取灵感，重新阅读意味着重访嵌在输入提示中的问题信息。这种方法与认知增强的原则完美契合，使LLM能够深入洞察、识别复杂的模式、建立 mor

    Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish mor
    
[^29]: LLM在Shell中的应用：生成式蜜罐

    LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])

    [http://arxiv.org/abs/2309.00155](http://arxiv.org/abs/2309.00155)

    本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。

    

    蜜罐是网络安全中的重要工具。然而，大多数蜜罐（即使是高交互式的）缺乏足够的真实感来欺骗攻击者。这个限制使得它们很容易被识别，从而影响到它们的有效性。本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐。初步结果表明，LLM能够创建可信且动态的蜜罐，能够解决以往蜜罐的重要局限性，如确定性响应、缺乏适应性等。我们通过与需要判断蜜罐回应是否虚假的攻击者进行实验来评估每个命令的真实性。我们提出的蜜罐，称为shelLM，达到了0.92的准确率。

    Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
    
[^30]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^31]: CADGE：基于图结构知识聚合的上下文感知对话生成

    CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])

    [http://arxiv.org/abs/2305.06294](http://arxiv.org/abs/2305.06294)

    本文提出了一种基于上下文感知的图注意力模型，可以将上下文增强的知识聚合过程与相关知识图的全局特征有效融合，将增强的图结构知识集成到基于上下文感知的对话生成模型中。实验证明，该模型在自动度量和人类评估方面均优于现有方法。

    

    常识知识（commonsense knowledge）对于自然语言处理任务来说至关重要。现有的方法通常将图知识与传统的图神经网络（GNNs）相结合，导致文本和图知识编码过程在串行流水线中被分离。我们认为，这些分离的表示学习阶段可能对神经网络学习包含在两种输入知识类型中的整体上下文是次优的。在本文中，我们提出了一种新颖的基于上下文感知的图注意力模型（Context-aware GAT），它可以基于上下文增强的知识聚合过程有效地融合相关知识图的全局特征。具体地，我们的框架利用了一种新颖的表示学习方法来处理异构特征——将图知识与文本相结合。据我们所知，这是第一次尝试在连接子图上分层应用图知识聚合以及上下文信息，并将增强的图结构知识集成到基于上下文感知的对话生成模型中。我们在两个基准数据集上的实验证明，所提出的模型在自动度量和人类评估方面均优于现有方法。

    Commonsense knowledge is crucial to many natural language processing tasks. Existing works usually incorporate graph knowledge with conventional graph neural networks (GNNs), leading to the text and graph knowledge encoding processes being separated in a serial pipeline. We argue that these separate representation learning stages may be suboptimal for neural networks to learn the overall context contained in both types of input knowledge. In this paper, we propose a novel context-aware graph-attention model (Context-aware GAT), which can effectively incorporate global features of relevant knowledge graphs based on a context-enhanced knowledge aggregation process. Specifically, our framework leverages a novel representation learning approach to process heterogeneous features - combining flattened graph knowledge with text. To the best of our knowledge, this is the first attempt at hierarchically applying graph knowledge aggregation on a connected subgraph in addition to contextual infor
    
[^32]: CKBP v2：一个通识知识库填充的专家注释评估集合

    CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population. (arXiv:2304.10392v1 [cs.CL])

    [http://arxiv.org/abs/2304.10392](http://arxiv.org/abs/2304.10392)

    本文介绍了CKBP v2, 一个使用专家注释而囊括对抗样本的高质量通识知识库填充基准，以解决CKBP v1由于众包注释和随机抽样导致的问题。实验结果表明，通识知识库填充任务对于现有技术水平仍然具有挑战性。

    

    填充通识知识库是NLP中一个重要但困难的任务，因为它处理外部来源、未见过的事件和实体的知识。 Fang等人提出了一个通识知识库填充基准，其中包括评估集CKBP v1。但是，CKBP v1采用由众包注释，存在相当大比例的错误答案，并且由于随机抽样，评估集与外部知识来源的对齐效果不佳。在本文中，我们引入了CKBP v2，一个新的高质量的通识知识库填充基准，通过使用专家而不是众包注释，并添加多样化的对抗样本来使评估集更具代表性来解决上述两个问题。我们在新的评估集上进行了各种实验，比较了用于通识知识库填充的最新方法，以用于未来的研究比较。实证结果表明，即使对于大型语言模型（LLM），填充任务仍然具有挑战性。

    Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task in NLP, as it tackles knowledge from external sources with unseen events and entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that suffer from a substantial fraction of incorrect answers, and the evaluation set is not well-aligned with the external knowledge source as a result of random sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB Population benchmark, which addresses the two mentioned problems by using experts instead of crowd-sourced annotation and by adding diversified adversarial samples to make the evaluation set more representative. We conduct extensive experiments comparing state-of-the-art methods for CSKB Population on the new evaluation set for future research comparisons. Empirical results show that the population task is still challenging, even for large language models (LLM) 
    

