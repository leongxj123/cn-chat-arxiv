# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs](https://arxiv.org/abs/2404.01461) | 该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证 |
| [^2] | [Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots](https://arxiv.org/abs/2403.19995) | 提出了一个融合视觉、本体感知和语言的大脑启发式神经网络模型，通过预测编码和主动推断的框架，基于自由能原理，实现了语言组合性和感觉运动技能的联合发展。 |
| [^3] | [SyllabusQA: A Course Logistics Question Answering Dataset](https://arxiv.org/abs/2403.14666) | SyllabusQA数据集是一个包含63个真实课程大纲的开源数据集，对36个专业涵盖5,078对多样化的开放式课程逻辑相关问题-答案对进行了详细收集，旨在评估答案事实性，多个强基线模型在该任务上表现出色，但仍存在与人类之间的显著差距。 |
| [^4] | [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636) | 提出了动态内存压缩（DMC）方法，用于在线关键-值缓存压缩，模型学习在不同的头部和层中应用不同的压缩率，并且通过将预训练的LLMs改装为DMC Transformers，在自回归推断中实现了高达~3.7倍的吞吐量增加。 |
| [^5] | [GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data](https://arxiv.org/abs/2402.14973) | 提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。 |
| [^6] | [How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts](https://arxiv.org/abs/2402.13220) | 对多模态LLMs的欺骗性提示进行了实证分析，提出包含850个测试样本的基准测试MAD-Bench，发现GPT-4V在该基准测试上准确率较高，而其他模型性能差距显著。 |
| [^7] | [GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models](https://arxiv.org/abs/2402.12881) | 该论文提出了一个名为GRAFFORD的基准数据集，用于测试语言和视觉模型对物体可供性知识的表现，实验结果显示当前预训练语言模型在理解不常见物体可供性方面存在推理能力的局限。 |
| [^8] | [Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection](https://arxiv.org/abs/2402.11406) | 本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。 |
| [^9] | [Multi-Cultural Commonsense Knowledge Distillation](https://arxiv.org/abs/2402.10689) | 提出了一种MANGO方法，通过从概念和文化两个入口点谨慎而迭代地提示LLMs，提炼高准确度、高召回率的文化知识断言，提供了大量高准确度断言，能够改善对话系统回应的质量、特异性和文化敏感性。 |
| [^10] | [Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning](https://arxiv.org/abs/2402.07204) | 本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。 |
| [^11] | [Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation](https://arxiv.org/abs/2311.09136) | 提出一种使用部分排序来优化LLMs的方法，能够通过训练模型优先考虑特定任务候选响应池中的最佳响应，从而改善响应生成能力。 |
| [^12] | [Schema-Driven Information Extraction from Heterogeneous Tables](https://arxiv.org/abs/2305.14336) | 本文探讨了大型语言模型在通过引入基于模式的信息提取任务进行多领域表格数据处理时的竞争性表现，而无需特定流水线或标签，同时保持成本效率。 |
| [^13] | [Scientific Large Language Models: A Survey on Biological & Chemical Domains.](http://arxiv.org/abs/2401.14656) | 这篇论文介绍了科学大型语言模型在生物和化学领域的综述，分析了最新进展，并提出了科学LLMs的重要性。 |
| [^14] | [Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility.](http://arxiv.org/abs/2401.13782) | 本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。 |
| [^15] | [Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering.](http://arxiv.org/abs/2401.10711) | 本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。 |
| [^16] | [Defending Our Privacy With Backdoors.](http://arxiv.org/abs/2310.08320) | 本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。 |

# 详细

[^1]: 请真正的琳达站出来...面对大语言模型？在LLMs中审视代表性启发式

    Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs

    [https://arxiv.org/abs/2404.01461](https://arxiv.org/abs/2404.01461)

    该研究调查了代表性启发式对大型语言模型推理的影响，并创建了专门的数据集进行实验验证

    

    尽管大型语言模型（LLMs）在理解文本和生成类似人类文本方面表现出色，但它们可能会展现出从训练数据中获得的偏见。具体而言，LLMs可能会容易受到人类决策中的一种常见认知陷阱影响，即代表性启发式。这是心理学中的一个概念，指的是根据事件与一个众所周知的原型或典型例子的相似程度来判断事件发生的可能性，而不考虑更广泛的事实或统计证据。本研究调查了代表性启发式对LLM推理的影响。我们创建了REHEAT（Representativeness Heuristic AI Testing），一个包含涵盖六种常见代表性启发式类型问题的数据集。实验显示，应用于REHEAT的四个LLMs都表现出代表性启发式偏见。我们进一步确定了模型的推理步骤

    arXiv:2404.01461v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps
    
[^2]: 通过交互学习语言和机器人动作实现组合性和泛化能力的发展

    Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots

    [https://arxiv.org/abs/2403.19995](https://arxiv.org/abs/2403.19995)

    提出了一个融合视觉、本体感知和语言的大脑启发式神经网络模型，通过预测编码和主动推断的框架，基于自由能原理，实现了语言组合性和感觉运动技能的联合发展。

    

    人类擅长将学到的行为应用于未学习过的情境。这种泛化行为的一个关键组成部分是我们能够将整体分解成可重复利用的部分的能力，即组合性。机器人领域的一个基本问题是涉及这种特征。“在个体只学习部分语言组合及其相应的感觉运动模式时，如何通过联想学习同时发展语言的组合性和感觉运动技能？”为了解决这个问题，我们提出了一个融合视觉、本体感知和语言的大脑启发式神经网络模型，将其纳入基于自由能原理的预测编码和主动推断框架中。通过与机器人手臂进行的各种模拟实验评估了这个模型的有效性和能力。我们的结果表明，在学习中对于遗忘。

    arXiv:2403.19995v1 Announce Type: new  Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlear
    
[^3]: SyllabusQA：一个课程逻辑问题回答数据集

    SyllabusQA: A Course Logistics Question Answering Dataset

    [https://arxiv.org/abs/2403.14666](https://arxiv.org/abs/2403.14666)

    SyllabusQA数据集是一个包含63个真实课程大纲的开源数据集，对36个专业涵盖5,078对多样化的开放式课程逻辑相关问题-答案对进行了详细收集，旨在评估答案事实性，多个强基线模型在该任务上表现出色，但仍存在与人类之间的显著差距。

    

    自动化教学助理和聊天机器人有显著潜力减轻人类教师的工作量，尤其是对于与课程逻辑相关的问题回答，这对学生很重要，但对教师来说是重复的。然而，由于隐私问题，缺乏公开可用的数据集。我们介绍了SyllabusQA，这是一个开源数据集，包含63个真实课程大纲，涵盖36个专业，包含5,078对多样化的开放式课程逻辑相关问题-答案对，问题类型和答案格式都是多样的。由于许多逻辑相关问题包含关键信息，如考试日期，评估答案的事实性很重要。我们在该任务上对几个强基线进行了基准测试，从大型语言模型提示到检索增强生成。我们发现，尽管在传统的文本相似性指标上接近人类表现，但在准确性方面仍存在显著差距。

    arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
    
[^4]: 动态内存压缩：用于加速推断的LLMs的改装

    Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference

    [https://arxiv.org/abs/2403.09636](https://arxiv.org/abs/2403.09636)

    提出了动态内存压缩（DMC）方法，用于在线关键-值缓存压缩，模型学习在不同的头部和层中应用不同的压缩率，并且通过将预训练的LLMs改装为DMC Transformers，在自回归推断中实现了高达~3.7倍的吞吐量增加。

    

    Transformers已经成为大型语言模型（LLMs）的支柱。然而，由于需要在内存中存储关键-值表示的缓存以用于过去的标记，其大小与输入序列长度和批处理大小呈线性比例，因此生成仍然低效。作为解决方案，我们提出了动态内存压缩（DMC），这是一种用于在线关键-值缓存压缩的方法。最重要的是，模型学习在不同的头部和层中应用不同的压缩率。我们将预训练的LLMs（如Llama 2（7B、13B和70B））改装为DMC Transformers，在NVIDIA H100 GPU上的自回归推断中实现了高达~3.7倍的吞吐量增加。DMC通过在原始数据的可忽略百分比上进行持续的预训练而应用，并且不添加任何额外参数。我们发现，在高达4倍缓存压缩的情况下，DMC保留了原始的下游性能，优于up-trained grouped-query a。

    arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a
    
[^5]: GenCeption：使用未标记的单模态数据评估多模态LLM

    GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data

    [https://arxiv.org/abs/2402.14973](https://arxiv.org/abs/2402.14973)

    提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。

    

    多模态大型语言模型（MLLMs）通常使用昂贵的带标注的多模态基准进行评估。然而，这些基准通常难以跟上MLLM评估的快速发展要求。我们提出了GenCeption，这是一个新颖的无需注释的MLLM评估框架，仅需要单模态数据来评估跨模态语义一致性，并反映出模型产生幻觉的倾向。类似于流行的DrawCeption游戏，GenCeption从一个非文本样本开始，并经历一系列迭代的描述和生成步骤。迭代之间的语义漂移使用GC@T指标进行量化。我们的实证发现验证了GenCeption的有效性，并显示出与流行的MLLM基准结果的强相关性。GenCeption可以通过利用普遍存在且以前未见的单模态数据来扩展，以减轻训练数据的污染。

    arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
    
[^6]: 有多容易欺骗多模态LLMs？关于欺骗性提示的实证分析

    How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts

    [https://arxiv.org/abs/2402.13220](https://arxiv.org/abs/2402.13220)

    对多模态LLMs的欺骗性提示进行了实证分析，提出包含850个测试样本的基准测试MAD-Bench，发现GPT-4V在该基准测试上准确率较高，而其他模型性能差距显著。

    

    多模态大型语言模型（MLLMs）的显著进展并没有使它们免疫各种挑战，特别是在处理带有欺骗性信息的提示时，会产生幻觉般的回应。为了定量评估这种脆弱性，我们提出了MAD-Bench，一个精心策划的基准测试，包含850个测试样本，分为6个类别，如不存在的对象、对象数量、空间关系和视觉混淆。我们对流行的MLLMs进行了全面分析，包括GPT-4V、Gemini-Pro，以及开源模型，如LLaVA-1.5和CogVLM。实证研究中，我们观察到GPT-4V和其他模型之间存在着显著的性能差距；之前的鲁棒指令调整模型，如LRV-Instruction和LLaVA-RLHF，在这个新基准测试中并不有效。虽然GPT-4V在MAD-Bench上取得了75.02%的准确率，但其他任何模型在我们的实验中都没有达到这一水平。

    arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper
    
[^7]: GRAFFORD: 用于测试语言和视觉模型对物体可供性知识的基准数据集

    GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models

    [https://arxiv.org/abs/2402.12881](https://arxiv.org/abs/2402.12881)

    该论文提出了一个名为GRAFFORD的基准数据集，用于测试语言和视觉模型对物体可供性知识的表现，实验结果显示当前预训练语言模型在理解不常见物体可供性方面存在推理能力的局限。

    

    我们调查了预训练语言模型（LMs）和预训练视觉-语言模型（VLMs）中关于物体可供性的知识。基于Transformer的大型预训练语言模型（PTLM）从大量未标记文本中学习上下文表示，并在下游NLU任务中表现出色。与此同时，越来越多的文献表明，PTLM在推理和基础方面存在不一致且不直观的失败。为了首次定量衡量基础（或缺乏）的影响，我们精心策划了一个关于物体可供性的新颖而全面的数据集-- GrAFFORD，包含15个可供性类别。与视觉和语言领域收集的可供性数据集不同，我们用现场句子标注了对象和可供性。实验结果显示，当涉及不常见的物体可供性时，PTLM表现出有限的推理能力。我们还观察到PTLM在理解不常见物体可供性时存在困难。

    arXiv:2402.12881v1 Announce Type: new  Abstract: We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pr
    
[^8]: 不要走向极端：揭示LLMs在隐式仇恨言论检测中的过度敏感性和校准限制

    Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection

    [https://arxiv.org/abs/2402.11406](https://arxiv.org/abs/2402.11406)

    本文研究了LLMs在检测隐式仇恨言论和表达信心时的能力，发现LLMs存在两个极端：对可能引起公平问题的群体或话题表现出过于敏感，同时置信度评分过度集中在一个范围内。

    

    大型语言模型（LLMs）的公平性和可信度越来越受到关注。隐式仇恨言论，利用间接语言传达仇恨意图，占据实践中的重要部分。然而，LLMs有效解决这一问题的程度尚未得到充分审查。本文探讨了LLMs检测隐式仇恨言论（分类任务）以及对其响应的信心进行表达（校准任务）的能力。我们的评估细致考虑了各种提示模式和主流的不确定性估计方法。我们的研究结果突出了LLMs展示了两个极端：（1）LLMs对可能导致公平性问题的群体或话题显示出过度的敏感性，导致将良性陈述错误分类为仇恨言论。 （2）LLMs对每种方法的置信度得分过度集中在一个固定范围上，无论数据集的复杂性如何也保持不变。

    arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
    
[^9]: 多元文化常识知识蒸馏

    Multi-Cultural Commonsense Knowledge Distillation

    [https://arxiv.org/abs/2402.10689](https://arxiv.org/abs/2402.10689)

    提出了一种MANGO方法，通过从概念和文化两个入口点谨慎而迭代地提示LLMs，提炼高准确度、高召回率的文化知识断言，提供了大量高准确度断言，能够改善对话系统回应的质量、特异性和文化敏感性。

    

    尽管最近取得了一定进展，但大型语言模型（LLMs）仍然面临着适当应对社会和文化惯例的挑战。本文提出了MANGO，一种用于提炼高准确度、高召回率文化知识断言的方法论。我们从概念和文化两个入口点谨慎而迭代地提示LLMs进行这一目的。通过聚类和生成摘要将输出结果巩固。运行MANGO方法，以GPT-3.5作为底层LLM，为30K个概念和11K个文化提供了167K个高准确度断言，大幅超过先前的资源。为了外部评估，我们探索了将对话系统与文化知识断言相结合的方法。我们发现，添加来自MANGO的知识可以提升对话回应的整体质量、特异性和文化敏感性，这是由人类标注者评判的。数据和代码可供下载。

    arXiv:2402.10689v1 Announce Type: new  Abstract: Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.
    
[^10]: 结合空间优化和大型语言模型的开放领域城市行程规划

    Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning

    [https://arxiv.org/abs/2402.07204](https://arxiv.org/abs/2402.07204)

    本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。

    

    本文首次提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程。OUIP与传统行程规划不同，传统规划限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型(LLM)在处理多样化任务方面表现出潜力。然而，由于非实时信息、不完整的知识和不足的空间意识，它们无法独立地提供满意的用户体验。鉴于此，我们提出了一个名为ItiNera的OUIP系统，将空间优化与大型语言模型(LLM)相结合，根据用户需求提供个性化的城市行程定制服务。具体来说，我们开发了一个基于LLM的流水线，用于提取和更新兴趣点特征，以创建用户自己的个性化兴趣点数据库。对于每个用户请求，我们利用LLM进行协同实现优化。

    In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
    
[^11]: 用部分排序对LLM响应进行排名以改善响应生成

    Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation

    [https://arxiv.org/abs/2311.09136](https://arxiv.org/abs/2311.09136)

    提出一种使用部分排序来优化LLMs的方法，能够通过训练模型优先考虑特定任务候选响应池中的最佳响应，从而改善响应生成能力。

    

    定制LLMs以适应特定任务涉及将有效响应与错误响应区分开。这种技能可以通过使用大量人类偏好数据进行监督微调来发展。然而，对于大多数任务来说，获取专家注释的偏好数据是昂贵的。在本文中，我们提出了一种使用排名度量来优化LLMs的新方法。该方法训练模型优先考虑为特定任务创建的候选响应池中的最佳响应。我们主张采用部分排序而不是传统的完全排序，因为就候选响应的完美顺序达成共识可能具有挑战性。我们的部分排序更加稳健，对噪声的敏感性较低，并且可以通过有限的人类注释或启发式方法来实现。我们使用基准数据集测试了我们系统的改进响应生成能力，包括最新的多文档问答任务。

    arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati
    
[^12]: 来自异构表格的基于模式的信息提取

    Schema-Driven Information Extraction from Heterogeneous Tables

    [https://arxiv.org/abs/2305.14336](https://arxiv.org/abs/2305.14336)

    本文探讨了大型语言模型在通过引入基于模式的信息提取任务进行多领域表格数据处理时的竞争性表现，而无需特定流水线或标签，同时保持成本效率。

    

    在本文中，我们探讨了大型语言模型是否能够支持高效地从表格中提取信息的问题。我们引入了基于模式的信息提取，这是一项将表格数据转换为按照人类编写的模式组织的记录的新任务。为了评估各种LLM在这一任务上的能力，我们提出了一个基准，包括来自四个不同领域的表格：机器学习论文、化学文献、材料科学期刊和网页。我们利用这个带有注释的表格集合来评估开源和基于API的语言模型从涵盖多种领域和数据格式的表格中提取信息的能力。我们的实验表明，即使不需要任务特定的流水线或标签，也可以实现出人意料的竞争性表现，F1分数范围从74.2到96.1，同时保持成本效率。此外，通过详细的消融研究

    arXiv:2305.14336v3 Announce Type: replace  Abstract: In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studie
    
[^13]: 科学大型语言模型：生物和化学领域的综述

    Scientific Large Language Models: A Survey on Biological & Chemical Domains. (arXiv:2401.14656v1 [cs.CL])

    [http://arxiv.org/abs/2401.14656](http://arxiv.org/abs/2401.14656)

    这篇论文介绍了科学大型语言模型在生物和化学领域的综述，分析了最新进展，并提出了科学LLMs的重要性。

    

    大型语言模型（LLMs）已成为提升自然语言理解的一种变革性力量，对于人工智能的发展迈出了重要的一步。LLMs的应用已超越传统的语言界限，包括在各种科学学科内开发的专门语言系统。这种不断增长的兴趣导致了科学LLMs的出现，这是一种专门为促进科学发现而设计的新型子类。作为人工智能科学社区中的新兴领域，科学LLMs值得全面探索。然而，目前缺乏系统且最新的综述介绍它们。在本文中，我们将系统地勾画“科学语言”的概念，并全面审查科学LLMs的最新进展。考虑到科学学科的广泛领域，我们的分析采用了一种聚焦的视角，专注于生物和化学领域。

    Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical dom
    
[^14]: 从推特到引用：揭示社交媒体影响者对人工智能研究可见性的影响

    Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])

    [http://arxiv.org/abs/2401.13782](http://arxiv.org/abs/2401.13782)

    本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。

    

    随着人工智能和机器学习会议上被接受的论文数量达到数千篇，研究人员如何获取和阅读研究论文变得不清楚。本文研究了社交媒体影响者在增强机器学习研究可见性中的作用，特别是他们分享的论文引用次数。我们编制了一个包括8000多篇论文的全面数据集，涵盖了2018年12月至2023年10月的推特，以及基于出版年份、会议地点和摘要主题进行1：1匹配的对照组。我们的分析揭示了这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还深入研究了被展示作者的地理、性别和机构多样性。这些发现突显了社交媒体在学术交流中的不断扩大的影响力，并强调了当今数字化时代不断发展的生态系统的重要性。

    As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
    
[^15]: 使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题

    Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])

    [http://arxiv.org/abs/2401.10711](http://arxiv.org/abs/2401.10711)

    本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。

    

    视频问答（VideoQA）旨在基于观察到的视频信息回答自然语言问题。尽管大型多模型（LMMs）在图像语言理解和推理方面取得了近期的成功，但它们在处理视频问答方面还不足够，仅仅是将均匀采样的帧作为视觉输入，忽略了与问题相关的视觉线索。此外，现有的视频问答数据集中没有针对问题关键时间戳的人工注释。基于此，我们提出了一种新的弱监督框架，强制LMMs使用问题关键时刻作为视觉输入推理出答案。具体来说，我们将问题和答案对合并为事件描述，以找到多个关键帧作为目标时刻，这些时刻将作为伪标签。通过将这些伪标签作为额外的弱监督，我们设计了一个轻量级的基于高斯的对比基础模块（GCG）。GCG学习多个高斯函数来描述时效结构。

    Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
    
[^16]: 使用后门技术保护我们的隐私

    Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])

    [http://arxiv.org/abs/2310.08320](http://arxiv.org/abs/2310.08320)

    本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。

    

    在使用未经筛选、常常包含敏感信息的网页数据训练大型人工智能模型的情况下，隐私问题成为了一个重要的关注点。其中一个问题是，攻击者可以利用隐私攻击的方法提取出训练数据的信息。然而，如何在不降低模型性能的情况下去除特定信息是一个不容易解决且具有挑战性的问题。我们提出了一个基于后门攻击的简单而有效的防御方法，用于从模型中删除私人信息，如个人姓名，特别是针对文本编码器的。具体而言，通过策略性地插入后门，我们将敏感短语的嵌入与中性术语的嵌入对齐，例如用"a person"代替人名。我们的实证结果通过对零样本分类器使用专门的隐私攻击测试表明了我们基于后门的防御方法的效果。我们的方法提供了一个新的"双重用途"的视角。

    The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
    

