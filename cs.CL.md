# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^2] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^3] | [HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/abs/2403.04307) | HaluEval-Wild是第一个专门设计用于评估实际环境中LLM幻觉的基准测试，收集了具有挑战性的用户查询并分类为五种不同类型，可以对LLM表现出的幻觉类型进行细粒度分析。 |
| [^4] | [Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People](https://arxiv.org/abs/2403.03640) | Apollo项目开发了多语言医学LLMs，创建了全球人口61亿的医学数据集，并发布了各种尺寸的最佳性能模型，其中Apollo-7B是最先进的多语言医学LLMs，可改善更大模型的多语言医学能力。 |
| [^5] | [ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs](https://arxiv.org/abs/2402.11764) | 本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。 |
| [^6] | [Discrete Neural Algorithmic Reasoning](https://arxiv.org/abs/2402.11628) | 这项工作提出了一种强制神经推理器维护执行轨迹作为有限预定义状态组合的方法，通过对算法状态转换的监督训练，使模型能够与原始算法完美对齐，并在基准测试中取得了完美的测试成绩。 |
| [^7] | [Puzzle Solving using Reasoning of Large Language Models: A Survey](https://arxiv.org/abs/2402.11291) | 本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。 |
| [^8] | [Zero-shot sampling of adversarial entities in biomedical question answering](https://arxiv.org/abs/2402.10527) | 在生物医学问题回答中，我们提出了一种在嵌入空间中进行零样本采样的方案，用于发现各种对抗实体作为干扰因素，相比随机采样，在对抗问答中表现出明显优势，揭示了不同特征的两种对抗性实体制度。 |
| [^9] | [Steering Conversational Large Language Models for Long Emotional Support Conversations](https://arxiv.org/abs/2402.10453) | 引入了Strategy-Relevant Attention（SRA）度量，评估大型语言模型在情感支持对话中遵循战略提示的有效性，研究发现应用SRA指导的提示可提高战略依从性，从而使长时间对话更可靠地展示所需的情感支持策略。 |
| [^10] | [LLMs and the Human Condition](https://arxiv.org/abs/2402.08403) | 本文提出了将三个成熟的人类决策理论整合到一起，形成了一个目的性人类行动模型。同时，将语言作为行动的观点应用于对话用户界面。通过理解ChatGPT的智能来源，可以在减少资源的同时获得对我们之间关系的认识。 |
| [^11] | [Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841) | 这项研究在大规模语言模型上对成员推断攻击进行了评估，发现在大部分设置中，攻击几乎只能比随机猜测稍好，这种糟糕的性能是由于大型数据集和少量训练迭代的组合，以及成员和非成员之间的边界困惑所导致的。 |
| [^12] | [Can GPT-3.5 Generate and Code Discharge Summaries?.](http://arxiv.org/abs/2401.13512) | GPT-3.5被用于生成和标注医疗文件以进行数据增强，结果显示其对ICD-10代码的编码性能良好，并且生成的文件在临床可接受性评估中得到了认可。 |
| [^13] | [VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM.](http://arxiv.org/abs/2401.01256) | VideoDrafter是一个利用LLM实现内容一致的多场景视频生成的框架，能够根据输入提示生成逻辑连贯的多场景脚本，并生成高质量的视频。 |
| [^14] | [Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation.](http://arxiv.org/abs/2309.07369) | 半混合注意力编码器-解码器模型通过分离声学模型和语言模型，以实现对传统文本语言模型适应技术的利用。在使用域外文本数据进行语言模型适应时，相对于传统模型，该模型可获得21\%的词错误率改进。 |
| [^15] | [Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.](http://arxiv.org/abs/2306.09841) | 本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。 |
| [^16] | [Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training.](http://arxiv.org/abs/2306.05278) | 本文探讨了Few-shot Intent Classification任务的解决方法。相较于传统的在外部资源上连续预训练，本文提出了直接微调预训练语言模型的方法，并通过实验证明其在少量标记数据情况下已经可以取得不错的结果，表明连续预训练并非必要。 |
| [^17] | [Interpretable Multimodal Misinformation Detection with Logic Reasoning.](http://arxiv.org/abs/2305.05964) | 本文提出了一种新的基于逻辑的多模态虚假信息检测神经模型，通过集成可解释性逻辑子句表达目标任务的推理过程，并使用神经表征参数化符号逻辑元素，从而便于自动生成和评估有意义的逻辑子句。此外，引入了五个元预测器来捕获虚假信息的基本模式。实验结果表明，该模型不仅性能显著优于当前方法，而且提供了透明且可解释的逻辑推理过程。 |

# 详细

[^1]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^2]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^3]: HaluEval-Wild：在实际环境中评估语言模型的幻觉

    HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild

    [https://arxiv.org/abs/2403.04307](https://arxiv.org/abs/2403.04307)

    HaluEval-Wild是第一个专门设计用于评估实际环境中LLM幻觉的基准测试，收集了具有挑战性的用户查询并分类为五种不同类型，可以对LLM表现出的幻觉类型进行细粒度分析。

    

    幻觉对于关键领域中大型语言模型（LLMs）的可靠性构成了重大挑战。最近设计用于评估LLM在传统NLP任务中的幻觉的基准测试，如知识密集型问答（QA）和摘要，不足以捕捉动态实际环境中用户-LLM交互的复杂性。为了弥补这一空白，我们介绍了HaluEval-Wild，这是第一个专门设计用于评估实际环境中LLM幻觉的基准测试。我们精心收集了来自现有实际用户-LLM交互数据集（包括ShareGPT）中具有挑战性的（经Alpaca对抗性过滤的）用户查询，以评估各种LLM的幻觉率。在分析收集到的查询后，我们将其分类为五种不同类型，这使得可以对LLM表现出的幻觉类型进行细粒度分析，并将引用答案与强大的GP合成。

    arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP
    
[^4]: Apollo：轻量级多语言医学LLMs：让医学人工智能普惠60亿人

    Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People

    [https://arxiv.org/abs/2403.03640](https://arxiv.org/abs/2403.03640)

    Apollo项目开发了多语言医学LLMs，创建了全球人口61亿的医学数据集，并发布了各种尺寸的最佳性能模型，其中Apollo-7B是最先进的多语言医学LLMs，可改善更大模型的多语言医学能力。

    

    尽管全球医学知识的庞大存储库主要是以英语为主，但在传递量身定制医疗服务方面，本地语言对于在医疗资源有限的地区尤为重要。为了将医学人工智能的进展扩展到更广泛的人群，我们旨在开发涵盖全球61亿人口的六种最常用语言的医学LLMs。这一努力最终促成了ApolloCorpora多语言医学数据集和XMedBench基准的创建。在多语言医学基准测试中，发布的Apollo模型，在各种相对较小尺寸（即0.5B、1.8B、2B、6B和7B）上取得了与同等大小模型最佳性能。特别地，Apollo-7B是迄今为止达到70B的最先进的多语言医学LLMs。此外，这些轻量级模型可用于在不需要微调的情况下改进较大模型的多语言医学能力。

    arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
    
[^5]: 基于ChatGPT的数据增强技术用于改善LLMs的参数高效去偏见化

    ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs

    [https://arxiv.org/abs/2402.11764](https://arxiv.org/abs/2402.11764)

    本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。

    

    大语言模型（LLMs）虽然功能强大，但存在有害的社会偏见。由于计算成本、数据约束和可能降低多任务语言能力，去偏见化通常具有挑战性。本文介绍了一种利用ChatGPT生成合成训练数据的新方法，旨在增强LLMs的去偏见化。我们提出了两种策略：目标提示，对已知偏见提供有效的去偏见化，但需要事先指定问题中的偏见; 一般提示，虽然效果稍逊，但能够跨各种类别进行去偏见化。我们利用适配器调整来实现资源高效的LLM去偏见化，并比较了我们的合成数据与现有去偏见化数据集的效果。我们的结果表明：（1）ChatGPT可以高效地生成用于去偏见化其他LLMs的高质量训练数据；（2）通过我们的方法生成的数据超越了现有数据集在去偏见化上的效果。

    arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
    
[^6]: 离散神经算法推理

    Discrete Neural Algorithmic Reasoning

    [https://arxiv.org/abs/2402.11628](https://arxiv.org/abs/2402.11628)

    这项工作提出了一种强制神经推理器维护执行轨迹作为有限预定义状态组合的方法，通过对算法状态转换的监督训练，使模型能够与原始算法完美对齐，并在基准测试中取得了完美的测试成绩。

    

    神经算法推理旨在通过学习模仿经典算法的执行来捕捉神经网络中的计算。尽管常见的架构足够表达正确的模型在权重空间中，但当前的神经推理器在处理超出分布数据时面临泛化困难。另一方面，经典计算不受分布变化的影响，因为它们可以描述为离散计算状态之间的转换。在这项工作中，我们提出强制神经推理器将执行轨迹作为有限预定义状态的组合进行维护。通过对算法状态转换的监督训练，这种模型能够与原始算法完美对齐。为了证明这一点，我们在SALSA-CLRS基准测试上评估我们的方法，在那里我们为所有任务获得了完美的测试成绩。此外，所提出的架构选择使我们能够证明...

    arXiv:2402.11628v1 Announce Type: new  Abstract: Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the 
    
[^7]: 使用大型语言模型的推理解决难题：一项调查

    Puzzle Solving using Reasoning of Large Language Models: A Survey

    [https://arxiv.org/abs/2402.11291](https://arxiv.org/abs/2402.11291)

    本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。

    

    探索大型语言模型（LLMs）在解决难题中的能力揭示了它们在人工智能中的潜力和挑战，标志着理解它们在复杂推理任务中的适用性迈出了重要的一步。本调查利用独特的分类法将难题分为基于规则和无规则两类，通过各种方法评估LLMs，包括提示技术、神经符号方法和微调。通过对相关数据集和基准的批判性审查，我们评估了LLMs在复杂难题场景中的表现，识别出复杂难题情境中的显著挑战。我们的研究结果突出了LLMs能力及类人推理之间的差距，特别是在需要高级逻辑推断的情况下。调查强调了需要新颖策略和更丰富数据集来提升LLMs的解谜能力并促进人工智能的发展。

    arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
    
[^8]: 生物医学问题回答中的零样本采样对抗实体

    Zero-shot sampling of adversarial entities in biomedical question answering

    [https://arxiv.org/abs/2402.10527](https://arxiv.org/abs/2402.10527)

    在生物医学问题回答中，我们提出了一种在嵌入空间中进行零样本采样的方案，用于发现各种对抗实体作为干扰因素，相比随机采样，在对抗问答中表现出明显优势，揭示了不同特征的两种对抗性实体制度。

    

    大型语言模型（LLM）中参数域知识的增加深度推动它们在现实世界应用中的快速部署。在高风险和知识密集型任务中，理解模型的漏洞对于量化模型预测的可信度和规范其使用至关重要。最近发现在自然语言处理任务中作为对抗示例的命名实体引发了关于它们在其他环境中可能的伪装的疑问。在这里，我们提出了一种在嵌入空间中的幂缩放距离加权采样方案，以发现多样化的对抗实体作为干扰因素。我们展示了它在生物医学主题的对抗性问题回答中优于随机采样的优势。我们的方法使得可以探索攻击表面上的不同区域，这揭示了两种在特征上明显不同的对抗性实体的制度。此外，我们展示了攻击方式如何...

    arXiv:2402.10527v1 Announce Type: new  Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks su
    
[^9]: 引导情感支持对话的大型语言模型进行长时间对话

    Steering Conversational Large Language Models for Long Emotional Support Conversations

    [https://arxiv.org/abs/2402.10453](https://arxiv.org/abs/2402.10453)

    引入了Strategy-Relevant Attention（SRA）度量，评估大型语言模型在情感支持对话中遵循战略提示的有效性，研究发现应用SRA指导的提示可提高战略依从性，从而使长时间对话更可靠地展示所需的情感支持策略。

    

    在这项研究中，我们解决了大型语言模型（LLMs）在长时间对话中一贯遵循情感支持策略的挑战。我们引入了Strategy-Relevant Attention（SRA）度量，这是一个模型不可知的指标，旨在评估LLMs在情感支持环境中遵循战略提示的有效性。通过使用LLaMA模型分析情感支持对话数据集（ESConv）中的对话，我们证明SRA与模型在整个互动过程中维持所述策略能力密切相关。我们的研究结果显示，应用基于SRA的提示可提高战略依从性，导致对话更可靠地展示长时间对话中所需的情感支持策略。此外，我们贡献了一个全面的、多分支的合成对话数据集，适用于ESConv，其中包含各种策略内容。

    arXiv:2402.10453v1 Announce Type: new  Abstract: In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model's ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy cont
    
[^10]: LLMs和人类条件

    LLMs and the Human Condition

    [https://arxiv.org/abs/2402.08403](https://arxiv.org/abs/2402.08403)

    本文提出了将三个成熟的人类决策理论整合到一起，形成了一个目的性人类行动模型。同时，将语言作为行动的观点应用于对话用户界面。通过理解ChatGPT的智能来源，可以在减少资源的同时获得对我们之间关系的认识。

    

    本文介绍了人类决策的三个成熟理论，并描述了如何将它们整合起来提供一个目的性人类行动的模型。同时，将语言作为行动的观点应用于对话用户界面。最近，基于理论的人工智能研究遇到了困难，本文旨在重新激发对理解LLMs实际执行的兴趣，而不仅仅是在所有数据上运行难以理解的机器学习例程。当一台售价不到50美元的树莓派电脑比第一台商业Cray超级计算机快400倍时，大型科技公司可以接近拥有无数随机打字并生成有意义文字的猴子。通过理解ChatGPT的表现智能的来源，也许我们可以用更少的资源进行同样的魔术，并在此过程中获得一些关于我们之间关系的理解。

    This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship
    
[^11]: 大型语言模型上的成员推断攻击是否奏效？

    Do Membership Inference Attacks Work on Large Language Models?

    [https://arxiv.org/abs/2402.07841](https://arxiv.org/abs/2402.07841)

    这项研究在大规模语言模型上对成员推断攻击进行了评估，发现在大部分设置中，攻击几乎只能比随机猜测稍好，这种糟糕的性能是由于大型数据集和少量训练迭代的组合，以及成员和非成员之间的边界困惑所导致的。

    

    成员推断攻击（MIAs）试图预测特定数据点是否属于目标模型的训练数据。尽管对传统机器学习模型进行了广泛研究，但在大型语言模型（LLMs）的预训练数据上对MIA的研究工作仍有限。我们对在Pile上训练的一系列语言模型（LMs）进行了大规模的MIA评估，参数范围从160M到12B。我们发现，在不同的LLM大小和领域的大多数设置中，MIAs几乎只能比随机猜测稍好。我们进一步分析发现，这种糟糕的性能可以归因于（1）大型数据集和少量训练迭代的组合，以及（2）成员和非成员之间的边界困惑。我们确定了LLMs易受成员推断攻击的特定设置，并表明在这些设置中取得的表面上的成功可以归因于分布的转变，例如当成员和非成员被绘制出来时。

    Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn
    
[^12]: GPT-3.5能否生成和标注出院摘要？

    Can GPT-3.5 Generate and Code Discharge Summaries?. (arXiv:2401.13512v1 [cs.CL])

    [http://arxiv.org/abs/2401.13512](http://arxiv.org/abs/2401.13512)

    GPT-3.5被用于生成和标注医疗文件以进行数据增强，结果显示其对ICD-10代码的编码性能良好，并且生成的文件在临床可接受性评估中得到了认可。

    

    目的：探究GPT-3.5在生成和标注具有ICD-10代码的医疗文件方面的应用，用于低资源标签的数据增强。材料和方法：利用GPT-3.5基于MIMIC-IV数据集中罕见（生成）代码的ICD-10代码描述列表生成和标注了9,606份出院摘要。将其与基线训练集结合，形成一个增强训练集。使用神经编码模型在基线和增强数据上进行训练，并在MIMIC-IV测试集上进行评估。我们报告了全代码集、生成代码及其所属代码族的微观和宏观F1得分。采用弱层次混淆矩阵来确定后面两个代码集中的代码族内和代码族外的编码错误。对GPT-3.5的编码性能进行了自行生成数据和真实MIMIC-IV数据的评估。临床专业人员对生成的文件进行了临床可接受性评估。结果和结论：增强微小

    Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels.  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset. Combined with the baseline training set, this formed an augmented training set. Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set. We report micro- and macro-F1 scores on the full codeset, generation codes, and their families. Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data. Clinical professionals evaluated the clinical acceptability of the generated documents.  Results: Augmentation slight
    
[^13]: VideoDrafter: 利用LLM实现内容一致的多场景视频生成

    VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM. (arXiv:2401.01256v1 [cs.CV])

    [http://arxiv.org/abs/2401.01256](http://arxiv.org/abs/2401.01256)

    VideoDrafter是一个利用LLM实现内容一致的多场景视频生成的框架，能够根据输入提示生成逻辑连贯的多场景脚本，并生成高质量的视频。

    

    最近扩展模型的创新和突破显著扩大了根据给定提示生成高质量视频的可能性。现有的大多数作品仅处理在单个背景中发生单个视频事件的单场景情况。然而，扩展到生成多场景视频并且在保持各个场景之间的逻辑一致同时保持视觉外观一致性方面并不简单。在本文中，我们提出了一种新颖的框架，即VideoDrafter，用于内容一致的多场景视频生成。技术上，VideoDrafter利用大型语言模型（LLM）将输入提示转化为综合的多场景脚本，该脚本从LLM学到的逻辑知识中受益。每个场景的脚本包括描述事件、前景/背景实体以及摄像机运动的提示。VideoDrafter识别脚本中的共同实体，并询问LLM来选择生成逻辑连贯的视频场景。

    The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoDrafter, for content-consistent multi-scene video generation. Technically, VideoDrafter leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoDrafter identifies the common entities throughout the script and asks LLM
    
[^14]: 半混合注意力编码器-解码器模型用于高效语言模型适应

    Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation. (arXiv:2309.07369v1 [eess.AS])

    [http://arxiv.org/abs/2309.07369](http://arxiv.org/abs/2309.07369)

    半混合注意力编码器-解码器模型通过分离声学模型和语言模型，以实现对传统文本语言模型适应技术的利用。在使用域外文本数据进行语言模型适应时，相对于传统模型，该模型可获得21\%的词错误率改进。

    

    基于注意力的编码器-解码器语音识别模型近年来取得了广泛的成功。然而，在端到端方式中联合优化声学模型和语言模型对于文本适应性提出了挑战。特别是，有效、快速和廉价地适应文本已成为在工业中部署注意力编码器-解码器系统的主要关注点。为了解决这个问题，我们提出了一种新颖的模型，即半混合注意力编码器-解码器语音识别模型，保留了传统混合自动语音识别系统的模块化特性。我们的半混合注意力编码器-解码器模型将声学模型和语言模型分离，使得可以使用传统的基于文本的语言模型适应技术。我们证明了在使用域外文本数据进行语言模型适应时，所提出的半混合注意力编码器-解码器模型相对于传统的基于注意力的模型在词错误率上实现了21\%的改进，并且在常规测试集上的词错误率只有轻微的降低。

    Attention-based encoder-decoder (AED) speech recognition model has been widely successful in recent years. However, the joint optimization of acoustic model and language model in end-to-end manner has created challenges for text adaptation. In particular, effectively, quickly and inexpensively adapting text has become a primary concern for deploying AED systems in industry. To address this issue, we propose a novel model, the hybrid attention-based encoder-decoder (HAED) speech recognition model that preserves the modularity of conventional hybrid automatic speech recognition systems. Our HAED model separates the acoustic and language models, allowing for the use of conventional text-based language model adaptation techniques. We demonstrate that the proposed HAED model yields 21\% Word Error Rate (WER) improvements in relative when out-of-domain text data is used for language model adaptation, and with only a minor degradation in WER on a general test set compared with conventional AE
    
[^15]: 大型语言模型真的是良好的逻辑推理者吗？基于演绎、归纳和阿布达斯观点的全面评估。

    Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])

    [http://arxiv.org/abs/2306.09841](http://arxiv.org/abs/2306.09841)

    本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。

    

    大型语言模型(LLMs)在各种自然语言任务中取得了巨大成功。对LLMs的具体推理能力进行评估，如多语言推理和数学推理，引起了广泛关注。然而，作为关键推理视角之一，逻辑推理能力还没有得到彻底评估。本文旨在填补这些差距并提供全面的评估。首先，为了进行系统化评估，本文选择了15个典型的逻辑推理数据集，并将它们组织成演绎、归纳、阿布达斯和混合形式的推理设置。考虑评估的全面性，我们选择了三个代表性的LLMs（text-davinci-003，ChatGPT和BARD），并在零样本、一次和三次的设置下对所有选择的数据集进行评估。其次，与以往仅依赖简单指标（如准确性）的评估不同，我们提出了从目标推理角度进行的精细级别评估。

    Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
    
[^16]: 重新审视使用PLMs的Few-shot Intent Classification: 直接微调 vs 连续预训练

    Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])

    [http://arxiv.org/abs/2306.05278](http://arxiv.org/abs/2306.05278)

    本文探讨了Few-shot Intent Classification任务的解决方法。相较于传统的在外部资源上连续预训练，本文提出了直接微调预训练语言模型的方法，并通过实验证明其在少量标记数据情况下已经可以取得不错的结果，表明连续预训练并非必要。

    

    本文考虑Few-shot Intent Classification任务，该任务涉及仅使用少量标记数据训练深度学习模型以基于其基础意图分类话语。解决此问题的当前方法是通过连续预训练，即在外部资源（例如会话语料库、公共意图检测数据集或自然语言理解数据集）上微调预训练语言模型（PLMs），然后使用它们作为话语编码器来训练意图分类器。在本文中，我们展示了连续预训练可能并非必要，因为PLMs在此任务上的过拟合问题可能并不像预期的那样严重。具体来说，我们发现，直接对仅有少量标记示例的PLMs进行微调已经可以产生相当不错的结果，而绩效差距随着标记数据量的增加迅速缩小。为了最大限度地利用有限的标记数据，我们提出了一种新的微调策略，即注意力流控（Attention Flow Control），其允许在不同的预训练层之间动态分配微调的重心。

    We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited a
    
[^17]: 多模态虚假信息解释性检测与逻辑推理

    Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])

    [http://arxiv.org/abs/2305.05964](http://arxiv.org/abs/2305.05964)

    本文提出了一种新的基于逻辑的多模态虚假信息检测神经模型，通过集成可解释性逻辑子句表达目标任务的推理过程，并使用神经表征参数化符号逻辑元素，从而便于自动生成和评估有意义的逻辑子句。此外，引入了五个元预测器来捕获虚假信息的基本模式。实验结果表明，该模型不仅性能显著优于当前方法，而且提供了透明且可解释的逻辑推理过程。

    

    在线社交平台上的多模态虚假信息由于多媒体内容的可信度和传播更容易而变得越来越重要。虽然现有的多模态检测方法已经达到了较高的性能，但缺乏解释性阻碍了这些系统的可靠性和实际部署。受到 NeuralSymbolic AI 的启发，该方法结合了神经网络的学习能力和符号学习的可解释性，我们提出了一种新的基于逻辑的多模态虚假信息检测神经模型，它集成了可解释性逻辑子句以表达目标任务的推理过程。为了使学习有效，我们使用神经表征来参数化符号逻辑元素，从而便于自动生成和评估有意义的逻辑子句。另外，为了使我们的框架可适用于各种虚假信息来源，我们在多模态融合网络中引入了五个元预测器来捕获虚假信息的基本模式。我们在实际的多模态虚假信息数据集上进行了大量实验，结果表明，我们的模型不仅显着优于现有方法，还为每个预测提供了透明且可解释的逻辑推理过程。

    Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-pre
    

