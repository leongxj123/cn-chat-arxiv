# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts](https://arxiv.org/abs/2404.02022) | 本文提出一种通用且便利的方法，通过利用小型编码器语言模型和交叉注意力，使原始语言模型可以覆盖更长的上下文，从而提高开放领域问答任务的性能。 |
| [^2] | [Developing Safe and Responsible Large Language Models -- A Comprehensive Framework](https://arxiv.org/abs/2404.01399) | 该论文介绍了一种新的模型SR$_{\text{LLM}}$，旨在通过引入全面的安全风险分类法和专家标注数据集来增强大型语言模型（LLM）在语言生成中的安全性，并通过指令和参数高效微调方法有效减少了不安全内容的生成。 |
| [^3] | [TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model](https://arxiv.org/abs/2404.01273) | 提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。 |
| [^4] | [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376) | 通过从医学教材提取的推理路径和多样化的遵循指令数据集，我们引入了具有70亿参数的Meerkat-7B医学人工智能系统，成功解决了商用大型语言模型在医学任务上隐私和推理能力不足的问题，取得了优于先前7B模型的显著成果。 |
| [^5] | [AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving](https://arxiv.org/abs/2403.19708) | AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。 |
| [^6] | [CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing](https://arxiv.org/abs/2403.13583) | CONLINE框架提出了通过在线搜索和正确性测试来增强复杂代码生成的方法，通过实验证明了其显著提高了代码生成质量。 |
| [^7] | [ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context](https://arxiv.org/abs/2403.02177) | 提出了一个计划-推理框架，用于在表格上的句子背景中回答用户查询，通过对Llama-2-7B进行微调，构建了ProTrix模型，广泛适用于不同表格任务，并表现出与GPT-3.5-turbo相当的性能水平，可生成准确且忠实的解释。 |
| [^8] | [LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language](https://arxiv.org/abs/2402.16929) | LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。 |
| [^9] | [If in a Crowdsourced Data Annotation Pipeline, a GPT-4](https://arxiv.org/abs/2402.16795) | 本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。 |
| [^10] | [GraphWiz: An Instruction-Following Language Model for Graph Problems](https://arxiv.org/abs/2402.16029) | GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。 |
| [^11] | [Are Large Language Models Rational Investors?](https://arxiv.org/abs/2402.12713) | 本研究引入了金融偏见指标（FBI）框架来评估大型语言模型（LLMs）的金融合理性，着重检验它们对金融信息的辨别和市场分析中可能存在的非理性偏见。 |
| [^12] | [Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations](https://arxiv.org/abs/2402.11975) | 提出了COmpressive Memory-Enhanced Dialogue sYstems（COMEDY）框架，通过“一对多”方法利用单一语言模型管理记忆生成、压缩和响应生成，核心概念是压缩记忆，支持大规模中文指导调优数据集Dolphin，比较评估证明了COMEDY的优越性。 |
| [^13] | [Integrating Pre-Trained Language Model with Physical Layer Communications](https://arxiv.org/abs/2402.11656) | 提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能 |
| [^14] | [How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?](https://arxiv.org/abs/2402.10770) | 本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。 |
| [^15] | [Recovering the Pre-Fine-Tuning Weights of Generative Models](https://arxiv.org/abs/2402.10208) | 该论文提出了一种恢复生成模型预微调权重的方法，通过少量低秩微调模型可以恢复准确的预微调权重，利用这个新漏洞攻击大规模模型。 |
| [^16] | [TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations](https://arxiv.org/abs/2402.06608) | 该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。 |
| [^17] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^18] | [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858) | CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。 |
| [^19] | [Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration](https://arxiv.org/abs/2402.00367) | 本论文研究了识别大型语言模型（LLM）知识盲区的方法，并提出了两种基于LLM协作的新方法，通过这些方法可以在面对知识盲区时放弃回答问题。实验证明，这些方法在提高放弃准确度方面取得了高达19.3％的改进。 |
| [^20] | [Question Translation Training for Better Multilingual Reasoning](https://arxiv.org/abs/2401.07817) | 本文探讨了通过问题对齐训练模型将推理问题翻译成英语的方法，以实现有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。 |
| [^21] | [Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark](https://arxiv.org/abs/2311.09122) | UNER是一个开放的、社区驱动的项目，旨在提供高质量、跨语言一致的命名实体识别基准，以促进和标准化多语言NER研究。 |
| [^22] | [Exploring the Potential of Large Language Models in Computational Argumentation](https://arxiv.org/abs/2311.09022) | 该研究旨在评估大型语言模型在计算辩论领域的性能，包括零样本和少样本设置，标准化了14个开源数据集，并介绍了一个新的反言生成基准数据集。 |
| [^23] | [Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning](https://arxiv.org/abs/2311.07945) | 较小的语言模型在多步骤数学推理中通过正确开始可以获得显着的性能提升，建议通过初始指导和自问指导的方式来引导模型开始正确。 |
| [^24] | [Does Writing with Language Models Reduce Content Diversity?](https://arxiv.org/abs/2309.05196) | 写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。 |
| [^25] | [CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering.](http://arxiv.org/abs/2401.13170) | CFMatch提出了一个在开放域问答中将自动答案等价评估与人工专家判断对齐的方法，通过提供明确一致的评估指南并引入高效、稳健且轻量级的判别式AE分类器匹配方法来解决当前评估指标与人类判断不一致的问题。 |
| [^26] | [A match made in consistency heaven: when large language models meet evolutionary algorithms.](http://arxiv.org/abs/2401.10510) | 大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。 |
| [^27] | [Zero-Shot Position Debiasing for Large Language Models.](http://arxiv.org/abs/2401.01218) | 本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。 |
| [^28] | [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning.](http://arxiv.org/abs/2311.12023) | LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。 |
| [^29] | [$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation.](http://arxiv.org/abs/2311.01862) | $R^3$-NL2GQL是一种通过利用较小和较大的Foundation Models进行重新排名、重写和细化的方法，以提高准确性和减轻幻觉，解决了NL2GQL任务中GQL生成能力和跨模式通用能力的挑战。 |
| [^30] | [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory.](http://arxiv.org/abs/2310.17884) | 本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。 |
| [^31] | [Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models.](http://arxiv.org/abs/2309.12763) | 使用音频增强为低资源自我监督语音模型的预训练提出一种有效的方法，并且综合增强（噪声/音高）是最佳的增强策略，超过了重音和语言知识转移。 |
| [^32] | [Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?.](http://arxiv.org/abs/2308.15399) | 本研究提出了一个灵活的框架，引导大型语言模型根据跨学科研究中建立的道德理论进行道德推理，解决了现有方法面临的问题。 |
| [^33] | [KoLA: Carefully Benchmarking World Knowledge of Large Language Models.](http://arxiv.org/abs/2306.09296) | 本研究提出了一个针对大型语言模型的知识导向评估基准 (KoLA)，通过模仿人类认知构建了四级知识相关能力的分类体系，并使用维基百科和新兴语料库进行评估。这个基准旨在全面、公正和实用地评估LLM的能力，以处理未见数据和不断发展的知识。 |
| [^34] | [Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation.](http://arxiv.org/abs/2305.13857) | 本研究发现任务导向对话系统中存在用户熟悉度偏见，而真实世界的应用场景很少符合封闭目标的设定。因此，在开放目标设置下，系统会出现严重问题，同时研究者发现了“不匹配错误”这一新型错误类型。 |
| [^35] | [The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research.](http://arxiv.org/abs/2305.02797) | 本文研究了工业界在自然语言处理研究中的存在和影响。研究发现在过去五年中，工业界的存在与影响呈现急剧增长，一些公司占据了大部分出版物，并向学术研究人员提供资金支持。 |
| [^36] | [Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models.](http://arxiv.org/abs/2303.09100) | 本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。 |
| [^37] | [Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification.](http://arxiv.org/abs/2303.08021) | 本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。 |
| [^38] | [End-to-End Training for Back-Translation with Categorical Reparameterization Trick.](http://arxiv.org/abs/2202.08465) | 本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。 |

# 详细

[^1]: 优化向量化上下文的检索增强开放领域问答

    Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts

    [https://arxiv.org/abs/2404.02022](https://arxiv.org/abs/2404.02022)

    本文提出一种通用且便利的方法，通过利用小型编码器语言模型和交叉注意力，使原始语言模型可以覆盖更长的上下文，从而提高开放领域问答任务的性能。

    

    在大型语言模型时代，应用检索增强生成等技术可以更好地解决开放领域问答问题。由于模型大小和计算资源等约束，上下文长度通常受限，让模型覆盖过长的上下文并回答来自开放领域的问题变得具有挑战性。本文提出了一种在开放领域问答任务中覆盖更长上下文的通用、方便方法。它利用一个小型编码器语言模型有效编码上下文，并对原始输入应用交叉注意力。通过我们的方法，原始语言模型可以覆盖几倍长的上下文，同时保持与基线接近的计算需求。我们的实验表明，在微调后，性能在两个保存的数据集、四个保留的数据集以及两个In Context

    arXiv:2404.02022v1 Announce Type: new  Abstract: In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Contex
    
[^2]: 开发安全和负责任的大型语言模型 - 一个全面框架

    Developing Safe and Responsible Large Language Models -- A Comprehensive Framework

    [https://arxiv.org/abs/2404.01399](https://arxiv.org/abs/2404.01399)

    该论文介绍了一种新的模型SR$_{\text{LLM}}$，旨在通过引入全面的安全风险分类法和专家标注数据集来增强大型语言模型（LLM）在语言生成中的安全性，并通过指令和参数高效微调方法有效减少了不安全内容的生成。

    

    鉴于人们对大型语言模型（LLM）的安全性和风险日益关注，发展减轻这些问题的方法至关重要。我们引入了安全和负责任的大型语言模型（SR$_{\text{LLM}}$），这个模型旨在通过使用LLM来增强语言生成的安全性。我们的方法结合了一个全面的LLM安全风险分类法，并利用专家注释的数据集与这种分类法相一致。SR$_{\text{LLM}}$旨在识别潜在的不安全内容并产生良性变化。它采用基于指令的和参数高效的微调方法，使得该模型不仅有效地增强安全性，而且资源高效且易于调整。在我们对五个基准数据集和两个专有数据集进行测试后，我们观察到不安全内容生成的显著减少。此外，在实施安全措施后，出现了...

    arXiv:2404.01399v1 Announce Type: new  Abstract: Given the growing concerns around the safety and risks of Large Language Models (LLMs), it is essential to develop methods for mitigating these issues. We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a model designed to enhance the safety of language generation using LLMs. Our approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$ is designed to identify potentially unsafe content and produce benign variations. It employs instruction-based and parameter-efficient fine-tuning methods, making the model not only effective in enhancing safety but also resource-efficient and straightforward to adjust. Through our testing on five benchmark datasets and two proprietary datasets, we observed notable reductions in the generation of unsafe content. Moreover, following the implementation of safety measures, there was a s
    
[^3]: TWIN-GPT: 基于大语言模型的临床试验数字孪生体

    TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model

    [https://arxiv.org/abs/2404.01273](https://arxiv.org/abs/2404.01273)

    提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    

    最近，对虚拟临床试验产生了日益增长的兴趣，这些试验模拟了现实世界情境，有望显著增强患者安全性，加快开发速度，降低成本，并为医疗领域的更广泛科学知识贡献力量。本文提出了一种基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
    
[^4]: 小型语言模型从医学教材中学习增强推理能力

    Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks

    [https://arxiv.org/abs/2404.00376](https://arxiv.org/abs/2404.00376)

    通过从医学教材提取的推理路径和多样化的遵循指令数据集，我们引入了具有70亿参数的Meerkat-7B医学人工智能系统，成功解决了商用大型语言模型在医学任务上隐私和推理能力不足的问题，取得了优于先前7B模型的显著成果。

    

    最近商用大型语言模型（LM）在医学任务中取得了有希望的成果，但其闭源性质引发了重要的隐私和安全问题，阻碍了它们在医学领域的广泛应用。针对这一问题，我们引入了Meerkat-7B，一个包含70亿参数的新型医学人工智能系统。Meerkat-7B使用我们新的合成数据集进行训练，该数据集包含从18本医学教材中获取的高质量思维链推理路径，以及多样的遵循指令数据集。我们的系统在七个医学基准测试中取得了显著的准确性，超过了GPT-3.5 13.1%，同时也优于以往最好的7B模型MediTron-7B和BioMistral-7B分别达到了13.4%和9.8%。

    arXiv:2404.00376v1 Announce Type: new  Abstract: While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notab
    
[^5]: AttentionStore: 在大型语言模型服务中实现多轮对话中的注意力成本效益复用

    AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving

    [https://arxiv.org/abs/2403.19708](https://arxiv.org/abs/2403.19708)

    AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。

    

    通过多轮对话与人类进行交互是大型语言模型（LLMs）的基本特征。然而，由于需要重复计算历史记号的键值（KV）缓存，导致现有用于执行多轮对话的LLM服务引擎效率低下，产生高昂的服务成本。为解决这一问题，本文提出了AttentionStore，一种新的注意力机制，实现了跨多轮对话的KV缓存复用（即 注意力复用），显著降低了重复计算开销。AttentionStore维护了一个层次结构的KV缓存系统，利用成本效益的内存/存储介质为所有请求保存KV缓存。为了减少慢速介质的KV缓存访问开销，AttentionStore采用逐层预加载和异步保存方案，将KV缓存访问与GPU计算重叠。为确保要访问的KV缓存…

    arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
    
[^6]: CONLINE: 复杂代码生成与在线搜索和正确性测试的精炼

    CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing

    [https://arxiv.org/abs/2403.13583](https://arxiv.org/abs/2403.13583)

    CONLINE框架提出了通过在线搜索和正确性测试来增强复杂代码生成的方法，通过实验证明了其显著提高了代码生成质量。

    

    大型语言模型（LLMs）通过将自然语言描述转换为可执行代码，彻底改变了代码生成能力。然而，在真实场景下生成复杂代码仍然具有挑战性，原因在于复杂的结构、微妙的错误、对高级数据类型的理解以及缺少辅助内容。为了解决这些挑战，我们引入了CONLINE框架，通过计划的在线搜索信息检索和自动正确性测试来增强代码生成，进行迭代精炼。CONLINE还串行化了复杂的输入和输出，以改善理解，并生成测试用例，确保框架适用于现实应用。CONLINE通过对DS-1000和ClassEval数据集进行严格实验验证。结果表明，CONLINE显著提高了复杂代码生成的质量，突显了其提升实践应用潜力。

    arXiv:2403.13583v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the pra
    
[^7]: ProTrix: 使用句子背景构建用于规划和推理表格的模型

    ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context

    [https://arxiv.org/abs/2403.02177](https://arxiv.org/abs/2403.02177)

    提出了一个计划-推理框架，用于在表格上的句子背景中回答用户查询，通过对Llama-2-7B进行微调，构建了ProTrix模型，广泛适用于不同表格任务，并表现出与GPT-3.5-turbo相当的性能水平，可生成准确且忠实的解释。

    

    在各个领域中，表格在传达信息方面起着至关重要的作用，是组织和呈现结构化数据的不可或缺工具。我们提出了一个计划-推理框架，用于回答带有句子背景的表格上的不同类型的用户查询。该框架首先规划上下文中的推理路径，然后将每个步骤分配给基于程序或文本的推理，以达到最终答案。我们根据该框架构建了一个指令调整集TrixtInstruct。我们的数据集涵盖了那些需要结合表格和句子信息来获得规划和推理能力的程序无法解决的查询。我们通过对TrixInstruct上的Llama-2-7B进行微调，提出了ProTrix。我们的实验表明，ProTrix对各种表格任务具有普遍性，并且达到了与GPT-3.5-turbo相当的性能。我们进一步证明ProTrix可以生成准确和忠实的解释来回答复杂的问题。

    arXiv:2403.02177v1 Announce Type: new  Abstract: Tables play a crucial role in conveying information in various domains, serving as indispensable tools for organizing and presenting data in a structured manner. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. We construct an instruction tuning set TrixInstruct following the framework. Our dataset cover queries that are program-unsolvable or need combining information from tables and sentences to obtain planning and reasoning abilities. We present ProTrix by finetuning Llama-2-7B on TrixInstruct. Our experiments show that ProTrix generalizes to diverse tabular tasks and achieves comparable performance to GPT-3.5-turbo. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex f
    
[^8]: LangGPT：重新思考面向LLMs的结构化可重复使用提示设计框架从编程语言出发

    LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language

    [https://arxiv.org/abs/2402.16929](https://arxiv.org/abs/2402.16929)

    LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。

    

    LLMs已经展示出在不同领域取得了令人瞩目的性能。然而，为了有效指导LLMs制定高质量的提示对于非AI专家来说是一个挑战。现有的提示工程研究建议了一些略显零碎的优化原则和设计，以及凭经验依赖的提示优化器。不幸的是，这些努力缺乏一个结构化的设计模板，导致学习成本高，重复使用性低。受结构化可重复使用的编程语言的启发，我们提出了LangGPT，作为LLMs的编程语言的双层提示设计框架。LangGPT具有易于学习的规范结构，并提供了一个扩展结构以进行迁移和重用。实验证明，与基准相比，LangGPT显著增强了LLMs产生高质量响应的能力。此外，LangGPT已被证明在引导LLMs生成高质量提示方面是有效的。

    arXiv:2402.16929v1 Announce Type: cross  Abstract: LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality promp
    
[^9]: 如果在一个众包数据标注管道中，GPT-4

    If in a Crowdsourced Data Annotation Pipeline, a GPT-4

    [https://arxiv.org/abs/2402.16795](https://arxiv.org/abs/2402.16795)

    本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。

    

    最近的研究表明GPT-4在数据标注准确性方面优于在线众包工作者，尤其是来自亚马逊机械土耳其（MTurk）的工作者。然而，这些研究因偏离标准众包实践并强调个别工作者的表现而受到批评，而不是整个数据标注过程。本文比较了GPT-4和一个道德且执行良好的MTurk管道，使用415名工作者标注了来自200篇学术文章的3,177个句段，使用了CODA-19方案。两个工作者界面产生了127,080个标签，然后通过八种标签聚合算法推断出最终的标签。我们的评估结果显示，尽管采用了最佳实践，MTurk管道的最高准确率为81.5%，而GPT-4达到了83.6%。有趣的是，当将GPT-4的标签与通过先进工作者界面收集的众包标签结合起来进行聚合时，8种算法中有2种实现了更高的准确率。

    arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
    
[^10]: GraphWiz：用于图问题的指令跟随语言模型

    GraphWiz: An Instruction-Following Language Model for Graph Problems

    [https://arxiv.org/abs/2402.16029](https://arxiv.org/abs/2402.16029)

    GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。

    

    大型语言模型（LLMs）在多个领域取得了令人印象深刻的成功，但它们在理解和解决复杂图问题方面的能力尚未得到充分探索。为弥合这一差距，我们引入了GraphInstruct，这是一个新颖而全面的指令调优数据集，旨在为语言模型提供处理各种图问题的能力，利用明确的推理路径。利用GraphInstruct，我们构建了GraphWiz，这是一个能够解决各种图问题类型并生成清晰推理过程的开源语言模型。为增强模型的能力和可靠性，我们将直接偏好优化（DPO）框架纳入图问题求解环境中。增强模型GraphWiz-DPO在九个具有不同复杂性水平的任务中取得了65%的平均准确率，超过了平均准确率为43.8%的GPT-4。此外，我们的研究深入探讨了...

    arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
    
[^11]: 大型语言模型是理性投资者吗？

    Are Large Language Models Rational Investors?

    [https://arxiv.org/abs/2402.12713](https://arxiv.org/abs/2402.12713)

    本研究引入了金融偏见指标（FBI）框架来评估大型语言模型（LLMs）的金融合理性，着重检验它们对金融信息的辨别和市场分析中可能存在的非理性偏见。

    

    大型语言模型（LLMs）正逐渐被引入金融分析领域，利用其丰富的知识库来解释复杂的市场数据和趋势。然而，它们在金融领域的应用受到固有偏见（即风险偏好偏见）和对市场复杂性的肤浅理解的挑战，强调了对它们的金融洞察力进行全面评估的必要性。本研究引入了一个新颖的框架，金融偏见指标（FBI），以对LLMs的金融合理性进行批判性评估，重点关注它们辨别和导航金融信息的微妙之处的能力，并识别可能扭曲市场分析的任何非理性偏见。

    arXiv:2402.12713v1 Announce Type: new  Abstract: Large Language Models (LLMs) are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends. However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight. This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of LLMs, focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis.   Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of LLMs. We conduct a comprehensive evaluation of 19 leading LLMs,
    
[^12]: 压缩以引人注目：释放压缩记忆在现实世界长期对话中的潜力

    Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations

    [https://arxiv.org/abs/2402.11975](https://arxiv.org/abs/2402.11975)

    提出了COmpressive Memory-Enhanced Dialogue sYstems（COMEDY）框架，通过“一对多”方法利用单一语言模型管理记忆生成、压缩和响应生成，核心概念是压缩记忆，支持大规模中文指导调优数据集Dolphin，比较评估证明了COMEDY的优越性。

    

    现有的基于检索的方法在维护长期对话方面取得了显著进展。然而，这些方法在记忆数据库管理和准确的记忆检索方面面临挑战，阻碍了它们在动态、真实世界互动中的有效性。该研究引入了一个新颖的框架，称为COmpressive Memory-Enhanced Dialogue sYstems（COMEDY），它摒弃了传统的检索模块和记忆数据库。相反，COMEDY采用了“一对多”方法，利用单一语言模型来管理记忆生成、压缩和响应生成。这一框架的核心概念是压缩记忆，它将会话特定摘要、用户-机器人动态和过去事件整合到简洁的记忆格式中。为了支持COMEDY，我们构建了一个大规模的中文指导调优数据集Dolphin，从真实用户-聊天机器人互动中得出。比较评估表明COMEDY的优越性。

    arXiv:2402.11975v1 Announce Type: new  Abstract: Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a ''One-for-All'' approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority ove
    
[^13]: 将预训练语言模型与物理层通信集成

    Integrating Pre-Trained Language Model with Physical Layer Communications

    [https://arxiv.org/abs/2402.11656](https://arxiv.org/abs/2402.11656)

    提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能

    

    在设备间人工智能通信的新兴领域中，设备直接通过嵌入式基础模型（如语言模型）交换信息，需要强大、高效且通用的通信框架。然而，将这些框架与现有无线系统集成并有效管理噪声和比特误差都面临着重大挑战。在本研究中，我们介绍了一个实用的设备间人工智能通信框架，集成了物理层通信功能，并通过链路级模拟器展示了其性能。我们的框架通过端到端训练结合信道噪声以增强韧性，采用向量量化变分自动编码器（VQ-VAE）实现高效稳健的通信，利用预训练编码-解码Transformer提升通用性能。在各种通信场景的模拟中，我们的框架展现出

    arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
    
[^14]: 自动评估方法在面向指令的LLM中有多可靠？

    How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?

    [https://arxiv.org/abs/2402.10770](https://arxiv.org/abs/2402.10770)

    本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。

    

    面向指令的大型语言模型(LLMs)的研究使用基于文本重叠和LLM判断的自动方法作为人工评估的成本有效替代方案。本文研究了这些方法在广泛的任务范围和跨语言环境中的可靠性。与先前的研究结果相反，我们观察到在任务类型不同的情况下，自动方法与人工评估者之间的相关性存在显著变化。具体而言，广泛使用的ROUGE-L度量在短答案英语任务中与人类判断强相关，但在自由形式生成任务和跨语言转移中不可靠。使用GPT-4作为评估员的有效性取决于在要求评估时包含参考答案，这可能导致在自由形式生成任务中评估过于严格。总的来说，我们发现，尽管自动评估方法可以近似人类判断，但其准确性可能因任务类型和评估设置而异。

    arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
    
[^15]: 恢复生成模型的预微调权重

    Recovering the Pre-Fine-Tuning Weights of Generative Models

    [https://arxiv.org/abs/2402.10208](https://arxiv.org/abs/2402.10208)

    该论文提出了一种恢复生成模型预微调权重的方法，通过少量低秩微调模型可以恢复准确的预微调权重，利用这个新漏洞攻击大规模模型。

    

    在生成建模中，主流模式包括两个步骤：i) 在大规模但不安全的数据集上进行预训练，ii) 通过微调将预训练模型与人类价值观对齐。这种做法被认为是安全的，因为目前没有一种方法可以恢复不安全的预微调模型权重。本文证明了这种假设通常是错误的。具体而言，我们提出了一种称为谱反调的方法，可以使用少量低秩（LoRA）微调模型恢复预微调模型的权重。与先前试图恢复预微调能力的攻击不同，我们的方法旨在恢复精确的预微调权重。我们的方法利用了这个新的对大规模模型的漏洞，例如个性化的稳定扩散和对齐的Mistral模型。

    arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
    
[^16]: TIC：利用LLMs和逻辑中间表示精确进行“文本到计划”的翻译-推断-编译

    TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations

    [https://arxiv.org/abs/2402.06608](https://arxiv.org/abs/2402.06608)

    该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。

    

    我们研究了为给定的自然语言计划任务请求生成计划的问题。一方面，LLMs在自然语言处理方面表现出色，但在计划方面表现不佳。另一方面，经典计划工具在计划任务方面表现出色，但需要使用结构化语言（如Planning Domain Definition Language（PDDL））作为输入。我们利用这两种技术的优点，通过使用LLMs生成计划任务请求的PDDL表示（任务PDDL），然后使用经典规划器计算计划。与直接使用LLMs生成任务PDDL的先前方法不同，我们的方法包括（a）翻译：仅使用LLMs生成自然语言任务描述的逻辑可解释的中间表示，（b）推断：使用逻辑推理器（目前是Answer Set Programming solver）从中间表示中推导出额外的逻辑相关信息，以及（c）编译：生成目标计划的PDDL描述的编译。

    We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
    
[^17]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^18]: CodeIt：具有优先级回顾重放的自我改进语言模型

    CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

    [https://arxiv.org/abs/2402.04858](https://arxiv.org/abs/2402.04858)

    CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。

    

    大型语言模型越来越能够解决通常被认为需要人类水平推理能力的任务。然而，这些模型在通用智能基准测试例如抽象和推理语料库（ARC）上表现仍然非常差。在本文中，我们将ARC视为一个以编程示例为基础的问题，并引入了一种名为Code Iteration（CodeIt）的新颖且可扩展的语言模型自我改进方法。我们的方法在1）程序抽样和回顾重标记以及2）基于优先级的经验回放之间进行迭代。通过将一个episode的目标（即给定输入的目标程序输出）重标记为采样程序产生的实际输出，我们的方法有效地处理了程序合成中奖励极度稀疏性的问题。应用CodeIt于ARC数据集，我们证明了优先级回顾重放、预训练和数据增强可以实现成功的跨任务泛化。CodeIt是第一个神经元-合成机制一体的自我改进语言模型方法。

    Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
    
[^19]: 不要幻觉，持观：通过多LLM协作识别LLM知识盲区

    Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration

    [https://arxiv.org/abs/2402.00367](https://arxiv.org/abs/2402.00367)

    本论文研究了识别大型语言模型（LLM）知识盲区的方法，并提出了两种基于LLM协作的新方法，通过这些方法可以在面对知识盲区时放弃回答问题。实验证明，这些方法在提高放弃准确度方面取得了高达19.3％的改进。

    

    尽管存在扩展大型语言模型（LLM）知识的努力，但由于知识的不断演化，LLM知识盲区——LLM中缺失或过时的信息可能会一直存在。在这项工作中，我们研究了识别LLM知识盲区和在存在知识盲区时放弃回答问题的方法。我们首先通过模型校准或适应的现有方法进行改进，并分析它们在避免生成低置信度输出方面的能力。受到它们在自我反思和过度依赖保留集方面的失败的启发，我们提出了两种基于模型协作的新方法，即LLM探测其他LLM的知识盲区，无论是合作还是竞争。通过在四个包含多样知识领域的问答任务上对三个LLM进行广泛实验，我们证明揭示LLM知识盲区的合作和竞争方法在放弃准确度方面取得了高达19.3％的提高。

    Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the s
    
[^20]: 为了更好的多语言推理能力而进行的问题翻译训练

    Question Translation Training for Better Multilingual Reasoning

    [https://arxiv.org/abs/2401.07817](https://arxiv.org/abs/2401.07817)

    本文探讨了通过问题对齐训练模型将推理问题翻译成英语的方法，以实现有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。

    

    大型语言模型在推理任务上表现出色，但在非英语语言上的表现往往较差。传统解决方案是将指导数据翻译成所有感兴趣的语言，然后在生成的多语言数据上进行训练，这被称为翻译训练。本文探讨了问题对齐的好处，通过在X-英语平行问题数据上微调，训练模型将推理问题翻译成英语。这种方法通过有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。实验结果表明在LLaMA2-13上

    arXiv:2401.07817v2 Announce Type: replace  Abstract: Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13
    
[^21]: 通用NER:一个金标准的多语言命名实体识别基准

    Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark

    [https://arxiv.org/abs/2311.09122](https://arxiv.org/abs/2311.09122)

    UNER是一个开放的、社区驱动的项目，旨在提供高质量、跨语言一致的命名实体识别基准，以促进和标准化多语言NER研究。

    

    我们介绍了通用NER（UNER），这是一个开放的，社区驱动的项目，旨在开发多种语言的金标准NER基准。UNER的总体目标是提供高质量、跨语言一致的标注，以促进和标准化多语言NER研究。UNER v1包含了在12种不同语言中使用跨语言一致模式标注的18个数据集。在本文中，我们详细介绍了UNER的数据集创建和组成；我们还提供了针对不同语言和跨语言学习设置的初始建模基线。我们向公众发布了数据、代码和拟合模型。

    arXiv:2311.09122v2 Announce Type: replace  Abstract: We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
    
[^22]: 探索大型语言模型在计算辩论中的潜力

    Exploring the Potential of Large Language Models in Computational Argumentation

    [https://arxiv.org/abs/2311.09022](https://arxiv.org/abs/2311.09022)

    该研究旨在评估大型语言模型在计算辩论领域的性能，包括零样本和少样本设置，标准化了14个开源数据集，并介绍了一个新的反言生成基准数据集。

    

    计算辩论已成为包括人工智能、法律和公共政策在内的各个领域中不可或缺的工具。作为自然语言处理中新兴的研究领域，计算辩论吸引了越来越多的关注。研究计算辩论主要涉及两类任务：辩论挖掘和辩论生成。鉴于大型语言模型在理解上下文和生成自然语言方面表现出色，评估LLMs在各种计算辩论任务中的性能是值得的。本工作旨在评估LLMs（例如ChatGPT、Flan和LLaMA2模型）在计算辩论领域的零样本和少样本设置下的表现。我们将现有任务分为六个主要类别，并对十四个开源数据集的格式进行了标准化。此外，我们提供了一个关于反言生成的新基准数据集。

    arXiv:2311.09022v2 Announce Type: replace  Abstract: Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into six main categories and standardise the format of fourteen open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, 
    
[^23]: 良好的开端是成功的一半：多步骤数学推理中开始正确的重要性

    Well begun is half done: Importance of Starting Right in Multi-Step Math Reasoning

    [https://arxiv.org/abs/2311.07945](https://arxiv.org/abs/2311.07945)

    较小的语言模型在多步骤数学推理中通过正确开始可以获得显着的性能提升，建议通过初始指导和自问指导的方式来引导模型开始正确。

    

    较小的语言模型通过学习为其预测生成原因，可以更好地解决复杂的推理任务。然而，我们观察到这些较小的模型有时会在开始时遇到困难，但在得到纠正后，可以解决原本困难的任务。我们提出了两种较小模型可以从初始指导中受益的方式：1）向LLM寻求初始指导，和2）自问指导，学生模型可以首先发起一个关于如何开始的问题，然后继续这一连锁。我们将初始基于问题的指导扩展到了一种称为QuestCoT的提示技术，该技术在进行推理链之前以一个问题开始是有益的。在两个多步数学推理数据集GSM8K和SVAMP上，我们展示了正确开始可以带来显著的性能提升（通过LLM指导最高高达+14分，通过QuestCoT最高高达+6分）。

    arXiv:2311.07945v2 Announce Type: replace  Abstract: Smaller language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. However, we observe that these smaller models can sometimes struggle to start correctly, but when corrected, can solve a task that they would otherwise have struggled with. We propose two ways in which a smaller model can benefit from initial guidance: 1) asking an LLM for initial guidance, and 2) self-questioning guidance, where the student model can first initiate a question regarding how to start and then continue that chain. We extend initial question-based guidance to a prompting technique called QuestCoT, where starting with a question before a chain of reasoning proves useful. On two multi-step math reasoning datasets GSM8K and SVAMP, we show that starting correctly can lead to a significant performance gain (up to $+14$ points with LLM guidance and $+6$ points with QuestCoT).
    
[^24]: 语言模型写作是否会降低内容多样性？

    Does Writing with Language Models Reduce Content Diversity?

    [https://arxiv.org/abs/2309.05196](https://arxiv.org/abs/2309.05196)

    写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。

    

    大型语言模型（LLMs）引发了与模型辅助合作写作的激增。当不同用户纳入同一模型的建议时，会存在内容多样性减少的风险，可能限制公共话语中的多元观点。本研究通过控制实验测量了协同写作对多样性的影响，在该实验中，用户以三种设置撰写议论性文章--使用基本LLM（GPT3）、经过反馈调整的LLM（InstructGPT）以及不使用模型帮助写作。我们开发了一组多样性指标，并发现使用InstructGPT进行写作（而不是GPT3）会导致多样性明显降低。具体而言，它增加了不同作者的写作之间的相似性，减少了整体的词汇和内容多样性。此外，我们还发现这种影响主要来源于InstructGPT对共同撰写的文本贡献较少。

    arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
    
[^25]: CFMatch: 将自动答案等价评估与人工专家判断在开放域问答中对齐

    CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])

    [http://arxiv.org/abs/2401.13170](http://arxiv.org/abs/2401.13170)

    CFMatch提出了一个在开放域问答中将自动答案等价评估与人工专家判断对齐的方法，通过提供明确一致的评估指南并引入高效、稳健且轻量级的判别式AE分类器匹配方法来解决当前评估指标与人类判断不一致的问题。

    

    问答系统只有在我们知道答案是否正确的情况下才能取得进展，但对于许多最具挑战和有趣的问答示例，当前用于确定答案等价性的评估指标通常与人类判断不一致，尤其是来自大型语言模型（LLM）的更冗长、自由形式的答案。存在两个挑战：缺乏数据和模型过大：基于LLM的评分器可以更好地与人工评判员相关联，但这个任务只在有限的问答数据集上进行了测试，即使可用，对模型的更新也有限，因为LLM过大且往往昂贵。我们通过提供明确一致的指南来解决这两个问题，这些指南用于从专业人工问答比赛中采纳机器问答在答案等价性评估方面的标准。我们还引入了一种标准评估和一种更高效、稳健且轻量级的判别式AE分类器匹配方法（CFMatch，大小小于1MB），经过训练和验证以更准确地评估答案等价性。

    Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu
    
[^26]: 天作之合：大型语言模型与进化算法的结合

    A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])

    [http://arxiv.org/abs/2401.10510](http://arxiv.org/abs/2401.10510)

    大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。

    

    预训练的大型语言模型（LLMs）在生成创造性的自然文本方面具有强大的能力。进化算法（EAs）可以发现复杂实际问题的多样解决方案。本文通过比较文本序列生成和进化的共同特点和方向性，阐述了LLMs与EAs之间的强大一致性，包括多个一对一的核心特征：标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化。在这种一致性视角下，分析了现有的耦合研究，包括进化微调和LLM增强型EAs。借助这些洞见，我们概述了未来在LLMs和EAs耦合方面的基本研究路线，并突出了其中的关键挑战。

    Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
    
[^27]: 大语言模型的零样本位置去偏方法

    Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])

    [http://arxiv.org/abs/2401.01218](http://arxiv.org/abs/2401.01218)

    本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。

    

    微调已被证明是改善大语言模型（LLMs）领域性能的有效方法。然而，LLMs可能适应数据集偏见和预测的捷径，导致生成性能差。实验结果显示，LLMs容易表现出位置偏差，即利用位于开头或末尾或输入中特定位置线索的信息。现有的减轻位置偏差的工作需要外部偏差知识或带注释的非偏倚样本，在实际中不太实用。在这项工作中，我们提出了一种零样本位置去偏（ZOE）框架对LLMs进行位置去偏。ZOE利用预训练的LLMs的无监督响应进行去偏，因此不需要任何外部知识或数据集。为了提高无监督响应的质量，我们提出了一种主从对齐（MSA）模块来修剪这些响应。对八个数据集和五个任务的实验表明，ZOE始终优于其他方法。

    Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
    
[^28]: LQ-LoRA: 低秩加量化矩阵分解用于有效的语言模型微调

    LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.12023](http://arxiv.org/abs/2311.12023)

    LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。

    

    我们提出了一种简单的方法，用于对预训练语言模型进行内存高效的自适应。我们的方法使用迭代算法将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分。在微调过程中，量化部分保持固定，只有低秩部分被更新。我们提出了量化部分的整数线性规划表达，可以根据总体内存预算动态配置量化参数（例如比特宽度、块大小）给定每个矩阵。我们进一步探索了数据感知版本的算法，该算法使用Fisher信息矩阵的近似来加权矩阵分解过程中的重构目标。在RoBERTa和LLaMA-2（7B和70B）的微调实验中，我们的低秩加量化矩阵分解方法（LQ-LoRA）优于强基线方法QLoRA和GPTQ-LoRA，并实现了激进的量化。

    We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
    
[^29]: $R^3$-NL2GQL:一种用于提高准确性和减轻幻觉的混合模型方法

    $R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])

    [http://arxiv.org/abs/2311.01862](http://arxiv.org/abs/2311.01862)

    $R^3$-NL2GQL是一种通过利用较小和较大的Foundation Models进行重新排名、重写和细化的方法，以提高准确性和减轻幻觉，解决了NL2GQL任务中GQL生成能力和跨模式通用能力的挑战。

    

    当前使用Foundation Models构建的NL2SQL任务取得了令人称赞的结果，然而直接将其应用于自然语言到图查询语言（NL2GQL）任务面临挑战，原因是GQL和SQL表达式之间存在显著差异，且GQL存在多种类型。我们的实验表明，在NL2GQL任务中，更大的Foundation Models展示了优越的跨模式通用能力，而较小的Foundation Models则通过微调难以提高其GQL生成能力。然而，在微调后，较小的模型表现出更好的意图理解和更高的语法准确性。与基于规则和槽填充技术不同，我们引入了R3-NL2GQL，该方法将较小和较大的Foundation Models用作重新排名、重写和细化器。该方法利用较小模型的理解能力进行信息的重新排名和重写，并利用卓越的通用化和生成能力进行细化。

    While current NL2SQL tasks constructed using Foundation Models have achieved commendable results, their direct application to Natural Language to Graph Query Language (NL2GQL) tasks poses challenges due to the significant differences between GQL and SQL expressions, as well as the numerous types of GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation Models demonstrate superior cross-schema generalization abilities, while smaller Foundation Models struggle to improve their GQL generation capabilities through fine-tuning. However, after fine-tuning, smaller models exhibit better intent comprehension and higher grammatical accuracy. Diverging from rule-based and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller and larger Foundation Models as reranker, rewriter and refiner. The approach harnesses the comprehension ability of smaller models for information reranker and rewriter, and the exceptional generalization and generation capabiliti
    
[^30]: LLM能保守秘密吗？通过上下文完整性理论测试语言模型的隐私影响

    Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])

    [http://arxiv.org/abs/2310.17884](http://arxiv.org/abs/2310.17884)

    本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。

    

    在AI助手（工作、家庭等）中交互使用大型语言模型（LLMs）引入了一系列新的推理时隐私风险：LLMs从多个来源的输入中获取不同类型的信息，并期望在给定的上下文中推理出在何种目的和与谁分享的内容。在这项工作中，我们通过提出ConfAIde，一个旨在识别指令调整的LLMs隐私推理能力中重要弱点的基准，来引起人们对上下文隐私这一极其关键但经常被忽视的概念的关注。我们的实验表明，即使是GPT-4和ChatGPT等最强大的模型，在人类不会的上下文中，也会泄露39％和57％的私人信息。即使我们使用保护隐私的提示或思维链推理，这种泄漏也会持续存在。我们的工作强调了迫切需要探索基于推理和理论的新型推理时隐私保护方法。

    The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
    
[^31]: 减少、复用、回收：与其他语言增强相比，被扰动数据对低资源自我监督语音模型更好吗？

    Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models. (arXiv:2309.12763v1 [eess.AS])

    [http://arxiv.org/abs/2309.12763](http://arxiv.org/abs/2309.12763)

    使用音频增强为低资源自我监督语音模型的预训练提出一种有效的方法，并且综合增强（噪声/音高）是最佳的增强策略，超过了重音和语言知识转移。

    

    自我监督表示学习（SSRL）已经改善了下游音素识别的性能，相对于受监督的模型。训练SSRL模型需要大量的预训练数据，这对于低资源语言是一个挑战。一种常用的方法是从其他语言中转移知识。相反，我们提出使用音频增强在低资源条件下预训练SSRL模型，并评估下游任务的音素识别。我们对增强技术进行了系统比较，包括音高变化、噪声添加、有重音的目标语音和其他语言的语音。我们发现综合增强（噪声/音高）是最好的增强策略，超过了重音和语言知识转移。我们比较了不同数量和类型的预训练数据的性能。我们考察了增强数据的缩放因子，以达到与预训练目标域语音模型相当的性能。我们的发现是...

    Self-supervised representation learning (SSRL) has improved the performance on downstream phoneme recognition versus supervised models. Training SSRL models requires a large amount of pre-training data and this poses a challenge for low resource languages. A common approach is transferring knowledge from other languages. Instead, we propose to use audio augmentation to pre-train SSRL models in a low resource condition and evaluate phoneme recognition as downstream task. We performed a systematic comparison of augmentation techniques, namely: pitch variation, noise addition, accented target-language speech and other language speech. We found combined augmentations (noise/pitch) was the best augmentation strategy outperforming accent and language knowledge transfer. We compared the performance with various quantities and types of pre-training data. We examined the scaling factor of augmented data to achieve equivalent performance to models pre-trained with target domain speech. Our findi
    
[^32]: 重新思考机器伦理 - LLM能否通过道德理论进行道德推理？

    Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?. (arXiv:2308.15399v1 [cs.CL])

    [http://arxiv.org/abs/2308.15399](http://arxiv.org/abs/2308.15399)

    本研究提出了一个灵活的框架，引导大型语言模型根据跨学科研究中建立的道德理论进行道德推理，解决了现有方法面临的问题。

    

    进行道德判断是发展伦理人工智能系统的重要一步。目前的方法大多数以自下而上的方式实施，通过使用大量的注释数据来训练基于众包意见的模型，来判断道德问题。这些方法因潜在过度普遍化有限的注释者道德立场并且缺乏可解释性而受到批评。相反，自上而下的方法是基于一套原则进行道德判断。然而，这个方法在概念上存在问题，因为之前的语言模型无法胜任，且道德原则之间存在未解决的辩论。在本研究中，我们提出了一个灵活的框架，可以引导大型语言模型（LLMs）根据跨学科研究中建立的道德理论进行道德推理。这个自上而下的理论引导框架可以融入各种道德理论。我们的实验验证了这个提出的框架在基于道德理论的数据集上的有效性。

    Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. In contrast, top-down approaches make moral judgments grounded in a set of principles. However, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. In this study, we propose a flexible framework to steer Large Language Models (LLMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore,
    
[^33]: KoLA: 认真基准大型语言模型的世界知识

    KoLA: Carefully Benchmarking World Knowledge of Large Language Models. (arXiv:2306.09296v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.09296](http://arxiv.org/abs/2306.09296)

    本研究提出了一个针对大型语言模型的知识导向评估基准 (KoLA)，通过模仿人类认知构建了四级知识相关能力的分类体系，并使用维基百科和新兴语料库进行评估。这个基准旨在全面、公正和实用地评估LLM的能力，以处理未见数据和不断发展的知识。

    

    大型语言模型 (LLM) 的前所未有的性能需要改进评估。我们认为，除了探索LLM能力的广度之外，细致和深思熟虑的设计对于全面、公正和实用的评估是必要的。鉴于全球知识对LLM的重要性，我们构建了一个以知识为导向的LLM评估基准(KoLA)，其中我们精心设计了三个关键因素：(1) 对于能力建模，我们模仿人类认知构建了一个四级知识相关能力的分类体系，涵盖了19个任务。(2) 对于数据，为了确保公正比较，我们使用了维基百科作为LLM普遍预训练的语料库，同时还使用了持续收集的新兴语料库，旨在评估处理未见数据和不断发展的知识的能力。(3) 对于评估标准，我们采用了对比系统，包括整体标准分数，以实现在任务和模型之间更好的数值比较性，以及独特的自对照指标。

    The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric f
    
[^34]: 通过交互式评估揭示任务导向对话中的用户熟悉度偏见

    Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation. (arXiv:2305.13857v1 [cs.CL])

    [http://arxiv.org/abs/2305.13857](http://arxiv.org/abs/2305.13857)

    本研究发现任务导向对话系统中存在用户熟悉度偏见，而真实世界的应用场景很少符合封闭目标的设定。因此，在开放目标设置下，系统会出现严重问题，同时研究者发现了“不匹配错误”这一新型错误类型。

    

    大多数任务导向对话(TOD)基准假定用户精确地知道如何使用系统，通过将用户行为限制在系统的能力范围内，即“用户熟悉度”偏见。当这种数据偏见与数据驱动的TOD系统相结合时，该偏见加深了，因为使用现有的静态评估无法理解其影响。因此，我们进行了一项交互式用户研究，揭示TOD系统在真实场景下的脆弱性。具体而言，我们比较了具有1）符合系统边界的详细目标说明（封闭目标）和2）通常不受支持但现实（开放目标）的模糊目标说明的用户。我们的研究发现，在开放目标设置下的对话会导致系统严重失败，92%的对话存在显著的问题。此外，我们进行了彻底的分析，通过错误注释识别两种设置之间的显著特征。从中我们发现了一种新的错误类型称为“不匹配错误”，这表明用户和系统无法建立共享的语境理解。本研究强调了在TOD评估中考虑用户熟悉度偏见的重要性，并开发更强大的系统来处理实际情况。

    Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system's capabilities via strict user goals, namely "user familiarity" bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a no
    
[^35]: 房间里的大象：分析大型科技公司在自然语言处理研究中的存在

    The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v1 [cs.CL])

    [http://arxiv.org/abs/2305.02797](http://arxiv.org/abs/2305.02797)

    本文研究了工业界在自然语言处理研究中的存在和影响。研究发现在过去五年中，工业界的存在与影响呈现急剧增长，一些公司占据了大部分出版物，并向学术研究人员提供资金支持。

    

    自然语言处理的深度学习方法的最新进展，创造了新的商业机会，并且使得NLP研究对产业发展至关重要。作为NLP领域的大玩家之一，连同政府和大学一起，跟踪产业对研究的影响非常重要。在本研究中，我们致力于量化和表征工业界在NLP社区中的存在。使用具有78,187篇NLP出版物和701个NLP作者简历的全面元数据语料库，我们探索了自上世纪90年代以来该领域中的工业存在。我们发现，NLP作者中的工业存在在过去五年中急剧增长（从2017年到2022年的增长率为180％）。一些公司占据了大部分出版物，并通过拨款和实习为学术研究人员提供资金支持。我们的研究表明，工业界对自然语言处理研究的存在和影响是显著的。

    Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are signi
    
[^36]: 视觉语言模型补丁-令牌对齐的贝叶斯提示学习

    Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])

    [http://arxiv.org/abs/2303.09100](http://arxiv.org/abs/2303.09100)

    本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。

    

    在视觉语言预训练模型的下游应用中，构建有效提示引起了极大关注。现有的提示工程方法要么需要费时费力的手动设计，要么将提示调优作为点估计问题进行优化，这可能无法描述类别的多样特征并限制了它们的应用。本文提出了一种基于贝叶斯概率的提示学习方法，其中通过从潜在分布中首先采样隐向量，然后采用轻量级生成模型来生成标签特定的随机提示。重要的是，我们将视觉知识与图像的语义规则化，并将图像和相应的提示视为补丁和令牌集，通过最优传输将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别。此外，所提出的模型还可以通过使用额外的基于文本的信息来生成更具信息量和准确性的提示。在各种视觉语言任务上的广泛实验表明，我们的补丁-令牌对齐的贝叶斯提示学习（PTBPL）优于现有的最先进模型。

    For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
    
[^37]: 用蜜蜂算法优化深度学习模型参数，提高医学文本分类准确性

    Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])

    [http://arxiv.org/abs/2303.08021](http://arxiv.org/abs/2303.08021)

    本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。

    

    本文介绍了一种使用蜜蜂算法对深度学习模型进行参数优化的新机制，这是一种最近很有前途的群智能算法。优化问题是在给定初始超参数的情况下，通过确定的迭代次数来最大化基于医学文本分类疾病的准确性。实验包括两个不同的数据集：英语和阿拉伯语。使用长短期记忆 (LSTM) 和蜜蜂算法，在英语数据集上获得了99.63%的最高准确率，在阿拉伯语数据集上使用AraBERT获得了88%的最高准确率。

    This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
    
[^38]: 基于分类重新参数化技巧的回译端到端训练

    End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08465](http://arxiv.org/abs/2202.08465)

    本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。

    

    回译是一种在神经机器翻译中有效的半监督学习框架。预先训练的神经机器翻译模型翻译单语句子并生成合成的双语句对以训练另一个神经机器翻译模型，反之亦然。将两个神经机器翻译模型分别理解为推理和生成模型。以往的研究采用了变分自动编码器（VAE）的培训框架。但是，由于翻译句子的离散属性使得梯度信息无法在两个NMT模型之间流动。本文提出了一种分类重新参数化技巧，使得神经机器翻译模型能够生成可微分的句子，使得VAE的训练框架可以以端到端方式工作。我们的实验表明，我们的方法有效地训练了NMT模型，并在WMT翻译任务的数据集上取得比以前基准测试更好的BLEU分数。

    Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
    

