# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Scaling Laws For Dense Retrieval](https://arxiv.org/abs/2403.18684) | 该研究探究了密集检索模型的性能是否遵循其他神经模型的缩放规律，并提出使用对比对数似然作为评估指标进行了广泛实验。 |
| [^2] | [Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms](https://arxiv.org/abs/2403.17806) | 提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实 |
| [^3] | [Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498) | 棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态 |
| [^4] | [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042) | LLM2LLM 提出了一种迭代数据增强策略，通过使用教师LLM生成合成数据并将其添加回训练数据，从而帮助低数据环境下的LLM进行微调。 |
| [^5] | [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299) | 本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。 |
| [^6] | [Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns](https://arxiv.org/abs/2403.10707) | 本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。 |
| [^7] | [SMART: Submodular Data Mixture Strategy for Instruction Tuning](https://arxiv.org/abs/2403.08370) | SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。 |
| [^8] | [Android in the Zoo: Chain-of-Action-Thought for GUI Agents](https://arxiv.org/abs/2403.02713) | 该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。 |
| [^9] | [Using LLMs for the Extraction and Normalization of Product Attribute Values](https://arxiv.org/abs/2403.02130) | 本文探讨了使用大型语言模型（LLMs）如GPT-3.5和GPT-4从产品标题和描述中提取和规范化属性值的潜力，引入了新的WDC PAVE数据集来支持实验。 |
| [^10] | [Large Language Models and Games: A Survey and Roadmap](https://arxiv.org/abs/2402.18659) | 这项研究调查了大型语言模型在游戏领域中的多种应用及其角色，指出了未开发领域和未来发展方向，同时探讨了在游戏领域中大型语言模型的潜力和限制。 |
| [^11] | [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition](https://arxiv.org/abs/2402.15220) | ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。 |
| [^12] | [Triple-Encoders: Representations That Fire Together, Wire Together](https://arxiv.org/abs/2402.12332) | 通过三重编码器计算话语混合，实现了对话模型的显着改进和零-shot泛化性能 |
| [^13] | [Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization](https://arxiv.org/abs/2402.10342) | 本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见 |
| [^14] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^15] | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | 本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。 |
| [^16] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^17] | [A Roadmap to Pluralistic Alignment](https://arxiv.org/abs/2402.05070) | 这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。 |
| [^18] | [Pedagogical Alignment of Large Language Models](https://arxiv.org/abs/2402.05000) | 本文介绍了教学对齐的大型语言模型（LLM）的新概念，并探讨了通过建设性反馈和提示指导学生解决复杂问题的方法。相比于传统方法，这种对齐方法以及采用人类反馈的强化学习方法能够更好地对齐LLM，提供更优质的教育支持。 |
| [^19] | [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530) | 通过将原始语料库转化为阅读理解文本来调整大型语言模型，使其在多个领域的各种任务中性能始终得到提升。 |
| [^20] | [Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770) | 该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。 |
| [^21] | [CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models.](http://arxiv.org/abs/2401.17043) | 这篇论文构建了一个大规模且更全面的中文基准测试，评估了检索增强生成系统的所有组件在各种应用场景中的性能。 |
| [^22] | [Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge.](http://arxiv.org/abs/2401.10712) | 本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。 |
| [^23] | [Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation.](http://arxiv.org/abs/2401.06310) | 本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。 |
| [^24] | [CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning.](http://arxiv.org/abs/2401.05544) | CodePrompt是一种利用Prompt学习和注意机制技术改进源代码相关分类任务的新方法。它能够提取源代码和相关文本中的丰富知识以提高准确性，并且减少了计算成本。 |
| [^25] | [DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning.](http://arxiv.org/abs/2310.12128) | DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。 |
| [^26] | [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning.](http://arxiv.org/abs/2309.15091) | 本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。 |
| [^27] | [BatchPrompt: Accomplish more with less.](http://arxiv.org/abs/2309.00384) | BatchPrompt是一种提示策略，它通过将多个数据点批量打包到一个提示中来提高LLM的令牌资源利用效率，从而缓解由于令牌计数差异导致的成本效率问题，提高推理速度和计算预算的利用率。 |
| [^28] | [Divide & Bind Your Attention for Improved Generative Semantic Nursing.](http://arxiv.org/abs/2307.10864) | 本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。 |
| [^29] | [pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks.](http://arxiv.org/abs/2106.09462) | pysentimiento是一个多语言的Python工具包，用于观点挖掘和社交自然语言处理任务，提供了易于使用的库和最先进的模型，研究人员可以利用这些技术进行研究。 |

# 详细

[^1]: 密集检索的扩展规律

    Scaling Laws For Dense Retrieval

    [https://arxiv.org/abs/2403.18684](https://arxiv.org/abs/2403.18684)

    该研究探究了密集检索模型的性能是否遵循其他神经模型的缩放规律，并提出使用对比对数似然作为评估指标进行了广泛实验。

    

    将神经模型扩展到更大规模已经在多项任务中取得了显著进展，特别是在语言生成方面。先前的研究发现，神经模型的性能常遵循可预测的扩展规律，与训练集大小和模型大小等因素相关。这一洞察力非常宝贵，尤其是随着大规模实验变得越来越耗费资源。然而，由于检索指标的离散性以及检索任务中训练数据和模型大小之间的复杂关系，密集检索中的这种扩展规律尚未得到充分探讨。在本研究中，我们调查了密集检索模型的性能是否遵循其他神经模型的缩放规律。我们建议使用对比对数似然作为评估指标，并对实现了不同参数数量并使用不同数量的数据训练的密集检索模型进行了广泛实验。

    arXiv:2403.18684v1 Announce Type: cross  Abstract: Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amo
    
[^2]: 坚信忠实：在找到模型机制时超越电路重叠

    Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms

    [https://arxiv.org/abs/2403.17806](https://arxiv.org/abs/2403.17806)

    提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实

    

    最近许多语言模型（LM）可解释性研究已采用电路框架，旨在找到解释LM在给定任务上行为的最小计算子图或电路。大多数研究通过独立对每个边执行因果干预来确定哪些边属于LM的电路，但这在模型规模较大时效率低下。边缘归因修补（EAP），一种基于梯度的近似干预方法，已成为解决这一问题的可扩展但不完美的解决方案。在本文中，我们介绍了一种新方法 - 带有集成梯度的EAP（EAP-IG），旨在更好地保持电路的核心属性：忠实。如果电路是忠实的，则可以去掉电路之外的所有模型边而不会改变模型在任务上的表现；忠实性是研究电路而不是完整模型的理由。我们的实验证明，使用EAP找到的电路不太忠实

    arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
    
[^3]: 棋类语言模型中的新颖世界模型和潜变量估计

    Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models

    [https://arxiv.org/abs/2403.15498](https://arxiv.org/abs/2403.15498)

    棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态

    

    语言模型展现了前所未有的能力，引发了关于其性能来源的讨论。是仅仅学习句法模式和表面统计结果，还是从文本中提取语义和世界模型？我们在象棋这个更复杂的领域扩展了之前的工作，通过在真实游戏中训练模型，使用线性探测和对比激活来研究模型的内部表示。尽管模型没有先验的游戏知识，仅仅通过下一个字符预测进行训练，我们发现了关于棋盘状态的内部表示的证据。

    arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
    
[^4]: LLM2LLM: 利用新的迭代数据增强技术增强LLM

    LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement

    [https://arxiv.org/abs/2403.15042](https://arxiv.org/abs/2403.15042)

    LLM2LLM 提出了一种迭代数据增强策略，通过使用教师LLM生成合成数据并将其添加回训练数据，从而帮助低数据环境下的LLM进行微调。

    

    预训练的大型语言模型（LLMs）目前是解决绝大多数自然语言处理任务的最先进技术。尽管许多现实世界应用仍需要微调以达到令人满意的性能水平，但其中许多应用处于低数据范围，使得微调变得具有挑战性。为了解决这个问题，我们提出了LLM2LLM，这是一种有针对性和迭代的数据增强策略，利用一个教师LLM来增强一个小的种子数据集，通过增加额外的数据用于针对特定任务的微调。LLM2LLM（1）在初始种子数据上微调基线学生LLM，（2）评估和提取模型错误的数据点，（3）使用教师LLM根据这些错误的数据点生成合成数据，然后将其添加回训练数据中。这种方法通过在训练过程中增强LLM对错误预测数据点的信号，并重新整合它们。

    arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in
    
[^5]: SQ-LLaVA：自问自答的大型视觉-语言助手

    SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant

    [https://arxiv.org/abs/2403.11299](https://arxiv.org/abs/2403.11299)

    本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。

    

    最近视觉-语言模型的发展在经过视觉指导调整后，在视觉-语言任务中展现出显着的泛化能力。然而，预训练视觉编码器和大型语言模型之间的鸿沟成为整个网络的瓶颈。为了改善跨模态对齐，现有的工作通常考虑涵盖更广泛的视觉任务范围的更多视觉指导数据，对模型进行微调以用于问答，但这种操作成本较高。然而，图像包含大量上下文信息，但这一方面一直鲜有人探索。本文首次尝试利用视觉指导数据内部被忽视的上下文，训练模型自我训练'学习'如何提出高质量问题。通过这种方式，我们引入了一个名为SQ-LLaVA的新颖框架：自问自答的大型视觉-语言助手。SQ-LLaVA在生成灵活且有意义的图像方面表现出高效性。

    arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
    
[^6]: 利用LLMs集成揭示社交媒体消息的潜在主题：气候运动案例研究

    Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns

    [https://arxiv.org/abs/2403.10707](https://arxiv.org/abs/2403.10707)

    本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。

    

    本文介绍了一种揭示和分析社交媒体消息主题的新方法。鉴于传统主题级分析的局限性，往往只捕捉到整体模式，本研究强调了对更精细、主题聚焦的探索的需求。传统的主题发现方法，涉及手动流程和人在循环中的方法，具有价值，但在伸缩性、一致性和资源强度方面面临挑战，涉及时间和成本。为了应对这些挑战，我们提出了一种利用大型语言模型（LLMs）先进功能的机器在循环中方法。这种方法允许更深入地调查社交媒体话语的主题方面，使我们能够揭示多样的主题，每个主题具有独特的特征和相关性，从而提供对更广泛主题内有的微妙细节的全面理解。

    arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
    
[^7]: SMART: 用于指令调整的子模块数据混合策略

    SMART: Submodular Data Mixture Strategy for Instruction Tuning

    [https://arxiv.org/abs/2403.08370](https://arxiv.org/abs/2403.08370)

    SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。

    

    指令调整涉及在一组以指令格式化的数据集上对语言模型进行微调，以增强模型对未见任务的泛化能力。研究表明，在微调过程中平衡不同任务比例的重要性，但找到合适的平衡仍然具有挑战性。目前除了手动调整或依赖从业者的直觉外，尚无系统方法。在本文中，我们介绍了SMART（Submodular data Mixture strAtegy for instRuction Tuning）- 一种利用子模块函数为任务分配重要性分数的新颖数据混合策略，然后用这些分数来确定混合权重。给定微调预算，SMART重新分配任务间的预算，并从每个任务中选择非冗余样本。实验结果表明，SMART显著优于传统方法，如例子比例混合和均等分配。

    arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
    
[^8]: Android在动物园中: GUI代理的动作思维链

    Android in the Zoo: Chain-of-Action-Thought for GUI Agents

    [https://arxiv.org/abs/2403.02713](https://arxiv.org/abs/2403.02713)

    该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。

    

    大型语言模型（LLM）导致智能手机上的大量自主GUI代理激增，这些代理通过预测API的一系列动作来完成由自然语言触发的任务。尽管该任务高度依赖于过去的动作和视觉观察，但现有研究通常很少考虑中间截图和屏幕操作传递的语义信息。为了解决这一问题，本文提出了动作思维链（CoAT），它考虑了先前动作的描述、当前屏幕，更重要的是分析应当执行的动作以及选择的动作带来的结果。我们证明，在使用现成LLM进行零次学习的情况下，CoAT相比于标准上下文建模显著提高了目标的完成情况。为了进一步促进这一研究领域的发展，我们构建了一个名为Android-In-The-Zoo（AitZ）的基准测试集，其中包含18,643个屏幕动作对。

    arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
    
[^9]: 使用LLMs提取和规范化产品属性值

    Using LLMs for the Extraction and Normalization of Product Attribute Values

    [https://arxiv.org/abs/2403.02130](https://arxiv.org/abs/2403.02130)

    本文探讨了使用大型语言模型（LLMs）如GPT-3.5和GPT-4从产品标题和描述中提取和规范化属性值的潜力，引入了新的WDC PAVE数据集来支持实验。

    

    在电子商务网站上的产品提供通常包括文本产品标题和文本产品描述。为了提供诸如分面产品过滤或基于内容的产品推荐等功能，网站需要从非结构化产品描述中提取属性值对。本文探讨了使用大型语言模型（LLMs），如OpenAI的GPT-3.5和GPT-4，从产品标题和产品描述中提取和规范化属性值的潜力。为了进行实验，我们引入了WDC产品属性-值提取（WDC PAVE）数据集。WDC PAVE包含来自提供schema.org注释的87个网站的产品提供。这些提供属于五个不同的类别，每个类别都具有一组特定的属性。该数据集以两种形式提供手动验证的属性-值对：（i）直接提取的值和（ii）规范化的属性值。

    arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
    
[^10]: 大型语言模型与游戏：调研与路线图

    Large Language Models and Games: A Survey and Roadmap

    [https://arxiv.org/abs/2402.18659](https://arxiv.org/abs/2402.18659)

    这项研究调查了大型语言模型在游戏领域中的多种应用及其角色，指出了未开发领域和未来发展方向，同时探讨了在游戏领域中大型语言模型的潜力和限制。

    

    近年来，大型语言模型（LLMs）的研究急剧增加，并伴随着公众对该主题的参与。尽管起初是自然语言处理中的一小部分，LLMs在广泛的应用和领域中展现出显著潜力，包括游戏。本文调查了LLMs在游戏中及为游戏提供支持的各种应用的最新技术水平，并明确了LLMs在游戏中可以扮演的不同角色。重要的是，我们讨论了尚未开发的领域和LLMs在游戏中未来应用的有前途的方向，以及在游戏领域中LLMs的潜力和限制。作为LLMs和游戏交叉领域的第一份综合调查和路线图，我们希望本文能够成为这一激动人心的新领域的开创性研究和创新的基础。

    arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
    
[^11]: ChunkAttention: 具有前缀感知KV缓存和两阶段分区的高效自注意力

    ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition

    [https://arxiv.org/abs/2402.15220](https://arxiv.org/abs/2402.15220)

    ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。

    

    自注意力是大型语言模型（LLMs）的重要组成部分，但对于长序列来说是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。本文介绍了ChunkAttention，一种具有前缀感知的自注意力模块，可以在运行时检测多个请求之间匹配的提示前缀，并共享它们的键/值张量以改进KV缓存的内存利用率。这是通过将整体键/值张量分解为较小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力内核，其中实现了两阶段分区算法，以改善自注意力计算中的数据局部性。

    arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
    
[^12]: 三重编码器：共同激活的表示一起连接

    Triple-Encoders: Representations That Fire Together, Wire Together

    [https://arxiv.org/abs/2402.12332](https://arxiv.org/abs/2402.12332)

    通过三重编码器计算话语混合，实现了对话模型的显着改进和零-shot泛化性能

    

    搜索导向的对话模型通常在每个轮次重新对对话历史进行编码，造成高昂的成本。曲率对比学习是一种最近展示出对话建模远超效率的表示学习方法，通过双编码器将话语之间的相对距离编码到嵌入空间中。高效率是通过独立编码话语实现的，然而这忽略了上下文化的重要性。为了解决这个问题，本研究引入了三重编码器，通过一种新颖的Hebbian启发的共存学习目标，从这些独立编码的话语中高效计算分布式话语混合，而不使用任何权重。实证上，我们发现三重编码器在比编码器上有显著改进，甚至比单向量表示模型实现更好的零-shot泛化，而无需重新编码。

    arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model
    
[^13]: 在RLHF中基于探索驱动的策略优化：关于有效数据利用的理论洞见

    Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization

    [https://arxiv.org/abs/2402.10342](https://arxiv.org/abs/2402.10342)

    本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见

    

    强化学习从人类反馈（RLHF）在依赖少量人类反馈的情况下取得了令人印象深刻的经验成功。然而，对于这种现象存在着有限的理论证明。此外，尽管最近的经验成功采用了基于策略的算法，但大多数最近的研究仍侧重于基于价值的算法。在这项工作中，我们考虑了基于策略优化（PO-RLHF）的RLHF算法。该算法基于流行的策略覆盖-策略梯度（PC-PG）算法，该算法假设对奖励函数有知识。在PO-RLHF中，不假设知道奖励函数，并且该算法依赖于基于轨迹的比较反馈来推断奖励函数。我们为PO-RLHF提供了低查询复杂度的性能界限，这为解释为什么少量的人类反馈可能足以在RLHF中获得良好性能提供了洞见。一个关键的创新是我们的轨迹级el

    arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
    
[^14]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^15]: 思想传播：扩散语言模型中的思维链推理

    Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

    [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)

    本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。

    

    扩散模型在文本处理中引起了关注，相对传统的自回归模型具有许多潜在优势。本文探讨了将扩散模型与思维链（CoT）集成的方法，CoT是一种在自回归语言模型中改进推理能力的成熟技术。我们提出了思维扩散（DoT）模型，允许推理步骤通过扩散过程在时间上传播。与传统的自回归语言模型逐个token从左到右做出决策的方式相比，DoT在计算和推理性能之间具有更大的灵活性。我们的实验证明了DoT在多位数乘法和小学数学问题中的有效性。此外，DoT展示了有希望的自我纠正能力，并从现有的增强推理技术（如自一致解码）中受益。我们的发现有助于理解和发展推理能力。

    Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
    
[^16]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^17]: 通往多元对齐的路线图

    A Roadmap to Pluralistic Alignment

    [https://arxiv.org/abs/2402.05070](https://arxiv.org/abs/2402.05070)

    这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。

    

    随着人工智能系统的权力和普及程度的增加，设计能够为不同价值观和观点的人服务的人工智能系统变得愈发重要。然而，将模型对齐以服务多元人类价值观仍然是一个待解决的研究问题。在本文中，我们提出了一条通向多元对齐的路线图，具体使用语言模型作为测试平台。我们确定和形式化了三种可能的方式来定义和实现人工智能系统中的多元主义：1）Overton多元模型，展示合理反应的光谱；2）可操控的多元模型，可以调整以反映特定的观点；3）分布多元模型，在分布中很好地校准给定人群的模型。我们还提出和形式化了三种可能的多元基准类别：1）多目标基准；2）权衡可操控基准，鼓励模型对任意权衡进行调整；3）陪审团多元基准，明确地模拟了不同陪审团的意见。

    With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
    
[^18]: 大型语言模型的教学对齐

    Pedagogical Alignment of Large Language Models

    [https://arxiv.org/abs/2402.05000](https://arxiv.org/abs/2402.05000)

    本文介绍了教学对齐的大型语言模型（LLM）的新概念，并探讨了通过建设性反馈和提示指导学生解决复杂问题的方法。相比于传统方法，这种对齐方法以及采用人类反馈的强化学习方法能够更好地对齐LLM，提供更优质的教育支持。

    

    在本文中，我们引入了教学对齐的大型语言模型（LLM）的新概念，这在教育背景下应用LLM具有转变性的意义。与直接回答用户问题不同，教学对齐的LLM作为辅助工具，将复杂问题分解为可管理的子问题，并通过建设性的反馈和提示指导学生找到最终答案。其目标是为学习者提供解决问题的策略，以加深他们对主题的理解和内化。先前的研究主要采用了监督微调方法，没有将目标定义为对齐问题，并未使用通过人类反馈的强化学习方法（RLHF）。本研究通过对齐的视角重新解释了这一论述，并展示了RLHF方法作为对齐LLM的优越替代方法。

    In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b
    
[^19]: 通过阅读理解调整大型语言模型

    Adapting Large Language Models via Reading Comprehension

    [https://arxiv.org/abs/2309.09530](https://arxiv.org/abs/2309.09530)

    通过将原始语料库转化为阅读理解文本来调整大型语言模型，使其在多个领域的各种任务中性能始终得到提升。

    

    我们探讨了在特定领域语料库上持续预训练对大型语言模型的影响，发现在原始语料库上进行训练赋予模型领域知识，但极大地损害了其回答问题的能力。受人类通过阅读理解学习的启发，即阅读后练习提高基于所学知识回答问题的能力，我们提出了一种将原始语料库转化为阅读理解文本的简单方法。每个原始文本都会被一系列与其内容相关的任务丰富。我们的方法非常可扩展，适用于任何预训练语料库，能够在三个不同领域（生物医学、金融和法律）的各种任务中持续提升性能。值得注意的是，我们的7B语言模型在竞争中表现出色，能与规模更大的领域特定模型（如BloombergGPT-50B）相媲美。此外，我们证明了领域特定模型可以带来更好的效果。

    arXiv:2309.09530v2 Announce Type: replace  Abstract: We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specif
    
[^20]: 大型语言模型中的偏见与公平性：一项调查

    Bias and Fairness in Large Language Models: A Survey

    [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770)

    该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。

    

    大型语言模型（LLMs）的快速发展使得人们能够处理、理解和生成类似人类文本，逐渐融入触及我们社交领域的系统。然而，尽管取得成功，这些模型可能学习、延续和放大有害的社会偏见。本文对LLMs的偏见评估和缓解技术进行了全面调查。我们首先整合、形式化和扩展自然语言处理中社会偏见和公平性的概念，定义了伤害的不同方面，并引入了几个实现LLMs公平性的必要条件。然后，我们通过提出三个直观的分类体系统一了文献，其中包括两个用于偏见评估的分类体系，即指标和数据集，以及一个用于缓解的分类体系。

    arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
    
[^21]: CRUD-RAG: 用于检索增强生成的大型语言模型的全面中文基准

    CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])

    [http://arxiv.org/abs/2401.17043](http://arxiv.org/abs/2401.17043)

    这篇论文构建了一个大规模且更全面的中文基准测试，评估了检索增强生成系统的所有组件在各种应用场景中的性能。

    

    检索增强生成（RAG）是一种通过整合外部知识源来增强大型语言模型（LLM）能力的技术。该方法解决了LLM的常见限制，包括过时的信息和产生不准确的“虚构”内容的倾向。然而，评估RAG系统具有挑战性，因为现有的基准测试在范围和多样性上存在限制。大多数当前的基准测试主要评估问答应用，忽视了RAG可能有优势的更广泛的场景。此外，它们只评估RAG流程中LLM组件的性能，并忽视检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模且更全面的基准测试，并在各种RAG应用场景中评估了RAG系统的所有组件。

    Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran
    
[^22]: Q&A提示：通过挖掘问题-回答提示来发现丰富的视觉线索，以满足对多样世界知识的视觉问答的需求

    Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])

    [http://arxiv.org/abs/2401.10712](http://arxiv.org/abs/2401.10712)

    本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。

    

    随着多模态大型语言模型的突破，回答需要高级推理能力和世界知识的复杂视觉问题比以往任何时候都更重要。然而，为AI模型配备强大的跨模态推理能力仍然具有挑战性，因为人类的认知方案尚未系统地被理解。在本文中，我们相信，如果我们能尽可能收集给定图像中的视觉线索，我们将能更准确地识别图像，更好地理解问题，更容易回忆相关知识，并最终推理出答案。我们通过在图像中挖掘问题-回答对来发现这些丰富的视觉线索，并将它们作为提示发送到多模态大型语言模型中。我们称之为Q&A提示的方法。具体而言，我们首先使用训练集中的图像-答案对和相应的问题作为输入和输出来训练一个视觉问题生成模型。

    With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
    
[^23]: 超越表面：文本到图像生成中视觉刻板印象的全球规模分析

    Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])

    [http://arxiv.org/abs/2401.06310](http://arxiv.org/abs/2401.06310)

    本论文提出了一种多方面的方法，利用现有的文本资源，基于135个全球范围内的身份群体对文本到图像生成（T2I）模型生成的图像中的地理文化刻板印象进行评估。研究结果表明，刻板属性在图像中的存在可能性是刻板属性的三倍。

    

    近期的研究已经强调了在文本到图像生成（T2I）模型生成的人物形象中存在的不同身份群体的刻板印象问题。然而，这些现有方法存在一些关键限制，包括在评估中对全球身份群体的覆盖率明显不足，以及相关刻板印象的范围。此外，它们通常缺乏对本质上是视觉刻板印象（如“瘦弱”或“墨西哥草帽”）和文化相关的刻板印象（如“吸引人”或“恐怖分子”）之间的重要区别。在本研究中，我们采用多方面的方法来解决这些限制，利用现有的文本资源来将我们对来自T2I模型生成的图像中与地理文化相关的刻板印象的评估进行基础绑定。我们使用现有的刻板印象基准来识别和评估全球范围内涉及135个基于国籍的身份群体的视觉刻板印象。我们证明，在图像中存在刻板印象的可能性是刻板属性的三倍。

    Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
    
[^24]: CodePrompt：通过Prompt学习的知识特征改进源代码相关分类

    CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])

    [http://arxiv.org/abs/2401.05544](http://arxiv.org/abs/2401.05544)

    CodePrompt是一种利用Prompt学习和注意机制技术改进源代码相关分类任务的新方法。它能够提取源代码和相关文本中的丰富知识以提高准确性，并且减少了计算成本。

    

    研究人员已经探索利用预训练语言模型（如CodeBERT）改进源代码相关任务的潜力。先前的研究主要依赖CodeBERT的文本嵌入能力和"[CLS]"句子嵌入信息作为下游源代码相关任务的语义表示进行微调。然而，这些方法需要额外的神经网络层来提取有效特征，导致计算成本更高。此外，现有方法没有利用源代码和相关文本中丰富的知识，可能导致准确性降低。本文提出了一种新的方法CodePrompt，通过Prompt学习和注意机制利用预训练模型中的丰富知识来改进源代码相关分类任务。

    Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
    
[^25]: DiagrammerGPT: 通过LLM规划生成开放领域、开放平台的图表

    DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])

    [http://arxiv.org/abs/2310.12128](http://arxiv.org/abs/2310.12128)

    DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。

    

    过去几年，文本到图像（T2I）生成取得了显著的发展。尽管如此，在使用T2I模型生成图表方面的研究很少。图表是一种使用结构丰富和空间复杂的可视化来解释信息的符号/示意性表示（例如，一种密集的相关对象、文本标签、方向箭头、连接线等组合）。现有的最先进的T2I模型在生成图表时经常失败，因为它们在许多对象通过复杂的关系（如箭头/线）密集连接时缺乏细粒度的对象布局控制，并且经常不能渲染出可理解的文本标签。为了填补这一空白，我们提出了DiagrammerGPT，一个新颖的两阶段文本到图表生成框架，它利用LLM（如GPT-4）的布局引导能力来生成更准确的开放领域、开放平台的图表。在第一阶段，我们使用LLM生成和迭代改进“图表规划”（在一个规划方案中）。

    Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
    
[^26]: VideoDirectorGPT: 通过LLM引导的规划实现一致的多场景视频生成

    VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])

    [http://arxiv.org/abs/2309.15091](http://arxiv.org/abs/2309.15091)

    本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。

    

    尽管最近的文本到视频生成方法取得了显著的进展，但大多数工作集中在生成单个事件和单一背景的短视频片段（即单场景视频）。与此同时，最近的大型语言模型（LLMs）已经证明了它们在生成布局和控制下游视觉模块（如图像生成模型）的程序方面的能力。这引发了一个重要问题：我们能否利用这些LLMs中嵌入的知识用于生成时间上一致的长视频？在本文中，我们提出了VideoDirectorGPT，这是一个用于一致的多场景视频生成的新型框架，它利用LLMs的知识进行视频内容规划和基于内容的视频生成。具体而言，我们首先将单个文本提示输入我们的视频规划器LLM（GPT-4）中，将其扩展为“视频计划”，其中包括生成场景描述、实体及其布局、每个场景的背景以及保持一致性等内容。

    Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
    
[^27]: BatchPrompt: 用更少的资源实现更多任务的策略

    BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])

    [http://arxiv.org/abs/2309.00384](http://arxiv.org/abs/2309.00384)

    BatchPrompt是一种提示策略，它通过将多个数据点批量打包到一个提示中来提高LLM的令牌资源利用效率，从而缓解由于令牌计数差异导致的成本效率问题，提高推理速度和计算预算的利用率。

    

    许多LLM（Language Model）被训练来使用基于指令的提示实现零样本或少样本推理。为这些LLM制作提示通常需要用户提供详细的任务描述、上下文和完成示例以及推理上下文的单个示例。本文将这种常规提示基准称为SinglePrompt。然而，在每个推理数据点不一定很长的NLP任务中，提示中的指令和少样本示例的令牌计数可能比数据点的令牌计数大得多，与Fine-tuned BERT等基于编码器的模型相比，导致令牌资源利用率降低。这个成本效率问题影响了推理速度和计算预算，抵消了LLM所能提供的许多好处。本文旨在通过将多个数据点批量打包到一个提示中来缓解上述问题，我们将这种提示策略称为BatchPrompt。这种策略增加了数据点的密度，

    Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, 
    
[^28]: 将注意力分割与绑定用于改进生成语义护理

    Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])

    [http://arxiv.org/abs/2307.10864](http://arxiv.org/abs/2307.10864)

    本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。

    

    新兴的大规模文本到图像生成模型，如稳定扩散（SD），展示了高度逼真的压倒性结果。尽管取得了巨大的进展，但当前最先进的模型仍然难以完全依照输入提示生成图像。先前的研究——关注与激发，引入了生成语义护理（GSN）的概念，旨在在推断时优化跨注意力以更好地融入语义。它在生成简单提示，如“一只猫和一只狗”，方面展示了有希望的结果。然而，它在处理更复杂的提示以及解决不适当的属性绑定问题方面的功效有所下降。为了应对复杂提示或涉及多个实体的场景所带来的挑战，并实现改进的属性绑定，我们提出了分割与绑定。我们引入了两个新的GSN损失目标：一种新的关注丢失和一种绑定丢失。我们的方法在其能够更好地将语义纳入图像生成过程中的特点上脱颖而出。

    Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
    
[^29]: pysentimiento: 一个用于观点挖掘和社交自然语言处理任务的Python工具包

    pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks. (arXiv:2106.09462v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.09462](http://arxiv.org/abs/2106.09462)

    pysentimiento是一个多语言的Python工具包，用于观点挖掘和社交自然语言处理任务，提供了易于使用的库和最先进的模型，研究人员可以利用这些技术进行研究。

    

    近年来，从用户生成的文本中提取观点和信息引起了很大的兴趣，主要是由于社交媒体中内容的前所未有的数量。然而，社会研究人员在采用最先进的工具进行这些任务时会遇到一些问题，因为这些工具通常落后于商业API，不适用于除英语以外的其他语言，或者对非专家来说非常复杂。为了解决这些问题，我们提出了pysentimiento，这是一个全面的多语言Python工具包，专为观点挖掘和其他社交自然语言处理任务而设计。这个开源库提供了易于使用的Python库，其中包含了用于西班牙语、英语、意大利语和葡萄牙语的最先进模型，可以让研究人员利用这些技术。我们对几种预训练语言模型在各种任务、语言和数据集上的性能进行了全面评估，包括对结果公平性的评估。

    In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.
    

