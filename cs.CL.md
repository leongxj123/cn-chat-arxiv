# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819) | 提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。 |
| [^2] | [Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging](https://arxiv.org/abs/2403.08002) | 本文针对生物医学应用中前沿模型尚存在的多模态能力差距，探讨了训练开源小型多模态模型以弥补临床需求的生物医学能力差距。 |
| [^3] | [GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing](https://arxiv.org/abs/2403.06399) | 该论文提出了GlossLM模型，通过利用跨语言转移和大规模多语言预训练，实现了低资源语言文字间注释的有效生成。 |
| [^4] | [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://arxiv.org/abs/2403.05750) | 大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。 |
| [^5] | [A Survey on Human-AI Teaming with Large Pre-Trained Models](https://arxiv.org/abs/2403.04931) | 本文调查了大型预训练模型与人工智能合作的重要性，强调了这些模型如何超越传统方法增强协作智能，并探讨了其在增强人类能力、改善AI模型、有效团队合作、道德考虑以及在各个领域广泛应用方面的潜在作用。 |
| [^6] | [Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning](https://arxiv.org/abs/2402.18344) | 大型语言模型在常识推理中表现出高水平的能力，但由于信息丢失问题，提出了新方法RIDERS来解释和减轻有害CoT问题 |
| [^7] | [EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings](https://arxiv.org/abs/2402.16040) | 该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。 |
| [^8] | [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) | MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法 |
| [^9] | [Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition](https://arxiv.org/abs/2402.14523) | 本文提出了Daisy-TTS设计，通过声调嵌入分解，模拟了更广泛的情感范围，包括 primary emotions、secondary emotions、intensity-level 和 emotions polarity。 |
| [^10] | [M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection](https://arxiv.org/abs/2402.11175) | 介绍了一个新的基准M4GT-Bench，涉及多语言、多领域和多生成器，用于检测机器生成文本，包括单语和多语种MGT检测、多模型检测和人机混合文本检测。 |
| [^11] | [NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models](https://arxiv.org/abs/2402.09773) | NutePrune是一种高效逐渐剪枝方法，通过加载一个完整模型并将其与掩码和LoRA模块集成，实现了在大型语言模型中进行高效的结构剪枝。 |
| [^12] | [AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis](https://arxiv.org/abs/2402.09742) | AI医院是一个框架，用于构建实时交互式诊断环境，通过与LLMs的交互评估和协作，提高临床诊断的准确性。 |
| [^13] | [Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping](https://arxiv.org/abs/2402.07610) | 本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。 |
| [^14] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^15] | [Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA](https://arxiv.org/abs/2401.15847) | 引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。 |
| [^16] | [Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions](https://arxiv.org/abs/2401.09395) | 通过引入数学和编码问题的扰动本体以及两个数据集，作者评估了LLMs在数字推理和编码任务中的能力，在全面评估中发现所有模型在扰动问题上表现显著下降，表明当前的LLMs缺乏稳健性。 |
| [^17] | [WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images](https://arxiv.org/abs/2311.16480) | 研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。 |
| [^18] | [Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?.](http://arxiv.org/abs/2401.10415) | 本研究探讨了大型语言模型在科学摘要任务中的可控性。通过控制风格特征，非微调的语言模型在评论生成任务中优于人类，同时基于关键词的引导可以改善模型的可控性。然而，模型在生成长摘要和高度抽象的简化摘要方面有限。总体而言，大型语言模型在摘要任务中表现出强大的通用能力，但在复杂控制方面有限。 |
| [^19] | [ReFT: Reasoning with Reinforced Fine-Tuning.](http://arxiv.org/abs/2401.08967) | ReFT是一种加强推理能力的强化微调方法，通过利用更多的推理路径进行微调，提高了大型语言模型在数学问题解决中的泛化能力。 |
| [^20] | [MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector.](http://arxiv.org/abs/2401.05060) | MuTox是第一个高度多语言的基于音频的毒性数据集，通过训练基于音频的毒性分类器，实现了跨多语言的零样本毒性检测，相较于现有基于文本的分类器，具有更好的性能和更广泛的语言覆盖，相较于基于词汇列表的分类器，精度和召回率提高了约2.5倍。 |
| [^21] | [{\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning.](http://arxiv.org/abs/2401.03183) | 本文提出了{\delta}-CAUSAL，这是一个用于研究因果推理中可废除性的基准数据集，并指出现有的因果强度度量无法在可废除环境中准确评估因果关系的变化。 |
| [^22] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^23] | [Theory of Mind for Multi-Agent Collaboration via Large Language Models.](http://arxiv.org/abs/2310.10701) | 本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。 |
| [^24] | [CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models.](http://arxiv.org/abs/2310.08279) | CP-KGC方法利用大型语言模型，通过约束式提示来补全知识图谱，提高推断效果，展示了在低资源计算条件下的有效性，并在数据集上取得了优于之前方法的结果。 |
| [^25] | [Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios.](http://arxiv.org/abs/2309.08316) | 本论文评估了语言模型在跨越主题、领域和语言变化的全面非分布场景中的泛化能力，并提出了改进策略，包括基于提示的精细调节和上下文学习。 |
| [^26] | [Assessing the nature of large language models: A caution against anthropocentrism.](http://arxiv.org/abs/2309.07683) | 通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。 |
| [^27] | [GCRE-GPT: A Generative Model for Comparative Relation Extraction.](http://arxiv.org/abs/2303.08601) | 本文介绍了一种生成模型GCRE-GPT, 可直接从比较文本中提取出高精度的比较关系。 |

# 详细

[^1]: 温度计：面向大型语言模型的通用校准

    Thermometer: Towards Universal Calibration for Large Language Models

    [https://arxiv.org/abs/2403.08819](https://arxiv.org/abs/2403.08819)

    提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。

    

    我们考虑大型语言模型（LLM）中的校准问题。最近的研究发现，常见的干预措施如指令调整通常会导致校准不佳的LLMs。尽管校准在传统应用中得到了很好的探讨，但对LLMs进行校准具有独特挑战。这些挑战不仅来自LLMs的严格计算要求，也来自它们的多功能性，使它们可以应用于各种任务。为了解决这些挑战，我们提出了一个针对LLMs的校准方法THERMOMETER。THERMOMETER通过学习来自多个任务的数据的辅助模型，用于校准LLM。它在计算上效率高，保持了LLM的准确性，并为新任务产生了更好的校准响应。对各种基准的广泛实证评估显示了所提方法的有效性。

    arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
    
[^2]: 训练小型多模态模型以填补生物医学能力差距：以放射学成像为例

    Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging

    [https://arxiv.org/abs/2403.08002](https://arxiv.org/abs/2403.08002)

    本文针对生物医学应用中前沿模型尚存在的多模态能力差距，探讨了训练开源小型多模态模型以弥补临床需求的生物医学能力差距。

    

    放大基础模型的尺度规律和非凡表现激励了在生物医学领域开发和利用这些大型模型。然而，尽管在一些生物医学基准测试中取得了早期有希望的结果，但在这些模型能够应用于真实世界的应用之前仍然存在一些重大挑战。像GPT-4V这样的前沿模型在生物医学应用中仍存在重大的多模态能力差距。此外，访问、成本、延迟和合规等实际问题使临床医生难以直接在私人患者数据上使用私人托管的最先进大型模型。在本文中，我们探讨训练开源小型多模态模型（SMMs）来填补未满足的临床需求的生物医学能力差距。为了最大化数据效率，我们采用模块化方法，将用于图像和文本模态的最先进预训练模型纳入，并侧重于t

    arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
    
[^3]: GlossLM: 低资源语言文字间注释的多语言预训练

    GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing

    [https://arxiv.org/abs/2403.06399](https://arxiv.org/abs/2403.06399)

    该论文提出了GlossLM模型，通过利用跨语言转移和大规模多语言预训练，实现了低资源语言文字间注释的有效生成。

    

    语言文献学的一个关键方面是以形式如文字间注释文本（IGT）的方式创建带注释的文本，IGT以逐词素的格式捕捉了精细的形态句法分析。先前的研究已探索了自动生成IGT的方法，以减少语言分析的时间成本。然而，许多语言（尤其是需要保护的语言）缺乏足够的IGT数据来训练有效的模型，跨语言转移被提出作为克服这一局限的方法。我们编制了来自各种来源的最大已有IGT数据语料库，涵盖了来自1.8k种语言的超过45万个例子，以便进行跨语言转移和IGT生成方面的研究。然后，我们在部分语料库上对一个大型多语言模型进行预训练，并进一步对特定语言进行微调。我们的模型在分割数据和大型单语数据方面与最先进的方法相竞争。

    arXiv:2403.06399v1 Announce Type: new  Abstract: A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation.   We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further finetune it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datas
    
[^4]: 解读AI笔: 检测AI生成文本的技术与挑战

    Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text

    [https://arxiv.org/abs/2403.05750](https://arxiv.org/abs/2403.05750)

    大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。

    

    大型语言模型(LLMs)通过展示生成类人文本的惊人能力，彻底颠覆了自然语言生成(NLG)领域。然而，它们广泛的应用带来挑战，需要深入审查、伦理审查和负责任的实践。本研究探讨了这些挑战，探索了现有的缓解策略，重点是识别AI生成文本作为最终解决方案。此外，我们从理论角度评估了检测的可行性，并提出了解决当前领域限制的新颖研究方向。

    arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
    
[^5]: 人工智能与大型预训练模型合作调查

    A Survey on Human-AI Teaming with Large Pre-Trained Models

    [https://arxiv.org/abs/2403.04931](https://arxiv.org/abs/2403.04931)

    本文调查了大型预训练模型与人工智能合作的重要性，强调了这些模型如何超越传统方法增强协作智能，并探讨了其在增强人类能力、改善AI模型、有效团队合作、道德考虑以及在各个领域广泛应用方面的潜在作用。

    

    在人工智能（AI）迅速发展的景观中，人类智能和AI系统之间的协作，即人工智能（HAI）合作，已成为推进问题解决和决策过程的基石。大型预训练模型（LPtM）的出现显著改变了这一景观，通过利用大量数据来理解和预测复杂模式，为人类提供了前所未有的能力。本文调查了LPtMs与HAI的关键整合，强调了这些模型如何超越传统方法增强协作智能。重点探讨了LPtMs在增强人类能力方面的协同潜力，讨论了这种协作对AI模型改进、有效的团队合作、道德考虑以及在各个领域的广泛应用影响。通过这一探索，研究揭示了LPtM增强HAI的变革性影响。

    arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
    
[^6]: 专注于你的问题！解释和减轻常识推理中的有害CoT问题

    Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning

    [https://arxiv.org/abs/2402.18344](https://arxiv.org/abs/2402.18344)

    大型语言模型在常识推理中表现出高水平的能力，但由于信息丢失问题，提出了新方法RIDERS来解释和减轻有害CoT问题

    

    大型语言模型表现出高水平的常识推理能力，尤其是通过Chain-of-Thought（CoT）等增强方法。然而，我们发现这些类似CoT的方法导致了原本正确的答案变得错误的问题，我们将其定义为有害的CoT问题。为了解释和减轻这一问题，我们首先利用属性跟踪和因果跟踪方法来探究LLM在CoT推理过程中的内部工作机制。通过比较，我们证明了模型在生成推理或答案时存在来自问题的信息丢失现象在浅层注意力层中。基于探究结果，我们设计了一种名为RIDERS（Residual decodIng and sERial-position Swap）的新方法，从解码和序列位置的角度补偿模型中的信息亏缺。通过对多个常识推理基准的广泛实验，我们验证了

    arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate 
    
[^7]: EHRNoteQA：用于在临床环境中评估大型语言模型的患者特定问题回答基准

    EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings

    [https://arxiv.org/abs/2402.16040](https://arxiv.org/abs/2402.16040)

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs），具有采用多项选择问题回答格式和需要分析多篇临床笔记的特点。

    

    该研究介绍了EHRNoteQA，这是一个新颖的患者特定问题回答基准，旨在评估临床环境中的大型语言模型（LLMs）。在MIMIC-IV电子健康记录（EHR）的基础上，由三位医疗专家团队精心策划了包含962个独特问题的数据集，每个问题都与特定患者的EHR临床笔记相关联。与现有基于EHR的基准不同的是：首先，它是第一个采用多项选择问题回答格式的数据集，这种设计选择在自动评估的背景下有效评估LLMs的得分性能，与其他格式相比。其次，它需要分析多篇临床笔记才能回答一个问题，反映了实际临床决策制定的复杂性，医生需要审查大量患者病史记录。我们对各种大型语言模型进行了全面评估。

    arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
    
[^8]: MobileLLM：优化亚十亿参数语言模型以用于设备端应用

    MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

    [https://arxiv.org/abs/2402.14905](https://arxiv.org/abs/2402.14905)

    MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法

    

    本文解决了移动设备上高效的大型语言模型(LLMs)的迫切需求问题，这是由于云成本和延迟问题不断增加所导致的。我们专注于设计具有不到十亿参数的顶级LLMs，这是移动部署的实际选择。与普遍的观点相反，强调数据和参数数量在确定模型质量方面的关键作用，我们的研究强调了亚十亿规模LLMs的模型架构的重要性。利用深度和瘦身结构，再加上嵌入共享和分组查询注意机制，我们建立了一个强大的基准网络，称为MobileLLM，其在将近125M/350M先进模型上分别获得了惊人的2.7%/4.3%的准确率提升。此外，我们提出了一种立即的块状权重共享方法，不增加模型大小，且仅具有极小的延迟开销。由此产生的模型被命名为MobileLLM-L

    arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
    
[^9]: Daisy-TTS: 通过声调嵌入分解模拟更广泛的情感范围

    Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition

    [https://arxiv.org/abs/2402.14523](https://arxiv.org/abs/2402.14523)

    本文提出了Daisy-TTS设计，通过声调嵌入分解，模拟了更广泛的情感范围，包括 primary emotions、secondary emotions、intensity-level 和 emotions polarity。

    

    我们经常以多方面的方式口头表达情感，它们在强度上可能有所变化，表达的不仅是单一的情感，还可能是各种情感的混合体。这种广泛的情感范围在情感结构模型中得到了深入研究，该模型将各种情感表示为原始情感的派生产品，具有不同程度的强度。在本文中，我们提出了一种情感文本转语音设计，旨在模拟基于结构模型的更广泛情感范围。我们提出的设计Daisy-TTS，结合了一个声调编码器，用于学习作为情感代理的可分离的声调嵌入。这种情感表示使模型能够模拟：（1）从训练样本中学到的原始情感，（2）作为原始情感的混合体的次级情感，（3）通过调整情感嵌入来实现强度级别，（4）通过否定情感嵌入来实现情感极性。

    arXiv:2402.14523v1 Announce Type: new  Abstract: We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of
    
[^10]: M4GT-Bench: 用于黑盒机器生成文本检测的评估基准

    M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection

    [https://arxiv.org/abs/2402.11175](https://arxiv.org/abs/2402.11175)

    介绍了一个新的基准M4GT-Bench，涉及多语言、多领域和多生成器，用于检测机器生成文本，包括单语和多语种MGT检测、多模型检测和人机混合文本检测。

    

    大型语言模型（LLMs）的出现带来了机器生成文本（MGT）在不同渠道的激增，这引发了人们对其潜在滥用和社会影响的关注。识别和区分这种内容与真实人类生成的文本对于对抗虚假信息、保持教育和科学领域的完整性以及保持通信信任至关重要。本文通过引入一个涉及多语言、多领域和多生成器的新基准M4GT-Bench来解决这一问题。它针对三个任务提出了不同的集合：（1）单语和多语种二分类MGT检测；（2）多模型检测确定生成文本的特定模型；以及（3）人机混合文本检测，应确定一个词边界来界定MGT和人工撰写内容。对于任务2的人类评估显示

    arXiv:2402.11175v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows
    
[^11]: NutePrune: 高效的大型语言模型逐渐剪枝方法，多个教师参与

    NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models

    [https://arxiv.org/abs/2402.09773](https://arxiv.org/abs/2402.09773)

    NutePrune是一种高效逐渐剪枝方法，通过加载一个完整模型并将其与掩码和LoRA模块集成，实现了在大型语言模型中进行高效的结构剪枝。

    

    大型语言模型（LLMs）的巨大尺寸给资源受限硬件带来了显著的部署挑战。结构剪枝为压缩LLMs提供了一种有效的方式，从而降低存储成本，提升推断速度，实现更高效的利用。在这项工作中，我们研究了数据效率和资源效率的结构剪枝方法，以获取更小但依然强大的模型。知识蒸馏非常适合剪枝，因为完整的模型可以作为剪枝后的学生的优秀教师。然而，在LLMs的背景下，由于内存限制，这变得具有挑战性。为了解决这个问题，我们提出了一种高效逐渐剪枝方法（NutePrune）。NutePrune通过只加载一个完整模型并将其与各种掩码和LoRA模块集成，在教师和学生角色之间无缝切换，从而减轻了过多的内存开销。

    arXiv:2402.09773v1 Announce Type: new  Abstract: The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach a
    
[^12]: AI医院：用于临床诊断的LLMs作为实习医生的交互式评估和协作

    AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis

    [https://arxiv.org/abs/2402.09742](https://arxiv.org/abs/2402.09742)

    AI医院是一个框架，用于构建实时交互式诊断环境，通过与LLMs的交互评估和协作，提高临床诊断的准确性。

    

    引入大型语言模型（LLMs）在医疗保健中的应用标志着重大的进展。然而，目前的应用主要局限于辨别和问答任务，没有充分发挥其交互潜力。为了解决这个局限，我们的论文提出了AI医院，一个旨在构建实时交互式诊断环境的框架。为了模拟过程，我们收集高质量的医疗记录，创建了患者、检查者和医疗主任代理。然后，利用AI医院进行LLMs的交互评估和协作。初始阶段，我们创建了一个多视图医学评估（MVME）基准，其中各种LLMs作为实习医生进行交互式诊断。随后，为了提高诊断准确性，我们引入了一种协作机制，涉及医疗主任的监督下的迭代讨论和争议解决过程。

    arXiv:2402.09742v1 Announce Type: new  Abstract: The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. I
    
[^13]: 踩脚调校：通过自助引导扩展LLM的自对齐能力的规模化方法

    Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping

    [https://arxiv.org/abs/2402.07610](https://arxiv.org/abs/2402.07610)

    本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。

    

    自对齐是一种降低人工注释成本并确保模型能力的有效方法。然而，大多数当前的方法在单次循环中完成数据收集和训练步骤，可能忽视了自对齐模型不断改进的能力。这引发了一个关键问题：如果我们进行多次自助引导自对齐，会增强模型性能还是导致快速退化？本文首次探索了自助引导自对齐对大型语言模型的影响。我们的研究结果表明，通过保证从上下文学习中获得的数据多样性，自助引导自对齐明显优于单次循环的方法。为了进一步发挥自助引导的能力，我们还研究并调整了数据的训练顺序，从而提高了模型的性能。基于这些发现，我们提出了踩脚调校（SOFT）的方法，利用模型的持续增强能力。

    Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
    
[^14]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^15]: 松饼还是吉娃娃？用多面板VQA挑战大型视觉语言模型

    Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA

    [https://arxiv.org/abs/2401.15847](https://arxiv.org/abs/2401.15847)

    引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。

    

    多面板图像，通常在网页截图、海报等中看到，充斥着我们的日常生活。这些图像以多个子图以不同布局组成，有效地向人们传达信息。为了构建高级的多模态人工智能应用，如能理解复杂场景并在网页中导航的代理程序，多面板视觉推理的技能是至关重要的，对模型在这方面进行全面评估是很重要的。因此，我们引入了多面板视觉问答（MultipanelVQA），这是一个新颖的基准，包括6,600个问题、答案和多面板图像三元组，专门挑战模型理解多面板图像。我们的评估表明，MultipanelVQA基准中的问题对测试的最先进的大型视觉语言模型（LVLMs）提出了重大挑战，即使人类可以获得约99%的准确率。

    arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
    
[^16]: 被理性的流沙所困，远离AGI峰会：通过本体引导干预评估LLMs的数学和编码能力

    Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions

    [https://arxiv.org/abs/2401.09395](https://arxiv.org/abs/2401.09395)

    通过引入数学和编码问题的扰动本体以及两个数据集，作者评估了LLMs在数字推理和编码任务中的能力，在全面评估中发现所有模型在扰动问题上表现显著下降，表明当前的LLMs缺乏稳健性。

    

    最近大型语言模型（LLMs）的先进发展展示了在现有逻辑推理基准测试中取得了引人注目的成果，其中一些模型甚至超过了人类表现。然而，它们在推理任务中的实际能力和稳健性仍然是一个未解之谜。因此，本文关注两个流行的推理任务：算术推理和代码生成。特别是，我们引入了：（i）数学和编码问题的通用扰动本体，（ii）一种半自动方法来应用这些扰动，以及（iii）两个数据集MORE和CORE，分别用于扰动数学和编码问题，以探究LLM在数字推理和编码任务中的能力极限。通过对封闭源和开源LLMs的全面评估，我们展示了所有模型对扰动问题的显著性能下降，表明当前的LLMs缺乏稳健性。

    arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
    
[^17]: 病理报告的多实例生成用于千亿像素全切片图像

    WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images

    [https://arxiv.org/abs/2311.16480](https://arxiv.org/abs/2311.16480)

    研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。

    

    全切片图像是用于癌症诊断和治疗的数字病理学的基础。撰写病理报告对经验不足的病理学家来说是费时且容易出错的。为了减少工作量并改善临床自动化，我们研究了如何生成给定全切片图像的病理报告。在数据端，我们整理了最大的WSI-文本数据集（TCGA-PathoText）。具体来说，我们通过识别和清理TCGA中叙述诊断幻灯片的病理报告，收集了近1万对高质量的WSI-文本配对，供视觉-语言模型使用。在模型端，我们提出了可以为千亿像素WSI生成病理报告的多实例生成模型（MI-Gen）。我们在TCGA-PathoText的最大子集上对我们的模型进行了基准测试。实验结果表明，我们的模型可以生成包含多个临床线索的病理报告。此外，WSI-文本预测可被视为一种方法。

    arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
    
[^18]: 大型语言模型摘要机能否适应不同科学传播目标？

    Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])

    [http://arxiv.org/abs/2401.10415](http://arxiv.org/abs/2401.10415)

    本研究探讨了大型语言模型在科学摘要任务中的可控性。通过控制风格特征，非微调的语言模型在评论生成任务中优于人类，同时基于关键词的引导可以改善模型的可控性。然而，模型在生成长摘要和高度抽象的简化摘要方面有限。总体而言，大型语言模型在摘要任务中表现出强大的通用能力，但在复杂控制方面有限。

    

    在这项工作中，我们研究了大型语言模型 (LLMs) 在科学摘要任务中的可控性。我们确定了表征论文评论、摘要和简化摘要等不同类型摘要的关键风格和内容覆盖因素。通过控制风格特征，我们发现非微调的LLMs在MuP评论生成任务中表现优于人类，无论是在与参考摘要的相似度还是在人类偏好方面。此外，我们还表明，我们可以通过基于关键词的无分类器引导 (CFG) 来改善LLMs的可控性，在arXiv和PubMed上实现与强微调基线相当的词汇重叠。然而，我们的结果还表明，LLMs无法一致地生成超过8个句子的长摘要。此外，这些模型在生成高度抽象的简化摘要方面能力有限。虽然LLMs表现出很强的通用摘要能力，但在不昂贵的微调措施下，对内容的复杂控制能力有限。

    In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
    
[^19]: ReFT: 加强强化微调的推理能力

    ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])

    [http://arxiv.org/abs/2401.08967](http://arxiv.org/abs/2401.08967)

    ReFT是一种加强推理能力的强化微调方法，通过利用更多的推理路径进行微调，提高了大型语言模型在数学问题解决中的泛化能力。

    

    增强大型语言模型（LLMs）的推理能力的一种方法是使用链式思考（CoT）注释进行监督微调（SFT）。然而，这种方法在泛化能力上并不十分强大，因为训练仅依赖于给定的CoT数据。例如，在数学问题解决中，训练数据中通常只有一个注释的推理路径用于每个问题。直观来说，让算法从给定的问题中学习多个注释的推理路径会更好。为了解决这个问题，我们提出了一种简单而有效的方法，称为加强强化微调（ReFT），以增强学习LLMs进行推理的泛化能力，以数学问题解决为例。ReFT首先使用SFT对模型进行热身，然后采用在线强化学习（具体来说，在本文中是使用PPO算法）进一步微调模型，其中根据问题自动采样了大量的推理路径。

    One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question
    
[^20]: MuTox: 通用多语言基于音频的毒性数据集和零样本检测器

    MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector. (arXiv:2401.05060v1 [cs.SD])

    [http://arxiv.org/abs/2401.05060](http://arxiv.org/abs/2401.05060)

    MuTox是第一个高度多语言的基于音频的毒性数据集，通过训练基于音频的毒性分类器，实现了跨多语言的零样本毒性检测，相较于现有基于文本的分类器，具有更好的性能和更广泛的语言覆盖，相较于基于词汇列表的分类器，精度和召回率提高了约2.5倍。

    

    语音模态（基于音频）自然语言处理中的毒性检测研究相对有限，特别是对于非英语语言而言。为了解决这些限制，并为真正多语言的基于音频的毒性检测奠定基础，我们引入了MuTox，这是第一个具有毒性标签的高度多语言的基于音频的数据集。该数据集包含20,000个英语和西班牙语音频片段，以及其他19种语言的4,000个片段。为了证明数据集的质量，我们训练了MuTox基于音频的毒性分类器，它能够在各种语言中进行零样本毒性检测。与现有的基于文本训练的分类器相比，该分类器的AUC性能提高了超过1%，同时扩大了语言覆盖范围十倍以上。与基于词汇列表的具有相似语言覆盖数量的分类器相比，MuTox的精度和召回率提高了约2.5倍。这个显著的改进突显了其潜在的创新性和贡献。

    Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 19 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier outperforms existing text-based trainable classifiers by more than 1% AUC, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves precision and recall by approximately 2.5 times. This significant improvement underscores the potenti
    
[^21]: {\delta}-CAUSAL：探索因果推理中的可废除性

    {\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning. (arXiv:2401.03183v1 [cs.CL])

    [http://arxiv.org/abs/2401.03183](http://arxiv.org/abs/2401.03183)

    本文提出了{\delta}-CAUSAL，这是一个用于研究因果推理中可废除性的基准数据集，并指出现有的因果强度度量无法在可废除环境中准确评估因果关系的变化。

    

    因果推理中的可废除性意味着因果关系可以被加强或削弱。也就是说，因果关系的强度应该随着加入支持者或驳斥者而增加或减少。然而，现有的研究忽视了因果推理中的可废除性，并未在可废除环境中评估现有的因果强度度量。在这项工作中，我们提出了第一个用于研究因果推理中的可废除性的基准数据集{\delta}-CAUSAL。{\delta}-CAUSAL包括约11K个涵盖十个领域的事件，其中包括支持者和驳斥者的可废除因果关系对。我们进一步展示了当前的因果强度度量无法反映{\delta}-CAUSAL中的支持者或驳斥者加入后的因果强度变化。为此，我们提出了CESAR（Causal Embedding aSsociation with Attention Rating）。

    Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present {\delta}-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. {\delta}-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters and defeaters. We further show current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in {\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating),
    
[^22]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^23]: 多智能体协作的大型语言模型理论

    Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])

    [http://arxiv.org/abs/2310.10701](http://arxiv.org/abs/2310.10701)

    本研究通过在多智能体合作游戏中评估基于大型语言模型的智能体，发现它们可以表现出协作行为和高级理论推理能力，并通过使用明确的信念状态表示来提高任务性能和理论推理准确性。

    

    大型语言模型在推理和规划方面取得了令人瞩目的成就，但它在多智能体协作方面的能力尚未得到深入探索。本研究通过对比多智能体强化学习和基于规划的基准方法，在多智能体合作文本游戏中评估了基于大型语言模型的智能体在理论推理任务上的表现。我们观察到基于大型语言模型的智能体出现了协作行为和高级理论推理能力的证据。我们的结果揭示了基于大型语言模型的智能体在长期规划上存在优化的局限性，以及对任务状态的错误认知。我们尝试使用明确的信念状态表示来缓解这些问题，并发现它可以提高大型语言模型智能体的任务性能和理论推理的准确性。

    While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
    
[^24]: CP-KGC: 利用大型语言模型的约束式提示对知识图谱进行补全

    CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])

    [http://arxiv.org/abs/2310.08279](http://arxiv.org/abs/2310.08279)

    CP-KGC方法利用大型语言模型，通过约束式提示来补全知识图谱，提高推断效果，展示了在低资源计算条件下的有效性，并在数据集上取得了优于之前方法的结果。

    

    知识图谱补全旨在利用现有知识推断和推测知识图谱中缺失的连接。SimKGC等基于文本的方法已经超过了图嵌入方法，展示了归纳式知识图谱补全的潜力。然而，基于文本的方法的效果取决于实体文本描述的质量。为了减轻LLM生成的文本中的幻觉，在本文中，我们引入了一种基于约束的提示方法，利用实体及其文本描述作为上下文约束来提高数据质量。我们的约束式提示知识图谱补全方法（CP-KGC）在低资源计算条件下表现出有效的推断能力，并超过了WN18RR和FB15K237数据集上的之前结果。这展示了LLMs在知识图谱补全任务中的整合，并为未来的研究提供了新的方向。

    Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
    
[^25]: 跨越主题、领域和语言变化：对全面的非分布场景进行评估

    Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios. (arXiv:2309.08316v1 [cs.CL])

    [http://arxiv.org/abs/2309.08316](http://arxiv.org/abs/2309.08316)

    本论文评估了语言模型在跨越主题、领域和语言变化的全面非分布场景中的泛化能力，并提出了改进策略，包括基于提示的精细调节和上下文学习。

    

    语言模型在独立且同分布的训练和测试数据中表现出色。然而，在实际应用中（如争论挖掘），它们的性能经常下降。这种降级发生在新话题出现，或其他文本领域和语言变得相关的情况下。为了评估语言模型在这些非分布场景中的泛化能力，我们通过有意地保留特定实例进行测试来模拟这种分布变化，例如社交媒体领域或太阳能主题。与先前关注特定变化和度量标准的研究不同，我们全面分析了泛化问题。我们定义了三个度量标准来确定泛化缺陷，并提出了涵盖主题、领域和语言变化的十一个分类任务。总体来说，我们发现基于提示的精细调节具有更卓越的性能，特别是在训练集和测试集在语义上主要有差异的情况下。同时，在上下文学习方面也有类似的发现。

    Language models (LMs) excel in in-distribution (ID) scenarios where train and test data are independent and identically distributed. However, their performance often degrades in real-world applications like argument mining. Such degradation happens when new topics emerge, or other text domains and languages become relevant. To assess LMs' generalization abilities in such out-of-distribution (OOD) scenarios, we simulate such distribution shifts by deliberately withholding specific instances for testing, as from the social media domain or the topic Solar Energy.  Unlike prior studies focusing on specific shifts and metrics in isolation, we comprehensively analyze OOD generalization. We define three metrics to pinpoint generalization flaws and propose eleven classification tasks covering topic, domain, and language shifts. Overall, we find superior performance of prompt-based fine-tuning, notably when train and test splits primarily differ semantically. Simultaneously, in-context learning
    
[^26]: 评估大规模语言模型的性质：对人类中心主义的警告

    Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])

    [http://arxiv.org/abs/2309.07683](http://arxiv.org/abs/2309.07683)

    通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。

    

    生成式人工智能模型通过OpenAI的聊天机器人ChatGPT的发布引起了公众的关注和猜测。目前存在两种意见阵营：一方对这些模型为人类任务带来的基本变革的可能性感到兴奋，另一方对这些模型的强大能力感到高度关切。为了应对这些关切，我们使用了标准、规范化和经过验证的认知和个性测量工具来评估GPT3.5。在这个初步项目中，我们开发了一套测试，可以估计这些模型的能力边界，它们在短时间内的稳定性以及与人类的比较。我们的结果表明，GPT 3.5很可能没有产生意识，尽管它对个性问卷的回答能力令人感兴趣。它在重复观察过程中显示出认知和个性测量方面的大量变异，这与具有人类般个性的模型是不符合预期的。

    Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
    
[^27]: GCRE-GPT: 一种用于比较关系提取的生成模型

    GCRE-GPT: A Generative Model for Comparative Relation Extraction. (arXiv:2303.08601v1 [cs.CL])

    [http://arxiv.org/abs/2303.08601](http://arxiv.org/abs/2303.08601)

    本文介绍了一种生成模型GCRE-GPT, 可直接从比较文本中提取出高精度的比较关系。

    

    给定比较文本，比较关系提取旨在提取两个目标（例如两个相机）的比较和它们被比较的方面（例如图像质量）。提取出的比较关系是进一步意见分析的基础。现有的解决方案将此任务作为一个序列标记任务，以提取目标和方面。然而，它们不能直接从文本中提取比较关系。本文通过生成模型展示出，可以直接以高精度提取出比较关系。基于GPT-2，我们提出了一种生成式比较关系提取器（GCRE-GPT）。实验结果表明，该模型在两个数据集上达到了最先进的准确性。

    Given comparative text, comparative relation extraction aims to extract two targets (\eg two cameras) in comparison and the aspect they are compared for (\eg image quality). The extracted comparative relations form the basis of further opinion analysis.Existing solutions formulate this task as a sequence labeling task, to extract targets and aspects. However, they cannot directly extract comparative relation(s) from text. In this paper, we show that comparative relations can be directly extracted with high accuracy, by generative model. Based on GPT-2, we propose a Generation-based Comparative Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves state-of-the-art accuracy on two datasets.
    

