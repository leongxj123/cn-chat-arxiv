# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text](https://arxiv.org/abs/2403.13107) | 提出了一种无监督问答系统，通过多级总结法对法律文本进行处理，实现了F1分数的显著提升 |
| [^2] | [The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing](https://arxiv.org/abs/2403.07825) | 本文提出了一种新的评估方法 GORA 和一种模型编辑方法 SORA，用以解决模型编辑中的隐藏空间中的涟漪效应问题。 |
| [^3] | [A Modular Approach for Multimodal Summarization of TV Shows](https://arxiv.org/abs/2403.03823) | 提出了一种模块化方法用于多模态电视节目摘要，包括检测场景边界、重新排列场景、将视觉信息转换为文本、总结对话以及将场景摘要融合的过程，并引入了一个新的衡量摘要质量的评价指标PREFS。 |
| [^4] | [Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges](https://arxiv.org/abs/2403.02990) | 探讨了大型语言模型（LLMs）对数据增强的转变性影响，独特挑战和机遇，突出了LLMs在数据增强中引入的范式转变。 |
| [^5] | [PROC2PDDL: Open-Domain Planning Representations from Texts](https://arxiv.org/abs/2403.00092) | Proc2PDDL是包含开放领域程序性文本和专家注释的第一个数据集，展示了对于动作的先决条件和效果的定义在语言模型方面的不足，为未来整合语言模型和形式规划提供了启示。 |
| [^6] | [GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2402.19255) | 通过引入对抗式小学数学数据集（GSM-Plus），评估了25个LLMs和4种提示技术，在广泛的问题变化中展示LLMs的数学推理能力，并发现它们的表现远非稳健。 |
| [^7] | [Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling](https://arxiv.org/abs/2402.17019) | 通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。 |
| [^8] | [GraphWiz: An Instruction-Following Language Model for Graph Problems](https://arxiv.org/abs/2402.16029) | GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。 |
| [^9] | [Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659) | 提出使用合成指南替换真实指南以增强隐私保护，并通过私密微调生成器生成此类合成指南，并通过新颖的过滤算法使合成指南的分布与真实指南一致，展示了在大型语言模型对齐中的高效用性。 |
| [^10] | [StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing](https://arxiv.org/abs/2402.12636) | StyleDubber提出了一种新的电影配音方法，通过在音素级别进行学习，解决了当前 V2C 模型中存在的音素发音不完整和身份稳定性差的问题。 |
| [^11] | [Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents](https://arxiv.org/abs/2402.12327) | 该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。 |
| [^12] | [A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models](https://arxiv.org/abs/2402.11469) | 本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。 |
| [^13] | [How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?](https://arxiv.org/abs/2402.10770) | 本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。 |
| [^14] | [HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation](https://arxiv.org/abs/2402.09390) | HGOT是一种用于检索增强上下文学习中事实性评估的分层思维图方法，通过利用大型语言模型的规划能力和思维质量评估指标来提高相关段落的检索和答案选择。 |
| [^15] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^16] | [Distilling Event Sequence Knowledge From Large Language Models](https://arxiv.org/abs/2401.07237) | 本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。 |
| [^17] | [Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images](https://arxiv.org/abs/2312.08906) | 本文利用眼动追踪技术研究了母语为汉语的人对语言景观的注意点，发现其关注程度高于一般景观，可能是因为语言景观的信息密度较高。 |
| [^18] | [AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents.](http://arxiv.org/abs/2401.12963) | AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。 |
| [^19] | [Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency.](http://arxiv.org/abs/2309.17272) | 本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。 |
| [^20] | [STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation.](http://arxiv.org/abs/2309.15176) | STANCE-C3是一种通过对比学习和反事实生成进行领域自适应的跨目标立场检测模型，用于推断人们对于普遍或有争议话题的观点。在解决数据分布偏移和缺乏领域特定标注数据的挑战上具有重要贡献。 |
| [^21] | [Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models.](http://arxiv.org/abs/2305.13712) | 本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。 |

# 详细

[^1]: 面向法律文本的多级总结无监督问答系统

    Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text

    [https://arxiv.org/abs/2403.13107](https://arxiv.org/abs/2403.13107)

    提出了一种无监督问答系统，通过多级总结法对法律文本进行处理，实现了F1分数的显著提升

    

    这篇论文总结了团队SCaLAR在SemEval-2024任务5上的工作：民事程序中的法律论证推理。为了解决这个二元分类任务，由于涉及到的法律文本的复杂性而令人望而却步，我们提出了一种简单而又新颖的基于相似度和距离的无监督方法来生成标签。此外，我们探索了使用集成特征（包括CNN、GRU和LSTM）的多级Legal-Bert嵌入的融合。为了解决数据集中法律解释的冗长性，我们引入了基于T5的分段摘要，成功地保留了关键信息，提升了模型的性能。我们的无监督系统在开发集上见证了macro F1分数增加了20个百分点，在测试集上增加了10个百分点，考虑到其简单的架构，这是令人鼓舞的。

    arXiv:2403.13107v1 Announce Type: new  Abstract: This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based unsupervised approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including CNN, GRU, and LSTM. To address the lengthy nature of Legal explanation in the dataset, we introduce T5-based segment-wise summarization, which successfully retained crucial information, enhancing the model's performance. Our unsupervised system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.
    
[^2]: 模型编辑中的遗漏之处：深入探讨模型编辑带来的隐藏损害

    The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing

    [https://arxiv.org/abs/2403.07825](https://arxiv.org/abs/2403.07825)

    本文提出了一种新的评估方法 GORA 和一种模型编辑方法 SORA，用以解决模型编辑中的隐藏空间中的涟漪效应问题。

    

    大型语言模型以其卓越的效果彻底改变了许多任务。然而，对这些模型进行编辑，以修改过时或错误信息的关键性工作，往往会导致一个称为“隐藏空间中的涟漪效应”的复杂问题。这种效应虽然难以检测，但却会显著阻碍模型编辑任务的效果，并恶化模型性能。本文通过提出一种新颖的评估方法，基于图形特异值关系的评估(GORA)，来应对这一科学挑战，量化评估模型的适应性和编辑的后续影响。此外，我们引入了一种旨在减轻这种涟漪效应的模型编辑方法——选择性异常值重新编辑方法(SORA)。我们的全面评估揭示了隐藏空间中的涟漪效应在所有当前模型编辑方法中都是一个重要问题。然而，我们提出的方法，G

    arXiv:2403.07825v1 Announce Type: new  Abstract: Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, G
    
[^3]: 一种用于多模态电视节目摘要的模块化方法

    A Modular Approach for Multimodal Summarization of TV Shows

    [https://arxiv.org/abs/2403.03823](https://arxiv.org/abs/2403.03823)

    提出了一种模块化方法用于多模态电视节目摘要，包括检测场景边界、重新排列场景、将视觉信息转换为文本、总结对话以及将场景摘要融合的过程，并引入了一个新的衡量摘要质量的评价指标PREFS。

    

    在本文中，我们讨论了电视节目摘要的任务，涉及到人工智能研究中的关键领域：复杂推理、多模态和长篇叙事。我们提出了一种模块化方法，其中各个组件执行专门的子任务，我们认为与端到端方法相比，这种方法提供了更大的灵活性。我们的模块涉及检测场景边界，重新排列场景以尽量减少不同事件之间的切换次数，将视觉信息转换为文本，总结每个场景中的对话，并将场景摘要融合成整集的最终摘要。我们还提出了一个新的度量标准，PREFS（摘要事实的精确度和召回率评估），用于衡量生成摘要的精确度和召回率，我们将其分解为原子事实。在最近发布的SummScreen3D数据集Papalampidi和Lapata（2023）上进行测试，我们的方法产生了

    arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
    
[^4]: 使用LLMs的数据增强：数据视角、学习范式和挑战

    Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges

    [https://arxiv.org/abs/2403.02990](https://arxiv.org/abs/2403.02990)

    探讨了大型语言模型（LLMs）对数据增强的转变性影响，独特挑战和机遇，突出了LLMs在数据增强中引入的范式转变。

    

    在机器学习（ML）领域快速发展中，数据增强（DA）已成为一种关键技术，通过使训练样本多样化而无需额外数据收集来增强模型性能。本调查探讨了大型语言模型（LLMs）对数据增强的转变性影响，特别是在自然语言处理（NLP）领域及其他领域中它们提供的独特挑战和机遇。从数据视角和学习视角，我们研究了利用大型语言模型进行数据增强的各种策略，包括对LLM生成数据进行进一步训练的新颖学习范式的探索。此外，本文还阐明了该领域面临的主要挑战，从可控数据增强到多模态数据增强等。本调查突显了LLMs在数据增强中引入的范式转变，旨在作为一种...

    arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
    
[^5]: PROC2PDDL：来自文本的开放领域规划表示

    PROC2PDDL: Open-Domain Planning Representations from Texts

    [https://arxiv.org/abs/2403.00092](https://arxiv.org/abs/2403.00092)

    Proc2PDDL是包含开放领域程序性文本和专家注释的第一个数据集，展示了对于动作的先决条件和效果的定义在语言模型方面的不足，为未来整合语言模型和形式规划提供了启示。

    

    在基于文本的环境中进行规划仍然是人工智能系统的一项重大挑战。最近的方法使用语言模型来预测规划领域定义（例如，PDDL），但仅在封闭领域模拟环境中进行了评估。为了解决这个问题，我们提出了Proc2PDDL，这是第一个包含开放领域程序性文本和专家注释的PDDL表示的数据集。利用这个数据集，我们评估了最先进的模型在定义动作的先决条件和效果方面的表现。我们发现Proc2PDDL是非常具有挑战性的，GPT-3.5 的成功率接近于0%，而 GPT-4 的成功率约为35%。我们的分析显示出语法和语义错误，表明语言模型在生成领域特定程序和推理事件方面存在不足。我们希望这个分析和数据集有助于未来在整合语言模型和形式规划的最佳方面方面的进展。

    arXiv:2403.00092v1 Announce Type: new  Abstract: Planning in a text-based environment continues to be a major challenge for AI systems. Recent approaches have used language models to predict a planning domain definition (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL , the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate state-of-the-art models on defining the preconditions and effects of actions. We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific prgorams and reasoning about events. We hope this analysis and dataset helps future progress towards integrating the best of LMs and formal planning.
    
[^6]: GSM-Plus：评估LLMs作为数学问题解决者的稳健性的全面基准

    GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers

    [https://arxiv.org/abs/2402.19255](https://arxiv.org/abs/2402.19255)

    通过引入对抗式小学数学数据集（GSM-Plus），评估了25个LLMs和4种提示技术，在广泛的问题变化中展示LLMs的数学推理能力，并发现它们的表现远非稳健。

    

    大型语言模型（LLMs）在各种数学推理基准上取得了令人印象深刻的表现。然而，关于这些模型是否真正理解并应用数学知识，还是仅仅依赖于数学推理的捷径，存在越来越多的争论。一个基本且经常发生的证据是，当数学问题稍作更改时，LLMs可能会出现错误行为。这促使我们通过测试各种问题变化来评估LLMs的数学推理能力的稳健性。我们引入了对抗式小学数学（\datasetname）数据集，这是对GSM8K的扩展，并添加了各种数学扰动。我们对25个LLMs和4种提示技术进行的实验表明，虽然LLMs展现出不同水平的数学推理能力，但它们的表现远非稳健。特别是，即使是在GSM8K中已解决的问题，LLMs也可能出错。

    arXiv:2402.19255v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes 
    
[^7]: 利用大型语言模型通过讲故事学习复杂法律概念

    Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling

    [https://arxiv.org/abs/2402.17019](https://arxiv.org/abs/2402.17019)

    通过大型语言模型和故事讲述，本论文提出了一种新颖的法律教育方法，帮助非专业人士学习复杂的法律概念，并构建了一个包含法律故事和多项选择题的数据集。

    

    将法律知识变得更容易理解对于提升普通法律素养和鼓励公民参与民主至关重要。然而，对于没有法律背景的人来说，法律文件通常难以理解。本文提出了一种新颖的大型语言模型（LLMs）在法律教育中的应用，帮助非专业人士通过讲故事学习复杂的法律概念，讲故事是传达复杂和抽象概念的有效教学工具。我们还介绍了一个名为LegalStories的新数据集，其中包含295个复杂的法律原则，每个原则都附有一个故事和一组由LLMs生成的多项选择题。为了构建数据集，我们尝试使用各种LLMs生成解释这些概念的法律故事。此外，我们使用专家参与的方法来迭代设计多项选择题。然后，我们通过一

    arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
    
[^8]: GraphWiz：用于图问题的指令跟随语言模型

    GraphWiz: An Instruction-Following Language Model for Graph Problems

    [https://arxiv.org/abs/2402.16029](https://arxiv.org/abs/2402.16029)

    GraphWiz是一个开源语言模型，通过引入指令调优数据集和直接偏好优化框架，能够高效解决各种图问题类型，平均准确率达到65%，超过了GPT-4的43.8%。

    

    大型语言模型（LLMs）在多个领域取得了令人印象深刻的成功，但它们在理解和解决复杂图问题方面的能力尚未得到充分探索。为弥合这一差距，我们引入了GraphInstruct，这是一个新颖而全面的指令调优数据集，旨在为语言模型提供处理各种图问题的能力，利用明确的推理路径。利用GraphInstruct，我们构建了GraphWiz，这是一个能够解决各种图问题类型并生成清晰推理过程的开源语言模型。为增强模型的能力和可靠性，我们将直接偏好优化（DPO）框架纳入图问题求解环境中。增强模型GraphWiz-DPO在九个具有不同复杂性水平的任务中取得了65%的平均准确率，超过了平均准确率为43.8%的GPT-4。此外，我们的研究深入探讨了...

    arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
    
[^9]: 大型语言模型对齐的隐私保护指南

    Privacy-Preserving Instructions for Aligning Large Language Models

    [https://arxiv.org/abs/2402.13659](https://arxiv.org/abs/2402.13659)

    提出使用合成指南替换真实指南以增强隐私保护，并通过私密微调生成器生成此类合成指南，并通过新颖的过滤算法使合成指南的分布与真实指南一致，展示了在大型语言模型对齐中的高效用性。

    

    大型语言模型（LLM）应用的服务提供商在野外收集用户指南，并在进一步对齐LLM与用户意图中使用这些指南。这些潜在包含敏感信息的指南在流程中由人工工作者标注。这带来了新的隐私风险，而Typical Private Optimization没有解决这个问题。为此，我们提议使用合成指南替换数据标注和模型微调中的真实指南。通过使用私密微调生成器生成这些合成指南，可以确保形式差异隐私。在实现所需效用方面至关重要的是我们的新颖过滤算法，将合成指南的分布与实际指南的分布进行匹配。在有人反馈的受监督微调和强化学习中，我们的广泛实验表明，通过展示合成指南的最终集合的高效用性

    arXiv:2402.13659v1 Announce Type: cross  Abstract: Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by sho
    
[^10]: StyleDubber: 面向电影配音的多尺度风格学习

    StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing

    [https://arxiv.org/abs/2402.12636](https://arxiv.org/abs/2402.12636)

    StyleDubber提出了一种新的电影配音方法，通过在音素级别进行学习，解决了当前 V2C 模型中存在的音素发音不完整和身份稳定性差的问题。

    

    给定一份剧本，在电影配音（视觉语音克隆，V2C）中的挑战是根据参考音轨的语气，生成与视频在时间和情绪上都良好对齐的语音。现有的 V2C 模型根据视频帧间的间隔字断分割剧本的音素，这解决了时间对齐问题，但导致音素发音不完整和身份稳定性差。为了解决这个问题，我们提出 StyleDubber，它将配音学习从帧级别转为音素级别。它包含三个主要组件：（1）一个多模态风格适配器，以音素级别操作，从参考音频中学习发音风格，并生成受视频中呈现的面部情绪影响的中间表示；（2）一个以语句级别风格学习模块，引导中间表现的 mel-spectrogram 解码和细化过程。

    arXiv:2402.12636v1 Announce Type: new  Abstract: Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate e
    
[^11]: 我们应该交流吗：探索竞争LLM代理之间的自发合作

    Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents

    [https://arxiv.org/abs/2402.12327](https://arxiv.org/abs/2402.12327)

    该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。

    

    最近的进展表明，由大型语言模型（LLMs）驱动的代理具有模拟人类行为和社会动态的能力。然而，尚未研究LLM代理在没有明确指令的情况下自发建立合作关系的潜力。为了弥补这一空白，我们进行了三项案例研究，揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力。这一发现不仅展示了LLM代理模拟人类社会中竞争与合作的能力，也验证了计算社会科学的一个有前途的愿景。具体来说，这表明LLM代理可以用于建模人类社会互动，包括那些自发合作的互动，从而提供对社会现象的洞察。这项研究的源代码可在https://github.com/wuzengqing001225/SABM_ShallWe 找到。

    arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
    
[^12]: 在搜索训练数据与Transformer文本模型对抗性稳健性之间的相关性时的一个有趣案例

    A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models

    [https://arxiv.org/abs/2402.11469](https://arxiv.org/abs/2402.11469)

    本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。

    

    现有研究表明，经过微调的文本Transformer模型可以实现最先进的预测性能，但也容易受到对抗文本扰动的影响。传统的对抗性评估通常在对模型进行微调之后才进行，忽略了训练数据。本文旨在证明训练数据和模型鲁棒性之间也存在着强关联。为此，我们提取了代表广泛输入微调语料库属性的13种不同特征，并用它们来预测经过微调的模型的对抗性稳健性。我们主要关注仅编码器的Transformer模型BERT和RoBERTa，并附加了BART、ELECTRA和GPT2的其他结果，为我们的论点提供多样的证据。首先，经验证明，(a)提取的特征可与轻量级分类器（如随机森林）一起有效地预测攻击成功率。

    arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
    
[^13]: 自动评估方法在面向指令的LLM中有多可靠？

    How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?

    [https://arxiv.org/abs/2402.10770](https://arxiv.org/abs/2402.10770)

    本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。

    

    面向指令的大型语言模型(LLMs)的研究使用基于文本重叠和LLM判断的自动方法作为人工评估的成本有效替代方案。本文研究了这些方法在广泛的任务范围和跨语言环境中的可靠性。与先前的研究结果相反，我们观察到在任务类型不同的情况下，自动方法与人工评估者之间的相关性存在显著变化。具体而言，广泛使用的ROUGE-L度量在短答案英语任务中与人类判断强相关，但在自由形式生成任务和跨语言转移中不可靠。使用GPT-4作为评估员的有效性取决于在要求评估时包含参考答案，这可能导致在自由形式生成任务中评估过于严格。总的来说，我们发现，尽管自动评估方法可以近似人类判断，但其准确性可能因任务类型和评估设置而异。

    arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
    
[^14]: HGOT: 用于检索增强上下文学习中事实性评估的分层思维图

    HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation

    [https://arxiv.org/abs/2402.09390](https://arxiv.org/abs/2402.09390)

    HGOT是一种用于检索增强上下文学习中事实性评估的分层思维图方法，通过利用大型语言模型的规划能力和思维质量评估指标来提高相关段落的检索和答案选择。

    

    随着大型语言模型（LLMs）在许多应用中的广泛应用，事实性和幻觉的倾向引发了重大关切。为了解决这个问题，特别是在检索增强的上下文学习中，我们引入了分层思维图（HGOT），这是一种结构化的、多层次的图形方法，旨在增强在上下文学习过程中相关段落的检索。该框架利用LLMs的逐渐规划能力，采用分而治之的策略将复杂查询分解为可处理的子查询。它通过引入最近提出的引文回忆和精确度指标来评估思维质量，将答案的可信度与思维的质量内在地联系起来，从而改进了自洽性多数投票的答案选择方法。这种方法引入了一个加权系统，在多数投票中优先考虑答案。

    arXiv:2402.09390v1 Announce Type: new Abstract: With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers 
    
[^15]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^16]: 从大型语言模型中提取事件序列知识

    Distilling Event Sequence Knowledge From Large Language Models

    [https://arxiv.org/abs/2401.07237](https://arxiv.org/abs/2401.07237)

    本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。

    

    事件序列模型在事件的分析和预测中被发现是非常有效的。建立这样的模型需要丰富的高质量事件序列数据。然而，在某些应用中，干净的结构化事件序列不可用，自动化序列提取导致的数据太嘈杂和不完整。在这项工作中，我们探索了使用大型语言模型（LLMs）生成可以有效用于概率事件模型构建的事件序列的方法。这可以看作是从LLMs中提取事件序列知识的一种机制。我们的方法依赖于一个具有部分因果关系的事件概念的知识图（KG）来指导生成语言模型进行因果事件序列的生成。我们展示了我们的方法可以生成高质量的事件序列，填补了输入KG中的知识空白。此外，我们还探索了如何利用生成的序列来发现更有用和更复杂的内容。

    Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
    
[^17]: 使用眼动追踪研究母语为汉语的人对语言景观图像的注意力

    Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images

    [https://arxiv.org/abs/2312.08906](https://arxiv.org/abs/2312.08906)

    本文利用眼动追踪技术研究了母语为汉语的人对语言景观的注意点，发现其关注程度高于一般景观，可能是因为语言景观的信息密度较高。

    

    语言景观是社会语言学研究中的一个重要领域，眼动追踪技术是心理学研究中常用的技术。在对语言景观的研究中，很少使用眼动追踪来进行研究。本文利用眼动追踪技术研究了母语为汉语的人对语言景观的实际注意点，并发现在凝视时间和凝视次数这两个维度上，母语为汉语的人对语言景观的关注程度高于一般景观。本文认为这种现象是由于语言景观的信息密度较高。同时，本文还讨论了这一现象的其他可能原因。

    Linguistic landscape is an important field in sociolinguistic research. Eye tracking technology is a common technology in psychological research. There are few cases of using eye movement to study linguistic landscape. This paper uses eye tracking technology to study the actual fixation of the linguistic landscape and finds that in the two dimensions of fixation time and fixation times, the fixation of native Chinese speakers to the linguistic landscape is higher than that of the general landscape. This paper argues that this phenomenon is due to the higher information density of linguistic landscapes. At the same time, the article also discusses other possible reasons for this phenomenon.
    
[^18]: AutoRT：大规模编排机器人代理的具身基础模型

    AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])

    [http://arxiv.org/abs/2401.12963](http://arxiv.org/abs/2401.12963)

    AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。

    

    拥有语言、视觉和行动等功能的具身基础模型已经彻底改变了利用互联网规模的数据来推理有用任务的能力。然而，训练具身基础模型的一个关键挑战是缺乏基于物理世界的数据。在本文中，我们提出了AutoRT，一个利用现有的基础模型来扩展完全未知场景中操作机器人的部署的系统，只需要最少的人工监督。AutoRT利用视觉-语言模型(VLMs)实现场景理解和基础绑定，并进一步利用大型语言模型(LLMs)提出多样化和新颖的指令，供一组机器人执行。通过利用基础模型的知识来指导数据收集，AutoRT能够有效地推理自主权和安全性的权衡，同时显著扩大机器人学习的数据收集。我们演示了AutoRT向20多个机器人提议指令。

    Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
    
[^19]: 提升大规模语言模型在编码中的能力通过多角度自一致性

    Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])

    [http://arxiv.org/abs/2309.17272](http://arxiv.org/abs/2309.17272)

    本文提出了一个名为多角度自一致性（MPSC）的框架，用于提升大规模语言模型在复杂的代码生成任务中的性能。该框架通过从多个角度采样多个输出并构建一个多部分图，利用交叉一致性和内一致性信息来选择最优输出。

    

    大规模语言模型（LLMs）在文本生成方面展现了卓越的能力。然而，在复杂的推理任务，如代码生成中，LLMs仍然难以在一次尝试中生成正确的答案。先前的研究通过聚合多个输出，利用它们之间的一致性来探索解决方案。然而，这些研究没有全面地从不同的角度捕捉这种一致性。在本文中，我们提出了一种名为多角度自一致性（MPSC）框架的新的解码策略，用于LLM，它将来自多个角度的输出之间的交叉一致性和单个角度内的内一致性结合起来。具体而言，我们要求LLMs对给定查询从各个角度采样多个多样化的输出，并基于它们构建一个多部分图。通过两个预定义的一致性度量，我们将交叉一致性和内一致性信息嵌入到图中。最佳选择是根据这些一致性度量来选择输出。

    Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
    
[^20]: STANCE-C3: 通过对比学习和反事实生成进行领域自适应的跨目标立场检测

    STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation. (arXiv:2309.15176v1 [cs.CL])

    [http://arxiv.org/abs/2309.15176](http://arxiv.org/abs/2309.15176)

    STANCE-C3是一种通过对比学习和反事实生成进行领域自适应的跨目标立场检测模型，用于推断人们对于普遍或有争议话题的观点。在解决数据分布偏移和缺乏领域特定标注数据的挑战上具有重要贡献。

    

    立场检测是通过推断一个人在特定问题上的立场或观点，以推断对于普遍或有争议的话题的普遍看法，例如COVID-19疫情期间的健康政策。现有的立场检测模型在训练时往往在单个领域（例如COVID-19）和特定目标话题（例如口罩规定）上表现良好，但在其他领域或目标中往往表现不佳，这是由于数据的分布偏移。然而，构建高性能的领域特定立场检测模型需要大量与目标领域相关的已标注数据，但这样的数据集往往不容易获取。这就面临着一个挑战，因为标注数据的过程代价高昂且耗时。为了应对这些挑战，我们提出了一种新颖的立场检测模型，称为通过对比学习和反事实生成进行领域自适应的跨目标立场检测（STANCE-C3）。

    Stance detection is the process of inferring a person's position or standpoint on a specific issue to deduce prevailing perceptions toward topics of general or controversial interest, such as health policies during the COVID-19 pandemic. Existing models for stance detection are trained to perform well for a single domain (e.g., COVID-19) and a specific target topic (e.g., masking protocols), but are generally ineffectual in other domains or targets due to distributional shifts in the data. However, constructing high-performing, domain-specific stance detection models requires an extensive corpus of labeled data relevant to the targeted domain, yet such datasets are not readily available. This poses a challenge as the process of annotating data is costly and time-consuming. To address these challenges, we introduce a novel stance detection model coined domain-adaptive Cross-target STANCE detection via Contrastive learning and Counterfactual generation (STANCE-C3) that uses counterfactua
    
[^21]: 知识的知识：探索大型语言模型对未知-已知不确定性的理解

    Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])

    [http://arxiv.org/abs/2305.13712](http://arxiv.org/abs/2305.13712)

    本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。

    

    本文研究了大型语言模型（LLM）在理解自身知识和测量不确定性方面的能力，以缓解虚构现象。我们专门关注解决“已知-未知”问题，这种问题由于缺乏确定的答案而具有高度不确定性。为了促进我们的研究，我们收集了一个新的已知-未知问题（KUQ）数据集，并提出了一个新的分类方案来阐明不确定性的来源。随后，我们评估LLM区分已知和未知问题以及相应分类的能力。此外，我们在开放式QA环境中评估LLM的答案质量。为了量化答案中表达的不确定性，我们创建了一种语义评估方法，用于测量模型在表达已知vs未知问题的不确定性方面的准确性。

    This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
    

