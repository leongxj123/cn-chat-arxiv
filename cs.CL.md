# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning](https://arxiv.org/abs/2404.00459) | NumeroLogic提出了一种新的数字表示方法，通过在每个数字前包含数字的位数计数，为语言模型的数字推理能力提供了增强，从而改善了生成实际数字之前的推理过程。 |
| [^2] | [AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs](https://arxiv.org/abs/2403.15676) | 该论文引入了一种新方法，通过将算术电路约束编码为多项式方程系统，并通过代数计算在有限域上解决多项式方程系统，以精确定位ZKP电路中两种不同类型的错误。 |
| [^3] | [How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models](https://arxiv.org/abs/2403.02436) | 本研究探讨了建筑如何影响预训练语言模型的基本能力，发现了FFN-Wider变压器模型降低了多头注意力的贡献比，从而导致基本能力的下降。 |
| [^4] | [Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge](https://arxiv.org/abs/2403.01432) | 本文研究了微调和检索增强生成两种方法对大型语言模型在处理低频实体问题回答任务中的影响，发现微调显著提高了各种受欢迎程度的实体的性能，而检索增强生成方法则超过了其他方法。 |
| [^5] | [ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation](https://arxiv.org/abs/2402.12844) | 本文提出的ICON方法旨在通过改善放射学报告生成的报告间一致性，提升系统捕捉语义等效病变相似性的能力。 |
| [^6] | [An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference](https://arxiv.org/abs/2402.10712) | 通过实证研究，本文探讨了各种跨语言词汇适应方法对提高生成LLM推理效率的影响。 |
| [^7] | [Humans or LLMs as the Judge? A Study on Judgement Biases](https://arxiv.org/abs/2402.10669) | 提出了一种新框架来研究LLM和人类裁判的偏见，揭示人类和LLM裁判在面对干扰时的脆弱性，强调评估现有LLM性能的挑战。 |
| [^8] | [Recent Trends in Unsupervised Summarization.](http://arxiv.org/abs/2305.11231) | 本文综述了无监督摘要的最新技术和模型，包括抽取式、生成式和混合模型，并提出了一个基于方法的分类法。本文还介绍了一些数据集和评估方法。 |

# 详细

[^1]: NumeroLogic：增强LLMs数字推理的数字编码

    NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning

    [https://arxiv.org/abs/2404.00459](https://arxiv.org/abs/2404.00459)

    NumeroLogic提出了一种新的数字表示方法，通过在每个数字前包含数字的位数计数，为语言模型的数字推理能力提供了增强，从而改善了生成实际数字之前的推理过程。

    

    论文指出，语言模型在处理数值数据和执行算术运算时面临困难。我们假设这种限制部分归因于非直观的文本数字表示。为了解决这个问题，我们提出了一种简单的调整方法，即在每个数字前包含数字的位数计数。例如，我们建议使用"{2:42}"代替"42"作为新的格式。我们将这种方法称为NumeroLogic，它在数字生成中作为“思维链”提供了额外优势。通过要求模型首先考虑数字的位数，它增强了生成实际数字之前的推理过程。我们使用算术任务来展示NumeroLogic格式的有效性。

    arXiv:2404.00459v1 Announce Type: new  Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstr
    
[^2]: AC4：用于ZKP中电路约束的代数计算检查器

    AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs

    [https://arxiv.org/abs/2403.15676](https://arxiv.org/abs/2403.15676)

    该论文引入了一种新方法，通过将算术电路约束编码为多项式方程系统，并通过代数计算在有限域上解决多项式方程系统，以精确定位ZKP电路中两种不同类型的错误。

    

    ZKP系统已经引起了人们的关注，在当代密码学中发挥着基础性作用。 Zk-SNARK协议主导了ZKP的使用，通常通过算术电路编程范式实现。然而，欠约束或过约束的电路可能导致错误。 欠约束的电路指的是缺乏必要约束的电路，导致电路中出现意外解决方案，并导致验证者接受错误见证。 过约束的电路是指约束过度的电路，导致电路缺乏必要的解决方案，并导致验证者接受没有见证，使电路毫无意义。 本文介绍了一种新方法，用于找出ZKP电路中两种不同类型的错误。 该方法涉及将算术电路约束编码为多项式方程系统，并通过代数计算在有限域上解决多项式方程系统。

    arXiv:2403.15676v1 Announce Type: cross  Abstract: ZKP systems have surged attention and held a fundamental role in contemporary cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. Underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. Overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. T
    
[^3]: 建筑如何影响预训练语言模型的基本能力？基于FFN-Wider变压器模型的案例研究

    How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models

    [https://arxiv.org/abs/2403.02436](https://arxiv.org/abs/2403.02436)

    本研究探讨了建筑如何影响预训练语言模型的基本能力，发现了FFN-Wider变压器模型降低了多头注意力的贡献比，从而导致基本能力的下降。

    

    预训练语言模型已被证明具有强大的基本能力，不仅在分布式语言建模方面表现出色，而且在超出分布式语言建模、迁移学习和少样本学习方面也展现出强大的能力。与现有研究侧重于规模对基本能力的影响不同，我们的工作将重点放在了架构对其影响。具体地，我们关心的是：建筑如何影响预训练语言模型的基本能力？在这项工作中，我们试图解释并逆转FFN-Wider变压器的架构导致基本能力下降的情况，力求提供一些见解。通过分析，我们发现多头注意力（一种组合函数）对预训练语言建模的贡献比是影响基本能力的关键因素。FFN-Wider变压器减少了这种组合函数的贡献比，导致一种

    arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d
    
[^4]: 微调与检索增强生成用于不太流行知识的比较

    Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge

    [https://arxiv.org/abs/2403.01432](https://arxiv.org/abs/2403.01432)

    本文研究了微调和检索增强生成两种方法对大型语言模型在处理低频实体问题回答任务中的影响，发现微调显著提高了各种受欢迎程度的实体的性能，而检索增强生成方法则超过了其他方法。

    

    大型语言模型（LLMs）记忆了大量的事实知识，在各种任务和领域表现出色。然而，观察到当处理不太流行或低频概念和实体时，性能会下降，例如在领域特定应用中。本文探讨和评估了检索增强生成（RAG）和通过合成数据进行微调（FT）对定制LLMs处理低频实体问题回答任务的影响。研究结果表明，FT显著提升了各种受欢迎程度的实体的性能，特别是在最受欢迎和最不受欢迎的群体中，而RAG超越了其他方法。另外，检索和数据增强技术的进步加强了RAG和FT方法的成功。

    arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
    
[^5]: ICON：通过病变感知混合增强改善放射学报告生成的报告间一致性

    ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation

    [https://arxiv.org/abs/2402.12844](https://arxiv.org/abs/2402.12844)

    本文提出的ICON方法旨在通过改善放射学报告生成的报告间一致性，提升系统捕捉语义等效病变相似性的能力。

    

    放射学报告生成的先前研究在增加生成报告的临床准确性方面取得了显著进展。本文强调了其应具备的另一个至关重要的特质，即报告间一致性，指的是对语义上等效的X射线照片生成一致性报告的能力。ICON提出了一种方法，它通过改善放射学报告生成的报告间一致性来解决这一问题。

    arXiv:2402.12844v1 Announce Type: cross  Abstract: Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesion
    
[^6]: 一项关于跨语言词汇适应用于高效生成LLM推理的实证研究

    An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference

    [https://arxiv.org/abs/2402.10712](https://arxiv.org/abs/2402.10712)

    通过实证研究，本文探讨了各种跨语言词汇适应方法对提高生成LLM推理效率的影响。

    

    arXiv:2402.10712v1 通告类型: 跨领域 摘要: 最先进的生成大型语言模型(LLMs)的发展在很大程度上依赖于英语为中心的分词器、词汇和预训练数据。尽管一些LLMs具有多语言能力，但最近的研究表明，当生成英语以外的其他语言时，它们的推理效率会下降。这导致推理时间和成本增加。已经提出了跨语言词汇适应方法，用于将模型调整到目标语言，旨在提高下游性能。然而，这些方法对提高生成LLM推理效率的有效性尚未得到探究。在本文中，我们对五种生成LLMs（包括单语和多语模型）在四种语言类型多样且四种自然语言理解任务上进行了各种跨语言词汇适应方法的实证研究。

    arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
    
[^7]: 人类还是大型语言模型作为裁判？一项关于判决偏见的研究

    Humans or LLMs as the Judge? A Study on Judgement Biases

    [https://arxiv.org/abs/2402.10669](https://arxiv.org/abs/2402.10669)

    提出了一种新框架来研究LLM和人类裁判的偏见，揭示人类和LLM裁判在面对干扰时的脆弱性，强调评估现有LLM性能的挑战。

    

    采用人类和大型语言模型（LLM）作为裁判（即人类和LLM作为裁判）来评估现有LLM性能的做法近来备受关注。然而，这种方法同时可能引入人类和LLM裁判的潜在偏见，质疑评估结果的可靠性。本文提出了一种新颖的框架，用于研究LLM和人类裁判的5种偏见。我们整理了一个包含142个样本的数据集，涉及修订的布卢姆分类法，并进行了成千上万次的人类和LLM评估。结果表明，人类和LLM裁判在不同程度上都容易受到干扰，即使最尖端的裁判也存在相当大的偏见。我们进一步利用他们的弱点对LLM裁判进行攻击。希望我们的工作能提醒社群关于人类和LLM作为裁判在面对干扰时的脆弱性，以及发展的紧迫性。

    arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
    
[^8]: 无监督摘要的最新趋势

    Recent Trends in Unsupervised Summarization. (arXiv:2305.11231v1 [cs.CL])

    [http://arxiv.org/abs/2305.11231](http://arxiv.org/abs/2305.11231)

    本文综述了无监督摘要的最新技术和模型，包括抽取式、生成式和混合模型，并提出了一个基于方法的分类法。本文还介绍了一些数据集和评估方法。

    

    无监督摘要是一种强大的技术，可以在不需要标记数据集的情况下训练摘要模型。本综述涵盖了用于无监督摘要的不同技术和模型。我们涵盖了抽取式、生成式和混合模型以及用于实现无监督摘要的策略。尽管本综述的主要重点是最新研究，但我们也介绍了一些重要的以前的研究。我们还引入了一个分类法，根据研究对无监督训练的方法进行分类。最后，我们讨论了当前的方法，并提到了一些数据集和评估方法。

    Unsupervised summarization is a powerful technique that enables training summarizing models without requiring labeled datasets. This survey covers different recent techniques and models used for unsupervised summarization. We cover extractive, abstractive, and hybrid models and strategies used to achieve unsupervised summarization. While the main focus of this survey is on recent research, we also cover some of the important previous research. We additionally introduce a taxonomy, classifying different research based on their approach to unsupervised training. Finally, we discuss the current approaches and mention some datasets and evaluation methods.
    

