# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TraveLER: A Multi-LMM Agent Framework for Video Question-Answering](https://arxiv.org/abs/2404.01476) | TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题 |
| [^2] | [GPT-4 Understands Discourse at Least as Well as Humans Do](https://arxiv.org/abs/2403.17196) | GPT-4在标准化语篇理解测试中表现出与人类相当的能力，尤其在推断未明确陈述信息方面显示出显著实力 |
| [^3] | [Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models](https://arxiv.org/abs/2403.14859) | 通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。 |
| [^4] | [Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines](https://arxiv.org/abs/2403.05846) | 提出了一种分析文本到图像模型中文本编码器的方法，并通过生成中间表示的图像来深入研究，揭示了在复合提示和知识检索方面的一些重要发现。 |
| [^5] | [Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization](https://arxiv.org/abs/2403.00067) | 本研究旨在通过将相同输入上下文的查询组合为单个提示，以最小化重复调用来优化使用大型语言模型在会议摘要中的推理。 |
| [^6] | [Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models](https://arxiv.org/abs/2402.15518) | 评估会话式大型语言模型生成文本的语言特征以及这些特征如何取决于模型参数是理解其潜在影响的关键一步。 |
| [^7] | [Mitigating Biases of Large Language Models in Stance Detection with Calibration](https://arxiv.org/abs/2402.14296) | 本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。 |
| [^8] | [CriticBench: Evaluating Large Language Models as Critic](https://arxiv.org/abs/2402.13764) | CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。 |
| [^9] | [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116) | 本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。 |
| [^10] | [SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning](https://arxiv.org/abs/2402.12806) | SymBa提出了一种符号化向后推理方法，在多步自然语言推理中取得了显著的性能和效率提升，能够生成可解释的结构化证明。 |
| [^11] | [What Changed? Converting Representational Interventions to Natural Language](https://arxiv.org/abs/2402.11355) | 将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。 |
| [^12] | [Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification](https://arxiv.org/abs/2402.10940) | 研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。 |
| [^13] | [Tandem Transformers for Inference Efficient LLMs](https://arxiv.org/abs/2402.08644) | 该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。 |
| [^14] | [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900) | 本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。 |
| [^15] | [SpiRit-LM: Interleaved Spoken and Written Language Model](https://arxiv.org/abs/2402.05755) | SPIRIT-LM是一个基于预训练文本语言模型的多模态语言模型，通过将文本和语音连续训练，实现了口语和书面语言的混合模型。它展示了文本模型的语义能力和语音模型的表现能力。此外，SPIRIT-LM还能以少量样本的方式学习新任务。 |
| [^16] | [INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://arxiv.org/abs/2402.03744) | 该论文提出了一种在LLMs内部状态中保留密集语义信息的方法来进行幻觉检测。通过使用EigenScore度量方法评估回答的自洽性，并探索测试时间的特征剪切方法，以减少过度自信的估计。 |
| [^17] | [Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning](https://arxiv.org/abs/2401.17686) | 本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。 |
| [^18] | [Self-Contradictory Reasoning Evaluation and Detection](https://arxiv.org/abs/2311.09603) | 研究了大型语言模型在推理任务中自相矛盾的现象，发现在涉及上下文信息理解或常识的任务中经常存在自相矛盾，而高准确性并不总是对应较低的自相矛盾率。 |
| [^19] | [Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators](https://arxiv.org/abs/2311.07879) | 本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。 |
| [^20] | [Benchmarking LLM-based Machine Translation on Cultural Awareness](https://arxiv.org/abs/2305.14328) | 介绍了一个新的数据整理流程来构建文化相关的平行语料库，并设计了一种新的评估指标，通过GPT-4无参考评估翻译的可理解性。 |
| [^21] | [The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models.](http://arxiv.org/abs/2401.05618) | 本文研究了在大型语言模型中使用简洁的思维链提示对问题求解的影响，实验结果表明简洁性不仅降低了回答长度，且对问题解决性能影响可以忽略。然而在数学问题上有一定的性能下降。这对AI系统工程师和研究人员都有实际意义。 |
| [^22] | [Aligning Translation-Specific Understanding to General Understanding in Large Language Models.](http://arxiv.org/abs/2401.05072) | 这项研究旨在解决大型语言模型在机器翻译中的性能限制问题，通过提出一种跨语言解释困难词的新方法来对齐翻译特定理解和一般理解。 |
| [^23] | [Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence.](http://arxiv.org/abs/2309.14379) | 本研究提出了一种机器辅助的混合方法框架，利用大规模语言模型在人文社科领域的数据分析中的应用潜力，展示了16个案例研究，并涵盖了多种任务，包括语言分析、文本挖掘、社交网络推断等。 |
| [^24] | [Persona-aware Generative Model for Code-mixed Language.](http://arxiv.org/abs/2309.02915) | 本论文提出了一种针对混合语言的人物感知生成模型PARADOX，它能够生成类似于真实个体代码混合文本的文本。该模型以用户的人物形象为条件来编码对话，并生成不带单语参考数据的代码混合文本。模型还进行对齐，使生成的文本更接近真实的代码混合文本。这种方法在语义上更有意义，在语言上更有效。 |
| [^25] | [Reverse Stable Diffusion: What prompt was used to generate this image?.](http://arxiv.org/abs/2308.01472) | 本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。 |
| [^26] | [Enhancing Robustness of AI Offensive Code Generators via Data Augmentation.](http://arxiv.org/abs/2306.05079) | 本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。 |
| [^27] | [Deep Emotion Recognition in Textual Conversations: A Survey.](http://arxiv.org/abs/2211.09172) | 本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。 |

# 详细

[^1]: TraveLER：用于视频问答的多重LMM代理框架

    TraveLER: A Multi-LMM Agent Framework for Video Question-Answering

    [https://arxiv.org/abs/2404.01476](https://arxiv.org/abs/2404.01476)

    TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题

    

    最近，大型多模态模型（LMMs）在视频问答方面取得了重要进展，通过利用大规模、基于图像的预训练以零样本方式以帧为单位进行处理。虽然基于图像的视频方法展现了令人印象深刻的性能，但目前的局限是它们经常忽视了如何选择关键时间戳，并且无法在确定错误时间戳时进行调整。此外，它们无法提取与问题相关的细节，而是提供帧的一般描述。为了克服这一点，我们设计了一个多重LMM代理框架，它沿着视频进行移动，通过交互式提问的方式迭代地从关键帧收集相关信息，直到获得足够的信息来回答问题。具体来说，我们提出了TraveLER，这是一个可以制定“遍历”视频计划的模型，询问关于单个帧的问题以“定位”并存储关键信息

    arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
    
[^2]: GPT-4至少能够像人类一样理解语篇

    GPT-4 Understands Discourse at Least as Well as Humans Do

    [https://arxiv.org/abs/2403.17196](https://arxiv.org/abs/2403.17196)

    GPT-4在标准化语篇理解测试中表现出与人类相当的能力，尤其在推断未明确陈述信息方面显示出显著实力

    

    我们测试了一种领先的AI系统GPT-4是否像人类一样理解语篇，使用了一项标准化的语篇理解测试。参与者会被呈现简短的故事，然后回答八个是/否问题，探究他们对故事的理解。这些问题的格式旨在评估直接性（陈述 vs. 暗示）和显著性（主要观点 vs. 细节）的独立影响。鉴于人类表现水平非常高，GPT-4的表现略好于人类，但并无统计学显著差异。GPT-4和人类都表现出强大的能力，能够推断故事中未明确陈述的信息，这是对理解力的重要测试。

    arXiv:2403.17196v1 Announce Type: new  Abstract: We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.
    
[^3]: 在基础模型和指令调优的大型语言模型中比较可信度估计

    Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models

    [https://arxiv.org/abs/2403.14859](https://arxiv.org/abs/2403.14859)

    通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。

    

    指令调优的LLM可以响应明确制定为提示的查询，这极大地促进了与人类用户的交互。然而，基于提示的方法可能并不总是能够利用LLM在预训练期间获得的隐式知识。本文对评估LLM中语义可信度的方法进行了全面研究。我们通过（a）明确提示和（b）直接读取模型分配给字符串的概率的隐式估计，在英语句子可信度任务中比较了基础和指令调优LLM的性能。实验1表明，跨模型架构和可信度数据集，（i）对数似然（LL）分数是句子可信度最可靠的指标，零照射提示产生不一致且通常效果不佳的结果；（ii）基于LL的性能仍低于人类表现；（iii）指令调优模型有

    arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
    
[^4]: 扩散镜头：解释文本编码器在文本到图像管道中的作用

    Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines

    [https://arxiv.org/abs/2403.05846](https://arxiv.org/abs/2403.05846)

    提出了一种分析文本到图像模型中文本编码器的方法，并通过生成中间表示的图像来深入研究，揭示了在复合提示和知识检索方面的一些重要发现。

    

    arXiv:2403.05846v1 通告类型：跨 存在文本到图像扩散模型（T2I）使用文本提示的潜在表示来引导图像生成过程。然而，编码器产生文本表示的过程是未知的。我们提出了扩散镜头，一种分析 T2I 模型文本编码器的方法，通过生成其中间表示的图像。使用扩散镜头，我们对两个最近的 T2I 模型进行了广泛分析。在探索复合提示时，我们发现描述多个对象的复杂场景相对于简单场景是逐步且较慢地构建的；在探索知识检索时，我们发现表示不常见概念需要比常见概念更多的计算，并且知识检索在层之间是渐进的。总体而言，我们的发现为 T2I 管道中的文本编码器组件提供了宝贵的见解。

    arXiv:2403.05846v1 Announce Type: cross  Abstract: Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
    
[^5]: Query-OPT：通过多查询指令优化大型语言模型在会议摘要中的推理

    Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization

    [https://arxiv.org/abs/2403.00067](https://arxiv.org/abs/2403.00067)

    本研究旨在通过将相同输入上下文的查询组合为单个提示，以最小化重复调用来优化使用大型语言模型在会议摘要中的推理。

    

    这项工作关注基于查询的会议摘要任务，在此任务中，针对特定查询对上下文（会议记录）生成摘要。使用大型语言模型（LLMs）进行此任务时，即使上下文保持不变，每个新查询也需要对LLM推理端点/API进行一次新调用。然而，反复调用LLM推理端点会显著增加在生产中使用它们的成本，这使得许多实际用例中LLMs都不切实际。为解决这一问题，在本文中，我们研究了是否可以成功地将相同输入上下文的查询组合为单个提示以最小化重复调用，在会议摘要中使用。在这方面，我们通过比较各种流行的LLM（GPT-4、PaLM-2、LLaMA-2、Mistral和FLAN-T5）在单查询和多查询设置中的表现进行了广泛实验。

    arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
    
[^6]: 当心言辞：评估会话式大型语言模型的词汇丰富度

    Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models

    [https://arxiv.org/abs/2402.15518](https://arxiv.org/abs/2402.15518)

    评估会话式大型语言模型生成文本的语言特征以及这些特征如何取决于模型参数是理解其潜在影响的关键一步。

    

    在很多不同任务中正在评估会话式大型语言模型（LLMs）的性能，特别是ChatGPT，从逻辑推理或数学到回答各种主题的问题。然而，对这些LLMs生成的文本的语言特征的研究却少之又少。这是令人惊讶的，因为LLMs是语言模型，了解它们如何使用语言是重要的。事实上，会话式LLMs可能对语言的演变产生重要影响，因为它们最终可能主导新文本的创作。因此，评估它们生成的文本的语言特征以及这些特征如何取决于模型参数是了解潜在影响的第一步。

    arXiv:2402.15518v1 Announce Type: new  Abstract: The performance of conversational Large Language Models (LLMs) in general, and of ChatGPT in particular, is currently being evaluated on many different tasks, from logical reasoning or maths to answering questions on a myriad of topics. Instead, much less attention is being devoted to the study of the linguistic features of the texts generated by these LLMs. This is surprising since LLMs are models for language, and understanding how they use the language is important. Indeed, conversational LLMs are poised to have a significant impact on the evolution of languages as they may eventually dominate the creation of new text. This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether. Therefore, evaluating the linguistic features of the text they produce and how those depend on the model parameters is the first step toward understanding the potential im
    
[^7]: 在立场检测中通过校准减轻大语言模型的偏见

    Mitigating Biases of Large Language Models in Stance Detection with Calibration

    [https://arxiv.org/abs/2402.14296](https://arxiv.org/abs/2402.14296)

    本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。

    

    大语言模型（LLMs）在许多自然语言处理任务中取得了显著进展。然而，我们的实验证明，在立场检测任务中，LLMs可能会生成偏见立场，这是由于虚假情感-立场相关性和对某些个人和主题的偏好，从而损害了它们的性能。因此，在本文中，我们提出通过校准来减轻LLMs在立场检测中的偏见（MB-Cal）。在其中，设计了一种新颖的门控校准网络，以减轻LLMs产生的立场推理结果上的偏见。此外，为了使校准更准确和可推广，我们构建了反事实增强数据来矫正立场偏见。针对目标和零射击立场检测任务的实验结果表明，所提出的MB-Cal可以有效减轻LLMs的偏见，取得了最先进的结果。

    arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
    
[^8]: CriticBench: 将大型语言模型作为评论家进行评估

    CriticBench: Evaluating Large Language Models as Critic

    [https://arxiv.org/abs/2402.13764](https://arxiv.org/abs/2402.13764)

    CriticBench是一个旨在全面和可靠地评估大型语言模型的评论能力的新型基准，展示了评论能力与任务、响应质量和模型规模之间的关系。

    

    论文提出了 CriticBench，这是一个旨在全面和可靠地评估大型语言模型（LLMs）的四个关键评论能力维度（反馈、比较、改进和元反馈）的新型基准。CriticBench包含九个不同的任务，每个任务评估LLMs在不同质量细粒度水平上评论响应的能力。对开源和闭源LLMs进行的广泛评估揭示了评论能力与任务、响应质量和模型规模之间有趣的关系。CriticBench的数据集、资源和评估工具包将在https://github.com/gmftbyGMFTBY/Cri上公开发布。

    arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
    
[^9]: 对大型语言模型知识蒸馏的调查

    A Survey on Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2402.13116](https://arxiv.org/abs/2402.13116)

    本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。

    

    本调查对大型语言模型（LLMs）领域中知识蒸馏（KD）技术进行了深入探讨，重点关注KD在将诸如GPT-4之类的专有巨头的复杂能力转移到可访问的开源模型（如LLaMA和Mistral）中起着关键作用。在不断发展的人工智能领域，本项工作阐明了专有和开源LLMs之间的关键差异，展示了KD如何成为第二者赋予第一者先进功能和细致理解的重要媒介。我们的调查围绕算法、技能和垂直化这三个基础支柱精心构建，全面探讨了KD机制、特定认知能力的增强以及它们在不同领域的实际影响。重要的是，调查引导着数据增强（DA）和KD之间错综复杂的相互作用。

    arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
    
[^10]: SymBa：符号化向后推理用于多步自然语言推理

    SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning

    [https://arxiv.org/abs/2402.12806](https://arxiv.org/abs/2402.12806)

    SymBa提出了一种符号化向后推理方法，在多步自然语言推理中取得了显著的性能和效率提升，能够生成可解释的结构化证明。

    

    最近大型语言模型（LLMs）展示了在一系列思维提示中出色的推理能力，但忠实的多步推理依然是一个挑战。我们专注于向后推理，即通过逻辑规则递归地分解查询，直到证明为止。为了解决当前向后推理实现的局限性，我们提出了SymBa（符号化向后推理）。在SymBa中，符号化自顶向下求解器控制整个证明过程，当求解器遇到死胡同时，才调用LLM生成单个推理步骤。通过这种新颖的求解器-LLM集成，SymBa在各种多步推理基准（ProofWriter，Birds-Electricity，GSM8k，CLUTRR-TF，ECtHR Article 6）中相比向后推理基线取得了性能、证明忠实性和效率显著提高，能够生成可解释的结构化证明。

    arXiv:2402.12806v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.
    
[^11]: 改变了什么？将表征干预转化为自然语言

    What Changed? Converting Representational Interventions to Natural Language

    [https://arxiv.org/abs/2402.11355](https://arxiv.org/abs/2402.11355)

    将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。

    

    针对语言模型（LMs）表征空间的干预方法已经被证明是影响模型行为的有效手段。这些方法被用来消除或改变模型表示中的人口统计信息（如性别）的编码，创建一个反事实的表示。然而，由于干预操作在表示空间内，准确理解它修改了哪些特征是一个挑战。我们展示了表征空间的反事实可以转化为自然语言的反事实。我们证明了这种方法使我们能够分析对应于给定表示空间干预的语言变化，并解释用于编码特定概念的特征。此外，由此产生的反事实可以用于减轻分类中的偏见。

    arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
    
[^12]: 临床程序代码的神经机器翻译用于医学诊断和不确定性量化

    Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification

    [https://arxiv.org/abs/2402.10940](https://arxiv.org/abs/2402.10940)

    研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。

    

    临床决策支持系统（CDSS）旨在通过将系统生成的建议与医学专业知识结合来增强临床医生的决策能力。本研究引入了医学熵的概念，通过基于手术ICD-9代码的神经机器翻译来量化患者预测结果中的不确定性。我们的实验结果不仅展示了程序代码与实际医疗结果之间的强相关性，

    arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
    
[^13]: 用于推断高效LLMs的串联Transformer

    Tandem Transformers for Inference Efficient LLMs

    [https://arxiv.org/abs/2402.08644](https://arxiv.org/abs/2402.08644)

    该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。

    

    传统的大型语言模型( LLMs )具有自回归的特性，这使得推断速度受到限制，因为词元是按顺序生成的。尽管有些预测和并行解码技术试图减轻这个问题，但它们都有限制：要么依赖更精简但准确度较低的模型进行生成，要么没有充分利用基础LLM的表示。我们提出了一种新颖的架构，即串联Transformer，来解决这些问题。这种架构独特地结合了(1)一个小型自回归模型和(2)一个以块模式运行的大模型(同时处理多个词元)。通过让小模型关注大模型更丰富的表示，大幅提升小模型的预测准确性。在PaLM2预训练数据集上，PaLM2-Bison和PaLM2-Gecko的串联相较独立的PaLM2-Gecko，在下一个词元预测准确性上提升了3.3%，与具有相似下游任务的PaLM2-Otter模型相比，提供了1.16倍的加速比。

    The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
    
[^14]: LLM能够识别毒性吗？结构化毒性调查框架和基于语义的度量

    Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric

    [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)

    本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。

    

    在开发遵守社会标准的大型语言模型（LLMs）的过程中，识别生成文本中的毒性存在至关重要。现有的大多数毒性度量依赖于在特定毒性数据集上训练的编码模型。然而，这些编码器容易受到分布外的问题的影响，并且依赖于数据集中所假定的毒性定义。本文介绍了一种基于LLMs的自动鲁棒度量，用于区分模型回应是否具有毒性。我们首先分析了毒性因素，然后研究了LLMs的内在毒性属性，以确定它们作为评估器的适用性。随后，我们对评估数据集上的度量指标LLMs As ToxiciTy Evaluators（LATTE）进行了评估。实证结果表明，在不进行训练过程的情况下，我们的度量在测量毒性方面表现出色，F1得分比现有技术指标提高了12个百分点。我们还展示了上游毒性对度量结果的影响。

    In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
    
[^15]: SpiRit-LM: 交织的口语和书面语言模型

    SpiRit-LM: Interleaved Spoken and Written Language Model

    [https://arxiv.org/abs/2402.05755](https://arxiv.org/abs/2402.05755)

    SPIRIT-LM是一个基于预训练文本语言模型的多模态语言模型，通过将文本和语音连续训练，实现了口语和书面语言的混合模型。它展示了文本模型的语义能力和语音模型的表现能力。此外，SPIRIT-LM还能以少量样本的方式学习新任务。

    

    我们引入了SPIRIT-LM，这是一个基于文本和语音自由混合的多模态语言模型。我们的模型基于预训练的文本语言模型，并通过连续在文本和语音单元上进行训练将其扩展到语音模态。语音和文本序列被连接为一组单词，并使用一个小型自动筛选的语音-文本平行语料库来进行词级交织的训练方法。SPIRIT-LM有两个版本：一个是使用语音语义单元的BASE版本，另一个是在语义单元之外还使用了音高和风格单元来模拟表现力的EXPRESSIVE版本。对于这两个版本，文本是用子词BPE标记编码的。结果模型展示了文本模型的语义能力和语音模型的表现能力。此外，我们还证明了SPIRIT-LM能够在跨模态（即ASR、TTS、语音分类）中以少量样本的方式学习新任务。

    We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).
    
[^16]: INSIDE: LLMs的内部状态保留了幻觉检测的能力

    INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection

    [https://arxiv.org/abs/2402.03744](https://arxiv.org/abs/2402.03744)

    该论文提出了一种在LLMs内部状态中保留密集语义信息的方法来进行幻觉检测。通过使用EigenScore度量方法评估回答的自洽性，并探索测试时间的特征剪切方法，以减少过度自信的估计。

    

    知识幻觉对部署的LLMs的安全性和可靠性提出了广泛关注。先前的努力主要集中在对幻觉的检测上，采用了逻辑级别的不确定性估计或语言级别的自洽性评估，在解码过程中不可避免地丢失了语义信息。因此，我们提出探索LLMs内部状态中保留的密集语义信息用于幻觉检测（INSIDE）。具体而言，我们提出了一种简单而有效的EigenScore度量方法，以更好地评估回答的自洽性，它利用回答的协方差矩阵的特征值来衡量密集嵌入空间中的语义一致性/多样性。此外，从自洽的幻觉检测角度出发，我们探索了一种测试时间的特征剪切方法，用于截断内部状态中的极端激活，以减少过度自信的估计。

    Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident g
    
[^17]: 推理束搜索：为链式思维推断寻找可推导的理由

    Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning

    [https://arxiv.org/abs/2401.17686](https://arxiv.org/abs/2401.17686)

    本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。

    

    最近的研究通过各种方法，尤其是链式思维推理，极大增强了大型语言模型（LLMs）的推理能力。然而，以往的方法未能解决中间步骤的推理错误问题，导致错误的累积。本文提出了一种称为推理束搜索（DBS）的方法，它将链式思维和演绎推理与逐步束搜索无缝集成到LLMs中。我们的方法部署了一个验证器，用于验证推理步骤及其前提的可推导性，从而减少错误的累积。此外，我们引入了一种可扩展且无需人工劳动的数据构建方法，来增强我们模型的验证能力。广泛的实验证明，我们的方法显著提升了各种规模的LLMs（7B、13B、70B和ChatGPT）的基础性能，在3种不同的推理场景（算术、常识和符号）的8个推理数据集中都表现出色。此外，我们的分析证明了

    Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
    
[^18]: 自相矛盾推理评估与检测

    Self-Contradictory Reasoning Evaluation and Detection

    [https://arxiv.org/abs/2311.09603](https://arxiv.org/abs/2311.09603)

    研究了大型语言模型在推理任务中自相矛盾的现象，发现在涉及上下文信息理解或常识的任务中经常存在自相矛盾，而高准确性并不总是对应较低的自相矛盾率。

    

    在最近的大量工作中，大型语言模型展示了令人印象深刻的推理能力，但许多提出的下游推理任务主要关注性能评估。然而，仍然存在两个基本问题：1）推理质量有多可靠，2）模型能否检测到不可靠的推理？本文研究了自相矛盾（Self-Contra）推理，即模型推理不支持预测的情况。为了解决第一个问题，我们评估了四个数据集中的Self-Contra率，并深入探讨了自相矛盾推理的更细粒度类别。我们发现，大型语言模型在进行涉及上下文信息理解或常识的推理任务时经常自相矛盾。重要的是，更高的准确性并不一定对应更低的自相矛盾率。模型可能会产生正确答案，但在推理过程中可能会采取捷径或忽略上下文证据。

    arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa
    
[^19]: 毒性检测并不是你所需要的全部：弥合支持志愿内容管理员的差距

    Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators

    [https://arxiv.org/abs/2311.07879](https://arxiv.org/abs/2311.07879)

    本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。

    

    人工智能模型在识别有毒、冒犯和令人讨厌的内容方面取得了长足的进展，旨在减轻管理员的工作负担。然而，目前尚不清楚这些任务的改进是否真正满足了管理员在工作中的需求。本文揭示了过去研究努力致力于为内容管理的各个方面提供自动化支持与志愿内容管理员的需求之间存在的差距，尤其是在识别违反各种管理规则方面。为此，我们在Hugging Face上对模型进行了调查，以揭示涵盖三个示范论坛的各种管理规则和指南的模型的可用性。我们进一步对最先进的LLM进行了测试，评估这些模型在标记某个特定论坛的平台规则违规方面的表现。最后，我们进行了用户调查研究。

    arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
    
[^20]: 在文化意识上基于LLM的机器翻译基准测试

    Benchmarking LLM-based Machine Translation on Cultural Awareness

    [https://arxiv.org/abs/2305.14328](https://arxiv.org/abs/2305.14328)

    介绍了一个新的数据整理流程来构建文化相关的平行语料库，并设计了一种新的评估指标，通过GPT-4无参考评估翻译的可理解性。

    

    翻译文化特定内容对于有效的跨文化沟通至关重要。然而，许多机器翻译系统仍然难以准确理解和翻译包含文化特定实体的句子。最近关于上下文学习的进展利用轻量级提示指导大型语言模型(LLMs)在机器翻译任务中。然而，这种方法在提高具有文化意识的机器翻译的有效性方面仍然存在不确定性。为了填补这一空白，我们引入了一个新的数据整理流程，构建了一个包含文化相关并丰富了文化特定项目注释的平行语料库。此外，我们设计了一种新颖的评估指标，通过GPT-4以无参考方式评估翻译的可理解性。我们使用我们的数据集评估了各种神经机器翻译(NMT)和LLM-based MT系统。此外，我们提出了几种提示策略。

    arXiv:2305.14328v2 Announce Type: replace  Abstract: Translating cultural-specific content is crucial for effective cross-cultural communication. However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably. Recent advancements in in-context learning utilize lightweight prompts to guide large language models (LLMs) in machine translation tasks. Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain. To address this gap, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific items. Furthermore, we devise a novel evaluation metric to assess the understandability of translations in a reference-free manner by GPT-4. We evaluate a variety of neural machine translation (NMT) and LLM-based MT systems using our dataset. Additionally, we propose several prompting strategies
    
[^21]: 在大型语言模型的问题求解中，简洁的思维链的好处

    The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])

    [http://arxiv.org/abs/2401.05618](http://arxiv.org/abs/2401.05618)

    本文研究了在大型语言模型中使用简洁的思维链提示对问题求解的影响，实验结果表明简洁性不仅降低了回答长度，且对问题解决性能影响可以忽略。然而在数学问题上有一定的性能下降。这对AI系统工程师和研究人员都有实际意义。

    

    在本文中，我们介绍了简洁的思维链(CCoT)提示。我们将标准的CoT和CCoT提示进行比较，以了解简洁性对回答长度和正确答案准确性的影响。我们使用GPT-3.5和GPT-4进行了多项选择问答(MCQA)基准的评估。CCoT将GPT-3.5和GPT-4的平均回答长度分别减少了48.70％，对问题解决性能几乎没有影响。然而，在数学问题上，带有CCoT的GPT-3.5会导致性能下降27.69％。总体而言，CCoT导致每个标记的成本平均降低了22.67％。这些结果对于使用CoT提示工程技术的AI系统工程师来解决真实世界问题的LLM具有实际意义。此外，这些结果为研究LLM中逐步推理的形成行为的AI研究人员提供了更广泛的见解。

    In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
    
[^22]: 将大型语言模型中的翻译特定理解与一般理解对齐

    Aligning Translation-Specific Understanding to General Understanding in Large Language Models. (arXiv:2401.05072v1 [cs.CL])

    [http://arxiv.org/abs/2401.05072](http://arxiv.org/abs/2401.05072)

    这项研究旨在解决大型语言模型在机器翻译中的性能限制问题，通过提出一种跨语言解释困难词的新方法来对齐翻译特定理解和一般理解。

    

    虽然大型语言模型（LLMs）展现出了令人惊讶的语言理解和生成能力，但在机器翻译领域尚未取得突破性进展。造成性能有限的一个潜在原因是LLMs中翻译特定理解与一般理解的不一致。为了将翻译特定理解与一般理解对齐，我们提出了一种新颖的翻译过程xIoD（跨语言解释困难词），明确地融入一般理解对产生不一致的理解以指导翻译。具体而言，xIoD对难以翻译的单词进行跨语言解释，并通过生成的解释增强翻译。此外，我们重新构建了外部工具QE，以解决xIoD在检测困难词和生成有帮助的解释方面的挑战。我们在实验中进行了测试。

    Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation. Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on th
    
[^23]: 机器辅助的混合方法：用人工智能增强人文社科研究

    Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])

    [http://arxiv.org/abs/2309.14379](http://arxiv.org/abs/2309.14379)

    本研究提出了一种机器辅助的混合方法框架，利用大规模语言模型在人文社科领域的数据分析中的应用潜力，展示了16个案例研究，并涵盖了多种任务，包括语言分析、文本挖掘、社交网络推断等。

    

    大规模语言模型（LLM）的不断进化为人文社科领域的数据分析提供了前所未有的机会，能够在以前通常由人力完成的定性分析任务中实现规模化、自动化。本研究提出了一种系统的混合方法框架，以利用定性分析专业知识、机器的可扩展性和严谨的量化方法，同时注重透明度和可复制性。研究展示了16个机器辅助的案例研究作为概念验证。任务包括语言和话语分析、词汇语义变化检测、采访分析、历史事件因果推断和文本挖掘、政治立场检测、文本和思想重复使用、文学和电影中的文类构成、社交网络推断、自动词典编纂、元数据补充和多模态视觉文化分析。与现有LLM应用文献中对英文的关注不同，本研究涵盖多种语言。

    The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many exampl
    
[^24]: 《针对混合语言的人物感知生成模型》

    Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])

    [http://arxiv.org/abs/2309.02915](http://arxiv.org/abs/2309.02915)

    本论文提出了一种针对混合语言的人物感知生成模型PARADOX，它能够生成类似于真实个体代码混合文本的文本。该模型以用户的人物形象为条件来编码对话，并生成不带单语参考数据的代码混合文本。模型还进行对齐，使生成的文本更接近真实的代码混合文本。这种方法在语义上更有意义，在语言上更有效。

    

    在在线社交网络和多语言社会中，代码混合和脚本混合非常普遍。然而，用户对于代码混合的偏好取决于用户的社会经济地位、人口统计信息和当地环境，而现有的生成模型在生成代码混合文本时大多忽视了这些因素。在这项工作中，我们首次尝试开发一种人物感知的生成模型，以生成类似于真实个体代码混合文本的文本。我们提出了一种针对代码混合生成的人物感知生成模型（PARADOX），这是一种基于Transformer编码器-解码器的新型模型，该模型在给定用户的人物形象的条件下对话进行编码，并生成不带单语参考数据的代码混合文本。我们提出了一个对齐模块，对生成的序列进行重新校准，使其更接近真实的代码混合文本。PARADOX生成的代码混合文本在语义上更有意义，在语言上更有效。

    Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
    
[^25]: 反向稳定扩散：生成该图像所使用的提示是什么？

    Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])

    [http://arxiv.org/abs/2308.01472](http://arxiv.org/abs/2308.01472)

    本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。

    

    文本到图像扩散模型，如稳定扩散，最近吸引了许多研究人员的兴趣，反向扩散过程在更好地理解生成过程和如何设计提示以获得所需图像方面起着重要作用。为此，我们引入了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。我们结合了一系列白盒和黑盒模型（有和无对扩散网络权重进行访问）来处理所提出的任务。我们提出了一个新颖的学习框架，包括联合提示回归和多标签词汇分类目标，生成改进的提示。为了进一步改进我们的方法，我们采用了一个课程学习过程，促进了具有更低标注噪声（即更好对齐）的图像提示对的学习，并且使用相似性进行无监督领域自适应核学习方法。

    Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
    
[^26]: 通过数据增强提升AI攻击性代码生成器的鲁棒性

    Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])

    [http://arxiv.org/abs/2306.05079](http://arxiv.org/abs/2306.05079)

    本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。

    

    本研究提出了一种将扰动添加到安全性代码上下文中的代码描述中的方法，即来自善意开发者的自然语言输入（NL），并分析了扰动如何以及在什么程度上影响AI攻击性代码生成器的性能。我们的实验表明，NL描述中的扰动高度影响代码生成器的性能。为了增强代码生成器的鲁棒性，我们使用该方法执行数据增强，即增加训练数据的变异性和多样性，并证明其对扰动和非扰动的代码描述的有效性。

    In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
    
[^27]: 文字对话中的深度情感识别：一项调研

    Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09172](http://arxiv.org/abs/2211.09172)

    本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。

    

    虽然近年来对话中的情感识别取得了巨大的进展，但新的应用和实施场景带来了新的挑战和机遇。这些挑战包括利用对话语境、说话人和情感动态建模，解释常识表达、非正式语言和讽刺，应对实时情感识别的挑战，识别情感原因，不同数据集中的多种分类法，多语言情感识别以及解释性。本调研首先介绍了情感识别在对话中的应用，详细说明了与此任务相关的挑战和机遇。然后，它介绍了情感分类法和多种使用该分类法的情感识别基准数据集的描述。接下来，它描述了情感识别中最重要的作品，并解释了所使用的深度学习架构。最后，它提供了对于更好的框架的建议性情感识别实践，详细说明了处理主观性的方法。

    While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
    

