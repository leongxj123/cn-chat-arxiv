<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16048</link><description>&lt;p&gt;
&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#32467;&#26500;&#38480;&#21046;: &#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Structural restrictions in local causal discovery: identifying direct causes of a target variable. (arXiv:2307.16048v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#35266;&#23519;&#32852;&#21512;&#20998;&#24067;&#20013;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#19968;&#32452;&#30452;&#25509;&#21407;&#22240;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#34920;&#31034;&#22240;&#26524;&#32467;&#26500;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24403;&#23436;&#25972;&#30340;DAG&#20174;&#20998;&#24067;&#20013;&#21487;&#35782;&#21035;&#26102;&#65292;&#24050;&#30693;&#26377;&#19968;&#20123;&#32467;&#26524;&#65292;&#20363;&#22914;&#20551;&#35774;&#38750;&#32447;&#24615;&#39640;&#26031;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#21482;&#23545;&#35782;&#21035;&#19968;&#20010;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65288;&#23616;&#37096;&#22240;&#26524;&#32467;&#26500;&#65289;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;DAG&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#30446;&#26631;&#21464;&#37327;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#19979;&#30452;&#25509;&#21407;&#22240;&#38598;&#21512;&#21487;&#20197;&#20174;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#26469;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23545;&#38500;&#30446;&#26631;&#21464;&#37327;&#20043;&#22806;&#30340;&#21464;&#37327;&#22522;&#26412;&#19978;&#27809;&#26377;&#20219;&#20309;&#20551;&#35774;&#12290;&#38500;&#20102;&#26032;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#31181;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20272;&#35745;&#30452;&#25509;&#21407;&#22240;&#30340;&#23454;&#29992;&#31639;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Often, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. In this paper, we discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.11536</link><description>&lt;p&gt;
&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;IPNN&#30340;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#20256;&#32479;&#27010;&#29575;&#35770;&#20013;&#65292;&#27010;&#29575;&#30340;&#35745;&#31639;&#26159;&#22522;&#20110;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20960;&#20046;&#19981;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#23427;&#26159;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20351;&#32463;&#20856;&#27010;&#29575;&#35770;&#25104;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#34987;&#23450;&#20041;&#20026;&#27010;&#29575;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;IPNN&#23637;&#29616;&#20102;&#26032;&#30340;&#29305;&#24615;&#65306;&#23427;&#22312;&#36827;&#34892;&#20998;&#31867;&#30340;&#21516;&#26102;&#21487;&#20197;&#25191;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;IPNN&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#22823;&#30340;&#20998;&#31867;&#65292;&#20363;&#22914;100&#20010;&#36755;&#20986;&#33410;&#28857;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#31867;10&#20159;&#31867;&#21035;&#12290;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;IPNN&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2005.09048</link><description>&lt;p&gt;
&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#36890;&#36807;&#22810;&#21442;&#25968;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.09048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24230;-Rips&#26500;&#36896;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23494;&#24230;&#25935;&#24863;&#30340;&#22810;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#20998;&#26512;&#20102;&#23427;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#20174;&#24230;-Rips&#20013;&#21462;&#26576;&#20123;&#19968;&#21442;&#25968;&#20999;&#29255;&#21487;&#20197;&#24674;&#22797;&#20986;&#24050;&#30693;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#22810;&#21442;&#25968;&#23545;&#35937;&#30340;&#24230;-Rips&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24230;-Rips&#20013;&#21462;&#20999;&#29255;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#31283;&#23450;&#24615;&#23646;&#24615;&#30340;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#26041;&#38754;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we
&lt;/p&gt;</description></item></channel></rss>