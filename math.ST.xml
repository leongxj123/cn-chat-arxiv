<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14925</link><description>&lt;p&gt;
&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Unbiased Sparsification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21521;&#37327;$p\in \mathbb{R}^n$&#30340;&#26080;&#20559;$m$-&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#24179;&#22343;&#20540;&#20026;$p$&#65292;&#26368;&#22810;&#26377;$m&lt;n$&#20010;&#38750;&#38646;&#22352;&#26631;&#30340;&#38543;&#26426;&#21521;&#37327;$Q\in \mathbb{R}^n&#12290; &#26080;&#20559;&#31232;&#30095;&#21270;&#21487;&#20197;&#21387;&#32553;&#21407;&#22987;&#21521;&#37327;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#65307;&#23427;&#20986;&#29616;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#65292;&#27604;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#37319;&#26679;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26080;&#20559;&#31232;&#30095;&#21270;&#36824;&#24212;&#35813;&#26368;&#23567;&#21270;&#19968;&#20010;&#24230;&#37327;$Q$&#19982;&#21407;&#22987;$p$&#20043;&#38388;&#36317;&#31163;&#26377;&#22810;&#36828;&#30340;&#20998;&#35010;&#20989;&#25968;$\mathsf{Div}(Q,p)$&#30340;&#26399;&#26395;&#20540;&#12290; &#22914;&#26524;$Q$&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#37027;&#20040;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#25928;&#12290; &#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25551;&#36848;&#20102;&#23545;&#20110;&#26082;&#26159;&#25490;&#21015;&#19981;&#21464;&#21448;&#26159;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#30340;&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25490;&#21015;&#19981;&#21464;&#20998;&#35010;&#20989;&#25968;&#30340;&#34920;&#24449;&#23545;&#20110;&#20998;&#35010;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#20581;&#22766;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#26368;&#20248;$Q$&#30340;&#31867;&#19982;&#25105;&#20204;&#30340;&#31867;&#37325;&#21512;&#20102;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.00833</link><description>&lt;p&gt;
&#33258;&#19979;&#32780;&#19978;&#20309;&#26102;&#20987;&#36133;&#33258;&#19978;&#32780;&#19979;&#36827;&#34892;&#20998;&#23618;&#31038;&#21306;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Bottom-up Beat Top-down in Hierarchical Community Detection?. (arXiv:2306.00833v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#30340;&#20998;&#23618;&#32858;&#31867;&#26159;&#25351;&#26597;&#25214;&#19968;&#32452;&#31038;&#21306;&#30340;&#26641;&#24418;&#32467;&#26500;&#65292;&#20854;&#20013;&#23618;&#27425;&#32467;&#26500;&#30340;&#36739;&#20302;&#32423;&#21035;&#26174;&#31034;&#26356;&#32454;&#31890;&#24230;&#30340;&#31038;&#21306;&#32467;&#26500;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31639;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#33258;&#19978;&#32780;&#19979;&#30340;&#31639;&#27861;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;&#20998;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#31181;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24674;&#22797;&#26465;&#20214;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#33258;&#19978;&#32780;&#19979;&#31639;&#27861;&#30340;&#26465;&#20214;&#26469;&#35828;&#65292;&#38480;&#21046;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for to
&lt;/p&gt;</description></item></channel></rss>