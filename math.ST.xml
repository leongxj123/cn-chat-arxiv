<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36229;&#36234;&#27604;&#20363;&#28176;&#36817;&#24773;&#20917;&#65292;&#37325;&#26032;&#23457;&#35270;&#23545;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#23725;&#22238;&#24402;&#65292;&#20801;&#35768;&#29305;&#24449;&#21521;&#37327;&#26159;&#39640;&#32500;&#29978;&#33267;&#26080;&#38480;&#32500;&#30340;&#65292;&#20026;&#32479;&#35745;&#23398;&#20013;&#30340;&#33258;&#28982;&#35774;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2210.08571</link><description>&lt;p&gt;
&#26080;&#32500;&#24230;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Dimension free ridge regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.08571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36229;&#36234;&#27604;&#20363;&#28176;&#36817;&#24773;&#20917;&#65292;&#37325;&#26032;&#23457;&#35270;&#23545;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#23725;&#22238;&#24402;&#65292;&#20801;&#35768;&#29305;&#24449;&#21521;&#37327;&#26159;&#39640;&#32500;&#29978;&#33267;&#26080;&#38480;&#32500;&#30340;&#65292;&#20026;&#32479;&#35745;&#23398;&#20013;&#30340;&#33258;&#28982;&#35774;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24050;&#25104;&#20026;&#39640;&#32500;&#32479;&#35745;&#23398;&#21644;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#27604;&#20363;&#28176;&#36817;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21015;&#25968;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#34892;&#25968;&#25104;&#27604;&#20363;&#22686;&#38271;&#12290;&#36825;&#22312;&#32479;&#35745;&#23398;&#20013;&#24182;&#19981;&#24635;&#26159;&#26368;&#33258;&#28982;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21015;&#23545;&#24212;&#21327;&#21464;&#37327;&#65292;&#34892;&#23545;&#24212;&#26679;&#26412;&#12290;&#20026;&#20102;&#36229;&#36234;&#27604;&#20363;&#28176;&#36817;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#23545;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;$(x_i, y_i)$&#65292;$i\le n$&#36827;&#34892;&#23725;&#22238;&#24402;&#65288;$\ell_2$-&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#65289;&#65292;&#20854;&#20013;$x_i$&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;$y_i = \beta^\top x_i +\epsilon_i \in\mathbb{R}$&#20026;&#21709;&#24212;&#12290;&#25105;&#20204;&#20801;&#35768;&#29305;&#24449;&#21521;&#37327;&#26159;&#39640;&#32500;&#30340;&#65292;&#29978;&#33267;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#27492;&#26102;&#23427;&#23646;&#20110;&#21487;&#20998;Hilbert&#31354;&#38388;&#65292;&#24182;&#19988;&#20551;&#35774;$z_i := \Sigma^{-1/2}x_i$&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26465;&#30446;&#65292;&#25110;&#32773;&#28385;&#36275;&#26576;&#31181;&#20984;&#38598;&#20013;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.08571v2 Announce Type: replace-cross  Abstract: Random matrix theory has become a widely useful tool in high-dimensional statistics and theoretical machine learning. However, random matrix theory is largely focused on the proportional asymptotics in which the number of columns grows proportionally to the number of rows of the data matrix. This is not always the most natural setting in statistics where columns correspond to covariates and rows to samples. With the objective to move beyond the proportional asymptotics, we revisit ridge regression ($\ell_2$-penalized least squares) on i.i.d. data $(x_i, y_i)$, $i\le n$, where $x_i$ is a feature vector and $y_i = \beta^\top x_i +\epsilon_i \in\mathbb{R}$ is a response. We allow the feature vector to be high-dimensional, or even infinite-dimensional, in which case it belongs to a separable Hilbert space, and assume either $z_i := \Sigma^{-1/2}x_i$ to have i.i.d. entries, or to satisfy a certain convex concentration property. With
&lt;/p&gt;</description></item></channel></rss>