<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#38750;&#21442;&#25968; logistic &#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22312; Hellinger &#36317;&#31163;&#19979;&#25512;&#23548;&#20986;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12482</link><description>&lt;p&gt;
&#38750;&#21442;&#25968; logistic &#22238;&#24402;&#19982;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Nonparametric logistic regression with deep learning. (arXiv:2401.12482v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#38750;&#21442;&#25968; logistic &#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22312; Hellinger &#36317;&#31163;&#19979;&#25512;&#23548;&#20986;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#38750;&#21442;&#25968; logistic &#22238;&#24402;&#38382;&#39064;&#12290;&#22312; logistic &#22238;&#24402;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#32771;&#34385;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#32780;&#36807;&#24230;&#39118;&#38505;&#26159;&#30495;&#23454;&#26465;&#20214;&#31867;&#27010;&#29575;&#21644;&#20272;&#35745;&#26465;&#20214;&#31867;&#27010;&#29575;&#20043;&#38388; Kullback-Leibler (KL) &#25955;&#24230;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#21442;&#25968; logistic &#22238;&#24402;&#20013;&#65292;KL &#25955;&#24230;&#24456;&#23481;&#26131;&#21457;&#25955;&#65292;&#22240;&#27492;&#65292;&#36807;&#24230;&#39118;&#38505;&#30340;&#25910;&#25947;&#24456;&#38590;&#35777;&#26126;&#25110;&#19981;&#25104;&#31435;&#12290;&#33509;&#24178;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#20551;&#35774;&#19979; KL &#25955;&#24230;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#30495;&#23454;&#30340;&#26465;&#20214;&#31867;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#19981;&#38656;&#35201;&#20998;&#26512;&#36807;&#24230;&#39118;&#38505;&#26412;&#36523;&#65292;&#21482;&#38656;&#22312;&#26576;&#20123;&#21512;&#36866;&#30340;&#24230;&#37327;&#19979;&#35777;&#26126;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#21363;&#21487;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#32479;&#19968;&#30340;&#26041;&#27861;&#20998;&#26512;&#38750;&#21442;&#25968;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120; (NPMLE)&#65292;&#30452;&#25509;&#25512;&#23548;&#20986; NPMLE &#22312; Hellinger &#36317;&#31163;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the nonparametric logistic regression problem. In the logistic regression, we usually consider the maximum likelihood estimator, and the excess risk is the expectation of the Kullback-Leibler (KL) divergence between the true and estimated conditional class probabilities. However, in the nonparametric logistic regression, the KL divergence could diverge easily, and thus, the convergence of the excess risk is difficult to prove or does not hold. Several existing studies show the convergence of the KL divergence under strong assumptions. In most cases, our goal is to estimate the true conditional class probabilities. Thus, instead of analyzing the excess risk itself, it suffices to show the consistency of the maximum likelihood estimator in some suitable metric. In this paper, using a simple unified approach for analyzing the nonparametric maximum likelihood estimator (NPMLE), we directly derive the convergence rates of the NPMLE in the Hellinger distance under mild assumptions. 
&lt;/p&gt;</description></item></channel></rss>