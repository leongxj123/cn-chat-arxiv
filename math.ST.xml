<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;Mendelson&#22797;&#26434;&#24615;&#21644;&#24230;&#37327;&#29109;&#26469;&#21051;&#30011;&#20854;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#26368;&#20248;&#27604;&#29575;&#38543;&#30528;&#32500;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#20851;&#31995;&#30340;&#21464;&#21270;&#21576;&#29616;&#20986;&#22810;&#27425;&#19979;&#38477;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.04268</link><description>&lt;p&gt;
&#22823;&#32500;&#24230;&#24773;&#20917;&#19979;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimal Rate of Kernel Regression in Large Dimensions. (arXiv:2309.04268v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;Mendelson&#22797;&#26434;&#24615;&#21644;&#24230;&#37327;&#29109;&#26469;&#21051;&#30011;&#20854;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#26368;&#20248;&#27604;&#29575;&#38543;&#30528;&#32500;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#20851;&#31995;&#30340;&#21464;&#21270;&#21576;&#29616;&#20986;&#22810;&#27425;&#19979;&#38477;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#65288;&#26679;&#26412;&#22823;&#23567;$n$&#19982;&#26679;&#26412;&#32500;&#24230;$d$&#30340;&#20851;&#31995;&#20026;&#22810;&#39033;&#24335;&#65292;&#21363;$n\asymp d^{\gamma}$&#65292;&#20854;&#20013;$\gamma&gt;0$&#65289;&#30340;&#26680;&#22238;&#24402;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;Mendelson&#22797;&#26434;&#24615;$\varepsilon_{n}^{2}$&#21644;&#24230;&#37327;&#29109;$\bar{\varepsilon}_{n}^{2}$&#26469;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#65292;&#29992;&#20110;&#21051;&#30011;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;&#19982;$\mathbb{S}^{d}$&#19978;&#23450;&#20041;&#30340;&#65288;&#19968;&#33324;&#65289;&#20869;&#31215;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;RKHS&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#24037;&#20855;&#26469;&#23637;&#31034;&#26680;&#22238;&#24402;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#29575;&#26159;$n^{-1/2}$&#65292;&#24403;$n\asymp d^{\gamma}$&#65292;&#20854;&#20013;$\gamma=2, 4, 6, 8, \cdots$&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23545;&#20110;&#25152;&#26377;$\gamma&gt;0$&#65292;&#26680;&#22238;&#24402;&#36807;&#37327;&#39118;&#38505;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;$\gamma$&#30340;&#21464;&#21270;&#65292;&#26368;&#20248;&#27604;&#29575;&#30340;&#26354;&#32447;&#23637;&#29616;&#20986;&#20960;&#20010;&#26032;&#29616;&#35937;&#65292;&#21253;&#25324;&#22810;&#27425;&#19979;&#38477;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma &gt;0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma&gt;0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior
&lt;/p&gt;</description></item></channel></rss>