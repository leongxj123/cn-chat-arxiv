<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09698</link><description>&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#36807;&#28388;&#22120;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Combining Evidence Across Filtrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#26102;&#21051;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#29702;&#20013;&#65292;&#24050;&#30693;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#25512;&#29702;&#26041;&#27861;&#24517;&#39035;&#22522;&#20110;&#27979;&#35797;&#38789;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24191;&#20041;&#21270;&#65292;&#31216;&#20026;e&#36827;&#31243;&#65292;&#23427;&#20204;&#26159;&#38750;&#36127;&#36827;&#31243;&#65292;&#20854;&#22312;&#20219;&#20309;&#20219;&#24847;&#20572;&#26102;&#30340;&#26399;&#26395;&#19978;&#30028;&#19981;&#36229;&#36807;&#19968;&#12290;e&#36827;&#31243;&#37327;&#21270;&#20102;&#38024;&#23545;&#22797;&#21512;&#38646;&#20551;&#35774;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#30340;&#32047;&#31215;&#35777;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#21363;&#36807;&#28388;&#22120;&#65289;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#21512;&#24182;&#26041;&#27861;&#65292;&#38024;&#23545;&#19968;&#20010;&#38646;&#20551;&#35774;&#12290;&#23613;&#31649;&#22312;&#30456;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#21487;&#20197;&#36731;&#26494;&#22320;&#21512;&#24182;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#24179;&#22343;&#65289;&#65292;&#20294;&#22312;&#19981;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#19981;&#33021;&#37027;&#20040;&#23481;&#26131;&#22320;&#21512;&#24182;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36739;&#31895;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#19981;&#33021;&#36716;&#25442;&#20026;&#22312;&#26356;&#32454;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25991;&#29486;&#20013;&#19977;&#20010;&#20855;&#20307;&#20363;&#23376;&#65306;&#21487;&#20132;&#25442;&#24615;&#27979;&#35797;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09698v1 Announce Type: cross  Abstract: In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#28041;&#21450;&#30340;&#22823;&#31867;&#22788;&#29702;&#25928;&#24212;&#21442;&#25968;&#30340;&#26377;&#25928;&#20272;&#35745;&#65292;&#21253;&#25324;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12289;&#20998;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#12289;&#23616;&#37096;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#31561;&#12290;</title><link>http://arxiv.org/abs/2307.15181</link><description>&lt;p&gt;
&#20851;&#20110;&#32454;&#20998;&#23454;&#39564;&#25928;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficiency of Finely Stratified Experiments. (arXiv:2307.15181v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#28041;&#21450;&#30340;&#22823;&#31867;&#22788;&#29702;&#25928;&#24212;&#21442;&#25968;&#30340;&#26377;&#25928;&#20272;&#35745;&#65292;&#21253;&#25324;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12289;&#20998;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#12289;&#23616;&#37096;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#28041;&#21450;&#30340;&#22823;&#31867;&#22788;&#29702;&#25928;&#24212;&#21442;&#25968;&#30340;&#26377;&#25928;&#20272;&#35745;&#12290;&#22312;&#36825;&#37324;&#65292;&#25928;&#29575;&#26159;&#25351;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#22788;&#29702;&#20998;&#37197;&#26041;&#26696;&#32780;&#35328;&#30340;&#65292;&#20854;&#20013;&#20219;&#20309;&#21333;&#20301;&#34987;&#20998;&#37197;&#21040;&#22788;&#29702;&#30340;&#36793;&#38469;&#27010;&#29575;&#31561;&#20110;&#39044;&#20808;&#25351;&#23450;&#30340;&#20540;&#65292;&#20363;&#22914;&#19968;&#21322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#35201;&#27714;&#22788;&#29702;&#29366;&#24577;&#26159;&#20197;i.i.d.&#30340;&#26041;&#24335;&#20998;&#37197;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#36866;&#24212;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#22788;&#29702;&#20998;&#37197;&#26041;&#26696;&#65292;&#22914;&#20998;&#23618;&#38543;&#26426;&#21270;&#21644;&#21305;&#37197;&#23545;&#12290;&#25152;&#32771;&#34385;&#30340;&#21442;&#25968;&#31867;&#21035;&#26159;&#21487;&#20197;&#34920;&#31034;&#20026;&#24050;&#30693;&#35266;&#27979;&#25968;&#25454;&#30340;&#19968;&#20010;&#24050;&#30693;&#20989;&#25968;&#30340;&#26399;&#26395;&#30340;&#32422;&#26463;&#30340;&#35299;&#30340;&#37027;&#20123;&#21442;&#25968;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#25324;&#22788;&#29702;&#20998;&#37197;&#36793;&#38469;&#27010;&#29575;&#30340;&#39044;&#20808;&#25351;&#23450;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#21442;&#25968;&#21253;&#25324;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12289;&#20998;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#12289;&#23616;&#37096;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. Here, efficiency is understood to be with respect to a broad class of treatment assignment schemes for which the marginal probability that any unit is assigned to treatment equals a pre-specified value, e.g., one half. Importantly, we do not require that treatment status is assigned in an i.i.d. fashion, thereby accommodating complicated treatment assignment schemes that are used in practice, such as stratified block randomization and matched pairs. The class of parameters considered are those that can be expressed as the solution to a restriction on the expectation of a known function of the observed data, including possibly the pre-specified value for the marginal probability of treatment assignment. We show that this class of parameters includes, among other things, average treatment effects, quantile treatment effects, local average treatment effect
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.09816</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Manifold Learning with Sparse Regularised Optimal Transport. (arXiv:2307.09816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#65288;&#32454;&#32990;&#12289;&#25991;&#26723;&#12289;&#22270;&#20687;&#12289;&#20998;&#23376;&#65289;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#23884;&#20837;&#22312;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#28982;&#32780;&#25968;&#25454;&#22266;&#26377;&#30340;&#33258;&#30001;&#24230;&#36890;&#24120;&#36828;&#36828;&#23569;&#20110;&#29615;&#22659;&#32500;&#24230;&#30340;&#25968;&#37327;&#12290;&#26816;&#27979;&#25968;&#25454;&#23884;&#20837;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#35768;&#22810;&#19979;&#28216;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#32463;&#24120;&#21463;&#21040;&#22122;&#22768;&#35266;&#27979;&#21644;&#25277;&#26679;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#21462;&#20851;&#20110;&#28508;&#22312;&#27969;&#24418;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#29256;&#26412;&#30340;&#26368;&#20248;&#20256;&#36755;&#21644;&#20108;&#27425;&#27491;&#21017;&#21270;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#21452;&#38543;&#26426;&#26680;&#24402;&#19968;&#21270;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#20135;&#29983;&#30340;&#26680;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness
&lt;/p&gt;</description></item></channel></rss>