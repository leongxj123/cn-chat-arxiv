<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.00327</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#21270;&#65306;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#33021;&#22815;&#36731;&#26494;&#22320;&#25554;&#20540;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#35768;&#22810;&#30740;&#31350;&#37117;&#26088;&#22312;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#35760;&#24518;&#33021;&#21147;&#65306;&#21363;&#22312;&#20219;&#24847;&#25918;&#32622;&#36825;&#20123;&#28857;&#24182;&#20219;&#24847;&#20998;&#37197;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#26550;&#26500;&#33021;&#22815;&#25554;&#20540;&#30340;&#26368;&#22823;&#28857;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#38469;&#25968;&#25454;&#65292;&#20154;&#20204;&#30452;&#35273;&#22320;&#26399;&#26395;&#23384;&#22312;&#19968;&#31181;&#33391;&#24615;&#32467;&#26500;&#65292;&#20351;&#24471;&#25554;&#20540;&#22312;&#27604;&#35760;&#24518;&#33021;&#21147;&#24314;&#35758;&#30340;&#36739;&#23567;&#32593;&#32476;&#23610;&#23544;&#19978;&#24050;&#32463;&#21457;&#29983;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#23454;&#20363;&#29305;&#23450;&#30340;&#35266;&#28857;&#26469;&#30740;&#31350;&#25554;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#20986;&#19968;&#20010;&#25554;&#20540;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#36825;&#20004;&#20010;&#31867;&#30340;&#20960;&#20309;&#29305;&#24615;&#21450;&#20854;&#30456;&#20114;&#25490;&#21015;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
&lt;/p&gt;</description></item></channel></rss>