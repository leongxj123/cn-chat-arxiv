<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#35760;&#24518;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;&#22312;&#20855;&#26377;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#20559;&#24046;&#19968;&#33324;&#38750;&#38646;&#12290;&#27492;&#22806;&#65292;&#24403;&#21442;&#25968;&#20272;&#35745;&#20351;&#29992;&#24179;&#22343;&#27861;&#26102;&#65292;&#20272;&#35745;&#20540;&#25910;&#25947;&#21040;&#28176;&#36817;&#26080;&#20559;&#65292;&#19988;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.02944</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#35760;&#24518;&#35781;&#21650;&#65306;&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Curse of Memory in Stochastic Approximation: Extended Version. (arXiv:2309.02944v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#35760;&#24518;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;&#22312;&#20855;&#26377;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#20559;&#24046;&#19968;&#33324;&#38750;&#38646;&#12290;&#27492;&#22806;&#65292;&#24403;&#21442;&#25968;&#20272;&#35745;&#20351;&#29992;&#24179;&#22343;&#27861;&#26102;&#65292;&#20272;&#35745;&#20540;&#25910;&#25947;&#21040;&#28176;&#36817;&#26080;&#20559;&#65292;&#19988;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#26368;&#26089;&#30340;&#26085;&#23376;&#20197;&#26469;&#65292;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#22312;&#25511;&#21046;&#31995;&#32479;&#30340;&#31038;&#21306;&#20013;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#26412;&#25991;&#20197;&#26032;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#21463;&#21040;&#26368;&#36817;&#30340;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#35813;&#32467;&#26524;&#35777;&#26126;&#20351;&#29992;&#65288;&#36275;&#22815;&#23567;&#30340;&#65289;&#24658;&#23450;&#27493;&#38271;&#945;&gt;0&#30340;SA&#20855;&#26377;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#37319;&#29992;&#24179;&#22343;&#27861;&#33719;&#21462;&#26368;&#32456;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#21017;&#20272;&#35745;&#20540;&#22312;&#28176;&#36817;&#26080;&#20559;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#19979;&#25910;&#25947;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#38024;&#23545;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#31995;&#25968;&#30340;&#38543;&#26426;&#32447;&#24615;SA&#36882;&#24402;&#33719;&#24471;&#30340;&#12290;&#26412;&#25991;&#22312;&#26356;&#24120;&#35265;&#30340;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#32467;&#35770;&#65306;&#65288;i&#65289;&#22312;&#38750;&#32447;&#24615;SA&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#20986;&#20102;&#8220;&#30446;&#26631;&#20559;&#24046;&#8221;&#65292;&#24182;&#19988;&#19968;&#33324;&#19978;&#19981;&#20026;&#38646;&#12290;&#20854;&#20313;&#30340;&#32467;&#26524;&#26159;&#38024;&#23545;&#32447;&#24615;SA&#36882;&#24402;&#24314;&#31435;&#30340;&#65306;&#65288;ii&#65289;&#21452;&#21464;&#37327;&#21442;&#25968;&#25200;&#21160;&#36807;&#31243;&#22312;&#25299;&#25169;&#24847;&#20041;&#19978;&#20855;&#26377;&#20960;&#20309;&#36941;&#21382;&#24615;&#65307;&#65288;iii&#65289;&#20559;&#24046;&#30340;&#34920;&#31034;&#20855;&#26377;&#31616;&#21333;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory and application of stochastic approximation (SA) has grown within the control systems community since the earliest days of adaptive control. This paper takes a new look at the topic, motivated by recent results establishing remarkable performance of SA with (sufficiently small) constant step-size $\alpha&gt;0$. If averaging is implemented to obtain the final parameter estimate, then the estimates are asymptotically unbiased with nearly optimal asymptotic covariance. These results have been obtained for random linear SA recursions with i.i.d.\ coefficients. This paper obtains very different conclusions in the more common case of geometrically ergodic Markovian disturbance: (i) The \textit{target bias} is identified, even in the case of non-linear SA, and is in general non-zero. The remaining results are established for linear SA recursions: (ii) the bivariate parameter-disturbance process is geometrically ergodic in a topological sense; (iii) the representation for bias has a simple
&lt;/p&gt;</description></item></channel></rss>