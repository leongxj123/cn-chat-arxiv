<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26391;&#20043;&#20961;&#25193;&#25955;&#21644;&#26410;&#35843;&#25972;&#26391;&#20043;&#20961;&#31639;&#27861;&#20013;&#38543;&#26426;&#21464;&#37327;&#29420;&#31435;&#21270;&#36895;&#29575;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20989;&#25968;&#24378;&#23545;&#25968;&#20985;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#20114;&#20449;&#24687;&#20250;&#20197;&#25351;&#25968;&#36895;&#29575;&#25910;&#25947;&#20110;$0$&#12290;</title><link>https://arxiv.org/abs/2402.17067</link><description>&lt;p&gt;
&#20851;&#20110;&#26391;&#20043;&#20961;&#25193;&#25955;&#21644;&#26410;&#35843;&#25972;&#26391;&#20043;&#20961;&#31639;&#27861;&#20013;&#29420;&#31435;&#26679;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17067
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26391;&#20043;&#20961;&#25193;&#25955;&#21644;&#26410;&#35843;&#25972;&#26391;&#20043;&#20961;&#31639;&#27861;&#20013;&#38543;&#26426;&#21464;&#37327;&#29420;&#31435;&#21270;&#36895;&#29575;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20989;&#25968;&#24378;&#23545;&#25968;&#20985;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#20114;&#20449;&#24687;&#20250;&#20197;&#25351;&#25968;&#36895;&#29575;&#25910;&#25947;&#20110;$0$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#21021;&#22987;&#21644;&#24403;&#21069;&#38543;&#26426;&#21464;&#37327;&#29420;&#31435;&#21270;&#30340;&#36895;&#29575;&#65292;&#37325;&#28857;&#20851;&#27880;&#36830;&#32493;&#26102;&#38388;&#20013;&#30340;&#26391;&#20043;&#20961;&#25193;&#25955;&#21644;&#31163;&#25955;&#26102;&#38388;&#20013;&#30340;&#26410;&#35843;&#25972;&#26391;&#20043;&#20961;&#31639;&#27861;&#65288;ULA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23427;&#20204;&#30340;&#20114;&#20449;&#24687;&#24230;&#37327;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23545;&#20110;&#26391;&#20043;&#20961;&#25193;&#25955;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#30446;&#26631;&#20989;&#25968;&#24378;&#23545;&#25968;&#20985;&#26102;&#65292;&#20114;&#20449;&#24687;&#20197;&#25351;&#25968;&#36895;&#29575;&#25910;&#25947;&#20110;$0$&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#24369;&#23545;&#25968;&#20985;&#26102;&#65292;&#20197;&#22810;&#39033;&#24335;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#20123;&#36895;&#29575;&#31867;&#20284;&#20110;&#22312;&#31867;&#20284;&#26465;&#20214;&#19979;&#26391;&#20043;&#20961;&#25193;&#25955;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#23545;&#20110;ULA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#30446;&#26631;&#20989;&#25968;&#24378;&#23545;&#25968;&#20985;&#19988;&#20809;&#28369;&#26102;&#65292;&#20114;&#20449;&#24687;&#20197;&#25351;&#25968;&#36895;&#29575;&#25910;&#25947;&#20110;$0$&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#23637;&#36825;&#20123;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#20114;&#20449;&#24687;&#29256;&#26412;&#30340;&#28151;&#21512;&#26102;&#38388;&#20998;&#26512;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#26391;&#20043;&#20961;&#25193;&#25955;&#30340;&#24378;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#26367;&#20195;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17067v1 Announce Type: cross  Abstract: We study the rate at which the initial and current random variables become independent along a Markov chain, focusing on the Langevin diffusion in continuous time and the Unadjusted Langevin Algorithm (ULA) in discrete time. We measure the dependence between random variables via their mutual information. For the Langevin diffusion, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave, and at a polynomial rate when the target is weakly log-concave. These rates are analogous to the mixing time of the Langevin diffusion under similar assumptions. For the ULA, we show the mutual information converges to $0$ exponentially fast when the target is strongly log-concave and smooth. We prove our results by developing the mutual version of the mixing time analyses of these Markov chains. We also provide alternative proofs based on strong data processing inequalities for the Langevin diffusion 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.10947</link><description>&lt;p&gt;
&#20351;&#29992;&#36895;&#29575;&#20989;&#25968;&#29702;&#35299;&#25554;&#20540;&#21306;&#38388;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#26032;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#29992;&#23454;&#25968;&#20540;&#65288;&#22914;&#26435;&#37325;&#33539;&#25968;&#65289;&#26469;&#34920;&#24449;&#27169;&#22411;&#30340;&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#23454;&#20540;&#20989;&#25968;&#26469;&#25551;&#36848;&#24179;&#28369;&#24230;&#12290;&#22522;&#20110;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;$\ell_2$-&#35268;&#33539;&#21270;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#19981;&#21464;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#21270;&#65289;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#36807;&#31243;&#65292;&#36825;&#20123;&#36807;&#31243;&#20351;&#20248;&#21270;&#22120;&#20559;&#21521;&#20110;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#65292;&#32780;&#26681;&#25454;&#36825;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#26159;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#25554;&#20540;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
&lt;/p&gt;</description></item></channel></rss>