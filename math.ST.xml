<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;</title><link>https://arxiv.org/abs/2402.14264</link><description>&lt;p&gt;
&#21452;&#31283;&#20581;&#23398;&#20064;&#22312;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#32467;&#26500;&#19981;&#21487;&#30693;&#24615;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14264
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20272;&#35745;&#31574;&#30053;&#65292;&#26368;&#36817;&#36824;&#32435;&#20837;&#20102;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#32479;&#35745;&#19979;&#30028;&#32467;&#26500;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#24178;&#25200;&#20989;&#25968;&#27809;&#26377;&#32467;&#26500;&#24615;&#36136;&#20551;&#35774;&#65292;&#38500;&#20102;&#35775;&#38382;&#40657;&#30418;&#20272;&#35745;&#22120;&#20197;&#36798;&#21040;&#23567;&#35823;&#24046;&#65307;&#24403;&#21482;&#24895;&#24847;&#32771;&#34385;&#20351;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20998;&#31867;&#31070;&#35861;&#20316;&#20026;&#40657;&#30418;&#23376;&#36807;&#31243;&#30340;&#20272;&#35745;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#23545;&#20110;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
&lt;/p&gt;</description></item><item><title>&#19968;&#38454;&#21435;&#20559;&#26041;&#27861;&#22312;&#26368;&#23567;&#20108;&#20056;&#24847;&#20041;&#19979;&#22312;&#24178;&#25200;&#20989;&#25968;&#29983;&#23384;&#22312;&#29305;&#23450;&#20989;&#25968;&#31354;&#38388;&#26102;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#65292;&#36825;&#20419;&#36827;&#20102;&#8220;&#39640;&#38454;&#8221;&#21435;&#20559;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04116</link><description>&lt;p&gt;
&#32467;&#26500;&#26080;&#20851;&#20989;&#25968;&#20272;&#35745;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Fundamental Limits of Structure-Agnostic Functional Estimation. (arXiv:2305.04116v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04116
&lt;/p&gt;
&lt;p&gt;
&#19968;&#38454;&#21435;&#20559;&#26041;&#27861;&#22312;&#26368;&#23567;&#20108;&#20056;&#24847;&#20041;&#19979;&#22312;&#24178;&#25200;&#20989;&#25968;&#29983;&#23384;&#22312;&#29305;&#23450;&#20989;&#25968;&#31354;&#38388;&#26102;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#65292;&#36825;&#20419;&#36827;&#20102;&#8220;&#39640;&#38454;&#8221;&#21435;&#20559;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#22240;&#26524;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#30340;&#21457;&#23637;&#37117;&#28304;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#22312;&#38750;&#24120;&#24369;&#30340;&#26465;&#20214;&#19979;&#65292;&#32463;&#20856;&#30340;&#19968;&#27493;&#65288;&#19968;&#38454;&#65289;&#21435;&#20559;&#26041;&#27861;&#25110;&#23427;&#20204;&#36739;&#26032;&#30340;&#26679;&#26412;&#20998;&#21106;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#27604;&#25554;&#34917;&#20272;&#35745;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;&#36825;&#20123;&#19968;&#38454;&#26657;&#27491;&#20197;&#40657;&#30418;&#23376;&#26041;&#24335;&#25913;&#21892;&#25554;&#34917;&#20272;&#35745;&#20540;&#65292;&#22240;&#27492;&#32463;&#24120;&#19982;&#24378;&#22823;&#30340;&#29616;&#25104;&#20272;&#35745;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#24178;&#25200;&#20989;&#25968;&#29983;&#23384;&#22312;Holder&#22411;&#20989;&#25968;&#31354;&#38388;&#20013;&#26102;&#65292;&#36825;&#20123;&#19968;&#38454;&#26041;&#27861;&#22312;&#26368;&#23567;&#20108;&#20056;&#24847;&#20041;&#19979;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#31181;&#19968;&#38454;&#21435;&#20559;&#30340;&#27425;&#20248;&#24615;&#20419;&#36827;&#20102;&#8220;&#39640;&#38454;&#8221;&#21435;&#20559;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20272;&#35745;&#37327;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#22312;Holder&#31867;&#22411;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#26512;&#19982;&#22522;&#30784;&#20989;&#25968;&#31354;&#38388;&#30340;&#24615;&#36136;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in causal inference, and functional estimation problems more generally, have been motivated by the fact that classical one-step (first-order) debiasing methods, or their more recent sample-split double machine-learning avatars, can outperform plugin estimators under surprisingly weak conditions. These first-order corrections improve on plugin estimators in a black-box fashion, and consequently are often used in conjunction with powerful off-the-shelf estimation methods. These first-order methods are however provably suboptimal in a minimax sense for functional estimation when the nuisance functions live in Holder-type function spaces. This suboptimality of first-order debiasing has motivated the development of "higher-order" debiasing methods. The resulting estimators are, in some cases, provably optimal over Holder-type spaces, but both the estimators which are minimax-optimal and their analyses are crucially tied to properties of the underlying function space
&lt;/p&gt;</description></item></channel></rss>