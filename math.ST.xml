<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#20197;&#21450;Wasserstein&#36317;&#31163;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis of the noise schedule for score-based generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#20197;&#21450;Wasserstein&#36317;&#31163;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26088;&#22312;&#36890;&#36807;&#20165;&#20351;&#29992;&#30446;&#26631;&#25968;&#25454;&#30340;&#22122;&#22768;&#25200;&#21160;&#26679;&#26412;&#26469;&#23398;&#20064;&#24471;&#20998;&#20989;&#25968;&#65292;&#20174;&#32780;&#20272;&#35745;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;KL&#25955;&#24230;&#21644;Wasserstein&#36317;&#31163;&#26469;&#34913;&#37327;&#29983;&#25104;&#36136;&#37327;&#12290;&#33267;&#20170;&#20026;&#27490;&#65292;&#25152;&#26377;&#29616;&#26377;&#32467;&#26524;&#37117;&#26159;&#38024;&#23545;&#26102;&#38388;&#22343;&#21248;&#21464;&#21270;&#30340;&#22122;&#22768;&#35843;&#24230;&#24471;&#21040;&#30340;&#12290;&#22312;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#65292;&#26126;&#30830;&#20381;&#36182;&#20110;&#20219;&#20309;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#20551;&#35774;&#24471;&#20998;&#26159;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;Wasserstein&#36317;&#31163;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#26377;&#21033;&#30340;&#25910;&#32553;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#19978;&#30028;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target. Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances.  All existing results  have been obtained so far for time-homogeneous speed of the noise schedule.  Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. Assuming that the score is Lipschitz continuous, we provide an improved error bound in Wasserstein distance, taking advantage of favourable underlying contraction mechanisms. We also propose an algorithm to automatically tune the noise schedule using the proposed upper bound. We illustrate empirically the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.14335</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26680;&#20998;&#24067;&#22238;&#24402;&#23398;&#20064;&#29702;&#35770;&#19982;&#20004;&#38454;&#27573;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved learning theory for kernel distribution regression with two-stage sampling. (arXiv:2308.14335v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#28085;&#30422;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#26680;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#39318;&#36873;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26680;&#20998;&#24067;&#22238;&#24402;&#22312;&#35745;&#31639;&#19978;&#26159;&#26377;&#21033;&#30340;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#29702;&#35770;&#30340;&#25903;&#25345;&#12290;&#35813;&#29702;&#35770;&#36824;&#35299;&#20915;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21482;&#26377;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#23884;&#20837;&#30340;&#26680;&#65292;&#36825;&#20123;&#26680;&#21253;&#21547;&#20102;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#20837;&#30340;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#20998;&#26512;&#25552;&#20379;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#23545;&#19977;&#20010;&#37325;&#35201;&#30340;&#26680;&#31867;&#21035;&#25104;&#31435;&#65292;&#36825;&#20123;&#26680;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#24179;&#22343;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution regression problem encompasses many important statistics and machine learning tasks, and arises in a large range of applications. Among various existing approaches to tackle this problem, kernel methods have become a method of choice. Indeed, kernel distribution regression is both computationally favorable, and supported by a recent learning theory. This theory also tackles the two-stage sampling setting, where only samples from the input distributions are available. In this paper, we improve the learning theory of kernel distribution regression. We address kernels based on Hilbertian embeddings, that encompass most, if not all, of the existing approaches. We introduce the novel near-unbiased condition on the Hilbertian embeddings, that enables us to provide new error bounds on the effect of the two-stage sampling, thanks to a new analysis. We show that this near-unbiased condition holds for three important classes of kernels, based on optimal transport and mean embedd
&lt;/p&gt;</description></item></channel></rss>