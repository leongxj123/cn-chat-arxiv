<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;$L^1$&#20445;&#30495;&#24230;&#26465;&#20214;&#19979;&#65292;&#20174;&#22122;&#22768;&#35266;&#27979;&#20013;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21807;&#19968;&#33021;&#22815;&#24341;&#20837;&#32447;&#24615;&#26465;&#20214;&#20013;&#20301;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#20182;$L^p$&#25439;&#22833;&#65292;&#24182;&#35266;&#23519;&#21040;&#23545;&#20110;$p \in [1,2]$&#65292;&#39640;&#26031;&#20998;&#24067;&#26159;&#21807;&#19968;&#24341;&#20837;&#32447;&#24615;&#26368;&#20248;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25193;&#23637;&#36824;&#28085;&#30422;&#20102;&#29305;&#23450;&#25351;&#25968;&#26063;&#26465;&#20214;&#20998;&#24067;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.09129</link><description>&lt;p&gt;
$L^1$&#20272;&#35745;&#65306;&#32447;&#24615;&#20272;&#35745;&#22120;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
$L^1$ Estimation: On the Optimality of Linear Estimators. (arXiv:2309.09129v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;$L^1$&#20445;&#30495;&#24230;&#26465;&#20214;&#19979;&#65292;&#20174;&#22122;&#22768;&#35266;&#27979;&#20013;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21807;&#19968;&#33021;&#22815;&#24341;&#20837;&#32447;&#24615;&#26465;&#20214;&#20013;&#20301;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#20182;$L^p$&#25439;&#22833;&#65292;&#24182;&#35266;&#23519;&#21040;&#23545;&#20110;$p \in [1,2]$&#65292;&#39640;&#26031;&#20998;&#24067;&#26159;&#21807;&#19968;&#24341;&#20837;&#32447;&#24615;&#26368;&#20248;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25193;&#23637;&#36824;&#28085;&#30422;&#20102;&#29305;&#23450;&#25351;&#25968;&#26063;&#26465;&#20214;&#20998;&#24067;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;$L^1$&#20445;&#30495;&#24230;&#26465;&#20214;&#19979;&#65292;&#32771;&#34385;&#20174;&#22122;&#22768;&#35266;&#27979;$Y=X+Z$&#20013;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$Z$&#26159;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#26159;&#26465;&#20214;&#20013;&#20301;&#25968;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#26465;&#20214;&#20013;&#20301;&#25968;&#20013;&#24341;&#20837;&#32447;&#24615;&#30340;&#21807;&#19968;&#20808;&#39564;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#20854;&#20182;&#20960;&#20010;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#35777;&#26126;&#20102;&#22914;&#26524;&#23545;&#20110;&#25152;&#26377;$y$&#65292;&#26465;&#20214;&#20998;&#24067;$P_{X|Y=y}$&#37117;&#26159;&#23545;&#31216;&#30340;&#65292;&#21017;$X$&#24517;&#39035;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20854;&#20182;&#30340;$L^p$&#25439;&#22833;&#65292;&#24182;&#35266;&#23519;&#21040;&#20197;&#19979;&#29616;&#35937;&#65306;&#23545;&#20110;$p \in [1,2]$&#65292;&#39640;&#26031;&#20998;&#24067;&#26159;&#21807;&#19968;&#24341;&#20837;&#32447;&#24615;&#26368;&#20248;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#23545;&#20110;$p \in (2,\infty)$&#65292;&#26377;&#26080;&#31351;&#22810;&#20010;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#24341;&#20837;&#32447;&#24615;&#24615;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#25193;&#23637;&#65292;&#20197;&#28085;&#30422;&#23548;&#33268;&#29305;&#23450;&#25351;&#25968;&#26063;&#26465;&#20214;&#20998;&#24067;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the problem of estimating a random variable $X$ from noisy observations $Y = X+ Z$, where $Z$ is standard normal, under the $L^1$ fidelity criterion. It is well known that the optimal Bayesian estimator in this setting is the conditional median. This work shows that the only prior distribution on $X$ that induces linearity in the conditional median is Gaussian.  Along the way, several other results are presented. In particular, it is demonstrated that if the conditional distribution $P_{X|Y=y}$ is symmetric for all $y$, then $X$ must follow a Gaussian distribution. Additionally, we consider other $L^p$ losses and observe the following phenomenon: for $p \in [1,2]$, Gaussian is the only prior distribution that induces a linear optimal Bayesian estimator, and for $p \in (2,\infty)$, infinitely many prior distributions on $X$ can induce linearity. Finally, extensions are provided to encompass noise models leading to conditional distributions from certain exponential families.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item></channel></rss>