<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2401.08468</link><description>&lt;p&gt;
&#20445;&#30041;&#36824;&#26159;&#20002;&#24323;&#65311;&#19968;&#31181;&#35780;&#20272;&#26377;&#22122;&#22768;ICA&#35299;&#20915;&#26041;&#26696;&#30340;&#38750;&#21442;&#25968;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#65288;ICA&#65289;&#20110;20&#19990;&#32426;80&#24180;&#20195;&#24341;&#20837;&#65292;&#20316;&#20026;&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#27169;&#22411;&#65292;&#25351;&#30340;&#26159;&#22312;&#23545;&#28151;&#21512;&#20449;&#21495;&#36827;&#34892;&#24674;&#22797;&#26102;&#65292;&#23545;&#28304;&#20449;&#21495;&#25110;&#28151;&#21512;&#36807;&#31243;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#31934;&#23494;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#20294;&#19981;&#21516;&#26041;&#27861;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;ICA&#31639;&#27861;&#21644;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#12290;&#35813;&#20998;&#25968;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#21482;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#26377;&#38480;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#26469;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#23545;&#27604;&#20989;&#25968;&#21644;&#31639;&#27861;&#65292;&#23427;&#20204;&#20855;&#26377;&#19982;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;FASTICA&#21644;JADE&#65289;&#30456;&#21516;&#30340;&#24555;&#36895;&#35745;&#31639;&#24615;&#33021;&#65292;&#20294;&#22312;&#21069;&#32773;&#21487;&#33021;&#22833;&#36133;&#30340;&#39046;&#22495;&#20013;&#24037;&#20316;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20063;&#21487;&#33021;&#23384;&#22312;&#32570;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14077</link><description>&lt;p&gt;
&#35686;&#24789;&#23574;&#23792;&#65306;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. (arXiv:2305.14077v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36798;&#21040;&#25509;&#36817;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#21363;&#20351;&#20272;&#35745;&#22120;&#25554;&#20540;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#36824;&#26159;&#20855;&#26377;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#26576;&#20123;&#23398;&#20064;&#26041;&#27861;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#24050;&#32463;&#30830;&#23450;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#23545;&#20110;&#20856;&#22411;&#20869;&#26680;&#26041;&#27861;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#38656;&#35201;&#39640;&#32500;&#24230;&#35774;&#32622;&#65292;&#20854;&#20013;&#32500;&#25968;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#26159;&#20851;&#38190;&#65292;&#32780;&#19981;&#26159;&#32500;&#25968;&#65306;&#21482;&#26377;&#24403;&#20272;&#35745;&#22120;&#30340;&#23548;&#25968;&#36275;&#22815;&#22823;&#26102;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#25165;&#21487;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#25554;&#20540;&#27169;&#22411;&#21644;&#26356;&#22810;&#20869;&#26680;&#65292;&#20197;&#34920;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a seque
&lt;/p&gt;</description></item></channel></rss>