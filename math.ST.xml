<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15312</link><description>&lt;p&gt;
&#27700;&#26031;&#22374;&#35270;&#35282;&#19979;&#30340;&#26222;&#36890; GANs
&lt;/p&gt;
&lt;p&gt;
A Wasserstein perspective of Vanilla GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15312
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#23454;&#35777;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#29702;&#35770;&#30740;&#31350;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#32479;&#35745;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#27700;&#26031;&#22374;GANs&#21450;&#20854;&#25193;&#23637;&#19978;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#20855;&#26377;&#33391;&#22909;&#30340;&#38477;&#32500;&#29305;&#24615;&#12290;&#23545;&#20110;&#26222;&#36890;GANs&#65292;&#21363;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#32479;&#35745;&#32467;&#26524;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#65292;&#38656;&#35201;&#20551;&#35774;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#21644;&#28508;&#31354;&#38388;&#19982;&#21608;&#22260;&#31354;&#38388;&#30340;&#32500;&#24230;&#30456;&#31561;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#29616;&#26377;&#30340;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;&#26222;&#36890;GANs&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27700;&#26031;&#22374;&#36317;&#31163;&#20013;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;&#36825;&#20010;&#31070;&#35861;&#19981;&#31561;&#24335;&#30340;&#20551;&#35774;&#26088;&#22312;&#30001;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#65292;&#22914;&#21069;&#39304;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15312v1 Announce Type: cross  Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative resu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16048</link><description>&lt;p&gt;
&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#32467;&#26500;&#38480;&#21046;: &#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Structural restrictions in local causal discovery: identifying direct causes of a target variable. (arXiv:2307.16048v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#35266;&#23519;&#32852;&#21512;&#20998;&#24067;&#20013;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#19968;&#32452;&#30452;&#25509;&#21407;&#22240;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#34920;&#31034;&#22240;&#26524;&#32467;&#26500;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24403;&#23436;&#25972;&#30340;DAG&#20174;&#20998;&#24067;&#20013;&#21487;&#35782;&#21035;&#26102;&#65292;&#24050;&#30693;&#26377;&#19968;&#20123;&#32467;&#26524;&#65292;&#20363;&#22914;&#20551;&#35774;&#38750;&#32447;&#24615;&#39640;&#26031;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#21482;&#23545;&#35782;&#21035;&#19968;&#20010;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65288;&#23616;&#37096;&#22240;&#26524;&#32467;&#26500;&#65289;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;DAG&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#30446;&#26631;&#21464;&#37327;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#19979;&#30452;&#25509;&#21407;&#22240;&#38598;&#21512;&#21487;&#20197;&#20174;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#26469;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23545;&#38500;&#30446;&#26631;&#21464;&#37327;&#20043;&#22806;&#30340;&#21464;&#37327;&#22522;&#26412;&#19978;&#27809;&#26377;&#20219;&#20309;&#20551;&#35774;&#12290;&#38500;&#20102;&#26032;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#31181;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20272;&#35745;&#30452;&#25509;&#21407;&#22240;&#30340;&#23454;&#29992;&#31639;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Often, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. In this paper, we discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10413</link><description>&lt;p&gt;
&#20351;&#29992;Lasso&#30340;&#31614;&#21517;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#19981;&#21516;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#23450;&#20041;&#21644;&#38543;&#26426;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Lasso&#22238;&#24402;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31614;&#21517;&#21464;&#25442;&#26159;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36845;&#20195;&#36335;&#24452;&#31215;&#20998;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#38750;&#32447;&#24615;&#36890;&#36807;&#32447;&#24615;&#21270;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37325;&#26032;&#23457;&#35270;&#20102;Lasso&#22238;&#24402;&#23545;&#20110;&#31614;&#21517;&#21464;&#25442;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26356;&#25509;&#36817;&#24067;&#26391;&#36816;&#21160;&#25110;&#20855;&#26377;&#36739;&#24369;&#36328;&#32500;&#24230;&#30456;&#20851;&#24615;&#30340;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#31614;&#21517;&#23450;&#20041;&#20026;It\^o&#31215;&#20998;&#30340;Lasso&#22238;&#24402;&#26356;&#20855;&#19968;&#33268;&#24615;&#65307;&#23545;&#20110;&#22343;&#20540;&#22238;&#24402;&#36807;&#31243;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#31614;&#21517;&#23450;&#20041;&#20026;Stratonovich&#31215;&#20998;&#22312;Lasso&#22238;&#24402;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#31614;&#21517;&#21644;&#38543;&#26426;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#20808;&#20998;&#26512;&#35745;&#21010;&#30340;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#26045;&#20855;&#26377;&#21487;&#34892;&#24615;&#30340;&#32479;&#35745;&#20915;&#31574;&#35268;&#21017;&#65292;&#21457;&#29616;&#20808;&#20998;&#26512;&#35745;&#21010;&#22312;&#23454;&#26045;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20551;&#35774;&#26816;&#39564;&#26469;&#35828;&#65292;&#26368;&#20248;&#25298;&#32477;&#35268;&#21017;&#38656;&#35201;&#39044;&#20808;&#27880;&#20876;&#26377;&#25928;&#30340;&#26816;&#39564;&#24182;&#23545;&#26410;&#25253;&#21578;&#30340;&#25968;&#25454;&#20570;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2208.09638</link><description>&lt;p&gt;
&#26368;&#20248;&#20808;&#20998;&#26512;&#35745;&#21010;&#65306;&#21463;&#38480;&#20110;&#21487;&#23454;&#26045;&#24615;&#30340;&#32479;&#35745;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability. (arXiv:2208.09638v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#20808;&#20998;&#26512;&#35745;&#21010;&#30340;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#26045;&#20855;&#26377;&#21487;&#34892;&#24615;&#30340;&#32479;&#35745;&#20915;&#31574;&#35268;&#21017;&#65292;&#21457;&#29616;&#20808;&#20998;&#26512;&#35745;&#21010;&#22312;&#23454;&#26045;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20551;&#35774;&#26816;&#39564;&#26469;&#35828;&#65292;&#26368;&#20248;&#25298;&#32477;&#35268;&#21017;&#38656;&#35201;&#39044;&#20808;&#27880;&#20876;&#26377;&#25928;&#30340;&#26816;&#39564;&#24182;&#23545;&#26410;&#25253;&#21578;&#30340;&#25968;&#25454;&#20570;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#26159;&#20808;&#20998;&#26512;&#35745;&#21010;&#30340;&#30446;&#30340;&#65292;&#24212;&#35813;&#22914;&#20309;&#35774;&#35745;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#20381;&#36182;&#20110;&#20998;&#26512;&#24072;&#36873;&#25321;&#24615;&#20294;&#30495;&#23454;&#30340;&#25253;&#21578;&#12290;&#20998;&#26512;&#24072;&#20855;&#26377;&#25968;&#25454;&#35775;&#38382;&#26435;&#65292;&#19988;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#23454;&#26045;&#32479;&#35745;&#20915;&#31574;&#35268;&#21017;&#65288;&#26816;&#39564;&#12289;&#20272;&#35745;&#37327;&#65289;&#38656;&#35201;&#19968;&#20010;&#28608;&#21169;&#20860;&#23481;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#21738;&#20123;&#20915;&#31574;&#35268;&#21017;&#21487;&#20197;&#34987;&#23454;&#26045;&#12290;&#28982;&#21518;&#25551;&#36848;&#20102;&#21463;&#38480;&#20110;&#21487;&#23454;&#26045;&#24615;&#30340;&#26368;&#20248;&#32479;&#35745;&#20915;&#31574;&#35268;&#21017;&#12290;&#25105;&#20204;&#34920;&#26126;&#23454;&#26045;&#38656;&#35201;&#20808;&#20998;&#26512;&#35745;&#21010;&#12290;&#37325;&#28857;&#25918;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#25298;&#32477;&#35268;&#21017;&#39044;&#20808;&#27880;&#20876;&#20102;&#19968;&#20010;&#23545;&#20110;&#25152;&#26377;&#25968;&#25454;&#25253;&#21578;&#30340;&#26377;&#25928;&#26816;&#39564;&#65292;&#24182;&#23545;&#26410;&#25253;&#21578;&#30340;&#25968;&#25454;&#20570;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#12290;&#26368;&#20248;&#26816;&#39564;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#26469;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the purpose of pre-analysis plans, and how should they be designed? We propose a principal-agent model where a decision-maker relies on selective but truthful reports by an analyst. The analyst has data access, and non-aligned objectives. In this model, the implementation of statistical decision rules (tests, estimators) requires an incentive-compatible mechanism. We first characterize which decision rules can be implemented. We then characterize optimal statistical decision rules subject to implementability. We show that implementation requires pre-analysis plans. Focussing specifically on hypothesis tests, we show that optimal rejection rules pre-register a valid test for the case when all data is reported, and make worst-case assumptions about unreported data. Optimal tests can be found as a solution to a linear-programming problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.14233</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;Bandits&#36890;&#36807;&#20581;&#22766;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#32463;&#24120;&#21516;&#26102;&#38754;&#23545;&#35768;&#22810;&#30456;&#20851;&#20294;&#24322;&#36136;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#23398;&#20064;&#23454;&#20363;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65292;&#20351;&#29992;&#20581;&#22766;&#32479;&#35745;&#23398;&#65288;&#22312;&#30456;&#20284;&#23454;&#20363;&#19978;&#23398;&#20064;&#65289;&#21644;LASSO&#22238;&#24402;&#65288;&#21435;&#20559;&#24046;&#32467;&#26524;&#65289;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
&lt;/p&gt;</description></item></channel></rss>