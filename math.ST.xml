<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.10160</link><description>&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#19979;&#20266;&#26631;&#31614;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift. (arXiv:2302.10160v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#23398;&#20064;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#26368;&#23567;&#30340;&#22238;&#24402;&#20989;&#25968;&#65292;&#22522;&#20110;&#20174;&#30446;&#26631;&#20998;&#24067;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#24050;&#26631;&#35760;&#25968;&#25454;&#20998;&#25104;&#20004;&#20010;&#23376;&#38598;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#26680;&#23725;&#22238;&#24402;&#65292;&#20197;&#33719;&#24471;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#21644;&#19968;&#20010;&#22635;&#20805;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21518;&#32773;&#22635;&#20805;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#38750;&#28176;&#36817;&#24615;&#36807;&#37327;&#39118;&#38505;&#30028;&#34920;&#26126;&#65292;&#22312;&#30456;&#24403;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#20197;&#21450;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#32467;&#26500;&#12290;&#23427;&#33021;&#22815;&#23454;&#29616;&#28176;&#36817;&#27491;&#24577;&#35823;&#24046;&#29575;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#30340;&#26368;&#23567;&#26497;&#38480;&#20248;&#21270;&#12290;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#20351;&#29992;&#20266;&#26631;&#31614;&#19981;&#20250;&#20135;&#29983;&#20027;&#35201;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate model accordingly. Our non-asymptotic excess risk bounds show that in quite general scenarios, our estimator adapts to the structure of the target distribution as well as the covariate shift. It achieves the minimax optimal error rate up to a logarithmic factor. The use of pseudo-labels in model selection does not have major negative impacts.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#23616;&#37096;&#22810;&#39033;&#24335;&#20272;&#35745;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#26032;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#22312;&#24369;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#30340;&#65292;&#19988;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/1904.05209</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models. (arXiv:1904.05209v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#23616;&#37096;&#22810;&#39033;&#24335;&#20272;&#35745;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#26032;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#22312;&#24369;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#30340;&#65292;&#19988;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24191;&#27867;&#31867;&#21035;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36817;&#29702;&#35770;&#26469;&#20272;&#35745;&#26102;&#38388;&#21464;&#21270;&#21442;&#25968;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#65288;&#20934;&#65289;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#12290;&#22312;&#24369;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#22823;&#26679;&#26412;&#19979;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#29702;&#35770;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#23545;&#20110;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21450;&#20854;&#20284;&#28982;&#20989;&#25968;&#30340;&#20809;&#28369;&#24615;&#21644;&#30697;&#26465;&#20214;&#35201;&#27714;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#39033;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#29702;&#35770;&#24212;&#29992;&#20110;&#23616;&#37096;&#20934;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26102;&#38388;&#21464;&#21270;VAR&#12289;ARCH&#21644;GARCH&#20197;&#21450;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26222;&#36866;&#32467;&#26524;&#30340;&#26377;&#30410;&#24615;&#12290;&#23545;&#20110;&#21069;&#19977;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#22823;&#22823;&#20943;&#24369;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#26465;&#20214;&#35201;&#27714;&#12290;&#23545;&#20110;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29616;&#26377;&#29702;&#35770;&#19981;&#33021;&#24212;&#29992;&#65292;&#32780;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel asymptotic theory for local polynomial (quasi-) maximum-likelihood estimators of time-varying parameters in a broad class of nonlinear time series models. Under weak regularity conditions, we show the proposed estimators are consistent and follow normal distributions in large samples. Our conditions impose weaker smoothness and moment conditions on the data-generating process and its likelihood compared to existing theories. Furthermore, the bias terms of the estimators take a simpler form. We demonstrate the usefulness of our general results by applying our theory to local (quasi-)maximum-likelihood estimators of a time-varying VAR's, ARCH and GARCH, and Poisson autogressions. For the first three models, we are able to substantially weaken the conditions found in the existing literature. For the Poisson autogression, existing theories cannot be be applied while our novel approach allows us to analyze it.
&lt;/p&gt;</description></item></channel></rss>