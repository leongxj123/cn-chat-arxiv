<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#28145;&#39532;&#36420;&#39640;&#26031;&#36807;&#31243;Deep-HGP&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20808;&#39564;&#65292;&#37319;&#29992;&#28145;&#39640;&#26031;&#36807;&#31243;&#24182;&#20801;&#35768;&#25968;&#25454;&#39537;&#21160;&#36873;&#25321;&#20851;&#38190;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#65292;&#23545;&#20110;&#38750;&#21442;&#25968;&#22238;&#24402;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#30495;&#23454;&#22238;&#24402;&#26354;&#32447;&#30340;&#20248;&#21270;&#22238;&#22797;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01737</link><description>&lt;p&gt;
&#28145;&#39532;&#36420;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Horseshoe Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01737
&lt;/p&gt;
&lt;p&gt;
&#28145;&#39532;&#36420;&#39640;&#26031;&#36807;&#31243;Deep-HGP&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20808;&#39564;&#65292;&#37319;&#29992;&#28145;&#39640;&#26031;&#36807;&#31243;&#24182;&#20801;&#35768;&#25968;&#25454;&#39537;&#21160;&#36873;&#25321;&#20851;&#38190;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#65292;&#23545;&#20110;&#38750;&#21442;&#25968;&#22238;&#24402;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#30495;&#23454;&#22238;&#24402;&#26354;&#32447;&#30340;&#20248;&#21270;&#22238;&#22797;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#28145;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#23545;&#35937;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#33021;&#25311;&#21512;&#29616;&#20195;&#25968;&#25454;&#26679;&#26412;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#29305;&#24449;&#65292;&#22914;&#32452;&#21512;&#32467;&#26500;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#33258;&#28982;&#22320;&#21033;&#29992;&#28145;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#23558;&#30456;&#24212;&#30340;&#21518;&#39564;&#20998;&#24067;&#29992;&#20110;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#39532;&#36420;&#39640;&#26031;&#36807;&#31243;Deep-HGP&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24102;&#26377;&#24179;&#26041;&#25351;&#25968;&#26680;&#30340;&#28145;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#31616;&#21333;&#20808;&#39564;&#65292;&#29305;&#21035;&#26159;&#20351;&#24471;&#21487;&#20197;&#23545;&#20851;&#38190;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#36873;&#25321;&#12290;&#23545;&#20110;&#38543;&#26426;&#35774;&#35745;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#24212;&#30340;&#35843;&#33410;&#21518;&#39564;&#20998;&#24067;&#20197;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#24335;&#65292;&#26368;&#20248;&#22320;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24847;&#20041;&#19979;&#24674;&#22797;&#26410;&#30693;&#30340;&#30495;&#22238;&#24402;&#26354;&#32447;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#25910;&#25947;&#36895;&#29575;&#21516;&#26102;&#23545;&#22238;&#24402;&#30340;&#24179;&#28369;&#24230;&#21644;&#35774;&#35745;&#32500;&#24230;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01737v1 Announce Type: cross  Abstract: Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated tempered posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regress
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#26469;&#25552;&#39640;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21644;&#25935;&#24863;&#24615;</title><link>http://arxiv.org/abs/2310.12140</link><description>&lt;p&gt;
&#22312;&#32447;&#20272;&#35745;&#19982;&#28378;&#21160;&#39564;&#35777;&#65306;&#36866;&#24212;&#24615;&#38750;&#21442;&#25968;&#20272;&#35745;&#19982;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data. (arXiv:2310.12140v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#26469;&#25552;&#39640;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21644;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#25928;&#35745;&#31639;&#21644;&#31454;&#20105;&#24615;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#32447;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21464;&#20307;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#19968;&#27425;&#21482;&#21462;&#19968;&#20010;&#26679;&#26412;&#28857;&#65292;&#24182;&#31435;&#21363;&#26356;&#26032;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20123;&#22312;&#32447;&#31639;&#27861;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#19968;&#31181;&#22312;&#32447;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#21464;&#20307;&#65292;&#23545;&#20110;&#35768;&#22810;&#20856;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26469;&#35828;&#65292;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#12290;&#31867;&#20284;&#20110;&#25209;&#37327;&#20132;&#21449;&#39564;&#35777;&#65292;&#23427;&#21487;&#20197;&#25552;&#21319;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#24456;&#31616;&#21333;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#20123;&#19968;&#33324;&#30340;&#32479;&#35745;&#31283;&#23450;&#24615;&#20551;&#35774;&#12290;&#27169;&#25311;&#30740;&#31350;&#24378;&#35843;&#20102;&#28378;&#21160;&#39564;&#35777;&#20013;&#21457;&#25955;&#26435;&#37325;&#22312;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#24456;&#23567;&#30340;&#20559;&#24046;&#65292;&#23427;&#30340;&#25935;&#24863;&#24615;&#20063;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and instantly update the parameter estimate of interest. In this work we consider model selection and hyperparameter tuning for such online algorithms. We propose a weighted rolling-validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators. Similar to batch cross-validation, it can boost base estimators to achieve a better, adaptive convergence rate. Our theoretical analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in rolling validation in practice and demonstrates its sensitivity even when there is only a slim
&lt;/p&gt;</description></item></channel></rss>