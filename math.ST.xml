<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item></channel></rss>