<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.13139</link><description>&lt;p&gt;
&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#31934;&#30830;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exact recovery for the non-uniform Hypergraph Stochastic Block Model. (arXiv:2304.13139v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#22312;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#38543;&#26426;&#36229;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#36793;&#29420;&#31435;&#22320;&#20197;&#26576;&#20123;&#32473;&#23450;&#27010;&#29575;&#20986;&#29616;&#65292;&#35813;&#27010;&#29575;&#20165;&#21462;&#20915;&#20110;&#20854;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#39318;&#27425;&#24314;&#31435;&#20102;&#22312;&#36825;&#31181;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#21463;&#21040;&#27425;&#35201;&#32422;&#26463;&#65307;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;K&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#23545;&#31216;&#20108;&#36827;&#21046;&#27169;&#22411;&#65288;K=2&#65289;&#12290;&#20851;&#38190;&#28857;&#26159;&#36890;&#36807;&#32858;&#21512;&#25152;&#26377;&#22343;&#21248;&#23618;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#32771;&#34385;&#27599;&#20010;&#23618;&#26102;&#20284;&#20046;&#19981;&#21487;&#33021;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#31934;&#30830;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#22312;&#38408;&#20540;&#20197;&#19978;&#23454;&#29616;&#20102;&#31934;&#30830;&#24674;&#22797;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Consider the community detection problem in random hypergraphs under the non-uniform hypergraph stochastic block model (HSBM), where each hyperedge appears independently with some given probability depending only on the labels of its vertices. We establish, for the first time in the literature, a sharp threshold for exact recovery under this non-uniform case, subject to minor constraints; in particular, we consider the model with $K$ classes as well as the symmetric binary model ($K=2$). One crucial point here is that by aggregating information from all the uniform layers, we may obtain exact recovery even in cases when this may appear impossible if each layer were considered alone. Two efficient algorithms that successfully achieve exact recovery above the threshold are provided. The theoretical analysis of our algorithms relies on the concentration and regularization of the adjacency matrix for non-uniform random hypergraphs, which could be of independent interest. We also address so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.08847</link><description>&lt;p&gt;
&#26377;&#38480;&#32500;&#19979;&#30340;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#39564;&#27979;&#24230;&#21253;&#21547;&#22312;&#19968;&#20010;&#33258;&#28982;&#30340;&#20984;&#38598;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#20250;&#23548;&#33268;&#25968;&#25454;&#28857;&#30340;coreset&#12290;&#25511;&#21046;&#36825;&#26679;&#19968;&#20010;coreset&#24517;&#39035;&#26377;&#22810;&#22823;&#30340;&#19968;&#20010;&#20851;&#38190;&#25968;&#37327;&#26159;&#21253;&#21547;&#22312;&#32463;&#39564;&#20984;&#38598;&#20013;&#30340;&#32463;&#39564;&#27979;&#37327;&#21608;&#22260;&#30340;&#26368;&#22823;&#29699;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23548;&#20986;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#29699;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#25216;&#26415;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#26680;&#23725;&#22238;&#24402;&#65292;&#26469;&#34917;&#20805;&#36825;&#31181;&#19979;&#38480;&#30340;&#27966;&#29983;&#12290;&#25105;&#20204;&#26368;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38480;&#32500;RKHS&#30340;&#26500;&#36896;&#65292;&#20854;&#20013;&#21387;&#32553;&#24456;&#24046;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods.Such an approximation gives under certain conditions rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions. We complement this derivation of the lower bound by developing techniques that allow us to apply the compression approach to concrete inference problems such as kernel ridge regression. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one face
&lt;/p&gt;</description></item></channel></rss>