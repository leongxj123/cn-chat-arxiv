<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2202.09134</link><description>&lt;p&gt;
&#22312;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#37327;&#21270;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#20010;&#20855;&#20307;&#27169;&#22411;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20123;&#35266;&#23519;&#65292;&#20294;&#20063;&#24471;&#20986;&#20102;&#24847;&#22806;&#30340;&#21457;&#29616;&#65306;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#32780;&#19981;&#26159;&#20943;&#23569;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#39044;&#27979;&#39118;&#38505;&#12290;&#23427;&#21487;&#20197;&#20805;&#24403;&#27491;&#21017;&#21270;&#22120;&#65292;&#20294;&#22312;&#26576;&#20123;&#39640;&#32500;&#38382;&#39064;&#20013;&#21364;&#26080;&#27861;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25913;&#21464;&#32463;&#39564;&#39118;&#38505;&#30340;&#21452;&#37325;&#19979;&#38477;&#23792;&#20540;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20998;&#26512;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#34987;&#36171;&#20104;&#30340;&#20960;&#20010;&#23646;&#24615;&#35201;&#20040;&#26159;&#30495;&#30340;&#65292;&#35201;&#20040;&#26159;&#20551;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;-&#29305;&#21035;&#26159;&#25968;&#25454;&#20998;&#24067;&#65292;&#20272;&#35745;&#22120;&#30340;&#23646;&#24615;&#20197;&#21450;&#26679;&#26412;&#22823;&#23567;&#65292;&#22686;&#24378;&#25968;&#37327;&#21644;&#32500;&#25968;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#24037;&#20855;&#26159;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
&lt;/p&gt;</description></item></channel></rss>