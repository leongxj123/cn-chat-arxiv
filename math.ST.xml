<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07388</link><description>&lt;p&gt;
&#26080;&#20551;&#35774;&#27979;&#35797;&#31639;&#27861;&#24615;&#33021;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Limits of Assumption-free Tests for Algorithm Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#35780;&#20215;&#21644;&#27604;&#36739;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#19968;&#20010;&#31639;&#27861;&#22312;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#65292;&#21738;&#20010;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65311;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#65292;&#36890;&#24120;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#30041;&#20986;&#25968;&#25454;&#28857;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#31243;&#24207;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#19979;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#38382;&#39064;: &#31639;&#27861;$A$&#22312;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#38598;&#19978;&#23398;&#20064;&#38382;&#39064;&#26377;&#22810;&#22909;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;$A$&#25152;&#20135;&#29983;&#30340;&#29305;&#23450;&#25311;&#21512;&#27169;&#22411;&#26377;&#22810;&#22909;&#65311;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#23558;&#31639;&#27861;&#35270;&#20026;&#40657;&#30418;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26080;&#27861;&#20934;&#30830;&#22320;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.00833</link><description>&lt;p&gt;
&#33258;&#19979;&#32780;&#19978;&#20309;&#26102;&#20987;&#36133;&#33258;&#19978;&#32780;&#19979;&#36827;&#34892;&#20998;&#23618;&#31038;&#21306;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Bottom-up Beat Top-down in Hierarchical Community Detection?. (arXiv:2306.00833v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#30340;&#20998;&#23618;&#32858;&#31867;&#26159;&#25351;&#26597;&#25214;&#19968;&#32452;&#31038;&#21306;&#30340;&#26641;&#24418;&#32467;&#26500;&#65292;&#20854;&#20013;&#23618;&#27425;&#32467;&#26500;&#30340;&#36739;&#20302;&#32423;&#21035;&#26174;&#31034;&#26356;&#32454;&#31890;&#24230;&#30340;&#31038;&#21306;&#32467;&#26500;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31639;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#33258;&#19978;&#32780;&#19979;&#30340;&#31639;&#27861;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;&#20998;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#31181;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24674;&#22797;&#26465;&#20214;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#33258;&#19978;&#32780;&#19979;&#31639;&#27861;&#30340;&#26465;&#20214;&#26469;&#35828;&#65292;&#38480;&#21046;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for to
&lt;/p&gt;</description></item></channel></rss>