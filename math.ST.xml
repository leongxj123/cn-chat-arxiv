<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#21644;No U-Turn Sampler (NUTS) &#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;NUTS&#20316;&#20026;&#21160;&#24577;HMC&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#20855;&#26377;&#36941;&#21382;&#24615;&#21644;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;HMC&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;HMC&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.03460</link><description>&lt;p&gt;
&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo&#21644;No U-Turn Samplers&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers. (arXiv:2307.03460v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#21644;No U-Turn Sampler (NUTS) &#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;NUTS&#20316;&#20026;&#21160;&#24577;HMC&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#20855;&#26377;&#36941;&#21382;&#24615;&#21644;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;HMC&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;HMC&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#65292;&#20363;&#22914;No U-Turn Sampler (NUTS)&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#20855;&#26377;&#25104;&#21151;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20294;&#20851;&#20110;&#23427;&#20204;&#34892;&#20026;&#30340;&#29702;&#35770;&#32467;&#26524;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31216;&#20026;&#21160;&#24577;HMC&#30340;&#36890;&#29992;MCMC&#31639;&#27861;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#36890;&#29992;&#26694;&#26550;&#28085;&#30422;&#20102;NUTS&#20316;&#20026;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#20010;&#38468;&#24102;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#30446;&#26631;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;NUTS&#19981;&#21487;&#32422;&#21644;&#38750;&#21608;&#26399;&#30340;&#26465;&#20214;&#65292;&#24182;&#20316;&#20026;&#25512;&#35770;&#32780;&#35777;&#26126;&#20102;&#36941;&#21382;&#24615;&#12290;&#22312;&#31867;&#20284;&#20110;HMC&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;NUTS&#20855;&#26377;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;HMC&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#23545;&#27493;&#38271;&#21644;leapfrog&#27493;&#25968;&#36827;&#34892;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is substantial empirical evidence about the success of dynamic implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler (NUTS), in many challenging inference problems but theoretical results about their behavior are scarce. The aim of this paper is to fill this gap. More precisely, we consider a general class of MCMC algorithms we call dynamic HMC. We show that this general framework encompasses NUTS as a particular case, implying the invariance of the target distribution as a by-product. Second, we establish conditions under which NUTS is irreducible and aperiodic and as a corrolary ergodic. Under conditions similar to the ones existing for HMC, we also show that NUTS is geometrically ergodic. Finally, we improve existing convergence results for HMC showing that this method is ergodic without any boundedness condition on the stepsize and the number of leapfrog steps, in the case where the target is a perturbation of a Gaussian distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13580</link><description>&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816; (EOT) &#26159;&#20248;&#21270;&#36755;&#36816; (OT) &#30340;&#19968;&#31181;&#26377;&#25928;&#19988;&#35745;&#31639;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25512;&#23548;&#20986;&#20102; EOT &#25104;&#26412;&#30340;&#26032;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#29109;&#27491;&#21017;&#21270;&#21442;&#25968; $\epsilon$ &#21644;&#26679;&#26412;&#22823;&#23567; $n$ &#30340;&#32479;&#35745;&#24615;&#33021;&#20165;&#21462;&#20915;&#20110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#20043;&#20013;&#36739;&#31616;&#21333;&#30340;&#37027;&#20010;&#12290;&#20363;&#22914;&#65292;&#22312;&#20805;&#20998;&#24179;&#28369;&#30340;&#25104;&#26412;&#19979;&#65292;&#36825;&#20250;&#20135;&#29983;&#20855;&#26377;$\epsilon^{-d/2}$&#22240;&#23376;&#30340;&#21442;&#25968;&#21270;&#36895;&#29575;$n^{-1/2}$&#65292;&#20854;&#20013;$d$&#26159;&#20004;&#20010;&#24635;&#20307;&#27979;&#24230;&#30340;&#26368;&#23567;&#32500;&#24230;&#12290;&#36825;&#30830;&#35748;&#20102;&#32463;&#39564;EOT&#20063;&#36981;&#24490;&#20102;&#26368;&#36817;&#25165;&#20026;&#26410;&#35268;&#21017;&#21270;OT&#30830;&#35748;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#30340;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#32463;&#39564;&#29109;Gromov-Wasserstein&#36317;&#31163;&#21450;&#20854;&#26410;&#35268;&#21017;&#21270;&#29256;&#26412;&#20063;&#36981;&#24490;&#27492;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip
&lt;/p&gt;</description></item></channel></rss>