<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13565</link><description>&lt;p&gt;
AdaTrans&#65306;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#19982;&#26679;&#26412;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#29305;&#24449;&#32500;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#12290;&#20026;&#20102;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#21487;&#33021;&#22312;&#29305;&#24449;&#25110;&#28304;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;-wise (F-AdaTrans)&#25110;&#26679;&#26412;-wise (S-AdaTrans)&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#24809;&#32602;&#26041;&#27861;&#65292;&#32467;&#21512;&#26435;&#37325;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#36873;&#25321;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471; F-AdaTrans &#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#23558;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#19982;&#30446;&#26631;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#28388;&#38500;&#38750;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#65292;S-AdaTrans&#21017;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#28304;&#26679;&#26412;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#24674;&#22797;&#29616;&#26377;&#30340;&#36817;&#26368;&#23567;&#20284;&#20046;&#26368;&#20248;&#36895;&#29575;&#12290;&#25928;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13565v1 Announce Type: cross  Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectivene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#20449;&#24687;&#20016;&#23500;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#65292;&#21516;&#26102;&#25511;&#21046;&#25152;&#36873;&#26679;&#26412;&#30340;&#34394;&#35686;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.12295</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#34394;&#35686;&#35206;&#30422;&#29575;&#36873;&#25321;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Selecting informative conformal prediction sets with false coverage rate control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12295
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#20449;&#24687;&#20016;&#23500;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#65292;&#21516;&#26102;&#25511;&#21046;&#25152;&#36873;&#26679;&#26412;&#30340;&#34394;&#35686;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21253;&#25324;&#22238;&#24402;&#21644;&#20998;&#31867;&#65292;&#31526;&#21512;&#26041;&#27861;&#20026;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#25552;&#20379;&#39044;&#27979;&#32467;&#26524;/&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#29575;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#36825;&#31181;&#39044;&#27979;&#38598;&#21512;&#26159;&#32463;&#36807;&#36873;&#25321;&#36807;&#31243;&#24471;&#21040;&#30340;&#12290;&#35813;&#36873;&#25321;&#36807;&#31243;&#35201;&#27714;&#36873;&#25321;&#30340;&#39044;&#27979;&#38598;&#22312;&#26576;&#31181;&#26126;&#30830;&#23450;&#20041;&#30340;&#24847;&#20041;&#19978;&#26159;&#8220;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#8221;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#35774;&#32622;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#20998;&#26512;&#20154;&#21592;&#21487;&#33021;&#21482;&#32771;&#34385;&#20855;&#26377;&#39044;&#27979;&#26631;&#31614;&#38598;&#25110;&#39044;&#27979;&#21306;&#38388;&#36275;&#22815;&#23567;&#12289;&#19981;&#21253;&#25324;&#31354;&#20540;&#25110;&#36981;&#23432;&#20854;&#20182;&#36866;&#24403;&#30340;&#8220;&#21333;&#35843;&#8221;&#32422;&#26463;&#30340;&#26679;&#26412;&#20026;&#20855;&#26377;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#12290;&#34429;&#28982;&#36825;&#28085;&#30422;&#20102;&#21508;&#31181;&#24212;&#29992;&#20013;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#35768;&#22810;&#35774;&#32622;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#26500;&#24314;&#36825;&#26679;&#30340;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#65292;&#21516;&#26102;&#25511;&#21046;&#25152;&#36873;&#26679;&#26412;&#19978;&#30340;&#34394;&#35686;&#35206;&#30422;&#29575;&#65288;FCR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12295v1 Announce Type: cross  Abstract: In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictors. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be `informative' in a well defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction label sets or prediction intervals small enough, excluding null values, or obeying other appropriate `monotone' constraints. While this covers many settings of possible interest in various applications, we develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01599</link><description>&lt;p&gt;
&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#24130;&#24459;&#34928;&#20943;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26576;&#20123;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#26088;&#22312;&#30830;&#23450;&#22312;&#19981;&#21516;&#28304;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#30830;&#20999;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20005;&#26684;&#32473;&#20986;&#20102;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;&#20197;&#21450;&#22823;&#31867;&#20998;&#26512;&#35889;&#31639;&#27861;&#65289;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#26680;&#25554;&#20540;&#30340;&#36817;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#28548;&#28165;&#20855;&#26377;&#26356;&#39640;&#36164;&#26684;&#30340;&#26680;&#22238;&#24402;&#31639;&#27861;&#30340;&#39281;&#21644;&#25928;&#24212;&#65292;&#31561;&#31561;&#12290;&#30001;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#30340;&#24110;&#21161;&#65292;&#36825;&#20123;&#32467;&#26524;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65292;&#21363;&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#35266;&#27979;&#20013;&#27979;&#37327;&#36830;&#32493;&#36712;&#36857;&#30340;&#31895;&#31961;&#25351;&#25968;&#65292;&#35813;&#20272;&#35745;&#22120;&#36866;&#29992;&#20110;&#38543;&#26426;&#27874;&#21160;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26679;&#26412;&#36335;&#24452;&#20013;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2307.02582</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#23454;&#29616;&#26041;&#24046;&#30340;&#35266;&#27979;&#20013;&#20272;&#35745;&#38543;&#26426;&#27874;&#21160;&#30340;&#31895;&#31961;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating the roughness exponent of stochastic volatility from discrete observations of the realized variance. (arXiv:2307.02582v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#35266;&#27979;&#20013;&#27979;&#37327;&#36830;&#32493;&#36712;&#36857;&#30340;&#31895;&#31961;&#25351;&#25968;&#65292;&#35813;&#20272;&#35745;&#22120;&#36866;&#29992;&#20110;&#38543;&#26426;&#27874;&#21160;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26679;&#26412;&#36335;&#24452;&#20013;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#38543;&#26426;&#27874;&#21160;&#27169;&#22411;&#20013;&#20272;&#35745;&#27874;&#21160;&#24615;&#30340;&#31895;&#31961;&#24230;&#65292;&#35813;&#27169;&#22411;&#26159;&#20316;&#20026;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;&#24102;&#28418;&#31227;&#65289;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#32780;&#20135;&#29983;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#37327;&#65292;&#35813;&#20272;&#35745;&#37327;&#27979;&#37327;&#36830;&#32493;&#36712;&#36857;&#30340;&#25152;&#35859;&#31895;&#31961;&#25351;&#25968;&#65292;&#22522;&#20110;&#20854;&#21407;&#20989;&#25968;&#30340;&#31163;&#25955;&#35266;&#27979;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20110;&#22522;&#30784;&#36712;&#36857;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20197;&#20005;&#26684;&#36335;&#24452;&#26041;&#24335;&#25910;&#25947;&#12290;&#28982;&#21518;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#20960;&#20046;&#27599;&#20010;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;&#24102;&#28418;&#31227;&#65289;&#26679;&#26412;&#36335;&#24452;&#20013;&#37117;&#24471;&#21040;&#28385;&#36275;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#22312;&#22823;&#31867;&#31895;&#27874;&#21160;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#24378;&#19968;&#33268;&#24615;&#23450;&#29702;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32463;&#36807;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#23610;&#24230;&#19981;&#21464;&#20462;&#25913;&#21518;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#31243;&#24207;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the roughness of the volatility in a stochastic volatility model that arises as a nonlinear function of fractional Brownian motion with drift. To this end, we introduce a new estimator that measures the so-called roughness exponent of a continuous trajectory, based on discrete observations of its antiderivative. We provide conditions on the underlying trajectory under which our estimator converges in a strictly pathwise sense. Then we verify that these conditions are satisfied by almost every sample path of fractional Brownian motion (with drift). As a consequence, we obtain strong consistency theorems in the context of a large class of rough volatility models. Numerical simulations show that our estimation procedure performs well after passing to a scale-invariant modification of our estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32676;&#19981;&#21464;&#24615;&#30340;&#20998;&#24067;&#21464;&#37327;&#22312;&#21464;&#20998;&#24046;&#24322;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#22312;&#32676;&#22823;&#23567;&#32500;&#24230;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#26377;&#25152;&#38477;&#20302;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.01915</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#23545;&#31216;&#24615;&#30340;&#27010;&#29575;&#24046;&#24322;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Probability Divergences under Group Symmetry. (arXiv:2302.01915v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32676;&#19981;&#21464;&#24615;&#30340;&#20998;&#24067;&#21464;&#37327;&#22312;&#21464;&#20998;&#24046;&#24322;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#22312;&#32676;&#22823;&#23567;&#32500;&#24230;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#26377;&#25152;&#38477;&#20302;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20110;&#20855;&#26377;&#32676;&#19981;&#21464;&#24615;&#30340;&#20998;&#24067;&#21464;&#37327;&#22312;&#21464;&#20998;&#24046;&#24322;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#20005;&#35880;&#30340;&#37327;&#21270;&#20998;&#26512;&#12290;&#22312;Wasserstein-1&#36317;&#31163;&#21644;Lipschitz&#27491;&#21017;&#21270;&#945;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#38477;&#20302;&#19982;&#32676;&#22823;&#23567;&#30340;&#32500;&#24230;&#30456;&#20851;&#12290;&#23545;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25913;&#36827;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#21462;&#20915;&#20110;&#32676;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#20869;&#26680;&#30340;&#36873;&#25321;&#12290; &#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We rigorously quantify the improvement in the sample complexity of variational divergence estimations for group-invariant distributions. In the cases of the Wasserstein-1 metric and the Lipschitz-regularized $\alpha$-divergences, the reduction of sample complexity is proportional to an ambient-dimension-dependent power of the group size. For the maximum mean discrepancy (MMD), the improvement of sample complexity is more nuanced, as it depends on not only the group size but also the choice of kernel. Numerical simulations verify our theories.
&lt;/p&gt;</description></item></channel></rss>