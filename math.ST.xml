<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15213</link><description>&lt;p&gt;
&#32479;&#35745;&#26080;&#20559;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22238;&#24402;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Agnostic Regression: a machine learning method to validate regression models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#22240;&#21464;&#37327;&#65288;&#36890;&#24120;&#31216;&#20026;&#21709;&#24212;&#21464;&#37327;&#65289;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#33258;&#21464;&#37327;&#65288;&#21363;&#35299;&#37322;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32447;&#24615;&#22238;&#24402;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#39044;&#27979;&#12289;&#39044;&#27979;&#25110;&#22240;&#26524;&#25512;&#26029;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#25191;&#34892;&#27492;&#20219;&#21153;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#8212;&#8212;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26159;&#26356;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#22522;&#30784;&#8212;&#8212;&#21518;&#32773;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#20294;&#27809;&#26377;&#23545;&#32479;&#35745;&#26174;&#33879;&#24615;&#36827;&#34892;&#27491;&#24335;&#23450;&#20041;&#12290;&#26368;&#22810;&#65292;&#22522;&#20110;&#32463;&#39564;&#27979;&#37327;&#65288;&#22914;&#27531;&#24046;&#25110;&#20934;&#30830;&#24230;&#65289;&#36827;&#34892;&#32622;&#25442;&#25110;&#22522;&#20110;&#32463;&#20856;&#20998;&#26512;&#65292;&#20197;&#21453;&#26144;ML&#20272;&#35745;&#23545;&#26816;&#27979;&#30340;&#26356;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item></channel></rss>