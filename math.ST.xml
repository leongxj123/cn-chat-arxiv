<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02520</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#22312;&#24378;&#22122;&#22768;&#20381;&#36182;&#20551;&#35774;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#32771;&#34385;&#22122;&#22768;&#20302;&#31209;&#21152;&#31232;&#30095;&#30697;&#38453;&#24674;&#22797;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20854;&#20013;&#22122;&#22768;&#30697;&#38453;&#21487;&#20197;&#26469;&#33258;&#20219;&#24847;&#20855;&#26377;&#20803;&#32032;&#38388;&#20219;&#24847;&#20381;&#36182;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20851;&#30456;&#20301;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#37117;&#26159;&#32039;&#33268;&#30340;&#65292;&#26082;&#28385;&#36275;&#30830;&#23450;&#24615;&#19979;&#30028;&#21448;&#21305;&#37197;&#26368;&#23567;&#21270;&#39118;&#38505;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#26029;&#35328;&#20004;&#20010;&#20219;&#24847;&#30340;&#20302;&#31209;&#26080;&#20851;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#24517;&#39035;&#22312;&#20854;&#20803;&#32032;&#19978;&#25193;&#25955;&#33021;&#37327;&#65292;&#25442;&#21477;&#35805;&#35828;&#19981;&#33021;&#22826;&#31232;&#30095;&#65292;&#36825;&#25581;&#31034;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#21487;&#33021;&#24341;&#36215;&#29420;&#31435;&#20852;&#36259;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20272;&#35745;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#30340;&#38382;&#39064;&#20013;&#65292;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
&lt;/p&gt;</description></item></channel></rss>