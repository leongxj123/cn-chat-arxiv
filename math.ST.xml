<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>