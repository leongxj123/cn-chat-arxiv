<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;</title><link>https://arxiv.org/abs/2311.17885</link><description>&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#26159;&#21542;&#19968;&#30452;&#22312;&#19981;&#26029;&#36827;&#27493;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Ensembles Getting Better all the Time?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17885
&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#32467;&#21512;&#20102;&#20960;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#22987;&#32456;&#23558;&#26356;&#22810;&#27169;&#22411;&#32435;&#20837;&#38598;&#25104;&#20250;&#25552;&#21319;&#20854;&#24179;&#22343;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#38598;&#25104;&#31867;&#22411;&#65292;&#20197;&#21450;&#36873;&#25321;&#30340;&#39044;&#27979;&#24230;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25152;&#26377;&#38598;&#25104;&#25104;&#21592;&#34987;&#39044;&#26399;&#34920;&#29616;&#30456;&#21516;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#20960;&#31181;&#27969;&#34892;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#25110;&#28145;&#24230;&#38598;&#25104;&#65289;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#25165;&#20250;&#19968;&#30452;&#21464;&#24471;&#26356;&#22909;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#25104;&#30340;&#24179;&#22343;&#25439;&#22833;&#26159;&#27169;&#22411;&#25968;&#37327;&#30340;&#20943;&#20989;&#25968;&#12290;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#32467;&#26524;&#65292;&#21487;&#20197;&#24635;&#32467;&#20026;&#65306;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#31967;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#23614;&#27010;&#29575;&#21333;&#35843;&#24615;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17885v2 Announce Type: replace-cross  Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabiliti
&lt;/p&gt;</description></item></channel></rss>