<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09236</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#27010;&#24565;&#65306;&#32479;&#19968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26377;&#20004;&#31181;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#22825;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21162;&#21147;&#26041;&#21521;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25237;&#20837;&#21162;&#21147;&#21435;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#27010;&#24565;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#25968;&#25454;&#20013;&#34987;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#26469;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2202.05560</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayes&#30028;&#38480;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#20165;&#38480;&#20110;&#24615;&#33021;&#30340;&#26631;&#37327;&#24230;&#37327;&#65292;&#22914;&#25439;&#22833;&#25110;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#20449;&#24687;&#30340;PAC-Bayes&#30028;&#38480;&#65292;&#36890;&#36807;&#30028;&#23450;&#19968;&#32452;M&#31181;&#38169;&#35823;&#31867;&#22411;&#30340;&#32463;&#39564;&#27010;&#29575;&#19982;&#30495;&#23454;&#27010;&#29575;&#20043;&#38388;&#30340;Kullback-Leibler&#24046;&#24322;&#26469;&#25511;&#21046;&#21487;&#33021;&#32467;&#26524;&#30340;&#25972;&#20010;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.05560v2 Announce Type: replace-cross  Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of M error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#35266;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#19968;&#33268;&#24615;&#30340;&#36275;&#22815;&#26465;&#20214;&#65292;&#24182;&#22312;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#30340;&#26410;&#30693;&#21442;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.07656</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#19968;&#33268;&#24615;&#21450;&#22312;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Consistency of MLE for partially observed diffusions, with application in market microstructure modeling. (arXiv:2201.07656v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#35266;&#27979;&#25193;&#25955;&#36807;&#31243;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#19968;&#33268;&#24615;&#30340;&#36275;&#22815;&#26465;&#20214;&#65292;&#24182;&#22312;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#30340;&#26410;&#30693;&#21442;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#36275;&#22815;&#26465;&#20214;&#65292;&#29992;&#20110;&#25551;&#36848;&#19982;&#23436;&#20840;&#35266;&#27979;&#30340;&#25193;&#25955;&#36807;&#31243;&#30456;&#20851;&#30340;&#31283;&#24577;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#20986;&#22312;&#26410;&#30693;&#21442;&#25968;&#20540;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#23558;&#35813;&#36275;&#22815;&#26465;&#20214;&#24212;&#29992;&#20110;&#24066;&#22330;&#24494;&#35266;&#32467;&#26500;&#30340;&#28508;&#22312;&#20215;&#26684;&#27169;&#22411;&#20013;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#19979;&#26410;&#30693;&#21442;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#30340;&#21382;&#21490;&#37329;&#34701;&#25968;&#25454;&#35745;&#31639;&#20102;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a tractable sufficient condition for the consistency of maximum likelihood estimators (MLEs) in partially observed diffusion models, stated in terms of stationary distribution of the associated fully observed diffusion, under the assumption that the set of unknown parameter values is finite. This sufficient condition is then verified in the context of a latent price model of market microstructure, yielding consistency of maximum likelihood estimators of the unknown parameters in this model. Finally, we compute the latter estimators using historical financial data taken from the NASDAQ exchange.
&lt;/p&gt;</description></item></channel></rss>