<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.17148</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Differentially private low-dimensional representation of high-dimensional data. (arXiv:2305.17148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#22788;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#26102;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#20351;&#29992;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#31934;&#24230;&#30028;&#38480;&#30340;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36807;&#31243;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#19982;&#20351;&#29992;Davis-Kahan&#23450;&#29702;&#36827;&#34892;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31169;&#26377;PCA&#20998;&#26512;&#19981;&#38656;&#35201;&#20551;&#35774;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from the standard perturbation analysis using the Davis-Kahan theorem, our analysis of private PCA works without assuming the spectral gap for the sample covariance matrix.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#27491;&#24577;&#20301;&#32622;&#27169;&#22411;&#20013;&#65292;&#38480;&#21046;&#20102;&#22343;&#20540;&#26041;&#24046;&#30340;&#20808;&#39564;&#65292;&#22312;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#26368;&#22823;&#35823;&#24046;&#65292;&#32467;&#26524;&#36866;&#29992;&#20110;&#21508;&#31181;&#20808;&#39564;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.08653</link><description>&lt;p&gt;
&#27491;&#24577;&#20301;&#32622;&#27169;&#22411;&#20013;&#30340;&#22343;&#20540;-&#26041;&#24046;&#21463;&#38480;&#20808;&#39564;&#26377;&#26377;&#38480;&#30340;&#26368;&#22823;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Mean-variance constrained priors have finite maximum Bayes risk in the normal location model. (arXiv:2303.08653v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08653
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#27491;&#24577;&#20301;&#32622;&#27169;&#22411;&#20013;&#65292;&#38480;&#21046;&#20102;&#22343;&#20540;&#26041;&#24046;&#30340;&#20808;&#39564;&#65292;&#22312;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#26368;&#22823;&#35823;&#24046;&#65292;&#32467;&#26524;&#36866;&#29992;&#20110;&#21508;&#31181;&#20808;&#39564;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#27491;&#24577;&#20301;&#32622;&#27169;&#22411;&#65292;&#20854;&#20013; $X \mid \theta \sim N(\theta, \sigma^2)$&#65292;$\sigma^2$&#24050;&#30693;&#12290;&#20551;&#35774; $\theta \sim G_0$&#65292;&#20854;&#20013;&#20808;&#39564; $G_0$ &#20855;&#26377;&#38646;&#22343;&#20540;&#21644;&#21333;&#20301;&#26041;&#24046;&#12290;&#20196; $G_1$ &#20026;&#21487;&#33021;&#23384;&#22312;&#35823;&#24046;&#30340;&#38646;&#22343;&#20540;&#21644;&#21333;&#20301;&#26041;&#24046;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312; $G_0, G_1, \sigma^2 &gt; 0$ &#33539;&#22260;&#20869;&#65292;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30340;&#21518;&#39564;&#22343;&#20540;&#30340;&#24179;&#26041;&#35823;&#24046;&#26377;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a normal location model $X \mid \theta \sim N(\theta, \sigma^2)$ with known $\sigma^2$. Suppose $\theta \sim G_0$, where the prior $G_0$ has zero mean and unit variance. Let $G_1$ be a possibly misspecified prior with zero mean and unit variance. We show that the squared error Bayes risk of the posterior mean under $G_1$ is bounded, uniformly over $G_0, G_1, \sigma^2 &gt; 0$.
&lt;/p&gt;</description></item></channel></rss>