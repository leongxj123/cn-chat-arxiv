<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08150</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20999;&#29255;&#36870;&#22238;&#24402;: &#26497;&#23567;&#26497;&#22823;&#24615;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sliced Inverse Regression: Minimax Optimality and Algorithm. (arXiv:2401.08150v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#38544;&#31169;&#20445;&#25252;&#24050;&#25104;&#20026;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20999;&#29255;&#36870;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#21327;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#23384;&#37325;&#35201;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20223;&#30495;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Proposed by Li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.03816</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#19982;&#21322;&#21442;&#25968;Bernstein-von-Mises&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reparametrization and the Semiparametric Bernstein-von-Mises Theorem. (arXiv:2306.03816v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#37096;&#20998;&#32447;&#24615;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22238;&#24402;&#20989;&#25968;&#30340;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#25152;&#20851;&#24515;&#30340;&#20302;&#32500;&#21442;&#25968;&#12290;&#21442;&#25968;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#29983;&#25104;&#20102;&#19968;&#20010;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#20272;&#35745;&#20302;&#32500;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#22823;&#26679;&#26412;&#20998;&#26512;&#25903;&#25345;&#20102;&#36825;&#31181;&#35828;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#20302;&#32500;&#21442;&#25968;&#30340;&#21518;&#39564;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#30495;&#23454;&#20540;&#25910;&#32553;&#65292;&#24182;&#19988;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#28176;&#36817;&#22320;&#27491;&#24577;&#20998;&#24067;&#12290;&#36825;&#20123;&#26465;&#20214;&#30456;&#23545;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#21407;&#22987;&#21442;&#25968;&#21270;&#20801;&#35768;&#26356;&#22823;&#31867;&#30340;&#24178;&#25200;&#21442;&#25968;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19968;&#20010;&#23884;&#20837;&#20102;Neyman&#27491;&#20132;&#24615;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#25104;&#20026;&#21322;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26377;&#29992;&#24037;&#20855;&#65292;&#20197;&#21435;&#20559;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers Bayesian inference for the partially linear model. Our approach exploits a parametrization of the regression function that is tailored toward estimating a low-dimensional parameter of interest. The key property of the parametrization is that it generates a Neyman orthogonal moment condition meaning that the low-dimensional parameter is less sensitive to the estimation of nuisance parameters. Our large sample analysis supports this claim. In particular, we derive sufficient conditions under which the posterior for the low-dimensional parameter contracts around the truth at the parametric rate and is asymptotically normal with a variance that coincides with the semiparametric efficiency bound. These conditions allow for a larger class of nuisance parameters relative to the original parametrization of the regression model. Overall, we conclude that a parametrization that embeds Neyman orthogonality can be a useful device for debiasing posterior distributions in semipa
&lt;/p&gt;</description></item></channel></rss>