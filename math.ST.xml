<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14294</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#20114;&#25442;&#24615;&#23454;&#29616;&#39640;&#21442;&#25968;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-arity PAC learning via exchangeability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#32500;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21363;&#22312;&#8220;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#8221;&#23384;&#22312;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#12290; &#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#20551;&#35774;&#21487;&#20197;&#26159;&#22270;&#24418;&#12289;&#36229;&#22270;&#65292;&#25110;&#32773;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26159;&#26377;&#38480;&#20851;&#31995;&#35821;&#35328;&#20013;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;i.i.d.&#25277;&#26679;&#34987;&#25277;&#26679;&#20135;&#29983;&#21487;&#20114;&#25442;&#20998;&#24067;&#30340;&#35825;&#23548;&#23376;&#32467;&#26500;&#21462;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#65292;&#36890;&#36807;&#34920;&#24449;&#39640;&#32500;&#65288;agnostic&#65289;PAC&#21487;&#23398;&#24615;&#65292;&#20197;&#32431;&#32452;&#21512;&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#21450;&#36866;&#24403;&#29256;&#26412;&#30340;&#22343;&#21248;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14294v1 Announce Type: new  Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.13867</link><description>&lt;p&gt;
Langevin-Based Non-Convex Sampling&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.
&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#37319;&#26679;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#20197;&#21450;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#29702;&#35770;&#19978;&#20173;&#23384;&#22312;&#35768;&#22810;&#37325;&#35201;&#25361;&#25112;&#65306;&#29616;&#26377;&#30340;&#20445;&#35777;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#24179;&#22343;&#36845;&#20195;&#32780;&#19981;&#26159;&#26356;&#29702;&#24819;&#30340;&#26368;&#21518;&#36845;&#20195;&#65292;&#32570;&#20047;&#25429;&#25417;&#21464;&#37327;&#23610;&#24230;&#65288;&#22914;Wasserstein&#36317;&#31163;&#65289;&#30340;&#25910;&#25947;&#24230;&#37327;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#31561;&#22522;&#26412;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;&#32467;&#21512;MCMC&#37319;&#26679;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#31435;&#21363;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
&lt;/p&gt;</description></item></channel></rss>