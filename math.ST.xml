<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#30740;&#31350;&#20102;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22312;&#23725;&#27491;&#21017;&#21270;&#21442;&#25968;&#36235;&#36817;&#20110;&#38646;&#26102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#21452;&#35895;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.20200</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#26041;&#24046;&#36718;&#24275;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#23725;&#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20200
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22312;&#23725;&#27491;&#21017;&#21270;&#21442;&#25968;&#36235;&#36817;&#20110;&#38646;&#26102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#21452;&#35895;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#29420;&#31435;&#20294;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#12290;&#20551;&#35774;&#35266;&#27979;&#21040;&#30340;&#39044;&#27979;&#21464;&#37327;&#38598;&#21512;&#26159;&#24102;&#26377;&#26041;&#24046;&#36718;&#24275;&#30340;&#38543;&#26426;&#30697;&#38453;&#65292;&#24182;&#19988;&#20854;&#32500;&#24230;&#20197;&#30456;&#24212;&#36895;&#29575;&#22686;&#38271;&#12290;&#22312;&#20551;&#35774;&#38543;&#26426;&#25928;&#24212;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36825;&#31181;&#26041;&#24046;&#36718;&#24275;&#30340;&#23725;&#20272;&#35745;&#22120;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;&#39044;&#27979;&#39118;&#38505;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#39118;&#38505;&#30340;&#30830;&#23450;&#24615;&#31561;&#20215;&#29289;&#20197;&#21450;&#23725;&#20272;&#35745;&#22120;&#30340;&#33258;&#30001;&#24230;&#12290;&#23545;&#20110;&#26576;&#20123;&#26041;&#24046;&#36718;&#24275;&#31867;&#21035;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#23725;&#27491;&#21017;&#21270;&#21442;&#25968;&#36235;&#20110;&#38646;&#26102;&#65292;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#26368;&#23567;&#27169;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#20986;&#29616;&#21452;&#35895;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20123;&#26041;&#24046;&#36718;&#24275;f...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20200v1 Announce Type: cross  Abstract: High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data. We propose to investigate high-dimensional regression models for independent but non-identically distributed data. To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate. Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile. In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator. For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero. We also exhibit variance profiles f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#35889;&#27491;&#21017;&#21270;&#30340;&#26680;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#22312;&#36873;&#25321;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#26102;&#33021;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#26412;&#26041;&#27861;&#36824;&#20811;&#26381;&#20102;&#20043;&#21069;&#26041;&#27861;&#23545;&#22343;&#20540;&#20803;&#32032;&#20026;&#38646;&#21644;&#31215;&#20998;&#25805;&#20316;&#31526;&#29305;&#24449;&#20989;&#25968;&#22343;&#21248;&#26377;&#30028;&#24615;&#26465;&#20214;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#35745;&#31639;&#26356;&#22810;&#31181;&#31867;&#30340;&#26680;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04561</link><description>&lt;p&gt;
&#20855;&#26377;&#35889;&#27491;&#21017;&#21270;&#30340;&#26680;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Spectral Regularized Kernel Goodness-of-Fit Tests. (arXiv:2308.04561v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#35889;&#27491;&#21017;&#21270;&#30340;&#26680;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#22312;&#36873;&#25321;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#26102;&#33021;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#26412;&#26041;&#27861;&#36824;&#20811;&#26381;&#20102;&#20043;&#21069;&#26041;&#27861;&#23545;&#22343;&#20540;&#20803;&#32032;&#20026;&#38646;&#21644;&#31215;&#20998;&#25805;&#20316;&#31526;&#29305;&#24449;&#20989;&#25968;&#22343;&#21248;&#26377;&#30028;&#24615;&#26465;&#20214;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#35745;&#31639;&#26356;&#22810;&#31181;&#31867;&#30340;&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24212;&#29992;&#20013;&#65292;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#22240;&#20854;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#21253;&#25324;&#38750;&#21442;&#25968;&#20551;&#35774;&#26816;&#39564;&#12290;&#26368;&#36817;&#65292;Balasubramanian&#31561;&#20154;(2021)&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;MMD&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#22312;&#36866;&#24403;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#26102;&#65292;&#24182;&#19981;&#26159;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#65292;&#32780;&#20854;Tikhonov&#27491;&#21017;&#21270;&#29256;&#26412;&#21017;&#26159;&#26368;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#30340;&#12290;&#28982;&#32780;&#65292;Balasubramanian&#31561;&#20154;(2021)&#30340;&#32467;&#26524;&#26159;&#22312;&#22343;&#20540;&#20803;&#32032;&#20026;&#38646;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#21644;&#31215;&#20998;&#25805;&#20316;&#31526;&#29305;&#24449;&#20989;&#25968;&#30340;&#22343;&#21248;&#26377;&#30028;&#24615;&#26465;&#20214;&#19979;&#33719;&#24471;&#30340;&#12290;&#27492;&#22806;&#65292;Balasubramanian&#31561;&#20154;(2021)&#25552;&#20986;&#30340;&#26816;&#39564;&#22312;&#35768;&#22810;&#26680;&#20989;&#25968;&#20013;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#19981;&#23454;&#29992;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#21253;&#25324;Tikhonov&#27491;&#21017;&#21270;&#22312;&#20869;&#30340;&#19968;&#33324;&#35889;&#27491;&#21017;&#21270;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) has enjoyed a lot of success in many machine learning and statistical applications, including non-parametric hypothesis testing, because of its ability to handle non-Euclidean data. Recently, it has been demonstrated in Balasubramanian et al.(2021) that the goodness-of-fit test based on MMD is not minimax optimal while a Tikhonov regularized version of it is, for an appropriate choice of the regularization parameter. However, the results in Balasubramanian et al. (2021) are obtained under the restrictive assumptions of the mean element being zero, and the uniform boundedness condition on the eigenfunctions of the integral operator. Moreover, the test proposed in Balasubramanian et al. (2021) is not practical as it is not computable for many kernels. In this paper, we address these shortcomings and extend the results to general spectral regularizers that include Tikhonov regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27491;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#21457;&#29616;&#24403;&#25968;&#25454;&#30340;&#20004;&#20010;&#32500;&#24230;&#26080;&#38480;&#22686;&#38271;&#26102;&#65292;&#20256;&#32479;&#30340;CCA&#20272;&#35745;&#36807;&#31243;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#37117;&#24456;&#22823;&#26102;&#26080;&#27861;&#35782;&#21035;&#27491;&#20856;&#21464;&#37327;&#30340;CCA&#36807;&#31243;&#30340;&#19981;&#21487;&#33021;&#24615;&#12290;&#24182;&#25552;&#20379;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#37327;&#32423;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;CCA&#20272;&#35745;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16393</link><description>&lt;p&gt;
&#39640;&#32500;&#27491;&#20856;&#30456;&#20851;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Canonical Correlation Analysis. (arXiv:2306.16393v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27491;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#21457;&#29616;&#24403;&#25968;&#25454;&#30340;&#20004;&#20010;&#32500;&#24230;&#26080;&#38480;&#22686;&#38271;&#26102;&#65292;&#20256;&#32479;&#30340;CCA&#20272;&#35745;&#36807;&#31243;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#37117;&#24456;&#22823;&#26102;&#26080;&#27861;&#35782;&#21035;&#27491;&#20856;&#21464;&#37327;&#30340;CCA&#36807;&#31243;&#30340;&#19981;&#21487;&#33021;&#24615;&#12290;&#24182;&#25552;&#20379;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#37327;&#32423;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;CCA&#20272;&#35745;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27491;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#23450;&#20041;&#27491;&#20856;&#21464;&#37327;&#30340;&#21521;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#30340;&#20004;&#20010;&#32500;&#24230;&#20849;&#21516;&#19988;&#25104;&#27604;&#20363;&#22320;&#22686;&#38271;&#21040;&#26080;&#31351;&#26102;&#65292;&#20256;&#32479;&#30340;CCA&#20272;&#35745;&#36807;&#31243;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#36825;&#26159;&#39318;&#27425;&#30740;&#31350;&#22312;&#25152;&#26377;&#32500;&#24230;&#37117;&#24456;&#22823;&#26102;&#26080;&#27861;&#35782;&#21035;&#27491;&#20856;&#21464;&#37327;&#30340;CCA&#36807;&#31243;&#30340;&#19981;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25512;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#22823;&#23567;&#65292;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#29992;&#20110;&#35780;&#20272;CCA&#20272;&#35745;&#30340;&#31934;&#30830;&#24230;&#12290;&#36824;&#25552;&#20379;&#20102;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#30707;&#28784;&#33609;&#22320;&#25968;&#25454;&#38598;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies high-dimensional canonical correlation analysis (CCA) with an emphasis on vectors which define canonical variables. The paper shows that when two dimensions of data grow to infinity jointly and proportionally the classical CCA procedure for estimating those vectors fails to deliver a consistent estimate. This provides the first result on impossibility of the identification of canonical variables in CCA procedure when all dimensions are large. To offset, the paper derives the magnitude of the estimation error, which can be used in practice to assess the precision of CCA estimates. An application of the results to limestone grassland data set is provided.
&lt;/p&gt;</description></item></channel></rss>