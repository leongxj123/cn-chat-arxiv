<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#22522;&#20110;&#38750;&#21442;&#25968;&#20998;&#24067;&#30340;&#32858;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.06108</link><description>&lt;p&gt;
&#22522;&#20110;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#32858;&#31867;&#30340;&#38750;&#21442;&#25968;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Nonparametric consistency for maximum likelihood estimation and clustering based on mixtures of elliptically-symmetric distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06108
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#22522;&#20110;&#38750;&#21442;&#25968;&#20998;&#24067;&#30340;&#32858;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#23545;&#20854;&#24635;&#20307;&#29256;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#20013;&#28508;&#22312;&#20998;&#24067;P&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19981;&#19968;&#23450;&#23646;&#20110;&#20272;&#35745;&#22120;&#25152;&#22522;&#20110;&#30340;&#28151;&#21512;&#31867;&#21035;&#12290;&#24403;P&#26159;&#36275;&#22815;&#20998;&#31163;&#20294;&#38750;&#21442;&#25968;&#30340;&#20998;&#24067;&#28151;&#21512;&#26102;&#65292;&#34920;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#24635;&#20307;&#29256;&#26412;&#30340;&#32452;&#20998;&#23545;&#24212;&#20110;P&#30340;&#33391;&#22909;&#20998;&#31163;&#32452;&#20998;&#12290;&#36825;&#20026;&#22312;P&#20855;&#26377;&#33391;&#22909;&#20998;&#31163;&#23376;&#24635;&#20307;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#26679;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#19978;&#30340;&#29702;&#25454;&#65292;&#21363;&#20351;&#36825;&#20123;&#23376;&#24635;&#20307;&#19982;&#28151;&#21512;&#27169;&#22411;&#25152;&#20551;&#35774;&#30340;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06108v2 Announce Type: replace-cross  Abstract: The consistency of the maximum likelihood estimator for mixtures of elliptically-symmetric distributions for estimating its population version is shown, where the underlying distribution $P$ is nonparametric and does not necessarily belong to the class of mixtures on which the estimator is based. In a situation where $P$ is a mixture of well enough separated but nonparametric distributions it is shown that the components of the population version of the estimator correspond to the well separated components of $P$. This provides some theoretical justification for the use of such estimators for cluster analysis in case that $P$ has well separated subpopulations even if these subpopulations differ from what the mixture model assumes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36866;&#24212;Hui-Walter&#33539;&#24335;&#65292;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;&#26041;&#27861;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26080;&#27861;&#33719;&#24471;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#20013;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#22312;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09376</link><description>&lt;p&gt;
&#35299;&#38145;&#26080;&#26631;&#31614;&#25968;&#25454;: Hui-Walter&#33539;&#24335;&#22312;&#22312;&#32447;&#21644;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings. (arXiv:2401.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36866;&#24212;Hui-Walter&#33539;&#24335;&#65292;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;&#26041;&#27861;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26080;&#27861;&#33719;&#24471;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#20013;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#22312;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#39046;&#22495;&#65292;&#20174;&#19994;&#20154;&#21592;&#24120;&#24120;&#22312;&#21487;&#35780;&#20272;&#21644;&#35757;&#32451;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#21487;&#35775;&#38382;&#30340;&#12289;&#38745;&#24577;&#30340;&#12289;&#24102;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#24448;&#24448;&#20559;&#31163;&#20102;&#29616;&#23454;&#65292;&#20854;&#20013;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#12289;&#21152;&#23494;&#30340;&#12289;&#38590;&#20197;&#27979;&#37327;&#30340;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;Hui-Walter&#33539;&#24335;&#35843;&#25972;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#22914;&#20551;&#38451;&#24615;&#29575;&#12289;&#20551;&#38452;&#24615;&#29575;&#21644;&#20808;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#31181;&#33539;&#24335;&#26469;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#65292;&#24320;&#36767;&#20102;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#20197;&#27169;&#25311;&#22810;&#20010;&#25968;&#25454;&#32676;&#20307;&#65288;&#22914;&#26524;&#27809;&#26377;&#33258;&#28982;&#32676;&#20307;&#21487;&#29992;&#65289;&#65292;&#24182;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#26469;&#22797;&#21046;&#22810;&#27425;&#27979;&#35797;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#20043;&#38388;&#20132;&#21449;&#21046;&#34920;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20108;&#20803;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning and statistical modeling, practitioners often work under the assumption of accessible, static, labeled data for evaluation and training. However, this assumption often deviates from reality where data may be private, encrypted, difficult- to-measure, or unlabeled. In this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method traditionally applied in epidemiology and medicine, to the field of machine learning. This approach enables us to estimate key performance metrics such as false positive rate, false negative rate, and priors in scenarios where no ground truth is available. We further extend this paradigm for handling online data, opening up new possibilities for dynamic data environments. Our methodology involves partitioning data into latent classes to simulate multiple data populations (if natural populations are unavailable) and independently training models to replicate multiple tests. By cross-tabulating binary outcomes across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.00978</link><description>&lt;p&gt;
&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Certified Multi-Fidelity Zeroth-Order Optimization. (arXiv:2308.00978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36817;&#20284;&#27700;&#24179;&#65288;&#20195;&#20215;&#19981;&#21516;&#65289;&#19978;&#35780;&#20272;&#20989;&#25968;$f$&#65292;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#20302;&#30340;&#20195;&#20215;&#20248;&#21270;$f$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\emph{&#35748;&#35777;}&#31639;&#27861;&#65292;&#23427;&#20204;&#39069;&#22806;&#35201;&#27714;&#36755;&#20986;&#19968;&#20010;&#23545;&#20248;&#21270;&#35823;&#24046;&#30340;&#25968;&#25454;&#39537;&#21160;&#19978;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#20197;&#31639;&#27861;&#21644;&#35780;&#20272;&#29615;&#22659;&#20043;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#24418;&#24335;&#26469;&#24418;&#24335;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#22312;&#20219;&#24847;Lipschitz&#20989;&#25968;$f$&#19978;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;$f$&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#31034;&#20363;&#35299;&#20915;&#20102;&#26377;&#22122;&#22768;&#65288;&#38543;&#26426;&#65289;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
&lt;/p&gt;</description></item></channel></rss>