<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11228</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#20248;&#21270;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Balancing for Optimal Random Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11228
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#26862;&#26519;&#36890;&#24120;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24212;&#24615;&#65292;&#25110;&#22312;&#31616;&#21333;&#12289;&#24179;&#28369;&#24773;&#26223;&#19979;&#22833;&#21435;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26641;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;Lipschitz&#31867;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#24179;&#28369;&#24615;&#27700;&#24179;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35813;&#29256;&#26412;&#22312;&#20219;&#24847;$q \in \mathbb{N}$&#21644;$\beta \in (0,1]$&#30340;H&#246;lder&#31867;$\mathcal{H}^{q,\beta}$&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24179;&#34913;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36807;&#24230;&#20381;&#36182;&#36741;&#21161;&#38543;&#26426;&#24615;&#21487;&#33021;&#20250;&#25439;&#23475;&#26641;&#27169;&#22411;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#26356;&#24179;&#34913;&#12289;&#26356;&#23569;&#38543;&#26426;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.14942</link><description>&lt;p&gt;
&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Optimality of Misspecified Spectral Algorithms. (arXiv:2303.14942v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;RKHSs&#19978;&#65292;&#35889;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20934;&#30830;&#35889;&#31639;&#27861;&#38382;&#39064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20551;&#35774;&#22320;&#19979;&#30495;&#23454;&#20989;&#25968;$f_{\rho}^{*} \in [\mathcal{H}]^{s}$&#65292;&#20854;&#20013;$\mathcal{H}$&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#36739;&#24179;&#28369;&#25554;&#20540;&#31354;&#38388;&#65292;$s\in (0,1)$&#12290;&#29616;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#32467;&#26524;&#35201;&#27714;$\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$&#65292;&#36825;&#38544;&#21547;&#22320;&#35201;&#27714;$s &gt; \alpha_{0}$&#65292;&#20854;&#20013;$\alpha_{0}\in (0,1)$&#26159;&#23884;&#20837;&#25351;&#25968;&#65292;&#19968;&#20010;&#20381;&#36182;&#20110;$\mathcal{H}$&#30340;&#24120;&#25968;&#12290;&#20851;&#20110;&#35889;&#31639;&#27861;&#26159;&#21542;&#23545;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26368;&#20248;&#30340;&#38382;&#39064;&#24050;&#32463;&#23384;&#22312;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35889;&#31639;&#27861;&#26159;&#23545;&#20110;&#20219;&#24847;&#30340;$\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#20854;&#20013;$\beta$&#26159;$\mathcal{H}$&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#20960;&#31867;&#28385;&#36275;$ \alpha_0 = \frac{1}{\beta} $&#30340;RKHSs&#65292;&#22240;&#27492;&#65292;&#35889;&#31639;&#27861;&#22312;&#36825;&#20123;RKHSs&#19978;&#23545;&#20110;&#25152;&#26377;&#30340;$s\in (0,1)$&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the misspecified spectral algorithms problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}&lt;\infty$ which implicitly requires $s &gt; \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the spectral algorithms are optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that spectral algorithms are minimax optimal for any $\alpha_{0}-\frac{1}{\beta} &lt; s &lt; 1$, where $\beta$ is the eigenvalue decay rate of $\mathcal{H}$. We also give several classes of RKHSs whose embedding index satisfies $ \alpha_0 = \frac{1}{\beta} $. Thus, the spectral algorithms are minimax optimal for all $s\in (0,1)$ on these RKHSs.
&lt;/p&gt;</description></item></channel></rss>