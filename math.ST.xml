<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07355</link><description>&lt;p&gt;
&#20174;&#22343;&#22330;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from the Mean-Field Stationary Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#65292;&#21363;&#21253;&#21547;&#20132;&#20114;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#26368;&#23567;&#21270;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27934;&#23519;&#26159;&#23558;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#35299;&#32806;&#65306;(1) &#36890;&#36807;&#26377;&#38480;&#31890;&#23376;&#31995;&#32479;&#36924;&#36817;&#22343;&#22330;SDE&#65292;&#36890;&#36807;&#26102;&#38388;&#22343;&#21248;&#20256;&#25773;&#28151;&#27788;&#65292;&#21644;(2) &#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20985;&#25277;&#26679;&#22120;&#20174;&#26377;&#38480;&#31890;&#23376;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#20854;&#28789;&#27963;&#24615;&#20801;&#35768;&#32467;&#21512;&#29992;&#20110;&#31639;&#27861;&#21644;&#29702;&#35770;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.07187</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#32508;&#36848;&#65306;&#36817;&#20284;&#65292;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models. (arXiv:2401.07187v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#35282;&#24230;&#22238;&#39038;&#20102;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#32479;&#35745;&#29702;&#35770;&#30340;&#25991;&#29486;&#12290;&#31532;&#19968;&#37096;&#20998;&#22238;&#39038;&#20102;&#22312;&#22238;&#24402;&#25110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#19979;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#24335;&#26500;&#36896;&#65292;&#20197;&#21450;&#37319;&#29992;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#23548;&#33268;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#36890;&#36807;&#36825;&#20123;&#26500;&#36896;&#65292;&#21487;&#20197;&#29992;&#26679;&#26412;&#22823;&#23567;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#20989;&#25968;&#24179;&#28369;&#24615;&#26469;&#34920;&#36798;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#22522;&#26412;&#20998;&#26512;&#20165;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#38750;&#20984;&#30340;&#20840;&#23616;&#26497;&#23567;&#20540;&#28857;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#31532;&#20108;&#37096;&#20998;&#22238;&#39038;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#37027;&#20123;&#35797;&#22270;&#22238;&#31572;&#8220;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#25214;&#21040;&#33021;&#22815;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#35299;&#8221;&#30340;&#35770;&#25991;&#12290;&#23588;&#20854;&#26159;&#20004;&#20010;&#30693;&#21517;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17582</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35745;&#31639;&#25968;&#25454;&#29983;&#25104;&#21644;&#20284;&#28982;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#23454;&#35777;&#34920;&#29616;&#19978;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;&#19982;&#30456;&#20851;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#31215;&#32047;&#29702;&#35770;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20110;&#22312;&#27491;&#21521;&#65288;&#25968;&#25454;&#21040;&#22122;&#22768;&#65289;&#21644;&#21453;&#21521;&#65288;&#22122;&#22768;&#21040;&#25968;&#25454;&#65289;&#26041;&#21521;&#19978;&#37117;&#26159;&#30830;&#23450;&#24615;&#30340;&#27969;&#27169;&#22411;&#30340;&#20998;&#26512;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#20013;&#23454;&#26045;Jordan-Kinderleherer-Otto&#65288;JKO&#65289;&#26041;&#26696;&#30340;&#25152;&#35859;JKO&#27969;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#28176;&#36827;&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;JKO&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;Kullback-Leibler&#65288;KL&#65289;&#20445;&#35777;&#20026;$O(\varepsilon^2)$&#65292;&#20854;&#20013;&#20351;&#29992;$N \lesssim \log (1/\varepsilon)$&#20010;JKO&#27493;&#39588;&#65288;&#27969;&#20013;&#30340;$N$&#20010;&#27531;&#24046;&#22359;&#65289;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#27599;&#27493;&#19968;&#38454;&#26465;&#20214;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;Bootstrap-Aided&#25512;&#26029;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#38750;&#21442;&#25968;Bootstrap&#38590;&#20197;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13598</link><description>&lt;p&gt;
&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;Bootstrap-Aided&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bootstrap-Assisted Inference for Generalized Grenander-type Estimators. (arXiv:2303.13598v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;Bootstrap-Aided&#25512;&#26029;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#38750;&#21442;&#25968;Bootstrap&#38590;&#20197;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Westling&#21644;Carone&#65288;2020&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#36825;&#26159;&#19968;&#31867;&#29992;&#20110;&#21333;&#35843;&#20989;&#25968;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#30340;&#22810;&#25165;&#22810;&#33402;&#30340;&#31867;&#12290;&#36825;&#20123;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#34920;&#31034;&#20026;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#22823;&#20984;&#25903;&#25745;&#32447;&#30340;&#24038;&#23548;&#25968;&#65292;&#35813;&#39640;&#26031;&#36807;&#31243;&#30340;&#21327;&#26041;&#24046;&#26680;&#21487;&#20197;&#24456;&#22797;&#26434;&#65292;&#20854;&#21333;&#39033;&#24335;&#22343;&#20540;&#21487;&#20197;&#26159;&#26410;&#30693;&#38454;&#25968;&#65288;&#22914;&#26524;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#30340;&#24179;&#22374;&#24230;&#26410;&#30693;&#65289;&#12290;&#26631;&#20934;&#30340;&#38750;&#21442;&#25968;bootstrap&#21363;&#20351;&#30693;&#36947;&#22343;&#20540;&#30340;&#21333;&#39033;&#24335;&#39034;&#24207;&#65292;&#20063;&#26080;&#27861;&#19968;&#33268;&#22320;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#22312;&#24212;&#29992;&#20013;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25512;&#26029;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;bootstrap&#36741;&#21161;&#25512;&#26029;&#31243;&#24207;&#12290;&#35813;&#31243;&#24207;&#20381;&#36182;&#20110;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#20294;&#33258;&#21160;&#21270;&#30340;&#21464;&#25442;e
&lt;/p&gt;
&lt;p&gt;
Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose covariance kernel can be complicated and whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#32676;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#65292;&#24182;&#33021;&#22815;&#20197;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#24369;&#30456;&#20851;&#38543;&#26426;&#36807;&#31243;&#30340;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
High-dimensional variable clustering based on sub-asymptotic maxima of a weakly dependent random process. (arXiv:2302.00934v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00934
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#32676;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#65292;&#24182;&#33021;&#22815;&#20197;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#28176;&#36817;&#29420;&#31435;&#22359; (AI-block) &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#24179;&#31283;&#28151;&#21512;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#26469;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#12290;&#35813;&#27169;&#22411;&#31867;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#31181;&#20559;&#24207;&#20851;&#31995;&#65292;&#20801;&#35768;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26080;&#38656;&#20107;&#20808;&#25351;&#23450;&#32676;&#38598;&#30340;&#25968;&#37327;&#21363;&#21487;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#27934;&#23519;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#22312;&#32500;&#24230;&#20013;&#26159;&#22810;&#39033;&#24335;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#12290;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#38750;&#21442;&#25968;&#22320;&#23398;&#20064;&#20986;&#20165;&#20165;&#26159;&#20122;&#28176;&#36817;&#30340;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#32676;&#32452;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new class of models for variable clustering called Asymptotic Independent block (AI-block) models, which defines population-level clusters based on the independence of the maxima of a multivariate stationary mixing random process among clusters. This class of models is identifiable, meaning that there exists a maximal element with a partial order between partitions, allowing for statistical inference. We also present an algorithm for recovering the clusters of variables without specifying the number of clusters \emph{a priori}. Our work provides some theoretical insights into the consistency of our algorithm, demonstrating that under certain conditions it can effectively identify clusters in the data with a computational complexity that is polynomial in the dimension. This implies that groups can be learned nonparametrically in which block maxima of a dependent process are only sub-asymptotic. To further illustrate the significance of our work, we applied our method to neu
&lt;/p&gt;</description></item></channel></rss>