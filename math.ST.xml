<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#21644;&#26080;&#38656;&#23494;&#24230;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#20102;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07296</link><description>&lt;p&gt;
&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating the Mixing Coefficients of Geometrically Ergodic Markov Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#21644;&#26080;&#38656;&#23494;&#24230;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#20102;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#23454;&#20540;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#21333;&#20010;&#946;-&#28151;&#21512;&#31995;&#25968;&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#26679;&#26412;&#36335;&#24452;X0&#65292;X1&#65292;...&#65292;Xn&#12290;&#22312;&#23545;&#23494;&#24230;&#30340;&#26631;&#20934;&#20809;&#28369;&#26465;&#20214;&#19979;&#65292;&#21363;&#23545;&#20110;&#27599;&#20010;m&#65292;&#23545;$(X_0,X_m)$&#23545;&#30340;&#32852;&#21512;&#23494;&#24230;&#37117;&#23646;&#20110;&#26576;&#20010;&#24050;&#30693;$s&gt;0$&#30340; Besov &#31354;&#38388;$B^s_{1,\infty}(\mathbb R^2)$&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25105;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\log(n) n^{-[s]/(2[s]+2)})$ &#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#30028;&#38480;&#36827;&#34892;&#20102;&#34917;&#20805;&#65292;&#24182;&#22312;&#29366;&#24577;&#31354;&#38388;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#36825;&#20123;&#30028;&#38480;&#30340;&#31867;&#27604;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#23494;&#24230;&#30340;&#20551;&#35774;&#65307;&#39044;&#26399;&#35823;&#24046;&#29575;&#26174;&#31034;&#20026;$\mathcal O(\log(
&lt;/p&gt;
&lt;p&gt;
We propose methods to estimate the individual $\beta$-mixing coefficients of a real-valued geometrically ergodic Markov process from a single sample-path $X_0,X_1, \dots,X_n$. Under standard smoothness conditions on the densities, namely, that the joint density of the pair $(X_0,X_m)$ for each $m$ lies in a Besov space $B^s_{1,\infty}(\mathbb R^2)$ for some known $s&gt;0$, we obtain a rate of convergence of order $\mathcal{O}(\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\footnote{We use $[s]$ to denote the integer part of the decomposition $s=[s]+\{s\}$ of $s \in (0,\infty)$ into an integer term and a {\em strictly positive} remainder term $\{s\} \in (0,1]$.}. We complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite. Naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\mathcal O(\log(
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>