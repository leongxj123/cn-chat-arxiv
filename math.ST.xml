<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24191;&#27867;&#27169;&#22411;&#20013;&#36827;&#34892;&#26368;&#20248;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20013;&#24230;&#20559;&#24046;&#21407;&#29702;&#26500;&#24314;&#39640;&#24230;&#20934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#28385;&#36275;&#25351;&#25968;&#31934;&#24230;&#12289;&#19968;&#33268;&#24615;&#21644;&#26368;&#22823;&#31934;&#24230;&#31561;&#26631;&#20934;&#65292;&#20026;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.14496</link><description>&lt;p&gt;
&#36890;&#36807;&#20013;&#24230;&#20559;&#24046;&#29702;&#35770;&#36827;&#34892;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learning via Moderate Deviations Theory. (arXiv:2305.14496v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#24191;&#27867;&#27169;&#22411;&#20013;&#36827;&#34892;&#26368;&#20248;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20013;&#24230;&#20559;&#24046;&#21407;&#29702;&#26500;&#24314;&#39640;&#24230;&#20934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#28385;&#36275;&#25351;&#25968;&#31934;&#24230;&#12289;&#19968;&#33268;&#24615;&#21644;&#26368;&#22823;&#31934;&#24230;&#31561;&#26631;&#20934;&#65292;&#20026;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#27867;&#27169;&#22411;&#20013;&#20351;&#29992;&#32622;&#20449;&#21306;&#38388;&#23398;&#20064;&#20989;&#25968;&#20540;&#30340;&#32479;&#35745;&#26368;&#20248;&#26041;&#27861;&#65292;&#21253;&#25324;&#25551;&#36848;&#20026;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#25110;&#21508;&#31181;SDE&#27169;&#22411;&#30340;&#26399;&#26395;&#25439;&#22833;&#30340;&#19968;&#33324;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20013;&#24230;&#20559;&#24046;&#21407;&#29702;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#26500;&#24314;&#39640;&#24230;&#20934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32622;&#20449;&#21306;&#38388;&#22312;&#32479;&#35745;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#28385;&#36275;&#20197;&#25351;&#25968;&#31934;&#24230;&#12289;&#26368;&#23567;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#35823;&#21028;&#27010;&#29575;&#20197;&#21450;&#26368;&#32456;&#30340;&#19968;&#33268;&#26368;&#22823;&#31934;&#24230;&#20026;&#26631;&#20934;&#30340;&#35201;&#27714;&#12290;&#35813;&#26041;&#27861;&#25552;&#20986;&#30340;&#32622;&#20449;&#21306;&#38388;&#26159;&#36890;&#36807;&#24378;&#21270;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26469;&#34920;&#36798;&#30340;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#24341;&#21457;&#30340;&#20013;&#24230;&#20559;&#24046;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23545;&#20110;&#35768;&#22810;&#27169;&#22411;&#65292;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#26131;&#20110;&#35299;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a statistically optimal approach for learning a function value using a confidence interval in a wide range of models, including general non-parametric estimation of an expected loss described as a stochastic programming problem or various SDE models. More precisely, we develop a systematic construction of highly accurate confidence intervals by using a moderate deviation principle-based approach. It is shown that the proposed confidence intervals are statistically optimal in the sense that they satisfy criteria regarding exponential accuracy, minimality, consistency, mischaracterization probability, and eventual uniformly most accurate (UMA) property. The confidence intervals suggested by this approach are expressed as solutions to robust optimization problems, where the uncertainty is expressed via the underlying moderate deviation rate function induced by the data-generating process. We demonstrate that for many models these optimization problems admit tractable r
&lt;/p&gt;</description></item></channel></rss>