<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#36890;&#36807;&#30740;&#31350;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#20960;&#20309;&#24341;&#23548;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.08269</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20960;&#20309;&#24341;&#23548;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Geometry-induced Implicit Regularization in Deep ReLU Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#20960;&#20309;&#24341;&#23548;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#27604;&#35757;&#32451;&#26679;&#26412;&#26356;&#22810;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#20250;&#36807;&#25311;&#21512;&#12290;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#65292;&#23545;&#8220;&#22909;&#8221;&#30340;&#32593;&#32476;&#26377;&#21033;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#32771;&#34385;&#8220;&#22909;&#8221;&#30340;&#32593;&#32476;&#65292;&#21442;&#25968;&#25968;&#37327;&#23601;&#19981;&#26159;&#19968;&#20010;&#36275;&#22815;&#34913;&#37327;&#22797;&#26434;&#24615;&#30340;&#25351;&#26631;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21738;&#20123;&#32593;&#32476;&#21463;&#21040;&#38738;&#30544;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#22266;&#23450;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38598;&#21512;&#30340;&#32500;&#24230;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#23616;&#37096;&#32500;&#24230;&#65292;&#21363;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#65292;&#20960;&#20046;&#24635;&#26159;&#30001;&#38544;&#34255;&#23618;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#20915;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#23545;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#23545;&#31216;&#24615;&#65288;&#31070;&#32463;&#20803;&#25490;&#21015;&#21644;&#27491;&#21521;&#32553;&#25918;&#65289;&#26159;&#19981;&#21464;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#36807;&#31243;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that neural networks with many more parameters than training examples do not overfit. Implicit regularization phenomena, which are still not well understood, occur during optimization and 'good' networks are favored. Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the 'good' ones. To better understand which networks are favored during optimization, we study the geometry of the output set as parameters vary. When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers. We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings. Empirically, we establish that the batch functional dimension decreases during optimization. As a consequence, optimization l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#30340;&#26032;&#39062;&#21644;&#32039;&#20945;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#26680;&#23545;&#24212;&#30340;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#19968;&#23450;&#27491;&#21017;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23545;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;</title><link>https://arxiv.org/abs/2312.14886</link><description>&lt;p&gt;
&#26469;&#33258;&#21327;&#26041;&#24046;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sample Path Regularity of Gaussian Processes from the Covariance Kernel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#30340;&#26032;&#39062;&#21644;&#32039;&#20945;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#26680;&#23545;&#24212;&#30340;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#19968;&#23450;&#27491;&#21017;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23545;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26159;&#23450;&#20041;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#24120;&#35265;&#24418;&#24335;&#20027;&#20041;&#12290;&#23613;&#31649;GPs&#30340;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#23545;&#20110;GP&#26679;&#26412;&#36335;&#24452;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21363;&#23427;&#20204;&#23450;&#20041;&#27010;&#29575;&#27979;&#24230;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#23578;&#32570;&#20047;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;GPs&#19981;&#26159;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#26500;&#24314;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22343;&#20540;&#20989;&#25968;&#21644;&#21327;&#26041;&#24046;&#26680;&#26500;&#24314;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#32473;&#23450;&#27491;&#21017;&#24615;&#25152;&#38656;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;H\"older&#27491;&#21017;&#24615;&#26694;&#26550;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#29305;&#21035;&#31616;&#21333;&#30340;&#26465;&#20214;&#65292;&#22312;&#24179;&#31283;&#21644;&#21508;&#21521;&#21516;&#24615;GPs&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#31616;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#20801;&#35768;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#26032;&#39062;&#19988;&#24322;&#24120;&#32039;&#20945;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14886v2 Announce Type: replace  Abstract: Gaussian processes (GPs) are the most common formalism for defining probability distributions over spaces of functions. While applications of GPs are myriad, a comprehensive understanding of GP sample paths, i.e. the function spaces over which they define a probability measure, is lacking. In practice, GPs are not constructed through a probability measure, but instead through a mean function and a covariance kernel. In this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding GP to attain a given regularity. We use the framework of H\"older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic GPs. We then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the GPs commonly used in machine learning applications, such as the Mat\'
&lt;/p&gt;</description></item></channel></rss>