<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14103</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#26469;&#33258;&#32500;&#24230;&#20026; $d$ &#30340; $k$-&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#30340; $n$ &#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35810;&#38382;&#20102;&#22312;&#26102;&#38388;&#22810;&#39033;&#24335;&#20013;&#30340;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#23545;&#36825; $n$ &#20010;&#26679;&#26412;&#36798;&#21040;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#12290;&#20449;&#24687;&#29702;&#35770;&#19978;&#65292;&#36825;&#21487;&#20197;&#29992; $\Theta(k \log (d/k))$ &#20010;&#26679;&#26412;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#24456;&#26174;&#33879;&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#38468;&#21152;&#23545;&#27169;&#22411;&#30340;&#20854;&#20182;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23569;&#20110; $\Theta(d)$ &#20010;&#26679;&#26412;&#36798;&#21040;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#31867;&#20284;&#22320;&#65292;&#29616;&#26377;&#30340;&#22256;&#38590;&#32467;&#26524;&#35201;&#20040;&#20165;&#38480;&#20110;&#36866;&#24403;&#35774;&#32622;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#20272;&#35745;&#20540;&#20063;&#24517;&#39035;&#26159;&#31232;&#30095;&#30340;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14103v1 Announce Type: new  Abstract: We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   We give evidence that efficient algorithms for this task require at least (roughly) $\Omega(
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wasserstein&#32479;&#35745;&#22312;&#20223;&#23556;&#21464;&#24418;&#32479;&#35745;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20960;&#20309;&#29305;&#24449;&#65292;&#27604;&#36739;&#20102;&#20449;&#24687;&#20960;&#20309;&#21644;Wasserstein&#20960;&#20309;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#21457;&#29616;Wasserstein&#20272;&#35745;&#37327;&#22312;&#26925;&#22278;&#23545;&#31216;&#20223;&#23556;&#21464;&#24418;&#27169;&#22411;&#20013;&#26159;&#30697;&#20272;&#35745;&#37327;&#65292;&#22312;&#27874;&#24418;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;&#19982;&#20449;&#24687;&#20960;&#20309;&#20272;&#35745;&#37327;&#37325;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.12508</link><description>&lt;p&gt;
&#24418;&#29366;&#21644;&#20223;&#23556;&#21464;&#24418;&#30340;Wasserstein&#32479;&#35745;&#30340;&#20449;&#24687;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations. (arXiv:2307.12508v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12508
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wasserstein&#32479;&#35745;&#22312;&#20223;&#23556;&#21464;&#24418;&#32479;&#35745;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20960;&#20309;&#29305;&#24449;&#65292;&#27604;&#36739;&#20102;&#20449;&#24687;&#20960;&#20309;&#21644;Wasserstein&#20960;&#20309;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#21457;&#29616;Wasserstein&#20272;&#35745;&#37327;&#22312;&#26925;&#22278;&#23545;&#31216;&#20223;&#23556;&#21464;&#24418;&#27169;&#22411;&#20013;&#26159;&#30697;&#20272;&#35745;&#37327;&#65292;&#22312;&#27874;&#24418;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;&#19982;&#20449;&#24687;&#20960;&#20309;&#20272;&#35745;&#37327;&#37325;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20960;&#20309;&#21644;Wasserstein&#20960;&#20309;&#26159;&#20171;&#32461;&#27010;&#29575;&#20998;&#24067;&#27969;&#24418;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#32467;&#26500;&#65292;&#23427;&#20204;&#25429;&#25417;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20223;&#23556;&#21464;&#24418;&#32479;&#35745;&#27169;&#22411;&#30340;Li&#21644;Zhao&#65288;2023&#65289;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;Wasserstein&#20960;&#20309;&#30340;&#29305;&#24449;&#65292;&#23427;&#26159;&#20301;&#32622;-&#23610;&#24230;&#27169;&#22411;&#30340;&#22810;&#32500;&#27867;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#21644;Wasserstein&#20960;&#20309;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#22312;Wasserstein&#20960;&#20309;&#20013;&#65292;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#29366;&#21644;&#20223;&#23556;&#21464;&#24418;&#26159;&#20998;&#31163;&#30340;&#65292;&#34920;&#26126;&#22312;&#23545;&#27874;&#24418;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20250;&#25439;&#22833;Fisher&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26925;&#22278;&#23545;&#31216;&#20223;&#23556;&#21464;&#24418;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;Wasserstein&#20272;&#35745;&#37327;&#26159;&#30697;&#20272;&#35745;&#37327;&#12290;&#23427;&#19982;&#20449;&#24687;&#20960;&#20309;&#20272;&#35745;&#37327;&#65288;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#65289;&#20165;&#22312;&#27874;&#24418;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;&#37325;&#21512;&#12290;Wasserstein&#25928;&#29575;&#30340;&#20316;&#29992;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Information geometry and Wasserstein geometry are two main structures introduced in a manifold of probability distributions, and they capture its different characteristics. We study characteristics of Wasserstein geometry in the framework of Li and Zhao (2023) for the affine deformation statistical model, which is a multi-dimensional generalization of the location-scale model. We compare merits and demerits of estimators based on information geometry and Wasserstein geometry. The shape of a probability distribution and its affine deformation are separated in the Wasserstein geometry, showing its robustness against the waveform perturbation in exchange for the loss in Fisher efficiency. We show that the Wasserstein estimator is the moment estimator in the case of the elliptically symmetric affine deformation model. It coincides with the information-geometrical estimator (maximum-likelihood estimator) when and only when the waveform is Gaussian. The role of the Wasserstein efficiency is 
&lt;/p&gt;</description></item></channel></rss>