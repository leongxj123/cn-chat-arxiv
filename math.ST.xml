<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07514</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning as a kernel method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07514
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#20102;&#25968;&#25454;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#24182;&#25552;&#39640;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#23558;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#29289;&#29702;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26222;&#36890;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#39564;&#39118;&#38505;&#30001;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#31243;&#37327;&#21270;&#20102;&#29289;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#32447;&#24615;&#24494;&#20998;&#20808;&#39564;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#26680;&#22238;&#24402;&#20219;&#21153;&#12290;&#21033;&#29992;&#26680;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#27491;&#21017;&#21270;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#34920;&#26126;&#23427;&#33267;&#23569;&#20197;Sobolev&#26368;&#23567;&#21270;&#36895;&#24230;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#29289;&#29702;&#35823;&#24046;&#30340;&#19981;&#21516;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#19968;&#32500;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#20010;&#21407;&#29702;&#65292;&#25903;&#25345;&#19968;&#20010;&#35770;&#28857;&#65306;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26469;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#23545;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#33021;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.07445</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Top-$K$ ranking with a monotone adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#27604;&#36739;&#22270;&#34987;&#38543;&#26426;&#29983;&#25104;&#19988;&#23545;&#25163;&#21487;&#20197;&#28155;&#21152;&#20219;&#24847;&#36793;&#30340;&#24773;&#20917;&#12290;&#32479;&#35745;&#23398;&#23478;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#20174;&#36825;&#20010;&#21322;&#38543;&#26426;&#27604;&#36739;&#22270;&#23548;&#20986;&#30340;&#20004;&#20004;&#27604;&#36739;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;Top-K&#30340;&#39318;&#36873;&#39033;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20986;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#23427;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;$log^2(n)$&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#34920;&#31034;&#27604;&#36739;&#39033;&#30340;&#25968;&#37327;&#12290;&#36825;&#24471;&#30410;&#20110;&#20998;&#26512;&#21644;&#31639;&#27861;&#21019;&#26032;&#30340;&#32467;&#21512;&#12290;&#22312;&#20998;&#26512;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26126;&#30830;&#12289;&#26356;&#32039;&#23494;&#30340;&#21152;&#26435;MLE&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#23427;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#26032;&#28041;&#21450;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\ell_\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development 
&lt;/p&gt;</description></item></channel></rss>