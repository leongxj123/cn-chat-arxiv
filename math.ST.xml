<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19516</link><description>&lt;p&gt;
&#38024;&#23545;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#23558;&#32858;&#31867;&#38382;&#39064;&#24314;&#27169;&#20026;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;DSBM&#65289;&#20013;&#28508;&#22312;&#31038;&#21306;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;DSBM&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#20174;&#32780;&#30830;&#23450;&#32473;&#23450;&#35266;&#23519;&#21040;&#30340;&#22270;&#32467;&#26500;&#26102;&#26368;&#21487;&#33021;&#30340;&#31038;&#21306;&#20998;&#37197;&#12290;&#38500;&#20102;&#32479;&#35745;&#35266;&#28857;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#36825;&#31181;MLE&#20844;&#24335;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#20248;&#21270;&#21551;&#21457;&#24335;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#35813;&#21551;&#21457;&#24335;&#21516;&#26102;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#26377;&#21521;&#22270;&#32479;&#35745;&#37327;&#65306;&#36793;&#23494;&#24230;&#21644;&#36793;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#31181;&#26377;&#21521;&#32858;&#31867;&#30340;&#26032;&#20844;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#35889;&#32858;&#31867;&#31639;&#27861;&#30340;&#38169;&#35823;&#32858;&#31867;&#39030;&#28857;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19516v1 Announce Type: cross  Abstract: This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algor
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#21487;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#36805;&#36895;&#20943;&#23567;&#65307;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#32531;&#24930;&#20943;&#23567;&#12290;&#36825;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08247</link><description>&lt;p&gt;
&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#20004;&#20010;&#38454;&#27573;&#30340;&#32553;&#25918;&#24459;
&lt;/p&gt;
&lt;p&gt;
Two Phases of Scaling Laws for Nearest Neighbor Classifiers. (arXiv:2308.08247v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08247
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#21487;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#36805;&#36895;&#20943;&#23567;&#65307;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#32531;&#24930;&#20943;&#23567;&#12290;&#36825;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#24459;&#26159;&#25351;&#24403;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#27169;&#22411;&#30340;&#27979;&#35797;&#24615;&#33021;&#20250;&#25552;&#39640;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#24555;&#36895;&#30340;&#32553;&#25918;&#24459;&#24847;&#21619;&#30528;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#23601;&#33021;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#26356;&#22810;&#25968;&#25454;&#30340;&#22909;&#22788;&#21487;&#33021;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#12290;&#25105;&#20204;&#21457;&#29616;&#32553;&#25918;&#24459;&#21487;&#33021;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#19988;&#24555;&#36895;&#20943;&#23567;&#65307;&#32780;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#19988;&#20943;&#23567;&#24471;&#24930;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#26174;&#20102;&#25968;&#25454;&#20998;&#24067;&#22312;&#20915;&#23450;&#27867;&#21270;&#35823;&#24046;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24403;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#20256;&#36882;&#26377;&#29992;&#20449;&#24687;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.05529</link><description>&lt;p&gt;
&#21033;&#29992;Birth-Death &#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#21152;&#36895;Langevin&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerate Langevin Sampling with Birth-Death process and Exploration Component. (arXiv:2305.05529v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#20256;&#36882;&#26377;&#29992;&#20449;&#24687;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#37319;&#26679;&#24050;&#30693;&#27010;&#29575;&#20998;&#24067;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#38024;&#23545;&#22810;&#23792;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#8220;&#19977;&#24605;&#32780;&#21518;&#34892;&#8221;&#12290;&#25105;&#20204;&#20445;&#30041;&#20004;&#32452;&#37319;&#26679;&#22120;&#65292;&#19968;&#32452;&#22312;&#36739;&#39640;&#28201;&#24230;&#19979;&#65292;&#19968;&#32452;&#22312;&#21407;&#22987;&#28201;&#24230;&#19979;&#12290;&#21069;&#32773;&#20316;&#20026;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#23558;&#26377;&#29992;&#20449;&#24687;&#20256;&#36882;&#32473;&#21518;&#32773;&#30340;&#20808;&#39537;&#65292;&#21518;&#32773;&#22312;&#25509;&#25910;&#20449;&#24687;&#21518;&#23545;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22343;&#22330;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#25506;&#32034;&#36807;&#31243;&#22914;&#20309;&#20915;&#23450;&#37319;&#26679;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#20197;&#21069;&#25991;&#29486;&#20013;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is \textit{look before you leap}. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration process determines sampling efficiency. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compared our methodology to previous ones.
&lt;/p&gt;</description></item></channel></rss>