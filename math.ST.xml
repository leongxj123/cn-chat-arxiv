<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.03242</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36817;&#20284;Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#65288;&#22914;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#65289;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#65292;&#20174;&#32780;&#20174;&#32473;&#23450;&#30340;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21463;Langevin Monte Carlo (LMC)&#31639;&#27861;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22522;&#20110;LMC&#25200;&#21160;&#32467;&#26524;&#65292;&#22312;Wasserstein-2&#36317;&#31163;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#23545;&#20110;&#24179;&#28369;&#30340;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#36895;&#24230;&#12290;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;&#25200;&#21160;LMC&#36807;&#31243;&#30340;&#20013;&#38388;&#24230;&#37327;&#30340;&#20122;&#39640;&#26031;&#24615;&#27010;&#24565;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#25200;&#21160;&#20551;&#35774;&#25512;&#23548;&#20986;&#20102;&#20013;&#38388;&#26041;&#24046;&#20195;&#29702;&#30340;&#22686;&#38271;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#36817;&#20284;&#26679;&#26412;&#19982;&#30446;&#26631;&#20998;&#24067;&#26144;&#23556;&#30340;&#34920;&#36798;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
&lt;/p&gt;</description></item></channel></rss>