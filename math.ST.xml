<rss version="2.0"><channel><title>Chat Arxiv math.ST</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for math.ST</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item></channel></rss>