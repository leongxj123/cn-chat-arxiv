<rss version="2.0"><channel><title>Chat Arxiv cs.GR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GR</description><item><title>&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;</title><link>https://arxiv.org/abs/2402.13251</link><description>&lt;p&gt;
FlashTex&#65306;&#20855;&#26377;LightControlNet&#30340;&#24555;&#36895;&#21487;&#37325;&#22609;&#32593;&#26684;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
FlashTex: Fast Relightable Mesh Texturing with LightControlNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#20026;3D&#32593;&#26684;&#21019;&#24314;&#32441;&#29702;&#36153;&#26102;&#36153;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#23478;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#32773;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#33258;&#21160;&#20026;&#36755;&#20837;&#30340;3D&#32593;&#26684;&#30528;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;/&#21453;&#23556;&#22312;&#29983;&#25104;&#30340;&#32441;&#29702;&#20013;&#35299;&#32806;&#65292;&#20197;&#20415;&#32593;&#26684;&#21487;&#20197;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#20013;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LightControlNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;ControlNet&#26550;&#26500;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20801;&#35768;&#23558;&#25152;&#38656;&#29031;&#26126;&#35268;&#26684;&#20316;&#20026;&#23545;&#27169;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#32441;&#29702;&#31649;&#36947;&#28982;&#21518;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#32441;&#29702;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;LightControlNet&#29983;&#25104;&#32593;&#26684;&#30340;&#19968;&#32452;&#31232;&#30095;&#30340;&#35270;&#35273;&#19968;&#33268;&#30340;&#21442;&#32771;&#35270;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#24212;&#29992;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#32441;&#29702;&#20248;&#21270;&#65292;&#36890;&#36807;LightControlNet&#26469;&#25552;&#39640;&#32441;&#29702;&#36136;&#37327;&#21516;&#26102;&#35299;&#32806;&#34920;&#38754;&#26448;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
&lt;/p&gt;</description></item><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item></channel></rss>