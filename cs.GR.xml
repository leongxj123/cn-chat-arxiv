<rss version="2.0"><channel><title>Chat Arxiv cs.GR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GR</description><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item></channel></rss>