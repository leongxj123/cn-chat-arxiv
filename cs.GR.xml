<rss version="2.0"><channel><title>Chat Arxiv cs.GR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GR</description><item><title>&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12034</link><description>&lt;p&gt;
VFusion3D: &#20174;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12034
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#12290;&#26500;&#24314;&#22522;&#30784;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;3D&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#19982;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#19981;&#21516;&#65292;3D&#25968;&#25454;&#19981;&#23481;&#26131;&#33719;&#21462;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#19982;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#25968;&#37327;&#30456;&#27604;&#23384;&#22312;&#26174;&#30528;&#30340;&#35268;&#27169;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#36890;&#36807;&#22823;&#37327;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;3D&#25968;&#25454;&#30340;&#30693;&#35782;&#28304;&#12290;&#36890;&#36807;&#24494;&#35843;&#35299;&#38145;&#20854;&#22810;&#35270;&#35282;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21069;&#39304;3D&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12034v1 Announce Type: cross  Abstract: This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compare
&lt;/p&gt;</description></item></channel></rss>