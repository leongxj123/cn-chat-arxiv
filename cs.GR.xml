<rss version="2.0"><channel><title>Chat Arxiv cs.GR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01391</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#30340;MLP&#23398;&#20064;SDF&#30340;&#26368;&#20248;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#22914;&#24418;&#29366;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#34920;&#31034;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;3D&#24418;&#29366;&#21644;&#25191;&#34892;&#30896;&#25758;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#38544;&#24335;&#22330;&#30001;&#24102;&#26377;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#36827;&#34892;&#32534;&#30721;&#20197;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24102;&#26377;PE&#30340;MLP&#30340;&#19968;&#20010;&#26174;&#33879;&#21103;&#20316;&#29992;&#26159;&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#22330;&#20013;&#23384;&#22312;&#22122;&#22768;&#20266;&#24433;&#12290;&#23613;&#31649;&#22686;&#21152;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#30340;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#30830;&#23450;&#23398;&#20064;&#31934;&#30830;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#36866;&#24403;&#37319;&#26679;&#29575;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#32593;&#32476;&#21709;&#24212;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#29992;&#20110;&#20272;&#35745;&#24102;&#26377;&#38543;&#26426;&#26435;&#37325;&#30340;&#32473;&#23450;&#32593;&#32476;&#30340;&#20869;&#22312;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network's responses. It is observed that
&lt;/p&gt;</description></item></channel></rss>