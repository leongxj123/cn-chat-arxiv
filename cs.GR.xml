<rss version="2.0"><channel><title>Chat Arxiv cs.GR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GR</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.10079</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of the Learning-based Camera and Lidar Simulation Methods for Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#23588;&#20854;&#26159;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#26159;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;(Autonomous Driving Systems&#65292;ADS)&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#20197;&#20570;&#20986;&#26126;&#26234;&#30340;&#39550;&#39542;&#21644;&#25511;&#21046;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36924;&#30495;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#65292;&#23545;&#20110;&#26377;&#25928;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;ADS&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20419;&#36827;&#20102;&#24863;&#30693;&#20256;&#24863;&#22120;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#26222;&#21450;&#12290;&#20256;&#32479;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#31995;&#32479;&#22914;ADS&#20013;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#28508;&#21147;&#22312;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21463;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10079v1 Announce Type: cross  Abstract: Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.17978</link><description>&lt;p&gt;
AutArch&#65306;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#32972;&#26223;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20174;&#24322;&#26500;&#30340;&#24050;&#21457;&#34920;&#36164;&#28304;&#20013;&#21019;&#24314;&#22823;&#35268;&#27169;&#32479;&#19968;&#30340;&#32771;&#21476;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;&#36951;&#29289;&#30446;&#24405;&#12290;&#35770;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#33268;&#32771;&#21476;&#25968;&#25454;&#32452;&#21512;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#29616;&#26377;&#35760;&#24405;&#22312;&#36136;&#37327;&#21644;&#35760;&#24405;&#26631;&#20934;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#26080;&#27861;&#31616;&#21333;&#22320;&#21512;&#24182;&#29616;&#26377;&#35760;&#24405;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20174;&#24050;&#21457;&#34920;&#30340;&#32771;&#21476;&#25554;&#22270;&#20013;&#37325;&#26032;&#21019;&#24314;&#35760;&#24405;&#12290;&#21482;&#26377;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#36825;&#25165;&#26159;&#21487;&#34892;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#32771;&#21476;&#36951;&#29289;&#30446;&#24405;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#20123;&#30446;&#24405;&#20316;&#20026;&#36951;&#30041;&#36164;&#28304;&#23384;&#22312;&#65292;&#27604;&#22914;&#22823;&#22411;&#26410;&#25490;&#24207;&#30340;PDF&#25991;&#20214;&#20013;&#30340;&#32771;&#21476;&#32472;&#22270;&#21644;&#29031;&#29255;&#65307;&#35813;&#24037;&#20316;&#27969;&#31243;&#20381;&#36182;&#20110;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#12289;&#29289;&#20307;&#26816;&#27979;&#20197;&#21450;&#39564;&#35777;&#21644;&#35843;&#25972;&#33258;&#21160;&#33719;&#21462;&#25968;&#25454;&#30340;&#20132;&#20114;&#25163;&#27573;&#30340;&#33258;&#23450;&#20041;&#36719;&#20214;&#65288;AutArch&#65289;&#12290;&#25105;&#20204;&#38598;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17978v2 Announce Type: replace-cross  Abstract: The context of this paper is the creation of large uniform archaeological datasets from heterogeneous published resources, such as find catalogues - with the help of AI and Big Data. The paper is concerned with the challenge of consistent assemblages of archaeological data. We cannot simply combine existing records, as they differ in terms of quality and recording standards. Thus, records have to be recreated from published archaeological illustrations. This is only a viable path with the help of automation. The contribution of this paper is a new workflow for collecting data from archaeological find catalogues available as legacy resources, such as archaeological drawings and photographs in large unsorted PDF files; the workflow relies on custom software (AutArch) supporting image processing, object detection, and interactive means of validating and adjusting automatically retrieved data. We integrate artificial intelligence (
&lt;/p&gt;</description></item></channel></rss>