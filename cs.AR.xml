<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#36716;&#25442;&#34920;&#31034;&#20026;&#21521;&#37327;&#24182;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;</title><link>http://arxiv.org/abs/2207.11437</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks. (arXiv:2207.11437v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#36716;&#25442;&#34920;&#31034;&#20026;&#21521;&#37327;&#24182;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36923;&#36753;&#32508;&#21512;&#38454;&#27573;&#65292;&#32508;&#21512;&#24037;&#20855;&#20013;&#30340;&#32467;&#26500;&#36716;&#25442;&#38656;&#35201;&#19982;&#20248;&#21270;&#24207;&#21015;&#32467;&#21512;&#65292;&#24182;&#20316;&#29992;&#20110;&#30005;&#36335;&#65292;&#20197;&#28385;&#36275;&#25351;&#23450;&#30340;&#30005;&#36335;&#38754;&#31215;&#21644;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#32508;&#21512;&#20248;&#21270;&#24207;&#21015;&#30340;&#36816;&#34892;&#26102;&#38388;&#36739;&#38271;&#65292;&#20026;&#30005;&#36335;&#23545;&#32508;&#21512;&#20248;&#21270;&#24207;&#21015;&#30340;&#32467;&#26524;&#36136;&#37327;&#65288;QoR&#65289;&#36827;&#34892;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#24037;&#31243;&#24072;&#26356;&#24555;&#22320;&#25214;&#21040;&#26356;&#22909;&#30340;&#20248;&#21270;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#30005;&#36335;-&#20248;&#21270;&#24207;&#21015;&#23545;&#30340;QoR&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23884;&#20837;&#26041;&#27861;&#23558;&#32467;&#26500;&#36716;&#25442;&#36716;&#21270;&#20026;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65288;Transformer&#65289;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#36807;&#31243;&#33021;&#22815;&#20174;&#30005;&#36335;&#27867;&#21270;&#21040;&#30005;&#36335;&#65292;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#34987;&#34920;&#31034;&#20026;&#37051;&#25509;&#30697;&#38453;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20110;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;
&lt;/p&gt;
&lt;p&gt;
In the logic synthesis stage, structure transformations in the synthesis tool need to be combined into optimization sequences and act on the circuit to meet the specified circuit area and delay. However, logic synthesis optimization sequences are time-consuming to run, and predicting the quality of the results (QoR) against the synthesis optimization sequence for a circuit can help engineers find a better optimization sequence faster. In this work, we propose a deep learning method to predict the QoR of unseen circuit-optimization sequences pairs. Specifically, the structure transformations are translated into vectors by embedding methods and advanced natural language processing (NLP) technology (Transformer) is used to extract the features of the optimization sequences. In addition, to enable the prediction process of the model to be generalized from circuit to circuit, the graph representation of the circuit is represented as an adjacency matrix and a feature matrix. Graph neural net
&lt;/p&gt;</description></item></channel></rss>