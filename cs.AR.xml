<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18595</link><description>&lt;p&gt;
EncodingNet: &#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;MAC&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#36328;  &#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35832;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#25191;&#34892;&#38656;&#35201;&#22312;&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#37327;&#30340;&#20056;-&#32047;&#31215;&#65288;MAC&#65289;&#36816;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#21151;&#32791;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#12290;&#22312;&#36825;&#31181;&#26032;&#35774;&#35745;&#20013;&#65292;&#20056;&#27861;&#22120;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#25152;&#21462;&#20195;&#65292;&#29992;&#20110;&#23558;&#32467;&#26524;&#25237;&#24433;&#21040;&#23485;&#27604;&#29305;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;&#27604;&#29305;&#25658;&#24102;&#21508;&#33258;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;&#25512;&#26029;&#31934;&#24230;&#12290;&#26032;&#20056;&#27861;&#22120;&#30340;&#36755;&#20986;&#36890;&#36807;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#36827;&#34892;&#30456;&#21152;&#65292;&#24182;&#19988;&#32047;&#31215;&#32467;&#26524;&#19982;&#29616;&#26377;&#35745;&#31639;&#24179;&#21488;&#20860;&#23481;&#65292;&#21487;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25110;&#38750;&#32479;&#19968;&#37327;&#21270;&#12290;&#30001;&#20110;&#20056;&#27861;&#20989;&#25968;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#25237;&#24433;&#25152;&#21462;&#20195;&#65292;&#23548;&#33268;&#33021;&#37327;&#25928;&#29575;&#21644;&#35745;&#31639;&#25928;&#26524;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to project the results onto a wide bit representation. These bits carry individual position weights, which can be trained for specific neural networks to enhance inference accuracy. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks with either uniform or non-uniform quantization. Since the multiplication function is replaced by simple logic projection, the c
&lt;/p&gt;</description></item></channel></rss>