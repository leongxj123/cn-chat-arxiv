<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;ODE&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#21644;RNN&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#27880;&#24847;&#26426;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#32452;&#20214;&#65292;&#20294;&#26159;&#35768;&#22810;Transformer&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#30456;&#27604;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#24182;&#23558;&#37096;&#20998;&#21367;&#31215;&#23618;&#26367;&#25442;&#20026;MHSA&#65288;&#22810;&#22836;&#33258;&#27880;&#24847;&#65289;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#65288;&#24120;&#24494;&#20998;&#26041;&#31243;&#65289;&#32780;&#19981;&#26159;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#65292;&#32780;&#19988;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#21488;&#36866;&#24230;&#35268;&#27169;&#30340;FPGA&#35774;&#22791;&#19978;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
&lt;/p&gt;</description></item></channel></rss>