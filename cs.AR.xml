<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16731</link><description>&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16731
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25191;&#34892;&#28041;&#21450;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#65292;&#21518;&#32773;&#22312;&#24635;&#26102;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#21463;&#25968;&#25454;&#22312;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#20043;&#38388;&#31227;&#21160;&#30340;&#20005;&#37325;&#29942;&#39048;&#25152;&#38480;&#21046;&#12290;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#31616;&#21333;&#22788;&#29702;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyGim&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;PIM&#31995;&#32479;&#19978;&#21152;&#36895;GNNs&#12290;&#25105;&#20204;&#20026;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#23450;&#21046;&#30340;GNN&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#25552;&#20986;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#24320;&#21457;&#20102;&#26041;&#20415;&#30340;Python API&#12290;&#25105;&#20204;&#25552;&#20379;&#28151;&#21512;&#24335;GNN&#25191;&#34892;&#65292;&#20854;&#20013;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#20998;&#21035;&#22312;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#21644;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#25191;&#34892;&#65292;&#20197;&#21305;&#37197;&#23427;&#20204;&#30340;&#31639;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;ODE&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#21644;RNN&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#27880;&#24847;&#26426;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#32452;&#20214;&#65292;&#20294;&#26159;&#35768;&#22810;Transformer&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#30456;&#27604;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#24182;&#23558;&#37096;&#20998;&#21367;&#31215;&#23618;&#26367;&#25442;&#20026;MHSA&#65288;&#22810;&#22836;&#33258;&#27880;&#24847;&#65289;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#65288;&#24120;&#24494;&#20998;&#26041;&#31243;&#65289;&#32780;&#19981;&#26159;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#65292;&#32780;&#19988;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#21488;&#36866;&#24230;&#35268;&#27169;&#30340;FPGA&#35774;&#22791;&#19978;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
&lt;/p&gt;</description></item></channel></rss>