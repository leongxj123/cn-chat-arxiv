<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16869</link><description>&lt;p&gt;
NeuralFuse: &#23398;&#20064;&#25913;&#21892;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16869
&lt;/p&gt;
&lt;p&gt;
NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#33021;&#37327;&#28040;&#32791;&#20173;&#28982;&#26159;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#26159;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#20013;&#65292;&#32780;SRAM&#20013;&#20250;&#21457;&#29983;&#38543;&#26426;&#20301;&#32763;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NeuralFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#22312;&#20302;&#30005;&#21387;&#29615;&#22659;&#20013;&#35299;&#20915;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;NeuralFuse&#22312;&#26631;&#31216;&#30005;&#21387;&#21644;&#20302;&#30005;&#21387;&#24773;&#20917;&#19979;&#37117;&#33021;&#20445;&#25252;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;NeuralFuse&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#26377;&#38480;&#35775;&#38382;&#30340;DNN&#65292;&#20363;&#22914;&#19981;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#25110;&#20113;&#31471;API&#30340;&#36828;&#31243;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1%&#30340;&#20301;&#38169;&#35823;&#29575;&#19979;&#65292;NeuralFuse&#21487;&#20197;&#23558;SRAM&#20869;&#23384;&#35775;&#38382;&#33021;&#37327;&#38477;&#20302;&#39640;&#36798;24%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
&lt;/p&gt;</description></item></channel></rss>