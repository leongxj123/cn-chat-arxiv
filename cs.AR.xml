<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.00849</link><description>&lt;p&gt;
NeuraLUT: &#22312;Boolean&#21512;&#25104;&#20989;&#25968;&#20013;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00849
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21152;&#36895;&#22120;&#24050;&#32463;&#35777;&#26126;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#36164;&#28304;&#20851;&#38190;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#31070;&#32463;&#32593;&#32476;&#20013;&#35745;&#31639;&#23494;&#38598;&#24230;&#26368;&#39640;&#30340;&#25805;&#20316;&#20043;&#19968;&#26159;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#20043;&#38388;&#30340;&#28857;&#31215;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;FPGA&#21152;&#36895;&#24037;&#20316;&#25552;&#20986;&#23558;&#20855;&#26377;&#37327;&#21270;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31070;&#32463;&#20803;&#30452;&#25509;&#26144;&#23556;&#21040;&#26597;&#25214;&#34920;&#65288;LUTs&#65289;&#20197;&#36827;&#34892;&#30828;&#20214;&#23454;&#29616;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#31070;&#32463;&#20803;&#30340;&#36793;&#30028;&#19982;LUTs&#30340;&#36793;&#30028;&#37325;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#25918;&#23485;&#36825;&#20123;&#36793;&#30028;&#65292;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#12290;&#30001;&#20110;&#23376;&#32593;&#32476;&#34987;&#21560;&#25910;&#21040;LUT&#20013;&#65292;&#20998;&#21306;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#20351;&#29992;&#20855;&#26377;&#28014;&#28857;&#31934;&#24230;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#36825;&#20123;&#23618;&#21463;&#30410;&#20110;&#25104;&#20026;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
&lt;/p&gt;</description></item></channel></rss>