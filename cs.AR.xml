<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13838</link><description>&lt;p&gt;
&#30005;&#36335;&#21464;&#21387;&#22120;&#65306;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#38376;&#23454;&#29616;&#31471;&#21040;&#31471;&#30005;&#36335;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#20154;&#31867;&#36890;&#36807;&#24207;&#21015;&#31526;&#21495;&#34920;&#36798;&#30340;&#31361;&#20986;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#35745;&#31639;&#19978;&#25484;&#25569;&#20102;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24040;&#22823;&#30340;&#31070;&#32463;&#27169;&#22411;&#19981;&#26029;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65292;LLMs&#23637;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#30005;&#36335;&#20316;&#20026;&#30005;&#23376;&#35774;&#35745;&#30340;&#8220;&#35821;&#35328;&#8221;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#30340;&#32423;&#32852;&#36830;&#25509;&#26469;&#25351;&#23450;&#30005;&#23376;&#35774;&#22791;&#30340;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#24449;&#26381;&#30005;&#23376;&#35774;&#35745;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13838v1 Announce Type: new  Abstract: Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the "language" of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large "circuit model", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first 
&lt;/p&gt;</description></item></channel></rss>