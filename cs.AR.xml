<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item></channel></rss>