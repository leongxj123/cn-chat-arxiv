<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item></channel></rss>