<rss version="2.0"><channel><title>Chat Arxiv cs.AR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AR</description><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00639</link><description>&lt;p&gt;
RL-MUL&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00639
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#27861;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#20056;&#27861;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#30005;&#36335;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#20248;&#21270;&#20056;&#27861;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL-MUL&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#22522;&#20110;&#36825;&#19968;&#34920;&#31034;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#20026;&#20195;&#29702;&#32593;&#32476;&#12290;&#20195;&#29702;&#21487;&#20197;&#23398;&#20064;&#26681;&#25454;&#23450;&#21046;&#21270;&#30340;&#21487;&#23481;&#24525;&#21306;&#22495;&#19982;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#26469;&#20248;&#21270;&#20056;&#27861;&#22120;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RL-MUL&#30340;&#33021;&#21147;&#34987;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;&#23454;&#39564;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#20056;&#27861;&#22120;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RL-MUL&#29983;&#25104;&#30340;&#20056;&#27861;&#22120;&#33021;&#22815;&#36229;&#36234;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00639v1 Announce Type: cross  Abstract: Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseli
&lt;/p&gt;</description></item></channel></rss>