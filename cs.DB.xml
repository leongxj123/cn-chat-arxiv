<rss version="2.0"><channel><title>Chat Arxiv cs.DB</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DB</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.05821</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#22411;&#24037;&#20316;&#36127;&#36733;&#20013;&#20248;&#21270;LLM&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Optimizing LLM Queries in Relational Workloads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20998;&#26512;&#24615;&#25968;&#25454;&#24211;&#25552;&#20379;&#21830;&#65288;&#20363;&#22914;Redshift&#12289;Databricks&#12289;BigQuery&#65289;&#24050;&#36805;&#36895;&#22686;&#21152;&#23545;&#36890;&#36807;&#26412;&#26426;&#29992;&#25143;&#33258;&#23450;&#20041;&#20989;&#25968;&#65288;UDFs&#65289;&#35843;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#20869;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#23454;&#20307;&#25552;&#21462;&#21644;&#32763;&#35793;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20248;&#21270;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#34892;&#20197;&#26368;&#22823;&#21270;LLM&#25512;&#29702;&#24341;&#25806;&#20869;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#37325;&#29992;&#65292;&#37325;&#26032;&#25490;&#24207;&#34892;&#20869;&#30340;&#21015;&#20197;&#36827;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 Announce Type: new  Abstract: Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking Large Language Models (LLMs) through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, LLM inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize LLM inference for analytical workloads that invoke LLMs within relational queries. We show that relational queries present novel opportunities for accelerating LLM inference, including reordering rows to maximize key-value (KV) cache reuse within the LLM inference engine, reordering columns within a row to further i
&lt;/p&gt;</description></item></channel></rss>