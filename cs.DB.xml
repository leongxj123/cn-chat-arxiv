<rss version="2.0"><channel><title>Chat Arxiv cs.DB</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DB</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;</title><link>https://arxiv.org/abs/2312.01678</link><description>&lt;p&gt;
Jellyfish&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jellyfish: A Large Language Model for Data Preprocessing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#31649;&#36947;&#20013;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#21033;&#20110;&#31616;&#21333;&#22788;&#29702;&#30340;&#24178;&#20928;&#26684;&#24335;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65288;DP&#65289;&#20013;LLMs&#30340;&#21033;&#29992;&#12290;&#19982;&#20351;&#29992;LLMs&#20026;DP&#35774;&#35745;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20852;&#36259;&#30456;&#27604;&#65292;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20513;&#35758;&#36890;&#24120;&#20381;&#36182;&#20110;GPT API&#65292;&#24341;&#21457;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#25968;&#25454;&#27844;&#38671;&#25285;&#24551;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#65288;7-13B&#27169;&#22411;&#65289;&#20316;&#20026;&#36890;&#29992;DP&#38382;&#35299;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;DP&#20219;&#21153;&#30340;&#22235;&#32452;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;DP&#23450;&#21046;&#30340;&#24207;&#21015;&#21270;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;DP&#25163;&#21160;&#21046;&#23450;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#21333;&#19968;&#21644;&#20215;&#26684;&#20302;&#24265;&#30340;GPU&#19978;&#36816;&#34892;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#23454;&#29616;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20026;DP&#25351;&#23548;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
&lt;/p&gt;</description></item></channel></rss>