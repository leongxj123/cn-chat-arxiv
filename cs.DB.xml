<rss version="2.0"><channel><title>Chat Arxiv cs.DB</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DB</description><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13804</link><description>&lt;p&gt;
UniTS: &#19968;&#31181;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning. (arXiv:2303.13804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13804
&lt;/p&gt;
&lt;p&gt;
UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#19981;&#21516;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#38754;&#20020;&#30528;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#26512;&#24182;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;UniTS&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;&#25110;&#39044;&#35757;&#32451;&#65289;&#12290; UniTS&#30340;&#32452;&#20214;&#20351;&#29992;&#31867;&#20284;&#20110;sklearn&#30340;API&#36827;&#34892;&#35774;&#35745;&#65292;&#20197;&#20801;&#35768;&#28789;&#27963;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#20351;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#25191;&#34892;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;UniTS&#22312;&#20116;&#20010;&#20027;&#27969;&#20219;&#21153;&#21644;&#20004;&#20010;&#23454;&#38469;&#35774;&#32622;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#27809;&#26377;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings.
&lt;/p&gt;</description></item></channel></rss>