<rss version="2.0"><channel><title>Chat Arxiv cs.DB</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DB</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Data Preprocessors. (arXiv:2308.16361v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16361
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#21644;Meta&#30340;LLaMA&#21464;&#20307;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#32463;&#36807;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#19978;&#20154;&#31867;&#21270;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;LLMs&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;GPT-3.5&#12289;GPT-4&#21644;Vicuna-13B&#65289;&#22312;&#38169;&#35823;&#26816;&#27979;&#12289;&#25968;&#25454;&#25554;&#34917;&#12289;&#27169;&#24335;&#21305;&#37197;&#21644;&#23454;&#20307;&#21305;&#37197;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38500;&#20102;&#23637;&#31034;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#21069;&#27839;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#20256;&#32479;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the perform
&lt;/p&gt;</description></item></channel></rss>