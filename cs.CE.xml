<rss version="2.0"><channel><title>Chat Arxiv cs.CE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CE</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06638</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#30340;&#21019;&#26032;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;Transformer&#27169;&#22411;&#20855;&#22791;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#29992;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23613;&#31649;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;Transformer&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#19982;NLP&#21644;CV&#20013;&#30340;&#25361;&#25112;&#30456;&#27604;&#65292;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#21040;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#25110;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#36235;&#21183;&#12289;&#27700;&#24179;&#21644;&#23395;&#33410;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#22312;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#19981;&#36275;&#20043;&#22788;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro
&lt;/p&gt;</description></item></channel></rss>