<rss version="2.0"><channel><title>Chat Arxiv cs.CE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>https://arxiv.org/abs/2403.14404</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#27491;&#24555;&#36895;&#25552;&#21319;&#20854;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36816;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39044;&#26399;&#20174;&#38544;&#21547;&#25968;&#25454;&#20998;&#24067;&#20013;&#21462;&#26679;&#30340;&#26679;&#26412;&#23558;&#36981;&#23432;&#29305;&#23450;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#22522;&#30784;&#32422;&#26463;&#36827;&#34892;&#20449;&#24687;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#26679;&#26412;&#19982;&#26045;&#21152;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#32422;&#26463;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#65292;&#21487;&#29992;&#20110;&#26045;&#21152;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#20197;&#21450;&#36741;&#21161;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
&lt;/p&gt;</description></item></channel></rss>