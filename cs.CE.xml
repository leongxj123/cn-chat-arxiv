<rss version="2.0"><channel><title>Chat Arxiv cs.CE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>https://arxiv.org/abs/2403.14404</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#27491;&#24555;&#36895;&#25552;&#21319;&#20854;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36816;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39044;&#26399;&#20174;&#38544;&#21547;&#25968;&#25454;&#20998;&#24067;&#20013;&#21462;&#26679;&#30340;&#26679;&#26412;&#23558;&#36981;&#23432;&#29305;&#23450;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#22522;&#30784;&#32422;&#26463;&#36827;&#34892;&#20449;&#24687;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#26679;&#26412;&#19982;&#26045;&#21152;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#32422;&#26463;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#65292;&#21487;&#29992;&#20110;&#26045;&#21152;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#20197;&#21450;&#36741;&#21161;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item></channel></rss>