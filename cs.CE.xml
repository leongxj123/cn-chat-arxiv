<rss version="2.0"><channel><title>Chat Arxiv cs.CE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CE</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.15166</link><description>&lt;p&gt;
&#20174;&#26080;&#25928;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning from Invalid Data: On Constraint Satisfaction in Generative Models. (arXiv:2306.15166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#31934;&#24230;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29983;&#25104;&#20986;&#29289;&#29702;&#19978;&#26080;&#25928;&#25110;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#25968;&#25454;&#12290;&#24403;&#29983;&#25104;&#30340;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#65292;&#20363;&#22914;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#28385;&#36275;&#20135;&#21697;&#35268;&#26684;&#25110;&#32773;&#22312;&#33258;&#28982;&#22330;&#26223;&#20013;&#36981;&#23432;&#29289;&#29702;&#23450;&#24459;&#12290;&#20026;&#20102;&#25552;&#39640;&#31934;&#24230;&#24182;&#20445;&#25345;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#29983;&#25104;&#20998;&#24067;&#19982;&#26377;&#25928;&#20808;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20102;&#19982;&#26080;&#25928;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23558;GAN&#21644;DDPM&#31561;&#29983;&#25104;&#27169;&#22411;&#19982;&#26080;&#25928;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#29983;&#25104;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Generative models have demonstrated impressive results in vision, language, and speech. However, even with massive datasets, they struggle with precision, generating physically invalid or factually incorrect data. This is particularly problematic when the generated data must satisfy constraints, for example, to meet product specifications in engineering design or to adhere to the laws of physics in a natural scene. To improve precision while preserving diversity and fidelity, we propose a novel training mechanism that leverages datasets of constraint-violating data points, which we consider invalid. Our approach minimizes the divergence between the generative distribution and the valid prior while maximizing the divergence with the invalid distribution. We demonstrate how generative models like GANs and DDPMs that we augment to train with invalid data vastly outperform their standard counterparts which solely train on valid data points. For example, our training procedure generates up 
&lt;/p&gt;</description></item></channel></rss>