<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25910;&#32553;&#20272;&#35745;&#37327;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22351;&#39118;&#38505;&#30456;&#23545;&#20110;&#24050;&#30693;&#20559;&#24046;&#19978;&#38480;&#30340;&#29702;&#35770;&#26368;&#20248;&#20272;&#35745;&#37327;&#30340;&#26368;&#22823;&#39118;&#38505;&#30340;&#30334;&#20998;&#27604;&#22686;&#21152;&#12290;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#32463;&#39564;&#30740;&#31350;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14265</link><description>&lt;p&gt;
&#36866;&#24212;&#27169;&#22411;&#38169;&#35823;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adapting to Misspecification. (arXiv:2305.14265v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14265
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25910;&#32553;&#20272;&#35745;&#37327;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22351;&#39118;&#38505;&#30456;&#23545;&#20110;&#24050;&#30693;&#20559;&#24046;&#19978;&#38480;&#30340;&#29702;&#35770;&#26368;&#20248;&#20272;&#35745;&#37327;&#30340;&#26368;&#22823;&#39118;&#38505;&#30340;&#30334;&#20998;&#27604;&#22686;&#21152;&#12290;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#32463;&#39564;&#30740;&#31350;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#26816;&#39564;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#30740;&#31350;&#36890;&#24120;&#28041;&#21450;&#21040;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#20154;&#21592;&#24819;&#35201;&#20272;&#35745;&#19968;&#20010;&#26631;&#37327;&#21442;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#20551;&#35774;&#26469;&#35774;&#35745;&#19968;&#20010;&#31934;&#20934;&#20294;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#20559;&#24046;&#30340;&#23616;&#38480;&#20272;&#35745;&#37327;&#65292;&#20063;&#21487;&#20197;&#25918;&#26494;&#19968;&#20123;&#20551;&#35774;&#24182;&#35774;&#35745;&#19968;&#20010;&#26356;&#21152;&#40065;&#26834;&#20294;&#21464;&#37327;&#36739;&#22823;&#30340;&#20272;&#35745;&#37327;&#12290;&#24403;&#23616;&#38480;&#20272;&#35745;&#37327;&#30340;&#20559;&#24046;&#19978;&#38480;&#24050;&#30693;&#26102;&#65292;&#23558;&#26080;&#38480;&#21046;&#20272;&#35745;&#37327;&#25910;&#32553;&#21040;&#23616;&#38480;&#20272;&#35745;&#37327;&#26159;&#26368;&#20248;&#30340;&#12290;&#23545;&#20110;&#23616;&#38480;&#20272;&#35745;&#37327;&#20559;&#24046;&#19978;&#38480;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25910;&#32553;&#20272;&#35745;&#37327;&#65292;&#35813;&#20272;&#35745;&#37327;&#26368;&#23567;&#21270;&#26368;&#22351;&#39118;&#38505;&#30456;&#23545;&#20110;&#24050;&#30693;&#20559;&#24046;&#19978;&#38480;&#30340;&#29702;&#35770;&#26368;&#20248;&#20272;&#35745;&#37327;&#30340;&#26368;&#22823;&#39118;&#38505;&#30340;&#30334;&#20998;&#27604;&#22686;&#21152;&#12290;&#25105;&#20204;&#35777;&#26126;&#33258;&#36866;&#24212;&#20272;&#35745;&#37327;&#26159;&#19968;&#20010;&#21152;&#26435;&#20984;&#26368;&#23567;&#21270;&#26368;&#22823;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26597;&#25214;&#34920;&#20197;&#20415;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;&#37325;&#26032;&#23457;&#35270;&#20102;&#20116;&#39033;&#23384;&#22312;&#27169;&#22411;&#35268;&#33539;&#38382;&#39064;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#24212;&#38169;&#35823;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#32780;&#19981;&#26159;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical research typically involves a robustness-efficiency tradeoff. A researcher seeking to estimate a scalar parameter can invoke strong assumptions to motivate a restricted estimator that is precise but may be heavily biased, or they can relax some of these assumptions to motivate a more robust, but variable, unrestricted estimator. When a bound on the bias of the restricted estimator is available, it is optimal to shrink the unrestricted estimator towards the restricted estimator. For settings where a bound on the bias of the restricted estimator is unknown, we propose adaptive shrinkage estimators that minimize the percentage increase in worst case risk relative to an oracle that knows the bound. We show that adaptive estimators solve a weighted convex minimax problem and provide lookup tables facilitating their rapid computation. Revisiting five empirical studies where questions of model specification arise, we examine the advantages of adapting to -- rather than testing for -
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2002.09377</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#24418;&#19979;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Misspecification-robust likelihood-free inference in high dimensions. (arXiv:2002.09377v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.09377
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#32479;&#35745;&#27169;&#22411;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#23454;&#36341;&#20013;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#25512;&#26029;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#26469;&#27010;&#29575;&#21270;&#22320;&#36924;&#36817;&#24046;&#24322;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#20110;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#21442;&#25968;&#20351;&#29992;&#21333;&#29420;&#30340;&#37319;&#38598;&#20989;&#25968;&#21644;&#24046;&#24322;&#20989;&#25968;&#26469;&#23454;&#29616;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#12290;&#26377;&#25928;&#30340;&#21152;&#24615;&#37319;&#38598;&#32467;&#26500;&#19982;&#25351;&#25968;&#25439;&#22833;-&#20284;&#28982;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35823;&#24046;&#27169;&#22411;&#35828;&#26126;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#36793;&#38469;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The me
&lt;/p&gt;</description></item></channel></rss>