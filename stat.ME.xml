<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01762</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel. (arXiv:2311.01762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#23427;&#22312;&#25968;&#25454;&#20013;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#21442;&#25968;&#20013;&#26159;&#32447;&#24615;&#30340;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#38381;&#24335;&#35299;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;&#30697;&#38453;&#27714;&#36870;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#23545;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#24102;&#23485;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#33267;&#38646;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#30340;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#24102;&#23485;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#24102;&#23485;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to
&lt;/p&gt;</description></item></channel></rss>