<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.06746</link><description>&lt;p&gt;
&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65306;&#36890;&#36807;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#22686;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules. (arXiv:2310.06746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06746
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#65292;&#24120;&#24120;&#38656;&#35201;&#20570;&#20986;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#21463;&#21040;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#24615;&#12289;&#25551;&#36848;&#24615;&#12289;&#30456;&#20851;&#24615;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#25551;&#36848;&#28508;&#22312;&#23376;&#32676;&#30340;&#31934;&#32454;&#22240;&#26524;&#35268;&#21017;&#38598;&#26469;&#20272;&#35745;&#21644;&#22686;&#24378;&#25105;&#20204;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#35268;&#21017;&#21457;&#29616;&#12289;&#35268;&#21017;&#36873;&#25321;&#21644;&#35268;&#21017;&#20998;&#26512;&#12290;&#22312;&#35268;&#21017;&#21457;&#29616;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#26862;&#26519;&#29983;&#25104;&#19968;&#32452;&#20855;&#26377;&#30456;&#24212;&#23376;&#32676;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#22240;&#26524;&#35268;&#21017;&#27744;&#12290;&#36873;&#25321;&#38454;&#27573;&#20351;&#29992;D-&#23398;&#20064;&#26041;&#27861;&#20174;&#36825;&#20123;&#35268;&#21017;&#20013;&#36873;&#25321;&#23376;&#38598;&#65292;&#23558;&#20010;&#20307;&#27700;&#24179;&#30340;&#27835;&#30103;&#25928;&#24212;&#20316;&#20026;&#23376;&#32676;&#27700;&#24179;&#25928;&#24212;&#30340;&#32447;&#24615;&#32452;&#21512;&#36827;&#34892;&#35299;&#26500;&#12290;&#36825;&#26377;&#21161;&#20110;&#22238;&#31572;&#20043;&#21069;&#25991;&#29486;&#24573;&#35270;&#30340;&#38382;&#39064;&#65306;&#22914;&#26524;&#19968;&#20010;&#20010;&#20307;&#21516;&#26102;&#23646;&#20110;&#22810;&#20010;&#19981;&#21516;&#30340;&#27835;&#30103;&#23376;&#32676;&#65292;&#20250;&#24590;&#20040;&#26679;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously bel
&lt;/p&gt;</description></item></channel></rss>