<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06925</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#20559;&#20506;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#36827;&#34892;&#26465;&#20214;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#25805;&#20316;&#23558;&#20855;&#26377;&#26126;&#30830;&#28508;&#22312;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#36716;&#25442;&#20026;&#27809;&#26377;&#27492;&#31867;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#26681;&#25454;&#21407;&#22987;SCM&#36873;&#25321;&#30340;&#20122;&#24635;&#20307;&#30340;&#22240;&#26524;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26465;&#20214;&#25805;&#20316;&#20445;&#25345;SCMs&#30340;&#31616;&#27905;&#24615;&#65292;&#26080;&#29615;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#24182;&#19982;&#36793;&#38469;&#21270;&#25805;&#20316;&#30456;&#31526;&#21512;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#19982;&#36793;&#38469;&#21270;&#21644;&#24178;&#39044;&#32467;&#21512;&#36215;&#26469;&#65292;&#26465;&#20214;&#25805;&#20316;&#20026;&#22312;&#28508;&#22312;&#32454;&#33410;&#24050;&#32463;&#21435;&#38500;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#20197;&#21253;&#25324;&#36873;&#25321;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.13124</link><description>&lt;p&gt;
&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#23558;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#25193;&#23637;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#22522;&#30784;&#20005;&#37325;&#24615;&#27169;&#22411;&#26159;&#38543;&#26426;&#26862;&#26519;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#38454;&#27573;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
&lt;/p&gt;</description></item></channel></rss>