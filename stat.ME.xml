<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01865</link><description>&lt;p&gt;
&#36890;&#36807;&#38170;&#22810;&#20803;&#20998;&#26512;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalisation via anchor multivariate analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01865
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#65292;&#20197;&#25913;&#21892;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#65288;MVA&#65289;&#31639;&#27861;&#65292;&#22914;&#65288;&#27491;&#20132;&#21270;&#65289;PLS&#12289;RRR&#21644;MLR&#65292;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31616;&#21333;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#27668;&#20505;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;&#20026;&#25152;&#36873;&#31639;&#27861;&#25552;&#20379;&#20102;&#20272;&#35745;&#22120;&#65292;&#23637;&#31034;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32463;&#39564;&#39564;&#35777;&#31361;&#26174;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19982;MVA&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#22686;&#24378;&#21487;&#22797;&#21046;&#24615;&#30340;&#21516;&#26102;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#20013;&#30340;&#20316;&#29992;&#12290;&#25193;&#23637;&#30340;AR&#26694;&#26550;&#25512;&#36827;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#65292;&#35299;&#20915;&#20102;&#21487;&#38752;OOD&#27867;&#21270;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#22343;&#20540;&#19979;&#30340;&#22823;&#32500;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#20854;&#20108;&#27425;&#25910;&#25947;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26631;&#20934;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.07045</link><description>&lt;p&gt;
Ledoit-Wolf&#32447;&#24615;&#25910;&#32553;&#26041;&#27861;&#22312;&#26410;&#30693;&#22343;&#20540;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;(arXiv:2304.07045v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
Ledoit-Wolf linear shrinkage with unknown mean. (arXiv:2304.07045v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#22343;&#20540;&#19979;&#30340;&#22823;&#32500;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#20854;&#20108;&#27425;&#25910;&#25947;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26631;&#20934;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26410;&#30693;&#22343;&#20540;&#19979;&#30340;&#22823;&#32500;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#12290;&#24403;&#32500;&#25968;&#21644;&#26679;&#26412;&#25968;&#25104;&#27604;&#20363;&#24182;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#32463;&#39564;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#22833;&#25928;&#65292;&#27492;&#26102;&#31216;&#20026;Kolmogorov&#28176;&#36827;&#24615;&#12290;&#24403;&#22343;&#20540;&#24050;&#30693;&#26102;&#65292;Ledoit&#21644;Wolf&#65288;2004&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#25910;&#32553;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#28436;&#36827;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24403;&#22343;&#20540;&#26410;&#30693;&#26102;&#65292;&#23578;&#26410;&#25552;&#20986;&#27491;&#24335;&#35777;&#26126;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;Ledoit&#21644;Wolf&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#23427;&#30340;&#20108;&#27425;&#25910;&#25947;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#32988;&#36807;&#20102;&#20854;&#20182;&#26631;&#20934;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses large dimensional covariance matrix estimation with unknown mean. The empirical covariance estimator fails when dimension and number of samples are proportional and tend to infinity, settings known as Kolmogorov asymptotics. When the mean is known, Ledoit and Wolf (2004) proposed a linear shrinkage estimator and proved its convergence under those asymptotics. To the best of our knowledge, no formal proof has been proposed when the mean is unknown. To address this issue, we propose a new estimator and prove its quadratic convergence under the Ledoit and Wolf assumptions. Finally, we show empirically that it outperforms other standard estimators.
&lt;/p&gt;</description></item></channel></rss>