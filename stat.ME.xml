<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#31243;&#24207;&#26469;&#20272;&#35745;&#20855;&#26377;&#36830;&#32493;&#20869;&#29983;&#21464;&#37327;&#21644;&#31163;&#25955;&#24037;&#20855;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#28176;&#36817;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/1905.07812</link><description>&lt;p&gt;
&#36830;&#32493;&#20869;&#29983;&#21464;&#37327;&#21644;&#31163;&#25955;&#24037;&#20855;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#36845;&#20195;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Iterative Estimation of Nonparametric Regressions with Continuous Endogenous Variables and Discrete Instruments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1905.07812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#31243;&#24207;&#26469;&#20272;&#35745;&#20855;&#26377;&#36830;&#32493;&#20869;&#29983;&#21464;&#37327;&#21644;&#31163;&#25955;&#24037;&#20855;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#20869;&#29983;&#29420;&#31435;&#21464;&#37327;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#24403;&#21482;&#26377;&#19982;&#35823;&#24046;&#39033;&#29420;&#31435;&#30340;&#31163;&#25955;&#24037;&#20855;&#21487;&#29992;&#26102;&#12290;&#34429;&#28982;&#36825;&#20010;&#26694;&#26550;&#22312;&#24212;&#29992;&#30740;&#31350;&#20013;&#38750;&#24120;&#30456;&#20851;&#65292;&#20294;&#20854;&#23454;&#29616;&#24456;&#40635;&#28902;&#65292;&#22240;&#20026;&#22238;&#24402;&#20989;&#25968;&#25104;&#20026;&#20102;&#38750;&#32447;&#24615;&#31215;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36807;&#31243;&#26469;&#20272;&#35745;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#20854;&#28176;&#36817;&#24615;&#36136;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#20026;&#20108;&#36827;&#21046;&#26102;&#20854;&#23454;&#29616;&#32454;&#33410;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20010;&#23454;&#35777;&#24212;&#29992;&#65292;&#20854;&#20013;&#25105;&#20204;&#30740;&#31350;&#20102;&#32654;&#22269;&#20960;&#20010;&#21439;&#30340;&#25151;&#20215;&#23545;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1905.07812v2 Announce Type: replace  Abstract: We consider a nonparametric regression model with continuous endogenous independent variables when only discrete instruments are available that are independent of the error term. While this framework is very relevant for applied research, its implementation is cumbersome, as the regression function becomes the solution to a nonlinear integral equation. We propose a simple iterative procedure to estimate such models and showcase some of its asymptotic properties. In a simulation experiment, we discuss the details of its implementation in the case when the instrumental variable is binary. We conclude with an empirical application in which we examine the effect of pollution on house prices in a short panel of U.S. counties.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#32447;&#24615;&#36807;&#31243;&#30340;&#20855;&#26377;&#30456;&#20851;&#21019;&#26032;&#30340;&#38598;&#20013;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#21033;&#29992;&#35813;&#19981;&#31561;&#24335;&#33719;&#24471;&#20102;&#32447;&#24615;&#36807;&#31243;&#28382;&#21518;&#33258;&#21327;&#26041;&#24046;&#30697;&#38453;&#26368;&#22823;&#20998;&#37327;&#33539;&#25968;&#30340;&#38598;&#20013;&#24230;&#30028;&#38480;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#20272;&#35745;&#39640;&#32500;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#12289;&#26102;&#38388;&#24207;&#21015;&#24341;&#23548;&#21644;&#38271;&#26399;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.12395</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#36807;&#31243;&#20013;&#20855;&#26377;&#30456;&#20851;&#21019;&#26032;&#30340;&#38598;&#20013;&#24230;
&lt;/p&gt;
&lt;p&gt;
Concentration for high-dimensional linear processes with dependent innovations. (arXiv:2307.12395v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#32447;&#24615;&#36807;&#31243;&#30340;&#20855;&#26377;&#30456;&#20851;&#21019;&#26032;&#30340;&#38598;&#20013;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#21033;&#29992;&#35813;&#19981;&#31561;&#24335;&#33719;&#24471;&#20102;&#32447;&#24615;&#36807;&#31243;&#28382;&#21518;&#33258;&#21327;&#26041;&#24046;&#30697;&#38453;&#26368;&#22823;&#20998;&#37327;&#33539;&#25968;&#30340;&#38598;&#20013;&#24230;&#30028;&#38480;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#20272;&#35745;&#39640;&#32500;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#12289;&#26102;&#38388;&#24207;&#21015;&#24341;&#23548;&#21644;&#38271;&#26399;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#23376;&#38886;&#24067;&#23572;&#23614;&#30340;&#28151;&#21512;&#24207;&#21015;&#19978;&#30340;&#32447;&#24615;&#36807;&#31243;&#30340;$l_\infty$&#33539;&#25968;&#24320;&#21457;&#20102;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#19981;&#31561;&#24335;&#21033;&#29992;&#20102;Beveridge-Nelson&#20998;&#35299;&#65292;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21521;&#37327;&#28151;&#21512;&#24207;&#21015;&#25110;&#20854;&#21152;&#26435;&#21644;&#30340;&#19978;&#30830;&#30028;&#33539;&#25968;&#30340;&#38598;&#20013;&#24230;&#12290;&#36825;&#20010;&#19981;&#31561;&#24335;&#29992;&#20110;&#24471;&#21040;&#32447;&#24615;&#36807;&#31243;&#30340;&#28382;&#21518;$h$&#33258;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26368;&#22823;&#20998;&#37327;&#33539;&#25968;&#30340;&#38598;&#20013;&#24230;&#30028;&#38480;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;$l_1$&#27491;&#21017;&#21270;&#20272;&#35745;&#30340;&#39640;&#32500;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#30340;&#20272;&#35745;&#30028;&#38480;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#32500;&#39640;&#26031;&#24341;&#23548;&#12289;&#20197;&#21450;&#38271;&#26399;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop concentration inequalities for the $l_\infty$ norm of a vector linear processes on mixingale sequences with sub-Weibull tails. These inequalities make use of the Beveridge-Nelson decomposition, which reduces the problem to concentration for sup-norm of a vector-mixingale or its weighted sum. This inequality is used to obtain a concentration bound for the maximum entrywise norm of the lag-$h$ autocovariance matrices of linear processes. These results are useful for estimation bounds for high-dimensional vector-autoregressive processes estimated using $l_1$ regularisation, high-dimensional Gaussian bootstrap for time series, and long-run covariance matrix estimation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;FAStEN&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#20989;&#25968;&#22238;&#24402;&#38382;&#39064;&#20013;&#25191;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#23545;&#20598;&#22686;&#24191;Lagrangian&#38382;&#39064;&#30340;&#31232;&#30095;&#24615;&#36136;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14801</link><description>&lt;p&gt;
&#39640;&#32500;&#20989;&#25968;&#22238;&#24402;&#20013;&#29305;&#24449;&#36873;&#25321;&#21644;&#20272;&#35745;&#30340;&#19968;&#31181;&#39640;&#25928;&#33258;&#36866;&#24212;&#26041;&#27861;--FAStEN
&lt;/p&gt;
&lt;p&gt;
FAStEN: an efficient adaptive method for feature selection and estimation in high-dimensional functional regressions. (arXiv:2303.14801v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;FAStEN&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#20989;&#25968;&#22238;&#24402;&#38382;&#39064;&#20013;&#25191;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#23545;&#20598;&#22686;&#24191;Lagrangian&#38382;&#39064;&#30340;&#31232;&#30095;&#24615;&#36136;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#22238;&#24402;&#20998;&#26512;&#26159;&#35768;&#22810;&#24403;&#20195;&#31185;&#23398;&#24212;&#29992;&#30340;&#24050;&#24314;&#31435;&#24037;&#20855;&#12290;&#28041;&#21450;&#22823;&#35268;&#27169;&#21644;&#22797;&#26434;&#25968;&#25454;&#38598;&#30340;&#22238;&#24402;&#38382;&#39064;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#21644;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#28789;&#27963;&#30340;&#12289;&#36229;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#39640;&#32500;&#20989;&#25968;&#22238;&#24402;&#38382;&#39064;&#20013;&#25191;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#37327;&#23545;&#20989;&#25968;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20989;&#25968;&#25968;&#25454;&#12289;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#25191;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#30340;&#29305;&#24615;&#20197;&#21450;&#23545;&#20598;&#22686;&#24191;Lagrangian&#38382;&#39064;&#30340;&#31232;&#30095;&#24615;&#36136;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#26041;&#26696;&#26469;&#25552;&#39640;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20339;&#29616;&#26377;&#31454;&#20105;&#23545;&#25163;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional regression analysis is an established tool for many contemporary scientific applications. Regression problems involving large and complex data sets are ubiquitous, and feature selection is crucial for avoiding overfitting and achieving accurate predictions. We propose a new, flexible, and ultra-efficient approach to perform feature selection in a sparse high dimensional function-on-function regression problem, and we show how to extend it to the scalar-on-function framework. Our method combines functional data, optimization, and machine learning techniques to perform feature selection and parameter estimation simultaneously. We exploit the properties of Functional Principal Components, and the sparsity inherent to the Dual Augmented Lagrangian problem to significantly reduce computational cost, and we introduce an adaptive scheme to improve selection accuracy. Through an extensive simulation study, we benchmark our approach to the best existing competitors and demonstrate a 
&lt;/p&gt;</description></item></channel></rss>