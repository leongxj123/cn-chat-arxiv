<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#26469;&#25552;&#39640;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21644;&#25935;&#24863;&#24615;</title><link>http://arxiv.org/abs/2310.12140</link><description>&lt;p&gt;
&#22312;&#32447;&#20272;&#35745;&#19982;&#28378;&#21160;&#39564;&#35777;&#65306;&#36866;&#24212;&#24615;&#38750;&#21442;&#25968;&#20272;&#35745;&#19982;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data. (arXiv:2310.12140v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#26469;&#25552;&#39640;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21644;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#25928;&#35745;&#31639;&#21644;&#31454;&#20105;&#24615;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#32447;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21464;&#20307;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#19968;&#27425;&#21482;&#21462;&#19968;&#20010;&#26679;&#26412;&#28857;&#65292;&#24182;&#31435;&#21363;&#26356;&#26032;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20123;&#22312;&#32447;&#31639;&#27861;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#28378;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#19968;&#31181;&#22312;&#32447;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#21464;&#20307;&#65292;&#23545;&#20110;&#35768;&#22810;&#20856;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26469;&#35828;&#65292;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#12290;&#31867;&#20284;&#20110;&#25209;&#37327;&#20132;&#21449;&#39564;&#35777;&#65292;&#23427;&#21487;&#20197;&#25552;&#21319;&#22522;&#26412;&#20272;&#35745;&#22120;&#30340;&#33258;&#36866;&#24212;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#24456;&#31616;&#21333;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#20123;&#19968;&#33324;&#30340;&#32479;&#35745;&#31283;&#23450;&#24615;&#20551;&#35774;&#12290;&#27169;&#25311;&#30740;&#31350;&#24378;&#35843;&#20102;&#28378;&#21160;&#39564;&#35777;&#20013;&#21457;&#25955;&#26435;&#37325;&#22312;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#24456;&#23567;&#30340;&#20559;&#24046;&#65292;&#23427;&#30340;&#25935;&#24863;&#24615;&#20063;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and instantly update the parameter estimate of interest. In this work we consider model selection and hyperparameter tuning for such online algorithms. We propose a weighted rolling-validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators. Similar to batch cross-validation, it can boost base estimators to achieve a better, adaptive convergence rate. Our theoretical analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in rolling validation in practice and demonstrates its sensitivity even when there is only a slim
&lt;/p&gt;</description></item></channel></rss>