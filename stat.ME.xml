<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15711</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Neural Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#26088;&#22312;&#20174;&#20302;&#32423;&#35266;&#27979;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#39640;&#32423;&#22240;&#26524;&#34920;&#24449;&#12290;&#23427;&#29305;&#21035;&#25797;&#38271;&#39044;&#27979;&#22312;&#26410;&#35265;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#22240;&#20026;&#36825;&#20123;&#21464;&#21270;&#36890;&#24120;&#21487;&#20197;&#35299;&#37322;&#20026;&#24178;&#39044;&#30340;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;{&#24050;&#35265;}&#20998;&#24067;&#21464;&#21270;&#25104;&#20026;&#24110;&#21161;&#35782;&#21035;&#22240;&#26524;&#34920;&#24449;&#30340;&#33258;&#28982;&#31574;&#30053;&#65292;&#36827;&#32780;&#26377;&#21161;&#20110;&#39044;&#27979;&#20197;&#21069;{&#26410;&#35265;}&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#30830;&#23450;&#36825;&#20123;&#20998;&#24067;&#21464;&#21270;&#30340;&#31867;&#22411;&#65288;&#25110;&#26465;&#20214;&#65289;&#23545;&#20110;&#22240;&#26524;&#34920;&#24449;&#30340;&#21487;&#35782;&#21035;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#24037;&#20316;&#24314;&#31435;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#65292;&#34920;&#24449;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15711v1 Announce Type: new  Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findin
&lt;/p&gt;</description></item></channel></rss>