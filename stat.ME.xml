<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13451</link><description>&lt;p&gt;
&#25235;&#20303;&#23427;&#20204;&#65306;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24120;&#22823;&#30340;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#22312;&#20854;&#20013;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Sussman&#31561;&#20154;&#25552;&#20986;&#30340;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21305;&#37197;&#28388;&#27874;&#31639;&#27861;&#20013;&#36845;&#20195;&#22320;&#24809;&#32602;&#21512;&#36866;&#30340;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#21305;&#37197;&#28388;&#27874;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#30340;Erdos-Renyi&#22270;&#35774;&#32622;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#39564;&#35777;&#65292;&#26174;&#31034;&#20854;&#22312;&#28201;&#21644;&#30340;&#27169;&#22411;&#26465;&#20214;&#19979;&#33021;&#22815;&#39034;&#24207;&#22320;&#21457;&#29616;&#22810;&#20010;&#27169;&#26495;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#27169;&#22411;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#20154;&#33041;&#36830;&#25509;&#32452;&#21644;&#22823;&#22411;&#20132;&#26131;&#30693;&#35782;&#24211;&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
&lt;/p&gt;</description></item><item><title>Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;</title><link>http://arxiv.org/abs/2307.00835</link><description>&lt;p&gt;
Engression: &#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#22806;&#25512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Engression: Extrapolation for Nonlinear Regression?. (arXiv:2307.00835v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00835
&lt;/p&gt;
&lt;p&gt;
Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#25512;&#23545;&#20110;&#35768;&#22810;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24120;&#24120;&#20250;&#36935;&#21040;&#36229;&#20986;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#35828;&#65292;&#22806;&#25512;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#36935;&#21040;&#22256;&#38590;&#65306;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#25903;&#25345;&#33539;&#22260;&#22806;&#25552;&#20379;&#36830;&#32493;&#30340;&#39044;&#27979;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24448;&#24448;&#21464;&#24471;&#19981;&#21487;&#25511;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20854;&#21487;&#38752;&#24615;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#19981;&#20250;&#31435;&#21363;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#21517;&#20026;&#8220;engression&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#65292;&#20854;&#20013;&#22122;&#22768;&#28155;&#21152;&#21040;&#21327;&#21464;&#37327;&#19978;&#24182;&#24212;&#29992;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36890;&#24120;&#36866;&#29992;&#20110;&#35768;&#22810;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;engression&#21487;&#20197;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#25104;&#21151;&#36827;&#34892;&#22806;&#25512;&#65292;&#20363;&#22914;&#20005;&#26684;&#38480;&#21046;&#22122;&#22768;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;Bootstrap-Aided&#25512;&#26029;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#38750;&#21442;&#25968;Bootstrap&#38590;&#20197;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13598</link><description>&lt;p&gt;
&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;Bootstrap-Aided&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bootstrap-Assisted Inference for Generalized Grenander-type Estimators. (arXiv:2303.13598v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;Bootstrap-Aided&#25512;&#26029;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#38750;&#21442;&#25968;Bootstrap&#38590;&#20197;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Westling&#21644;Carone&#65288;2020&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#29305;&#24615;&#65292;&#36825;&#26159;&#19968;&#31867;&#29992;&#20110;&#21333;&#35843;&#20989;&#25968;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#30340;&#22810;&#25165;&#22810;&#33402;&#30340;&#31867;&#12290;&#36825;&#20123;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#34920;&#31034;&#20026;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#22823;&#20984;&#25903;&#25745;&#32447;&#30340;&#24038;&#23548;&#25968;&#65292;&#35813;&#39640;&#26031;&#36807;&#31243;&#30340;&#21327;&#26041;&#24046;&#26680;&#21487;&#20197;&#24456;&#22797;&#26434;&#65292;&#20854;&#21333;&#39033;&#24335;&#22343;&#20540;&#21487;&#20197;&#26159;&#26410;&#30693;&#38454;&#25968;&#65288;&#22914;&#26524;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#30340;&#24179;&#22374;&#24230;&#26410;&#30693;&#65289;&#12290;&#26631;&#20934;&#30340;&#38750;&#21442;&#25968;bootstrap&#21363;&#20351;&#30693;&#36947;&#22343;&#20540;&#30340;&#21333;&#39033;&#24335;&#39034;&#24207;&#65292;&#20063;&#26080;&#27861;&#19968;&#33268;&#22320;&#36924;&#36817;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;&#22823;&#26679;&#26412;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#22312;&#24212;&#29992;&#20013;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25512;&#26029;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;Grenander&#22411;&#20272;&#35745;&#37327;&#30340;bootstrap&#36741;&#21161;&#25512;&#26029;&#31243;&#24207;&#12290;&#35813;&#31243;&#24207;&#20381;&#36182;&#20110;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#20294;&#33258;&#21160;&#21270;&#30340;&#21464;&#25442;e
&lt;/p&gt;
&lt;p&gt;
Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose covariance kernel can be complicated and whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#32676;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#65292;&#24182;&#33021;&#22815;&#20197;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#24369;&#30456;&#20851;&#38543;&#26426;&#36807;&#31243;&#30340;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
High-dimensional variable clustering based on sub-asymptotic maxima of a weakly dependent random process. (arXiv:2302.00934v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00934
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20122;&#28176;&#36817;&#26497;&#22823;&#20540;&#30340;&#39640;&#32500;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#32676;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#26469;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#65292;&#24182;&#33021;&#22815;&#20197;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#32858;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#28176;&#36817;&#29420;&#31435;&#22359; (AI-block) &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#32676;&#38598;&#38388;&#22810;&#21464;&#37327;&#24179;&#31283;&#28151;&#21512;&#38543;&#26426;&#36807;&#31243;&#30340;&#26497;&#22823;&#20540;&#30340;&#29420;&#31435;&#24615;&#26469;&#23450;&#20041;&#31181;&#32676;&#27700;&#24179;&#30340;&#32676;&#38598;&#12290;&#35813;&#27169;&#22411;&#31867;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#31181;&#20559;&#24207;&#20851;&#31995;&#65292;&#20801;&#35768;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26080;&#38656;&#20107;&#20808;&#25351;&#23450;&#32676;&#38598;&#30340;&#25968;&#37327;&#21363;&#21487;&#24674;&#22797;&#21464;&#37327;&#30340;&#32676;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#27934;&#23519;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#22312;&#32500;&#24230;&#20013;&#26159;&#22810;&#39033;&#24335;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#32676;&#38598;&#12290;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#38750;&#21442;&#25968;&#22320;&#23398;&#20064;&#20986;&#20165;&#20165;&#26159;&#20122;&#28176;&#36817;&#30340;&#20381;&#36182;&#36807;&#31243;&#30340;&#22359;&#26368;&#22823;&#20540;&#30340;&#32676;&#32452;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new class of models for variable clustering called Asymptotic Independent block (AI-block) models, which defines population-level clusters based on the independence of the maxima of a multivariate stationary mixing random process among clusters. This class of models is identifiable, meaning that there exists a maximal element with a partial order between partitions, allowing for statistical inference. We also present an algorithm for recovering the clusters of variables without specifying the number of clusters \emph{a priori}. Our work provides some theoretical insights into the consistency of our algorithm, demonstrating that under certain conditions it can effectively identify clusters in the data with a computational complexity that is polynomial in the dimension. This implies that groups can be learned nonparametrically in which block maxima of a dependent process are only sub-asymptotic. To further illustrate the significance of our work, we applied our method to neu
&lt;/p&gt;</description></item></channel></rss>