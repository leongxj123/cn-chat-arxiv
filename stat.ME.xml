<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13929</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21457;&#29616;&#20915;&#31574;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics. (arXiv:2401.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#65292;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#30340;&#35777;&#25454;&#34920;&#26126;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#21487;&#33021;&#20316;&#20026;MDD&#30340;&#34892;&#20026;&#26631;&#35760;&#12290;&#20026;&#20102;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#65292;&#24739;&#32773;&#25191;&#34892;&#28041;&#21450;&#20570;&#20986;&#36873;&#25321;&#25110;&#23545;&#19982;&#19981;&#21516;&#32467;&#26524;&#30456;&#20851;&#32852;&#30340;&#21050;&#28608;&#20316;&#20986;&#21453;&#24212;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#34987;&#25311;&#21512;&#20197;&#25552;&#21462;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#21508;&#20010;&#26041;&#38754;&#30340;&#21442;&#25968;&#65292;&#20197;&#34920;&#24449;&#24739;&#32773;&#22312;&#34892;&#20026;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#22522;&#20110;&#21333;&#20010;RL&#27169;&#22411;&#30340;&#22870;&#21169;&#23398;&#20064;&#34920;&#24449;&#19981;&#36275;; &#30456;&#21453;&#65292;&#20915;&#31574;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#20043;&#38388;&#30340;&#20999;&#25442;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31185;&#23398;&#38382;&#39064;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#21160;&#24577;&#22914;&#20309;&#24433;&#21709;MDD&#24739;&#32773;&#30340;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#27010;&#29575;&#22870;&#21169;&#20219;&#21153;(PRT)&#25152;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03731</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning continuous-valued treatment effects through representation balancing. (arXiv:2309.03731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#12289;&#21830;&#19994;&#12289;&#32463;&#27982;&#31561;&#39046;&#22495;&#65292;&#20272;&#35745;&#19982;&#27835;&#30103;&#21058;&#37327;&#30456;&#20851;&#30340;&#27835;&#30103;&#25928;&#26524;&#65288;&#21363;&#8220;&#21058;&#37327;&#21453;&#24212;&#8221;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#36890;&#24120;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24471;&#21040;&#30340;&#65292;&#32780;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#21058;&#37327;&#20998;&#37197;&#21463;&#21040;&#39044;&#22788;&#29702;&#21327;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#21040;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBRNet&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#30340;&#21058;&#37327;&#21453;&#24212;&#12290;CBRNet&#37319;&#29992;&#20102;Neyman-Rubin&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65292;&#24182;&#25193;&#23637;&#20102;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#20811;&#26381;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#24212;&#29992;&#34920;&#31034;&#24179;&#34913;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-v
&lt;/p&gt;</description></item></channel></rss>