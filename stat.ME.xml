<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.16336</link><description>&lt;p&gt;
&#22810;&#29615;&#22659;&#22330;&#26223;&#20013;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference in Multi-environment Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#36328;&#22810;&#20010;&#29615;&#22659;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#20004;&#31181;&#35206;&#30422;&#31867;&#22411;&#65292;&#25193;&#23637;&#20102;Jackknife&#21644;&#20998;&#35010;&#19968;&#33268;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#36825;&#31181;&#38750;&#20256;&#32479;&#30340;&#23618;&#27425;&#25968;&#25454;&#29983;&#25104;&#22330;&#26223;&#20013;&#33719;&#24471;&#26080;&#20998;&#24067;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#23545;&#38750;&#23454;&#20540;&#21709;&#24212;&#35774;&#32622;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#36825;&#20123;&#19968;&#33324;&#38382;&#39064;&#20013;&#39044;&#27979;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#29702;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#36825;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#25968;&#25454;&#30340;&#39044;&#27979;&#25512;&#26029;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#21450;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65307;&#36825;&#36890;&#36807;&#31070;&#32463;&#21270;&#23398;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#26497;&#20540;&#29702;&#35770;&#30340;EQRN&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23384;&#22312;&#22797;&#26434;&#39044;&#27979;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#27946;&#27700;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#25552;&#20379;&#19968;&#22825;&#21069;&#22238;&#24402;&#27700;&#24179;&#21644;&#36229;&#20986;&#27010;&#29575;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.07590</link><description>&lt;p&gt;
&#26497;&#31471;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#27946;&#27700;&#39118;&#38505;&#39044;&#27979;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk. (arXiv:2208.07590v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#26497;&#20540;&#29702;&#35770;&#30340;EQRN&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23384;&#22312;&#22797;&#26434;&#39044;&#27979;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#27946;&#27700;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#25552;&#20379;&#19968;&#22825;&#21069;&#22238;&#24402;&#27700;&#24179;&#21644;&#36229;&#20986;&#27010;&#29575;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#39118;&#38505;&#35780;&#20272;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#36229;&#20986;&#21382;&#21490;&#35266;&#27979;&#33539;&#22260;&#30340;&#39640;&#20998;&#20301;&#25968;&#12290;&#24403;&#39118;&#38505;&#20381;&#36182;&#20110;&#35266;&#27979;&#39044;&#27979;&#21464;&#37327;&#30340;&#20540;&#26102;&#65292;&#22238;&#24402;&#25216;&#26415;&#29992;&#20110;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#36827;&#34892;&#25554;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EQRN&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#26497;&#20540;&#29702;&#35770;&#30340;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#24418;&#25104;&#19968;&#31181;&#33021;&#22815;&#22312;&#22797;&#26434;&#39044;&#27979;&#21464;&#37327;&#30456;&#20851;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22806;&#25512;&#30340;&#26041;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#28982;&#22320;&#23558;&#25968;&#25454;&#20013;&#30340;&#38468;&#21152;&#32467;&#26500;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;EQRN&#30340;&#24490;&#29615;&#29256;&#26412;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#22797;&#26434;&#30340;&#39034;&#24207;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#29790;&#22763;Aare&#27969;&#22495;&#30340;&#27946;&#27700;&#39118;&#38505;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#22810;&#20010;&#21327;&#21464;&#37327;&#20449;&#24687;&#65292;&#25552;&#20379;&#19968;&#22825;&#21069;&#22238;&#24402;&#27700;&#24179;&#21644;&#36229;&#20986;&#27010;&#29575;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#36755;&#20986;&#34917;&#20805;&#20102;&#20256;&#32479;&#26497;&#20540;&#20998;&#26512;&#30340;&#38745;&#24577;&#22238;&#24402;&#27700;&#24179;&#65292;&#24182;&#19988;&#39044;&#27979;&#33021;&#22815;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecasting of flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedances probabilities. This output complements the static return level from a traditional extreme value analysis and the predictions are able to adapt to distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#20840;&#29699;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#20840;&#23616;&#32553;&#20943;&#21442;&#25968;&#65292;&#20351;&#29992;&#32452;&#21035;&#29305;&#23450;&#30340;&#32553;&#20943;&#21442;&#25968;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#35777;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#28857;&#12290;&#22312;&#31232;&#30095;/&#23494;&#38598;&#20808;&#39564;&#19979;&#65292;&#39044;&#27979;&#24615;&#33021;&#22240;&#35780;&#20272;&#30340;&#32463;&#27982;&#21464;&#37327;&#21644;&#26102;&#38388;&#26694;&#26550;&#32780;&#24322;&#65292;&#20294;&#21160;&#24577;&#27169;&#22411;&#24179;&#22343;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.04902</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#39044;&#27979;&#23439;&#35266;&#32463;&#27982;&#25968;&#25454;&#65306;&#31232;&#30095;&#36824;&#26159;&#23494;&#38598;&#65311;&#35201;&#30475;&#24773;&#20917;&#65281;
&lt;/p&gt;
&lt;p&gt;
Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!. (arXiv:2206.04902v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#20840;&#29699;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#20840;&#23616;&#32553;&#20943;&#21442;&#25968;&#65292;&#20351;&#29992;&#32452;&#21035;&#29305;&#23450;&#30340;&#32553;&#20943;&#21442;&#25968;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#35777;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#28857;&#12290;&#22312;&#31232;&#30095;/&#23494;&#38598;&#20808;&#39564;&#19979;&#65292;&#39044;&#27979;&#24615;&#33021;&#22240;&#35780;&#20272;&#30340;&#32463;&#27982;&#21464;&#37327;&#21644;&#26102;&#38388;&#26694;&#26550;&#32780;&#24322;&#65292;&#20294;&#21160;&#24577;&#27169;&#22411;&#24179;&#22343;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#27169;&#21644;&#39044;&#27979;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#26102;&#65292;&#21521;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;VARs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#32553;&#20943;&#20808;&#39564;&#26041;&#27861;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21322;&#20840;&#29699;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#29992;&#29305;&#23450;&#32452;&#21035;&#30340;&#32553;&#20943;&#21442;&#25968;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#20840;&#23616;&#32553;&#20943;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#21508;&#31181;&#32553;&#20943;&#20808;&#39564;&#65292;&#22914;&#20840;&#23616;-&#23616;&#37096;&#20808;&#39564;&#21644;&#38543;&#26426;&#25628;&#32034;&#21464;&#37327;&#36873;&#25321;&#20808;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23545;&#32654;&#22269;&#32463;&#27982;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27491;&#22312;&#36827;&#34892;&#30340;"&#31232;&#30095;&#20551;&#35937;"&#36777;&#35770;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#25506;&#35752;&#65292;&#21457;&#29616;&#22312;&#31232;&#30095;/&#23494;&#38598;&#20808;&#39564;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#22312;&#35780;&#20272;&#30340;&#32463;&#27982;&#21464;&#37327;&#21644;&#26102;&#38388;&#26694;&#26550;&#20013;&#21464;&#21270;&#24456;&#22823;&#12290;&#28982;&#32780;&#65292;&#21160;&#24577;&#27169;&#22411;&#24179;&#22343;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vectorautogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinking priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinking priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, ca
&lt;/p&gt;</description></item></channel></rss>