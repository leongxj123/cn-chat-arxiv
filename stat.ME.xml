<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.11665</link><description>&lt;p&gt;
&#32479;&#35745;&#23545;&#27969;&#24418;&#20551;&#35774;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Statistical exploration of the Manifold Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.11665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;&#29702;&#35770;&#65292;&#23427;&#35748;&#20026;&#21517;&#20041;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#23454;&#38469;&#19978;&#38598;&#20013;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#20013;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#36825;&#31181;&#36890;&#29992;&#19988;&#38750;&#24120;&#31616;&#21333;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#20016;&#23500;&#32780;&#26377;&#26102;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#12289;&#30456;&#20851;&#24615;&#21644;&#24179;&#31283;&#24615;&#31561;&#22522;&#26412;&#27010;&#24565;&#12290;&#36825;&#20026;&#20026;&#20160;&#20040;&#27969;&#24418;&#20551;&#35774;&#22312;&#36825;&#20040;&#22810;&#24773;&#20917;&#19979;&#20284;&#20046;&#25104;&#31435;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#22312;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism
&lt;/p&gt;</description></item></channel></rss>