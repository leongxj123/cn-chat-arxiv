<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#32047;&#31215;&#38543;&#26426;&#23454;&#29616;&#30340;&#23614;&#37096;&#27010;&#29575;&#21644;&#26399;&#26395;&#32447;&#24615;&#25439;&#22833;&#30340;&#26032;&#30340;&#26356;&#23574;&#38160;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#22312;&#22522;&#30784;&#20998;&#24067;&#21322;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19981;&#21463;&#38480;&#21046;&#65292;&#34917;&#20805;&#20102;&#24050;&#26377;&#30340;&#32467;&#26524;&#65292;&#24320;&#36767;&#20102;&#20016;&#23500;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.02400</link><description>&lt;p&gt;
&#20851;&#20110;&#32047;&#31215;&#38543;&#26426;&#23454;&#29616;&#30340;&#23614;&#37096;&#27010;&#29575;&#21644;&#26399;&#26395;&#25439;&#22833;&#30340;&#25913;&#36827;&#21322;&#21442;&#25968;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
On Improved Semi-parametric Bounds for Tail Probability and Expected Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#32047;&#31215;&#38543;&#26426;&#23454;&#29616;&#30340;&#23614;&#37096;&#27010;&#29575;&#21644;&#26399;&#26395;&#32447;&#24615;&#25439;&#22833;&#30340;&#26032;&#30340;&#26356;&#23574;&#38160;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#22312;&#22522;&#30784;&#20998;&#24067;&#21322;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19981;&#21463;&#38480;&#21046;&#65292;&#34917;&#20805;&#20102;&#24050;&#26377;&#30340;&#32467;&#26524;&#65292;&#24320;&#36767;&#20102;&#20016;&#23500;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24403;&#20010;&#21035;&#23454;&#29616;&#26159;&#29420;&#31435;&#30340;&#26102;&#65292;&#32047;&#31215;&#38543;&#26426;&#23454;&#29616;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#30340;&#22522;&#30784;&#20998;&#24067;&#26410;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#20102;&#23545;&#23614;&#37096;&#27010;&#29575;&#21644;&#26399;&#26395;&#32447;&#24615;&#25439;&#22833;&#30340;&#26032;&#30340;&#26356;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23574;&#38160;&#30028;&#38480;&#24456;&#22909;&#22320;&#34917;&#20805;&#20102;&#25991;&#29486;&#20013;&#24050;&#32463;&#24314;&#31435;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22522;&#20110;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#21518;&#32773;&#32463;&#24120;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#29420;&#31435;&#24615;&#24182;&#20351;&#29992;&#19981;&#22815;&#20248;&#38597;&#30340;&#35777;&#26126;&#12290;&#26032;&#30340;&#35265;&#35299;&#21253;&#25324;&#22312;&#38750;&#30456;&#21516;&#24773;&#20917;&#19979;&#30340;&#35777;&#26126;&#65292;&#36798;&#21040;&#30028;&#38480;&#30340;&#20998;&#24067;&#20855;&#26377;&#30456;&#31561;&#30340;&#33539;&#22260;&#23646;&#24615;&#65292;&#24182;&#19988;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#23545;&#24635;&#21644;&#30340;&#26399;&#26395;&#20540;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#23545;Korkine&#24658;&#31561;&#24335;&#30340;&#25512;&#24191;&#26469;&#23396;&#31435;&#20986;&#26469;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#30340;&#30028;&#38480;&#19981;&#20165;&#34917;&#20805;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#32780;&#19988;&#24320;&#25299;&#20102;&#22823;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#25913;&#36827;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02400v1 Announce Type: new  Abstract: We revisit the fundamental issue of tail behavior of accumulated random realizations when individual realizations are independent, and we develop new sharper bounds on the tail probability and expected linear loss. The underlying distribution is semi-parametric in the sense that it remains unrestricted other than the assumed mean and variance. Our sharp bounds complement well-established results in the literature, including those based on aggregation, which often fail to take full account of independence and use less elegant proofs. New insights include a proof that in the non-identical case, the distributions attaining the bounds have the equal range property, and that the impact of each random variable on the expected value of the sum can be isolated using an extension of the Korkine identity. We show that the new bounds not only complement the extant results but also open up abundant practical applications, including improved pricing 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17712</link><description>&lt;p&gt;
&#20351;&#29992;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#26377;&#21508;&#31181;&#24037;&#20855;&#21487;&#29992;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20316;&#31038;&#21306;&#26816;&#27979;/&#33410;&#28857;&#32858;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#38500;&#20102;&#35889;&#32858;&#31867;&#26041;&#27861;&#20043;&#22806;&#65292;&#23545;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#23398;&#20064;&#23884;&#20837;&#26041;&#27861;&#65292;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#30001;node2vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;node2vec&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#24212;&#29992;k-means&#32858;&#31867;&#21487;&#20197;&#23545;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#24369;&#19968;&#33268;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#32593;&#32476;&#25968;&#25454;&#30340;&#20854;&#20182;&#23884;&#20837;&#24037;&#20855;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
&lt;/p&gt;</description></item></channel></rss>