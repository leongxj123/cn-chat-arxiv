<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#30340;&#38754;&#26495;&#25968;&#25454;&#20013;&#22238;&#24402;&#31995;&#25968;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#21644;&#20559;&#20506;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#22240;&#32032;&#24369;&#24341;&#36215;&#30340;&#20559;&#20506;&#21644;&#22823;&#23567;&#22833;&#30495;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#22240;&#32032;&#26159;&#21542;&#24378;&#22766;&#65292;&#37117;&#33021;&#24471;&#21040;&#32479;&#19968;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.06639</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#30340;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#40065;&#26834;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust Estimation and Inference in Panels with Interactive Fixed Effects. (arXiv:2210.06639v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#30340;&#38754;&#26495;&#25968;&#25454;&#20013;&#22238;&#24402;&#31995;&#25968;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#21644;&#20559;&#20506;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#65292;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#22240;&#32032;&#24369;&#24341;&#36215;&#30340;&#20559;&#20506;&#21644;&#22823;&#23567;&#22833;&#30495;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#22240;&#32032;&#26159;&#21542;&#24378;&#22766;&#65292;&#37117;&#33021;&#24471;&#21040;&#32479;&#19968;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#65288;&#21363;&#20855;&#26377;&#22240;&#23376;&#32467;&#26500;&#65289;&#30340;&#38754;&#26495;&#25968;&#25454;&#20013;&#22238;&#24402;&#31995;&#25968;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#24320;&#21457;&#30340;&#20272;&#35745;&#22120;&#21644;&#32622;&#20449;&#21306;&#38388;&#21487;&#33021;&#22312;&#19968;&#20123;&#22240;&#32032;&#36739;&#24369;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#20506;&#21644;&#22823;&#23567;&#22833;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#21644;&#20559;&#20506;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#30340;&#20272;&#35745;&#22120;&#65292;&#26080;&#35770;&#22240;&#32032;&#26159;&#21542;&#24378;&#22766;&#37117;&#33021;&#20445;&#25345;&#32479;&#19968;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26368;&#23567;&#21270;&#32447;&#24615;&#20272;&#35745;&#29702;&#35770;&#65292;&#22312;&#21021;&#22987;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#30340;&#35823;&#24046;&#19978;&#20351;&#29992;&#26680;&#33539;&#25968;&#32422;&#26463;&#26469;&#24418;&#25104;&#19968;&#20010;&#26080;&#20559;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#25152;&#24471;&#20272;&#35745;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#21040;&#22240;&#32032;&#24369;&#24341;&#36215;&#30340;&#21097;&#20313;&#20559;&#24046;&#30340;&#20559;&#20506;&#24863;&#30693;&#32622;&#20449;&#21306;&#38388;&#12290;&#22312;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22240;&#32032;&#36739;&#24369;&#30340;&#24773;&#20917;&#19979;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#22240;&#32032;&#36739;&#24378;&#30340;&#24773;&#20917;&#19979;&#20960;&#20046;&#27809;&#26377;&#20272;&#35745;&#35823;&#24046;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider estimation and inference for a regression coefficient in panels with interactive fixed effects (i.e., with a factor structure). We show that previously developed estimators and confidence intervals (CIs) might be heavily biased and size-distorted when some of the factors are weak. We propose estimators with improved rates of convergence and bias-aware CIs that are uniformly valid regardless of whether the factors are strong or not. Our approach applies the theory of minimax linear estimation to form a debiased estimate using a nuclear norm bound on the error of an initial estimate of the interactive fixed effects. We use the obtained estimate to construct a bias-aware CI taking into account the remaining bias due to weak factors. In Monte Carlo experiments, we find a substantial improvement over conventional approaches when factors are weak, with little cost to estimation error when factors are strong.
&lt;/p&gt;</description></item></channel></rss>