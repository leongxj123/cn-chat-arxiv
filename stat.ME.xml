<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26469;&#25551;&#36848;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#36866;&#29992;&#20110;&#38750;&#21487;&#31215;&#24773;&#20917;&#65292;&#24182;&#35828;&#26126;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#20197;&#21450;&#35813;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01163</link><description>&lt;p&gt;
&#38750;&#21487;&#31215;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340;&#25193;&#23637;&#32500;&#23572;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
The extended Ville's inequality for nonintegrable nonnegative supermartingales. (arXiv:2304.01163v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26469;&#25551;&#36848;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#36866;&#29992;&#20110;&#38750;&#21487;&#31215;&#24773;&#20917;&#65292;&#24182;&#35828;&#26126;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#20197;&#21450;&#35813;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312; Robbins &#30340;&#21021;&#22987;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#20005;&#23494;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340;&#25193;&#23637;&#29702;&#35770;&#65292;&#19981;&#38656;&#35201;&#21487;&#31215;&#24615;&#25110;&#26377;&#38480;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102; Robbins &#39044;&#31034;&#30340;&#19968;&#20010;&#20851;&#38190;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#31216;&#20026;&#25193;&#23637;&#32500;&#23572;&#19981;&#31561;&#24335;&#65292;&#23427;&#21152;&#24378;&#20102;&#32463;&#20856;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#65288;&#36866;&#29992;&#20110;&#21487;&#31215;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65289;&#65292;&#24182;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#38750;&#21487;&#31215;&#35774;&#32622;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#25193;&#23637;&#30340;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340; $\sigma$- &#26377;&#38480;&#28151;&#21512;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#19968;&#20123;&#24212;&#29992;&#65292;&#22914;&#22312;&#25512;&#23548;&#38750;&#21442;&#25968;&#32622;&#20449;&#24207;&#21015;&#21644;&#65288;&#25193;&#23637;&#65289;e-&#36807;&#31243;&#20013;&#20351;&#29992;&#19981;&#36866;&#24403;&#28151;&#21512;&#65288;&#20808;&#39564;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following initial work by Robbins, we rigorously present an extended theory of nonnegative supermartingales, requiring neither integrability nor finiteness. In particular, we derive a key maximal inequality foreshadowed by Robbins, which we call the extended Ville's inequality, that strengthens the classical Ville's inequality (for integrable nonnegative supermartingales), and also applies to our nonintegrable setting. We derive an extension of the method of mixtures, which applies to $\sigma$-finite mixtures of our extended nonnegative supermartingales. We present some implications of our theory for sequential statistics, such as the use of improper mixtures (priors) in deriving nonparametric confidence sequences and (extended) e-processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#37325;&#40065;&#26834;&#36125;&#21494;&#26031;&#25512;&#26029;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#20559;&#24046;&#26657;&#27491;&#24182;&#24418;&#25104;&#20102;&#21487;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.16298</link><description>&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#21452;&#37325;&#40065;&#26834;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Double Robust Bayesian Inference on Average Treatment Effects. (arXiv:2211.16298v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21452;&#37325;&#40065;&#26834;&#36125;&#21494;&#26031;&#25512;&#26029;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#20559;&#24046;&#26657;&#27491;&#24182;&#24418;&#25104;&#20102;&#21487;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#20559;&#24615;&#19979;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#21452;&#37325;&#40065;&#26834;&#36125;&#21494;&#26031;&#25512;&#26029;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#35843;&#25972;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#26657;&#27491;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#30340;ATE&#30340;&#21518;&#39564;&#20998;&#24067;&#19978;&#24341;&#20837;&#19968;&#20010;&#37325;&#26032;&#23621;&#20013;&#26415;&#35821;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#21452;&#37325;&#40065;&#26834;&#24615;&#19979;&#30340;&#21322;&#21442;&#25968;Bernstein-von Mises&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#37327;&#21644;&#21452;&#37325;&#40065;&#26834;&#39057;&#29575;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#31561;&#20215;&#24615;&#65307;&#21363;&#65292;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#30340;&#32570;&#20047;&#24179;&#28369;&#24615;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#24471;&#20998;&#30340;&#39640;&#35268;&#21017;&#24615;&#36827;&#34892;&#34917;&#20607;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#22240;&#27492;&#65292;&#20135;&#29983;&#30340;&#36125;&#21494;&#26031;&#28857;&#20272;&#35745;&#20869;&#22312;&#21270;&#20102;&#39057;&#29575;&#22411;&#21452;&#37325;&#40065;&#26834;&#20272;&#35745;&#37327;&#30340;&#20559;&#24046;&#26657;&#27491;&#65292;&#32780;&#36125;&#21494;&#26031;&#21487;&#20449;&#38598;&#24418;&#25104;&#30340;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#28176;&#36817;&#31934;&#30830;&#30340;&#35206;&#30422;&#27010;&#29575;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#40065;&#26834;&#30340;&#36125;&#21494;&#26031;&#31243;&#24207;&#23548;&#33268;&#20102;&#26174;&#30528;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We study a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. Our robust Bayesian approach involves two adjustment steps: first, we make a correction for prior distributions of the conditional mean function; second, we introduce a recentering term on the posterior distribution of the resulting ATE. We prove asymptotic equivalence of our Bayesian estimator and double robust frequentist estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian point estimator internalizes the bias correction as the frequentist-type doubly robust estimator, and the Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, we find that this robust Bayesian procedure leads to significant
&lt;/p&gt;</description></item></channel></rss>