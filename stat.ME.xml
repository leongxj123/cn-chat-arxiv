<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#35299;&#20915;&#38543;&#26426;&#28436;&#21270;&#26041;&#31243;&#30340;&#31283;&#20581;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26377;&#25928;&#22320;&#27979;&#37327;&#20102;&#21327;&#21464;&#37327;&#30340;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33021;&#25581;&#31034;&#19982;&#21327;&#21464;&#37327;&#30340;&#32553;&#25918;&#26497;&#38480;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2401.16286</link><description>&lt;p&gt;
&#26080;&#38480;&#32500;&#24230;&#20013;&#38543;&#26426;&#28436;&#21270;&#26041;&#31243;&#30340;&#31283;&#20581;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions. (arXiv:2401.16286v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#35299;&#20915;&#38543;&#26426;&#28436;&#21270;&#26041;&#31243;&#30340;&#31283;&#20581;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26377;&#25928;&#22320;&#27979;&#37327;&#20102;&#21327;&#21464;&#37327;&#30340;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33021;&#25581;&#31034;&#19982;&#21327;&#21464;&#37327;&#30340;&#32553;&#25918;&#26497;&#38480;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Hilbert&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#29992;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#26469;&#31283;&#20581;&#22320;&#27979;&#37327;&#21327;&#21464;&#37327;&#30340;&#21464;&#21270;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#26041;&#31243;&#65292;&#22522;&#20110;&#27178;&#25130;&#38754;&#21327;&#26041;&#24046;&#30340;&#26631;&#20934;&#25216;&#26415;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#20986;&#32479;&#35745;&#19978;&#30456;&#20851;&#30340;&#38543;&#26426;&#39537;&#21160;&#22240;&#32032;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20272;&#35745;&#29702;&#35770;&#65292;&#29992;&#20110;&#20272;&#35745;&#26041;&#31243;&#30340;&#28508;&#22312;&#38543;&#26426;&#39537;&#21160;&#22240;&#32032;&#30340;&#36830;&#32493;&#20108;&#27425;&#21327;&#21464;&#37327;&#65292;&#32780;&#19981;&#26159;&#21487;&#35266;&#23519;&#35299;&#36807;&#31243;&#30340;&#38745;&#24577;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#24369;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#21487;&#36776;&#35782;&#24615;&#30340;&#32467;&#26524;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35013;&#28385;&#28176;&#36817;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#25552;&#20379;&#20102;&#38271;&#26102;&#38388;&#28176;&#36817;&#24615;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#20197;&#20272;&#35745;&#28508;&#22312;&#39537;&#21160;&#22240;&#32032;&#30340;&#38745;&#24577;&#21327;&#21464;&#37327;&#12290;&#24212;&#29992;&#20110;&#21033;&#29575;&#32467;&#26500;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#19982;&#21327;&#21464;&#37327;&#30340;&#32553;&#25918;&#26497;&#38480;&#30340;&#26681;&#26412;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article addresses the robust measurement of covariations in the context of solutions to stochastic evolution equations in Hilbert spaces using functional data analysis. For such equations, standard techniques for functional data based on cross-sectional covariances are often inadequate for identifying statistically relevant random drivers and detecting outliers since they overlook the interplay between cross-sectional and temporal structures. Therefore, we develop an estimation theory for the continuous quadratic covariation of the latent random driver of the equation instead of a static covariance of the observable solution process. We derive identifiability results under weak conditions, establish rates of convergence and a central limit theorem based on infill asymptotics, and provide long-time asymptotics for estimation of a static covariation of the latent driver. Applied to term structure data, our approach uncovers a fundamental alignment with scaling limits of covariations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25193;&#23637;&#30340;Gibbs&#20808;&#39564;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20271;&#24681;&#26031;&#22374;-&#20911;&#183;&#23494;&#26031;&#23450;&#29702;&#22312;&#27969;&#24418;&#19978;&#30340;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12882</link><description>&lt;p&gt;
&#24102;&#26377;&#24207;&#21015;Gibbs&#20808;&#39564;&#30340;&#36129;&#29486;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequential Gibbs Posteriors with Applications to Principal Component Analysis. (arXiv:2310.12882v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25193;&#23637;&#30340;Gibbs&#20808;&#39564;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20271;&#24681;&#26031;&#22374;-&#20911;&#183;&#23494;&#26031;&#23450;&#29702;&#22312;&#27969;&#24418;&#19978;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gibbs&#20808;&#39564;&#19982;&#20808;&#39564;&#20998;&#24067;&#20056;&#20197;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#25104;&#27604;&#20363;&#65292;&#20854;&#20013;&#20851;&#38190;&#35843;&#25972;&#21442;&#25968;&#22312;&#25439;&#22833;&#19982;&#20808;&#39564;&#20013;&#26435;&#37325;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#25511;&#21046;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;Gibbs&#20808;&#39564;&#20026;&#26080;&#20284;&#28982;&#36125;&#21494;&#26031;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#20294;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21333;&#19968;&#35843;&#25972;&#21442;&#25968;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#25193;&#23637;&#30340;Gibbs&#20808;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24207;&#21015;&#21518;&#39564;&#23637;&#31034;&#20102;&#38598;&#20013;&#24615;&#21644;&#19968;&#20010;&#20271;&#24681;&#26031;&#22374;-&#20911;&#183;&#23494;&#26031;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#27969;&#24418;&#19978;&#26131;&#20110;&#39564;&#35777;&#30340;&#26465;&#20214;&#19979;&#25104;&#31435;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20256;&#32479;&#22522;&#20110;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#22312;&#27969;&#24418;&#19978;&#30340;&#31532;&#19968;&#20010;&#20271;&#24681;&#26031;&#22374;-&#20911;&#183;&#23494;&#26031;&#23450;&#29702;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#26377;&#31034;&#20363;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gibbs posteriors are proportional to a prior distribution multiplied by an exponentiated loss function, with a key tuning parameter weighting information in the loss relative to the prior and providing a control of posterior uncertainty. Gibbs posteriors provide a principled framework for likelihood-free Bayesian inference, but in many situations, including a single tuning parameter inevitably leads to poor uncertainty quantification. In particular, regardless of the value of the parameter, credible regions have far from the nominal frequentist coverage even in large samples. We propose a sequential extension to Gibbs posteriors to address this problem. We prove the proposed sequential posterior exhibits concentration and a Bernstein-von Mises theorem, which holds under easy to verify conditions in Euclidean space and on manifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem for traditional likelihood-based Bayesian posteriors on manifolds. All methods are illustrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#38754;&#26495;&#25968;&#25454;&#27169;&#22411;&#20013;&#20272;&#35745;&#22266;&#23450;&#25928;&#24212;&#30340;&#26368;&#20248;&#32553;&#23567;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#33021;&#22815;&#20805;&#20998;&#22320;&#21033;&#29992;&#24207;&#21015;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#39044;&#27979;&#26410;&#26469;&#22266;&#23450;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12485</link><description>&lt;p&gt;
&#32447;&#24615;&#38754;&#26495;&#25968;&#25454;&#27169;&#22411;&#20013;&#22266;&#23450;&#25928;&#24212;&#26368;&#20248;&#32553;&#23567;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models. (arXiv:2308.12485v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#38754;&#26495;&#25968;&#25454;&#27169;&#22411;&#20013;&#20272;&#35745;&#22266;&#23450;&#25928;&#24212;&#30340;&#26368;&#20248;&#32553;&#23567;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#33021;&#22815;&#20805;&#20998;&#22320;&#21033;&#29992;&#24207;&#21015;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#39044;&#27979;&#26410;&#26469;&#22266;&#23450;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#23567;&#20272;&#35745;&#26041;&#27861;&#32463;&#24120;&#34987;&#29992;&#20110;&#20272;&#35745;&#22266;&#23450;&#25928;&#24212;&#65292;&#20197;&#20943;&#23569;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#32553;&#23567;&#20272;&#35745;&#20165;&#22312;&#24378;&#20998;&#24067;&#20551;&#35774;&#19979;&#25165;&#33021;&#20445;&#35777;&#38477;&#20302;&#22122;&#22768;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20272;&#35745;&#22266;&#23450;&#25928;&#24212;&#30340;&#20272;&#35745;&#22120;&#65292;&#22312;&#32553;&#23567;&#20272;&#35745;&#22120;&#31867;&#21035;&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#35813;&#31867;&#21035;&#21253;&#25324;&#20256;&#32479;&#30340;&#32553;&#23567;&#20272;&#35745;&#22120;&#65292;&#19988;&#26368;&#20248;&#24615;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#12290;&#35813;&#20272;&#35745;&#22120;&#20855;&#26377;&#30452;&#35266;&#30340;&#24418;&#24335;&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#22266;&#23450;&#25928;&#24212;&#20801;&#35768;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#20855;&#26377;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#32780;&#32553;&#23567;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#26368;&#20248;&#22320;&#32467;&#21512;&#24213;&#23618;&#30456;&#20851;&#32467;&#26500;&#12290;&#22312;&#36825;&#26679;&#30340;&#32972;&#26223;&#19979;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#39044;&#27979;&#26410;&#26469;&#19968;&#20010;&#26102;&#26399;&#22266;&#23450;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shrinkage methods are frequently used to estimate fixed effects to reduce the noisiness of the least square estimators. However, widely used shrinkage estimators guarantee such noise reduction only under strong distributional assumptions. I develop an estimator for the fixed effects that obtains the best possible mean squared error within a class of shrinkage estimators. This class includes conventional shrinkage estimators and the optimality does not require distributional assumptions. The estimator has an intuitive form and is easy to implement. Moreover, the fixed effects are allowed to vary with time and to be serially correlated, and the shrinkage optimally incorporates the underlying correlation structure in this case. In such a context, I also provide a method to forecast fixed effects one period ahead.
&lt;/p&gt;</description></item></channel></rss>