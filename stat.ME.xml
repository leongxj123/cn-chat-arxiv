<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30693;&#36947;&#28304;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23558;&#26679;&#26412;&#20174;&#28304;&#27169;&#22411;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#24402;&#32422;&#26041;&#27861;&#12290;&#36825;&#20123;&#24402;&#32422;&#26041;&#27861;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.07717</link><description>&lt;p&gt;
&#19968;&#20123;&#32479;&#35745;&#27169;&#22411;&#20043;&#38388;&#30340;&#39640;&#25928;&#24402;&#32422;
&lt;/p&gt;
&lt;p&gt;
Efficient reductions between some statistical models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30693;&#36947;&#28304;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23558;&#26679;&#26412;&#20174;&#28304;&#27169;&#22411;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#24402;&#32422;&#26041;&#27861;&#12290;&#36825;&#20123;&#24402;&#32422;&#26041;&#27861;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#30693;&#36947;&#28304;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#22320;&#23558;&#26469;&#33258;&#28304;&#32479;&#35745;&#27169;&#22411;&#30340;&#26679;&#26412;&#36716;&#25442;&#20026;&#30446;&#26631;&#32479;&#35745;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#36825;&#31181;&#32479;&#35745;&#23454;&#39564;&#20043;&#38388;&#30340;&#24402;&#32422;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31243;&#24207;&#65292;&#21487;&#20197;&#36817;&#20284;&#23558;&#22343;&#21248;&#20998;&#24067;&#12289;Erlang&#20998;&#24067;&#21644;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;&#20301;&#32622;&#27169;&#22411;&#24402;&#32422;&#21040;&#19968;&#33324;&#30340;&#30446;&#26631;&#26063;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20123;&#32463;&#20856;&#30340;&#39640;&#32500;&#38382;&#39064;&#20043;&#38388;&#30340;&#38750;&#28176;&#36817;&#24402;&#32422;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24402;&#32422;&#20445;&#25345;&#20102;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.
&lt;/p&gt;</description></item></channel></rss>