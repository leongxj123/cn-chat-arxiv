<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.14048</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#65306;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#38598;&#25104;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy. (arXiv:2308.14048v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#20135;&#29983;&#19982;&#30495;&#23454;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#26368;&#20026;&#37325;&#35201;&#19988;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#12290;GAN&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;VAE&#21017;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;GAN&#24573;&#35270;&#20102;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#36755;&#20986;&#31354;&#38388;&#65292;&#36825;&#23548;&#33268;&#19981;&#33021;&#23436;&#20840;&#20307;&#29616;&#30446;&#26631;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;VAE&#21017;&#24120;&#24120;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#23558;GAN&#21644;VAE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#21516;&#26102;&#20351;&#29992;&#20102;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#24182;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have emerged as a promising technique for producing high-quality images that are indistinguishable from real images. Generative adversarial networks (GANs) and variational autoencoders (VAEs) are two of the most prominent and widely studied generative models. GANs have demonstrated excellent performance in generating sharp realistic images and VAEs have shown strong abilities to generate diverse images. However, GANs suffer from ignoring a large portion of the possible output space which does not represent the full diversity of the target distribution, and VAEs tend to produce blurry images. To fully capitalize on the strengths of both models while mitigating their weaknesses, we employ a Bayesian non-parametric (BNP) approach to merge GANs and VAEs. Our procedure incorporates both Wasserstein and maximum mean discrepancy (MMD) measures in the loss function to enable effective learning of the latent space and generate diverse and high-quality samples. By fusing the di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16048</link><description>&lt;p&gt;
&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#32467;&#26500;&#38480;&#21046;: &#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Structural restrictions in local causal discovery: identifying direct causes of a target variable. (arXiv:2307.16048v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65292;&#36890;&#36807;&#19981;&#23545;&#20854;&#20182;&#21464;&#37327;&#20570;&#22826;&#22810;&#20551;&#35774;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#21644;&#20004;&#31181;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#35266;&#23519;&#32852;&#21512;&#20998;&#24067;&#20013;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#19968;&#32452;&#30452;&#25509;&#21407;&#22240;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#34920;&#31034;&#22240;&#26524;&#32467;&#26500;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24403;&#23436;&#25972;&#30340;DAG&#20174;&#20998;&#24067;&#20013;&#21487;&#35782;&#21035;&#26102;&#65292;&#24050;&#30693;&#26377;&#19968;&#20123;&#32467;&#26524;&#65292;&#20363;&#22914;&#20551;&#35774;&#38750;&#32447;&#24615;&#39640;&#26031;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#21482;&#23545;&#35782;&#21035;&#19968;&#20010;&#30446;&#26631;&#21464;&#37327;&#30340;&#30452;&#25509;&#21407;&#22240;&#65288;&#23616;&#37096;&#22240;&#26524;&#32467;&#26500;&#65289;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;DAG&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#30446;&#26631;&#21464;&#37327;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#19979;&#30452;&#25509;&#21407;&#22240;&#38598;&#21512;&#21487;&#20197;&#20174;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#26469;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23545;&#38500;&#30446;&#26631;&#21464;&#37327;&#20043;&#22806;&#30340;&#21464;&#37327;&#22522;&#26412;&#19978;&#27809;&#26377;&#20219;&#20309;&#20551;&#35774;&#12290;&#38500;&#20102;&#26032;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#31181;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20272;&#35745;&#30452;&#25509;&#21407;&#22240;&#30340;&#23454;&#29992;&#31639;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Often, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. In this paper, we discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark dataset
&lt;/p&gt;</description></item></channel></rss>