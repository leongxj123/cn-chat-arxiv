<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#25968;&#25454;&#27169;&#22411;&#30340;&#35823;&#35774;&#65292;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20998;&#31867;&#21709;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11954</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#40065;&#26834;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust Estimation and Inference in Categorical Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#25968;&#25454;&#27169;&#22411;&#30340;&#35823;&#35774;&#65292;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20998;&#31867;&#21709;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#35777;&#31185;&#23398;&#20013;&#65292;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#26159;&#20998;&#31867;&#30340;&#12290;&#19982;&#20219;&#20309;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#20110;&#20998;&#31867;&#21709;&#24212;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#35823;&#35774;&#65292;&#23548;&#33268;&#20272;&#35745;&#21487;&#33021;&#23384;&#22312;&#36739;&#22823;&#20559;&#24046;&#12290;&#19968;&#20010;&#29305;&#21035;&#40635;&#28902;&#30340;&#35823;&#35774;&#26469;&#28304;&#26159;&#22312;&#38382;&#21367;&#35843;&#26597;&#20013;&#30340;&#30095;&#24573;&#21709;&#24212;&#65292;&#20247;&#25152;&#21608;&#30693;&#36825;&#20250;&#21361;&#21450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEM&#65289;&#21644;&#20854;&#20182;&#22522;&#20110;&#35843;&#26597;&#30340;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#20998;&#31867;&#21709;&#24212;&#27169;&#22411;&#30340;&#35823;&#35774;&#40065;&#26834;&#30340;&#36890;&#29992;&#20272;&#35745;&#22120;&#12290;&#19982;&#36804;&#20170;&#20026;&#27490;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#20272;&#35745;&#22120;&#23545;&#20998;&#31867;&#21709;&#24212;&#27169;&#22411;&#30340;&#35823;&#35774;&#31243;&#24230;&#12289;&#22823;&#23567;&#25110;&#31867;&#22411;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#25512;&#24191;&#20102;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#26159;&#24378;&#19968;&#33268;&#30340;&#65292;&#28176;&#36817;&#39640;&#26031;&#30340;&#65292;&#20855;&#26377;&#19982;&#26497;&#22823;&#20284;&#28982;&#30456;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20998;&#31867;&#21709;&#24212;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26816;&#39564;&#65292;&#29992;&#20110;&#27979;&#35797;&#19968;&#20010;&#32473;&#23450;&#21709;&#24212;&#26159;&#21542; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11954v1 Announce Type: cross  Abstract: In empirical science, many variables of interest are categorical. Like any model, models for categorical responses can be misspecified, leading to possibly large biases in estimation. One particularly troublesome source of misspecification is inattentive responding in questionnaires, which is well-known to jeopardize the validity of structural equation models (SEMs) and other survey-based analyses. I propose a general estimator that is designed to be robust to misspecification of models for categorical responses. Unlike hitherto approaches, the estimator makes no assumption whatsoever on the degree, magnitude, or type of misspecification. The proposed estimator generalizes maximum likelihood estimation, is strongly consistent, asymptotically Gaussian, has the same time complexity as maximum likelihood, and can be applied to any model for categorical responses. In addition, I develop a novel test that tests whether a given response can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#39532;&#27663;&#36317;&#31163;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26925;&#22278;&#24418;&#20998;&#24067;&#30340;&#31454;&#20105;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#27969;&#34892;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08283</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#39532;&#27663;&#36317;&#31163;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classification Using Global and Local Mahalanobis Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#39532;&#27663;&#36317;&#31163;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26925;&#22278;&#24418;&#20998;&#24067;&#30340;&#31454;&#20105;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#27969;&#34892;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#35266;&#23519;&#20540;&#30340;&#39532;&#27663;&#36317;&#31163;&#30340;&#26032;&#22411;&#21322;&#21442;&#25968;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#26159;&#19968;&#20010;&#20855;&#26377;&#36923;&#36753;&#38142;&#25509;&#20989;&#25968;&#30340;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#20316;&#20026;&#29305;&#24449;&#26469;&#20272;&#35745;&#19981;&#21516;&#31867;&#21035;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#23613;&#31649;&#27969;&#34892;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#22914;&#32447;&#24615;&#21644;&#20108;&#27425;&#21028;&#21035;&#20998;&#26512;&#20027;&#35201;&#22522;&#20110;&#22522;&#30784;&#20998;&#24067;&#30340;&#27491;&#24577;&#24615;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#22120;&#26356;&#21152;&#28789;&#27963;&#65292;&#19981;&#21463;&#27492;&#31867;&#21442;&#25968;&#21270;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#26925;&#22278;&#20998;&#24067;&#30340;&#23494;&#24230;&#26159;&#39532;&#27663;&#36317;&#31163;&#30340;&#20989;&#25968;&#65292;&#24403;&#31454;&#20105;&#31867;&#21035;&#26159;&#65288;&#20960;&#20046;&#65289;&#26925;&#22278;&#24418;&#26102;&#65292;&#35813;&#20998;&#31867;&#22120;&#30340;&#25928;&#26524;&#24456;&#22909;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#32463;&#24120;&#32988;&#36807;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#65292;&#29305;&#21035;&#26159;&#24403;&#26679;&#26412;&#37327;&#30456;&#23545;&#20110;&#25968;&#25454;&#32500;&#25968;&#36739;&#23567;&#26102;&#12290;&#20026;&#20102;&#24212;&#23545;&#38750;&#26925;&#22278;&#21644;&#21487;&#33021;&#22810;&#23792;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39532;&#27663;&#36317;&#31163;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
We propose a novel semi-parametric classifier based on Mahalanobis distances of an observation from the competing classes. Our tool is a generalized additive model with the logistic link function that uses these distances as features to estimate the posterior probabilities of the different classes. While popular parametric classifiers like linear and quadratic discriminant analyses are mainly motivated by the normality of the underlying distributions, the proposed classifier is more flexible and free from such parametric assumptions. Since the densities of elliptic distributions are functions of Mahalanobis distances, this classifier works well when the competing classes are (nearly) elliptic. In such cases, it often outperforms popular nonparametric classifiers, especially when the sample size is small compared to the dimension of the data. To cope with non-elliptic and possibly multimodal distributions, we propose a local version of the Mahalanobis distance. Subsequently, we propose 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26679;&#30340;&#31232;&#30095;&#24773;&#20917;&#19979;&#20855;&#26377;&#23614;&#37096;&#33258;&#36866;&#24212;&#25910;&#32553;&#29305;&#24615;&#30340;&#40065;&#26834;&#31232;&#30095;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#20840;&#23616;-&#23616;&#37096;-&#23614;&#37096;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23454;&#29616;&#65292;&#33021;&#22815;&#26681;&#25454;&#31232;&#30095;&#31243;&#24230;&#33258;&#36866;&#24212;&#35843;&#25972;&#20808;&#39564;&#30340;&#23614;&#37096;&#37325;&#37327;&#20197;&#36866;&#24212;&#26356;&#22810;&#25110;&#26356;&#23569;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2007.02192</link><description>&lt;p&gt;
&#23614;&#37096;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#25910;&#32553;
&lt;/p&gt;
&lt;p&gt;
Tail-adaptive Bayesian shrinkage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.02192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26679;&#30340;&#31232;&#30095;&#24773;&#20917;&#19979;&#20855;&#26377;&#23614;&#37096;&#33258;&#36866;&#24212;&#25910;&#32553;&#29305;&#24615;&#30340;&#40065;&#26834;&#31232;&#30095;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#20840;&#23616;-&#23616;&#37096;-&#23614;&#37096;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#23454;&#29616;&#65292;&#33021;&#22815;&#26681;&#25454;&#31232;&#30095;&#31243;&#24230;&#33258;&#36866;&#24212;&#35843;&#25972;&#20808;&#39564;&#30340;&#23614;&#37096;&#37325;&#37327;&#20197;&#36866;&#24212;&#26356;&#22810;&#25110;&#26356;&#23569;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#19979;&#22810;&#26679;&#30340;&#31232;&#30095;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#25910;&#32553;&#20808;&#39564;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22312;&#25152;&#35859;&#30340;&#36229;&#31232;&#30095;&#39046;&#22495;&#20174;&#25104;&#21315;&#19978;&#19975;&#20010;&#39044;&#27979;&#21464;&#37327;&#20013;&#26816;&#27979;&#23569;&#25968;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#24403;&#31232;&#30095;&#31243;&#24230;&#36866;&#20013;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34920;&#29616;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#26679;&#31232;&#30095;&#24773;&#20917;&#19979;&#20855;&#26377;&#23614;&#37096;&#33258;&#36866;&#24212;&#25910;&#32553;&#29305;&#24615;&#30340;&#40065;&#26834;&#31232;&#30095;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#29305;&#24615;&#20013;&#65292;&#20808;&#39564;&#30340;&#23614;&#37096;&#37325;&#37327;&#20250;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#38543;&#30528;&#31232;&#30095;&#27700;&#24179;&#30340;&#22686;&#21152;&#25110;&#20943;&#23569;&#21464;&#24471;&#26356;&#22823;&#25110;&#26356;&#23567;&#65292;&#20197;&#36866;&#24212;&#20808;&#39564;&#22320;&#26356;&#22810;&#25110;&#26356;&#23569;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23616;&#37096;&#23614;&#37096;&#65288;GLT&#65289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20197;&#30830;&#20445;&#36825;&#31181;&#23646;&#24615;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#20808;&#39564;&#30340;&#23614;&#37096;&#25351;&#25968;&#19982;&#22522;&#30784;&#31232;&#30095;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;GLT&#21518;&#39564;&#20250;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.02192v4 Announce Type: replace-cross  Abstract: Robust Bayesian methods for high-dimensional regression problems under diverse sparse regimes are studied. Traditional shrinkage priors are primarily designed to detect a handful of signals from tens of thousands of predictors in the so-called ultra-sparsity domain. However, they may not perform desirably when the degree of sparsity is moderate. In this paper, we propose a robust sparse estimation method under diverse sparsity regimes, which has a tail-adaptive shrinkage property. In this property, the tail-heaviness of the prior adjusts adaptively, becoming larger or smaller as the sparsity level increases or decreases, respectively, to accommodate more or fewer signals, a posteriori. We propose a global-local-tail (GLT) Gaussian mixture distribution that ensures this property. We examine the role of the tail-index of the prior in relation to the underlying sparsity level and demonstrate that the GLT posterior contracts at the
&lt;/p&gt;</description></item></channel></rss>