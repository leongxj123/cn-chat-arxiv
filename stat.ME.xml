<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04341</link><description>&lt;p&gt;
&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;&#36951;&#25022;&#20998;&#24067;&#65306;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk. (arXiv:2304.04341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23436;&#20840;&#21051;&#30011;&#20102;&#31574;&#30053;&#35774;&#35745;&#20013;&#19977;&#20010;&#26399;&#26395;&#24615;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#24615;&#65292;&#23454;&#20363;&#30456;&#20851;&#30340;&#19968;&#33268;&#24615;&#21644;&#36731;&#23614;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#22914;&#20309;&#24433;&#21709;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#30340;&#34928;&#20943;&#29575;&#65292;&#21516;&#26102;&#21253;&#25324;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20197;&#34920;&#24449;&#23545;&#20110;&#20219;&#20309;&#36951;&#25022;&#38408;&#20540;&#30340;&#26368;&#20248;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;$\alpha \in [1/2, 1)$&#21644;$\beta \in [0, \alpha]$&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#24179;&#22343;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\alpha)$&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;$\alpha$-&#26368;&#20248;&#21644;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\beta)$&#30340;&#23454;&#20363;&#30456;&#20851;&#30340;$\beta$-&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20139;&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#21487;&#20197;&#36991;&#20813;$\tilde O(T^\delta)$&#30340;&#36951;&#25022;($\delta \geq \alpha$&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21644;$\delta \geq \beta$&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;)&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit problem. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to characterize the optimal regret tail probability for any regret threshold. Concretely, for any given $\alpha\in[1/2, 1)$ and $\beta\in[0, \alpha]$, our policy achieves a worst-case expected regret of $\tilde O(T^\alpha)$ (we call it $\alpha$-optimal) and an instance-dependent expected regret of $\tilde O(T^\beta)$ (we call it $\beta$-consistent), while enjoys a probability of incurring an $\tilde O(T^\delta)$ regret ($\delta\geq\alpha$ in the worst-case scenario and $\delta\geq\beta$ in the instance-dependent s
&lt;/p&gt;</description></item></channel></rss>