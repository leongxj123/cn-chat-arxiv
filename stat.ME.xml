<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#26465;&#20214;&#20934;&#38543;&#26426;&#21270;&#26816;&#39564;&#26469;&#35299;&#20915;&#32593;&#32476;&#20013;&#24178;&#25200;&#23384;&#22312;&#26102;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#38646;&#20551;&#35774;&#22312;&#21463;&#38480;&#20154;&#21475;&#19978;&#25104;&#20026;&#23574;&#38160;&#12290;</title><link>https://arxiv.org/abs/2403.16673</link><description>&lt;p&gt;
&#32593;&#32476;&#24178;&#25200;&#30340;&#20934;&#38543;&#26426;&#21270;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Quasi-randomization tests for network interference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16673
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26465;&#20214;&#20934;&#38543;&#26426;&#21270;&#26816;&#39564;&#26469;&#35299;&#20915;&#32593;&#32476;&#20013;&#24178;&#25200;&#23384;&#22312;&#26102;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#38646;&#20551;&#35774;&#22312;&#21463;&#38480;&#20154;&#21475;&#19978;&#25104;&#20026;&#23574;&#38160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32463;&#20856;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#20154;&#21475;&#21333;&#20301;&#20043;&#38388;&#23384;&#22312;&#24178;&#25200;&#26102;&#22833;&#25928;&#12290;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#21333;&#20301;&#30340;&#22788;&#29702;&#29366;&#24577;&#20250;&#24433;&#21709;&#20154;&#21475;&#20013;&#20854;&#20182;&#21333;&#20301;&#30340;&#28508;&#22312;&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#27979;&#35797;&#36825;&#31181;&#24433;&#21709;&#30340;&#38646;&#20551;&#35774;&#20250;&#20351;&#38646;&#20551;&#35774;&#38750;&#23574;&#38160;&#12290;&#35299;&#20915;&#36825;&#31181;&#35774;&#32622;&#20013;&#38646;&#20551;&#35774;&#38750;&#23574;&#38160;&#24615;&#30340;&#19968;&#20010;&#26377;&#36259;&#26041;&#27861;&#26159;&#26500;&#24314;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#65292;&#20351;&#24471;&#38646;&#20551;&#35774;&#22312;&#21463;&#38480;&#20154;&#21475;&#19978;&#26159;&#23574;&#38160;&#30340;&#12290;&#22312;&#38543;&#26426;&#23454;&#39564;&#20013;&#65292;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#24102;&#26469;&#35745;&#31639;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23454;&#39564;&#35774;&#35745;&#25214;&#21040;&#36825;&#20123;&#36866;&#24403;&#30340;&#23376;&#20154;&#21475;&#21487;&#33021;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;NP&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#21475;&#20043;&#38388;&#30340;&#32593;&#32476;&#35270;&#20026;&#19968;&#20010;&#38543;&#26426;&#21464;&#37327;&#32780;&#19981;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#26465;&#20214;&#20934;&#38543;&#26426;&#21270;&#26816;&#39564;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16673v1 Announce Type: cross  Abstract: Many classical inferential approaches fail to hold when interference exists among the population units. This amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for such spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity. Such approaches can pose computational challenges as finding these appropriate sub-populations based on experimental design can involve solving an NP-hard problem. In this paper, we view the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. Our main ide
&lt;/p&gt;</description></item><item><title>&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07307</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07307
&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20855;&#26377;&#30456;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#37319;&#21462;&#30456;&#21516;&#30340;&#34892;&#21160;&#12290;&#31526;&#21512;&#39044;&#27979;&#24110;&#21161;&#20915;&#31574;&#32773;&#37327;&#21270;&#21160;&#20316;&#30340;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#21463;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#23427;&#20135;&#29983;&#20102;&#26082;&#31526;&#21512;Venn-Abers&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#24341;&#21457;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21518;&#39564;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#22120;&#65292;&#25552;&#20379;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#38388;&#30340;&#25928;&#29575;&#21644;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04952</link><description>&lt;p&gt;
&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#22270;&#65292;&#35813;&#22270;&#32534;&#30721;&#20102;&#29983;&#25104;&#25968;&#25454;&#36807;&#31243;&#30340;&#22240;&#26524;&#22270;&#30340;&#22270;&#24418;&#20998;&#31163;&#21644;&#36830;&#25509;&#38472;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#24212;&#35813;&#21253;&#25324;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#65292;&#20197;&#34913;&#37327;&#36825;&#19968;&#26126;&#30830;&#30446;&#26631;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#20004;&#20010;&#22240;&#26524;&#22270;&#30340;&#20998;&#31163;/&#36830;&#25509;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29609;&#20855;&#31034;&#20363;&#12289;&#23454;&#35777;&#23454;&#39564;&#21644;&#20266;&#20195;&#30721;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process. In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth. We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming. We complement our theoretical analysis with toy examples, empirical experiments and pseudocode.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#20462;&#27491;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27491;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#24773;&#20917;&#19979;&#20248;&#20110;Bonferroni&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#65292;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2311.10900</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24378;&#22823;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A powerful rank-based correction to multiple testing under positive dependency. (arXiv:2311.10900v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10900
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#20462;&#27491;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27491;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#24773;&#20917;&#19979;&#20248;&#20110;Bonferroni&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#65292;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#21487;&#33021;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#27491;&#30456;&#20851;&#24615;&#30340;&#23478;&#26063;&#35823;&#24046;&#29575;(FWER)&#25511;&#21046;&#30340;&#26032;&#22411;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#20462;&#27491;&#31639;&#27861;$\texttt{max-rank}$&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27010;&#24565;&#19978;&#24456;&#30452;&#35266;&#65292;&#20381;&#36182;&#20110;&#22312;&#35745;&#31639;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#31209;&#22495;&#20351;&#29992;$\max$&#31639;&#23376;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#32463;&#24120;&#20351;&#29992;&#30340;Bonferroni&#20462;&#27491;&#65292;&#32780;&#22312;&#19981;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#31561;&#25928;&#12290;&#25105;&#20204;&#30340;&#20248;&#21183;&#38543;&#30528;&#27979;&#35797;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#21516;&#26102;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#30340;&#32972;&#26223;&#20013;&#65292;&#36825;&#26159;&#22312;&#25105;&#20204;&#20027;&#35201;&#24212;&#29992;&#30340;&#19968;&#31181;&#22797;&#26434;&#39044;&#27979;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel multiple hypothesis testing correction with family-wise error rate (FWER) control that efficiently exploits positive dependencies between potentially correlated statistical hypothesis tests. Our proposed algorithm $\texttt{max-rank}$ is conceptually straight-forward, relying on the use of a $\max$-operator in the rank domain of computed test statistics. We compare our approach to the frequently employed Bonferroni correction, theoretically and empirically demonstrating its superiority over Bonferroni in the case of existing positive dependency, and its equivalence otherwise. Our advantage over Bonferroni increases as the number of tests rises, and we maintain high statistical power whilst ensuring FWER control. We specifically frame our algorithm in the context of parallel permutation testing, a scenario that arises in our primary application of conformal prediction, a recently popularized approach for quantifying uncertainty in complex predictive settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item></channel></rss>