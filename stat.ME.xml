<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item></channel></rss>