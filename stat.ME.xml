<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#22312;&#22238;&#24402;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;Mondrian&#38543;&#26426;&#26862;&#26519;&#30340;&#20272;&#35745;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#21435;&#20559;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#21644;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#20272;&#35745;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09702</link><description>&lt;p&gt;
&#24102;&#26377;Mondrian&#38543;&#26426;&#26862;&#26519;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inference with Mondrian Random Forests. (arXiv:2310.09702v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22238;&#24402;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;Mondrian&#38543;&#26426;&#26862;&#26519;&#30340;&#20272;&#35745;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#21435;&#20559;&#36807;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#21644;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#20272;&#35745;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#21464;&#20307;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#20363;&#23376;&#26159;Mondrian&#38543;&#26426;&#26862;&#26519;&#65292;&#20854;&#20013;&#24213;&#23618;&#26641;&#26159;&#26681;&#25454;Mondrian&#36807;&#31243;&#26500;&#24314;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;Mondrian&#38543;&#26426;&#26862;&#26519;&#22312;&#22238;&#24402;&#35774;&#32622;&#19979;&#30340;&#20272;&#35745;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#24403;&#19982;&#20559;&#24046;&#34920;&#24449;&#21644;&#19968;&#33268;&#26041;&#24046;&#20272;&#35745;&#22120;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#20801;&#35768;&#36827;&#34892;&#28176;&#36817;&#26377;&#25928;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#22914;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#23545;&#26410;&#30693;&#30340;&#22238;&#24402;&#20989;&#25968;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21435;&#20559;&#36807;&#31243;&#65292;&#29992;&#20110;Mondrian&#38543;&#26426;&#26862;&#26519;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#36866;&#24403;&#30340;&#21442;&#25968;&#35843;&#25972;&#19979;&#23454;&#29616;$\beta$-H\"older&#22238;&#24402;&#20989;&#25968;&#30340;&#26368;&#23567;&#26497;&#22823;&#20272;&#35745;&#36895;&#29575;&#65292;&#23545;&#20110;&#25152;&#26377;&#30340;$\beta$&#21644;&#20219;&#24847;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forests are popular methods for classification and regression, and many different variants have been proposed in recent years. One interesting example is the Mondrian random forest, in which the underlying trees are constructed according to a Mondrian process. In this paper we give a central limit theorem for the estimates made by a Mondrian random forest in the regression setting. When combined with a bias characterization and a consistent variance estimator, this allows one to perform asymptotically valid statistical inference, such as constructing confidence intervals, on the unknown regression function. We also provide a debiasing procedure for Mondrian random forests which allows them to achieve minimax-optimal estimation rates with $\beta$-H\"older regression functions, for all $\beta$ and in arbitrary dimension, assuming appropriate parameter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.11908</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#28857;&#26641;&#30340;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#24314;&#31435;&#22312;&#20256;&#32479;&#38543;&#26426;&#26862;&#26519;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#26680;&#21152;&#26435;&#31639;&#27861;&#26469;&#26500;&#24314;&#20272;&#31639;&#22120;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26641;&#29983;&#38271;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#22522;&#20110;&#23450;&#28857;&#36845;&#20195;&#36817;&#20284;&#34920;&#31034;&#26799;&#24230;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#20026;&#27492;&#24320;&#21457;&#20102;&#28176;&#36817;&#29702;&#35770;&#12290;&#36825;&#26377;&#25928;&#22320;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#37327;&#30340;&#32500;&#24230;&#36866;&#20013;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
&lt;/p&gt;</description></item></channel></rss>