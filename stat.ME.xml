<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#25991;&#22312;&#31751;&#30456;&#20851;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20026;&#38750;&#21442;&#25968;&#26680;&#22238;&#24402;&#27169;&#22411;&#21457;&#23637;&#20102;&#19968;&#33324;&#28176;&#36817;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24102;&#23485;&#36873;&#25321;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#28176;&#36817;&#26041;&#24046;&#30340;&#20272;&#35745;&#37327;&#65292;&#24182;&#39564;&#35777;&#20102;&#38598;&#32676;&#31283;&#20581;&#24102;&#23485;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04766</link><description>&lt;p&gt;
&#38598;&#32676;&#25277;&#26679;&#19979;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Regression under Cluster Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31751;&#30456;&#20851;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20026;&#38750;&#21442;&#25968;&#26680;&#22238;&#24402;&#27169;&#22411;&#21457;&#23637;&#20102;&#19968;&#33324;&#28176;&#36817;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24102;&#23485;&#36873;&#25321;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#28176;&#36817;&#26041;&#24046;&#30340;&#20272;&#35745;&#37327;&#65292;&#24182;&#39564;&#35777;&#20102;&#38598;&#32676;&#31283;&#20581;&#24102;&#23485;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#31751;&#30456;&#20851;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20026;&#38750;&#21442;&#25968;&#26680;&#22238;&#24402;&#27169;&#22411;&#21457;&#23637;&#20102;&#19968;&#33324;&#28176;&#36817;&#29702;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12289;Nadaraya-Watson&#26680;&#22238;&#24402;&#21644;&#23616;&#37096;&#32447;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32771;&#34385;&#20102;&#22686;&#38271;&#21644;&#24322;&#36136;&#30340;&#31751;&#22823;&#23567;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#28176;&#36817;&#26465;&#20214;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#30830;&#31435;&#20102;&#19968;&#33268;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#24322;&#36136;&#30340;&#31751;&#22823;&#23567;&#19979;&#65292;&#28176;&#36817;&#26041;&#24046;&#21253;&#25324;&#19968;&#20010;&#21453;&#26144;&#31751;&#20869;&#30456;&#20851;&#24615;&#30340;&#26032;&#39033;&#65292;&#24403;&#20551;&#23450;&#31751;&#22823;&#23567;&#26377;&#30028;&#26102;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24102;&#23485;&#36873;&#25321;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#28176;&#36817;&#26041;&#24046;&#30340;&#20272;&#35745;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#38598;&#32676;&#31283;&#20581;&#24102;&#23485;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25512;&#23548;&#30340;&#38598;&#32676;&#31283;&#20581;&#32622;&#20449;&#21306;&#38388;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04766v1 Announce Type: new  Abstract: This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence. We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation. Our theory accommodates growing and heterogeneous cluster sizes. We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality. Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded. We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency. In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio. We 
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#24615;&#30340;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#22312;&#36873;&#25321;&#22238;&#24402;&#30697;&#38453;&#21644;&#20551;&#35774;&#26816;&#39564;&#19978;&#23384;&#22312;&#33030;&#24369;&#24615;&#65292;OLS&#33021;&#22815;&#25552;&#20379;&#26356;&#20581;&#22766;&#30340;&#32467;&#26524;&#32780;&#25928;&#29575;&#25439;&#22833;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2311.02299</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Fragility of Sparsity. (arXiv:2311.02299v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02299
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#30340;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#22312;&#36873;&#25321;&#22238;&#24402;&#30697;&#38453;&#21644;&#20551;&#35774;&#26816;&#39564;&#19978;&#23384;&#22312;&#33030;&#24369;&#24615;&#65292;OLS&#33021;&#22815;&#25552;&#20379;&#26356;&#20581;&#22766;&#30340;&#32467;&#26524;&#32780;&#25928;&#29575;&#25439;&#22833;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#23454;&#35777;&#24212;&#29992;&#23637;&#31034;&#20102;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#22312;&#20381;&#36182;&#31232;&#30095;&#24615;&#20551;&#35774;&#26102;&#23384;&#22312;&#20004;&#31181;&#33030;&#24369;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19981;&#24433;&#21709;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;(OLS)&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#22522;&#32447;&#31867;&#21035;&#30340;&#36873;&#25321;&#19982;&#20998;&#31867;&#25511;&#21046;&#30456;&#20851;&#65292;&#21487;&#33021;&#20250;&#20351;&#31232;&#30095;&#24615;&#20272;&#35745;&#20540;&#31227;&#21160;&#36229;&#36807;&#20004;&#20010;&#26631;&#20934;&#35823;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#22522;&#20110;&#23558;&#31232;&#30095;&#24615;&#20272;&#35745;&#19982;OLS&#20272;&#35745;&#36827;&#34892;&#27604;&#36739;&#30340;&#31232;&#30095;&#24615;&#20551;&#35774;&#26816;&#39564;&#12290;&#22312;&#25152;&#26377;&#19977;&#20010;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#26816;&#39564;&#20542;&#21521;&#20110;&#25298;&#32477;&#31232;&#30095;&#24615;&#20551;&#35774;&#12290;&#38500;&#38750;&#33258;&#21464;&#37327;&#30340;&#25968;&#37327;&#19982;&#26679;&#26412;&#37327;&#30456;&#24403;&#25110;&#36229;&#36807;&#26679;&#26412;&#37327;&#65292;&#21542;&#21017;OLS&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#25928;&#29575;&#25439;&#22833;&#20135;&#29983;&#26356;&#20581;&#22766;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show, using three empirical applications, that linear regression estimates which rely on the assumption of sparsity are fragile in two ways. First, we document that different choices of the regressor matrix that do not impact ordinary least squares (OLS) estimates, such as the choice of baseline category with categorical controls, can move sparsity-based estimates two standard errors or more. Second, we develop two tests of the sparsity assumption based on comparing sparsity-based estimators with OLS. The tests tend to reject the sparsity assumption in all three applications. Unless the number of regressors is comparable to or exceeds the sample size, OLS yields more robust results at little efficiency cost.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;CNN&#23398;&#20064;&#31354;&#38388;&#36807;&#31243;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#30830;&#20999;&#20284;&#28982;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#23398;&#20064;&#20284;&#28982;&#20989;&#25968;&#12290;&#20351;&#29992;Platt&#32553;&#25918;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#20284;&#28982;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04634</link><description>&lt;p&gt;
&#31354;&#38388;&#36807;&#31243;&#30340;&#31070;&#32463;&#20284;&#28982;&#38754;
&lt;/p&gt;
&lt;p&gt;
Neural Likelihood Surfaces for Spatial Processes with Computationally Intensive or Intractable Likelihoods. (arXiv:2305.04634v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04634
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;CNN&#23398;&#20064;&#31354;&#38388;&#36807;&#31243;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#30830;&#20999;&#20284;&#28982;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#23398;&#20064;&#20284;&#28982;&#20989;&#25968;&#12290;&#20351;&#29992;Platt&#32553;&#25918;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#20284;&#28982;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31354;&#38388;&#32479;&#35745;&#20013;&#65292;&#24403;&#25311;&#21512;&#31354;&#38388;&#36807;&#31243;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#65292;&#24555;&#36895;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25163;&#27573;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20284;&#28982;&#20989;&#25968;&#21487;&#33021;&#35780;&#20272;&#32531;&#24930;&#25110;&#38590;&#20197;&#22788;&#29702;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23398;&#20064;&#31354;&#38388;&#36807;&#31243;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#36890;&#36807;&#29305;&#23450;&#35774;&#35745;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#22320;&#23398;&#20064;&#20284;&#28982;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26174;&#24335;&#21487;&#29992;&#30340;&#30830;&#20999;&#20284;&#28982;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;&#19968;&#26086;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;Platt&#32553;&#25918;&#36827;&#34892;&#26657;&#20934;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#20284;&#28982;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26469;&#33258;&#31070;&#32463;&#20284;&#28982;&#38754;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#36817;&#20284;&#32622;&#20449;&#21306;&#38388;&#19982;&#20004;&#20010;&#19981;&#21516;&#31354;&#38388;&#36807;&#31243;&#65288;&#39640;&#26031;&#36807;&#31243;&#21644;&#23545;&#25968;&#39640;&#26031;Cox&#36807;&#31243;&#65289;&#30340;&#30456;&#24212;&#31934;&#30830;&#25110;&#36817;&#20284;&#30340;&#20284;&#28982;&#20989;&#25968;&#26500;&#25104;&#30340;&#31561;&#25928;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spatial statistics, fast and accurate parameter estimation coupled with a reliable means of uncertainty quantification can be a challenging task when fitting a spatial process to real-world data because the likelihood function might be slow to evaluate or intractable. In this work, we propose using convolutional neural networks (CNNs) to learn the likelihood function of a spatial process. Through a specifically designed classification task, our neural network implicitly learns the likelihood function, even in situations where the exact likelihood is not explicitly available. Once trained on the classification task, our neural network is calibrated using Platt scaling which improves the accuracy of the neural likelihood surfaces. To demonstrate our approach, we compare maximum likelihood estimates and approximate confidence regions constructed from the neural likelihood surface with the equivalent for exact or approximate likelihood for two different spatial processes: a Gaussian Pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.02022</link><description>&lt;p&gt;
&#22522;&#20110;MNL&#36873;&#25321;&#27169;&#22411;&#30340;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Joint Assortment-Inventory Optimization under MNL Choices. (arXiv:2304.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#39038;&#23458;&#30340;&#36873;&#25321;&#34892;&#20026;&#37117;&#36981;&#24490;Multinomial Logit&#65288;MNL&#65289;&#36873;&#25321;&#27169;&#22411;&#65292;&#21560;&#24341;&#21147;&#21442;&#25968;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#38646;&#21806;&#21830;&#36827;&#34892;&#21608;&#26399;&#24615;&#32452;&#21512;&#21644;&#24211;&#23384;&#20915;&#31574;&#65292;&#20197;&#21160;&#24577;&#22320;&#20174;&#23454;&#29616;&#30340;&#38656;&#27714;&#20013;&#23398;&#20064;&#21560;&#24341;&#21147;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#19978;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#24635;&#21033;&#28070;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#34913;&#32452;&#21512;&#21644;&#24211;&#23384;&#22312;&#32447;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;MNL&#21560;&#24341;&#21147;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#26576;&#20123;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#26469;&#28608;&#21169;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#38745;&#24577;&#21333;&#21608;&#26399;&#32452;&#21512;&#24211;&#23384;&#35268;&#21010;&#38382;&#39064;&#30340;&#20248;&#21270;oracle&#22522;&#30784;&#20043;&#19978;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#65292;&#20197;&#21450;&#20851;&#20110;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the realized demands about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance the exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, a novel approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#21327;&#21464;&#37327;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#21644;&#32039;&#20945;&#21442;&#25968;&#25903;&#25345;&#30340;&#33258;&#28982;&#38480;&#21046;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#21327;&#21464;&#37327;&#38382;&#39064;&#12290;&#19982;&#31454;&#20105;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20559;&#24046;&#20943;&#23567;&#21644;&#23610;&#23544;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20272;&#35745;&#27773;&#27833;&#38656;&#27714;&#30340;&#20215;&#26684;&#21644;&#25910;&#20837;&#24377;&#24615;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09255</link><description>&lt;p&gt;
&#32858;&#31867;&#21327;&#21464;&#37327;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Clustered Covariate Regression. (arXiv:2302.09255v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#21327;&#21464;&#37327;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#32858;&#31867;&#21644;&#32039;&#20945;&#21442;&#25968;&#25903;&#25345;&#30340;&#33258;&#28982;&#38480;&#21046;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#21327;&#21464;&#37327;&#38382;&#39064;&#12290;&#19982;&#31454;&#20105;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20559;&#24046;&#20943;&#23567;&#21644;&#23610;&#23544;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20272;&#35745;&#27773;&#27833;&#38656;&#27714;&#30340;&#20215;&#26684;&#21644;&#25910;&#20837;&#24377;&#24615;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20272;&#35745;&#20013;&#21327;&#21464;&#37327;&#32500;&#24230;&#30340;&#39640;&#24230;&#22686;&#21152;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#26080;&#24207;&#24615;&#25110;&#19981;&#21487;&#35266;&#27979;&#21442;&#25968;&#21521;&#37327;&#30340;&#31163;&#25955;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#32463;&#39564;&#32972;&#26223;&#19979;&#65292;&#32463;&#27982;&#29702;&#35770;&#21487;&#33021;&#19981;&#25903;&#25345;&#20219;&#20309;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#20998;&#32452;&#21442;&#25968;&#20272;&#35745;&#22120;&#65288;GPE&#65289;&#25918;&#24323;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#32780;&#36873;&#25321;&#21442;&#25968;&#25903;&#25345;&#26159;&#32039;&#20945;&#30340;&#33258;&#28982;&#38480;&#21046;&#12290;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#65292;GPE&#20855;&#26377;&#31283;&#20581;&#30340;&#22823;&#26679;&#26412;&#24615;&#36136;&#65292;&#24182;&#36866;&#24212;&#20102;&#25903;&#25345;&#21487;&#20197;&#36828;&#31163;&#38646;&#28857;&#30340;&#31232;&#30095;&#21644;&#38750;&#31232;&#30095;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#35777;&#26126;&#20102;&#19982;&#31454;&#20105;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;GPE&#22312;&#20559;&#24046;&#20943;&#23567;&#21644;&#23610;&#23544;&#25511;&#21046;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#23545;&#20110;&#20272;&#35745;&#27773;&#27833;&#38656;&#27714;&#30340;&#20215;&#26684;&#21644;&#25910;&#20837;&#24377;&#24615;&#30340;&#23454;&#35777;&#24212;&#29992;&#31361;&#26174;&#20102;GPE&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High covariate dimensionality is increasingly occurrent in model estimation, and existing techniques to address this issue typically require sparsity or discrete heterogeneity of the unobservable parameter vector. However, neither restriction may be supported by economic theory in some empirical contexts, leading to severe bias and misleading inference. The clustering-based grouped parameter estimator (GPE) introduced in this paper drops both restrictions in favour of the natural one that the parameter support be compact. GPE exhibits robust large sample properties under standard conditions and accommodates both sparse and non-sparse parameters whose support can be bounded away from zero. Extensive Monte Carlo simulations demonstrate the excellent performance of GPE in terms of bias reduction and size control compared to competing estimators. An empirical application of GPE to estimating price and income elasticities of demand for gasoline highlights its practical utility.
&lt;/p&gt;</description></item></channel></rss>