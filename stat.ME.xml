<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#36890;&#36807;&#26368;&#23567;&#21518;&#24724;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#35797;&#39564;&#20013;&#20248;&#21270;&#26679;&#26412;&#36873;&#25321;&#20197;&#23454;&#29616;&#24322;&#36136;&#20154;&#32676;&#20013;&#26368;&#20339;&#31119;&#21033;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.01386</link><description>&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#20013;&#30340;&#26368;&#23567;&#21518;&#24724;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Minimax-Regret Sample Selection in Randomized Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01386
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#21518;&#24724;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#35797;&#39564;&#20013;&#20248;&#21270;&#26679;&#26412;&#36873;&#25321;&#20197;&#23454;&#29616;&#24322;&#36136;&#20154;&#32676;&#20013;&#26368;&#20339;&#31119;&#21033;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#32463;&#24120;&#22312;&#23384;&#22312;&#35768;&#22810;&#21487;&#33021;&#23545;&#25152;&#35780;&#20272;&#30340;&#27835;&#30103;&#25928;&#26524;&#26377;&#24046;&#24322;&#30340;&#23376;&#20154;&#32676;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26679;&#26412;&#36873;&#25321;&#38382;&#39064;&#65292;&#21363;&#22312;&#24322;&#36136;&#20154;&#32676;&#20013;&#22914;&#20309;&#36873;&#25321;&#20837;&#32452;RRT&#65292;&#20197;&#20248;&#21270;&#31119;&#21033;&#12290;&#25105;&#20204;&#22312;&#26368;&#23567;&#21518;&#24724;&#26694;&#26550;&#19979;&#24418;&#24335;&#21270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#31181;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#20915;&#31574;&#22914;&#20309;&#23548;&#33268;&#26126;&#26174;&#19981;&#21516;&#30340;&#20851;&#20110;&#26368;&#20339;&#26679;&#26412;&#20998;&#37197;&#30340;&#25351;&#23548;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;COVID-19&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#20102;&#19968;&#39033;&#21512;&#25104;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01386v1 Announce Type: cross  Abstract: Randomized controlled trials (RCTs) are often run in settings with many subpopulations that may have differential benefits from the treatment being evaluated. We consider the problem of sample selection, i.e., whom to enroll in an RCT, such as to optimize welfare in a heterogeneous population. We formalize this problem within the minimax-regret framework, and derive optimal sample-selection schemes under a variety of conditions. We also highlight how different objectives and decisions can lead to notably different guidance regarding optimal sample allocation through a synthetic experiment leveraging historical COVID-19 trial data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02111</link><description>&lt;p&gt;
&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#65306;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#23601;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;(MLMC)&#26469;&#25552;&#39640;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#22810;&#27493;&#21069;&#30651;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26222;&#36890;&#33945;&#29305;&#21345;&#27931;&#30340;&#22797;&#26434;&#24230;&#22312;&#23884;&#22871;&#25805;&#20316;&#20013;&#20250;&#38477;&#20302;&#65292;&#32780;MLMC&#33021;&#22815;&#20197;&#35268;&#33539;&#33945;&#29305;&#21345;&#27931;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;&#21644;&#24179;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#19968;&#27493;&#21644;&#20004;&#27493;&#21069;&#30651;&#37319;&#38598;&#20989;&#25968;&#30340;&#36817;&#20284;&#25913;&#36827;&#65292;&#20294;&#27491;&#22914;&#25105;&#20204;&#25152;&#35752;&#35770;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#31181;&#26041;&#38754;&#26159;&#21487;&#25512;&#24191;&#30340;&#65292;&#21253;&#25324;&#36229;&#36234;BO&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;MLMC&#22312;BO&#20013;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#22312;&#36825;&#37324;&#33719;&#21462;&#65306;https://github.com/Shangda-Yang/MLMCBO&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. The complexity rate of naive Monte Carlo degrades for nested operations, whereas MLMC is capable of achieving the canonical Monte Carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available here https://github.com/Shangda-Yang/MLMCBO.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item></channel></rss>