<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.09104</link><description>&lt;p&gt;
&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32553;&#20943;&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks. (arXiv:2308.09104v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20943;&#23569;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#24674;&#22797;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32467;&#26500;&#31232;&#30095;&#65288;&#22914;&#33410;&#28857;&#31232;&#30095;&#65289;&#21387;&#32553;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#25512;&#29702;&#12289;&#26356;&#39640;&#30340;&#25968;&#25454;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32553;&#20943;&#25216;&#26415;&#65292;Lasso&#21644;Horseshoe&#65292;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso (SS-GL)&#21644;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe (SS-GHS)&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#21253;&#25324;&#23545;&#20271;&#21162;&#21033;&#21464;&#37327;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#32553;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational 
&lt;/p&gt;</description></item></channel></rss>