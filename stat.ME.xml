<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.04661</link><description>&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04661
&lt;/p&gt;
&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#25209;&#35780;&#21644;&#25361;&#25112;&#12290;&#30683;&#30462;&#30340;&#26159;&#65292;&#23427;&#20204;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#34920;&#26126;&#23545;&#24213;&#23618;&#25968;&#25454;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#37325;&#35201;&#30340;&#35299;&#37322;&#28508;&#21147;&#12290;&#20511;&#21161;&#30693;&#35782;&#31934;&#39311;&#30340;&#26032;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20851;&#20110;&#25968;&#25454;&#30340;&#30693;&#35782;&#20174;&#40657;&#30418;&#27169;&#22411;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#30693;&#35782;&#31934;&#39311;&#36807;&#31243;&#26500;&#24314;&#30340;DDT&#30340;&#21487;&#35299;&#37322;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20026;DDT&#30340;&#32467;&#26500;&#31283;&#23450;&#24615;&#24314;&#31435;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20854;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#32467;&#26500;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31639;&#27861;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
&lt;/p&gt;</description></item></channel></rss>