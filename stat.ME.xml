<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#32676;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25490;&#38500;&#21487;&#33021;&#21463;&#21040;&#24178;&#25200;&#24433;&#21709;&#30340;&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20559;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#20248;&#21270;&#20272;&#35745;&#22120;&#25910;&#25947;&#36895;&#29575;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18836</link><description>&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#30340;&#38598;&#32676;&#38543;&#26426;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Design of Cluster-Randomized Trials with Cross-Cluster Interference. (arXiv:2310.18836v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#32676;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25490;&#38500;&#21487;&#33021;&#21463;&#21040;&#24178;&#25200;&#24433;&#21709;&#30340;&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20559;&#24046;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#20248;&#21270;&#20272;&#35745;&#22120;&#25910;&#25947;&#36895;&#29575;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#38543;&#26426;&#35797;&#39564;&#32463;&#24120;&#28041;&#21450;&#31354;&#38388;&#19978;&#20998;&#24067;&#19981;&#35268;&#24459;&#19988;&#27809;&#26377;&#26126;&#26174;&#20998;&#31163;&#31038;&#21306;&#30340;&#21333;&#20803;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#65292;&#38598;&#32676;&#26500;&#24314;&#26159;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#20381;&#36182;&#20110;&#37096;&#20998;&#24178;&#25200;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#38598;&#32676;&#35270;&#20026;&#32473;&#23450;&#65292;&#24182;&#20551;&#35774;&#27809;&#26377;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#20801;&#35768;&#24178;&#25200;&#19982;&#21333;&#20803;&#20043;&#38388;&#30340;&#22320;&#29702;&#36317;&#31163;&#34928;&#20943;&#26469;&#25918;&#23485;&#36825;&#20010;&#20551;&#35774;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#20559;&#24046;-&#26041;&#24046;&#30340;&#26435;&#34913;&#65306;&#26500;&#24314;&#36739;&#23569;&#12289;&#36739;&#22823;&#30340;&#38598;&#32676;&#21487;&#20197;&#20943;&#23569;&#24178;&#25200;&#24341;&#36215;&#30340;&#20559;&#24046;&#65292;&#20294;&#20250;&#22686;&#21152;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#25490;&#38500;&#21487;&#33021;&#21463;&#21040;&#20132;&#21449;&#38598;&#32676;&#24178;&#25200;&#24433;&#21709;&#30340;&#21333;&#20803;&#65292;&#24182;&#26174;&#31034;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22343;&#20540;&#24046;&#20272;&#35745;&#22120;&#65292;&#36825;&#22823;&#22823;&#38477;&#20302;&#20102;&#28176;&#36817;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20248;&#21270;&#20272;&#35745;&#22120;&#25910;&#25947;&#36895;&#29575;&#30340;&#38598;&#32676;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#30340;&#27491;&#24335;&#35777;&#26126;&#65292;&#35813;&#35774;&#35745;&#36873;&#25321;&#20102;&#38598;&#32676;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster-randomized trials often involve units that are irregularly distributed in space without well-separated communities. In these settings, cluster construction is a critical aspect of the design due to the potential for cross-cluster interference. The existing literature relies on partial interference models, which take clusters as given and assume no cross-cluster interference. We relax this assumption by allowing interference to decay with geographic distance between units. This induces a bias-variance trade-off: constructing fewer, larger clusters reduces bias due to interference but increases variance. We propose new estimators that exclude units most potentially impacted by cross-cluster interference and show that this substantially reduces asymptotic bias relative to conventional difference-in-means estimators. We then study the design of clusters to optimize the estimators' rates of convergence. We provide formal justification for a new design that chooses the number of clus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;&#32479;&#35745;&#24615;&#36136;&#21644;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13966</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#26368;&#20248;&#26497;&#23567;&#21270;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression. (arXiv:2310.13966v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;&#32479;&#35745;&#24615;&#36136;&#21644;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20256;&#36882;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#21463;&#21040;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#20851;&#30740;&#31350;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#30740;&#31350;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#21560;&#24341;&#21147;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#30340;&#26159;&#32553;&#23567;&#23454;&#38469;&#25928;&#26524;&#19982;&#29702;&#35770;&#20445;&#35777;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#24050;&#30693;&#21487;&#20256;&#36882;&#30340;&#26469;&#28304;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#24050;&#30693;&#21487;&#20256;&#36882;&#30340;&#26469;&#28304;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#26680;&#20272;&#35745;&#22120;&#65292;&#20165;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#12290;&#23545;&#20110;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#32858;&#21512;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#24182;&#20943;&#36731;&#36127;&#38754;&#26469;&#28304;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25152;&#38656;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the 
&lt;/p&gt;</description></item></channel></rss>