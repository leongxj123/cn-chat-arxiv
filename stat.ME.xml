<rss version="2.0"><channel><title>Chat Arxiv stat.ME</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ME</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03527</link><description>&lt;p&gt;
&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#19968;&#33268;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Validation for Predictive Methods in Spatial Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#22825;&#27668;&#39044;&#25253;&#12289;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#20854;&#20182;&#31185;&#23398;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#23450;&#25105;&#20204;&#23545;&#32479;&#35745;&#25110;&#29289;&#29702;&#26041;&#27861;&#25152;&#20316;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#26159;&#31185;&#23398;&#32467;&#35770;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#39564;&#35777;&#20301;&#32622;&#21644;&#25105;&#20204;&#24076;&#26395;&#36827;&#34892;&#39044;&#27979;&#30340;&#65288;&#27979;&#35797;&#65289;&#20301;&#32622;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#36890;&#24120;&#19981;&#26159;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#19968;&#20010;&#23454;&#20363;&#65288;&#24120;&#24120;&#34987;&#24418;&#24335;&#21270;&#65289;&#65292;&#22240;&#20026;&#39564;&#35777;&#21644;&#27979;&#35797;&#20301;&#32622;&#26159;&#22266;&#23450;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32593;&#26684;&#19978;&#25110;&#36873;&#23450;&#30340;&#28857;&#19978;&#65289;&#65292;&#32780;&#19981;&#26159;&#20174;&#20004;&#20010;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#22320;&#37319;&#26679;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#23545;&#39564;&#35777;&#26041;&#27861;&#30340;&#26816;&#26597;&#65306;&#38543;&#30528;&#39564;&#35777;&#25968;&#25454;&#30340;&#23494;&#24230;&#36234;&#26469;&#36234;&#22823;&#65292;&#23427;&#20204;&#33021;&#22815;&#21464;&#24471;&#20219;&#24847;&#31934;&#30830;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20256;&#32479;&#26041;&#27861;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#26041;&#27861;&#21487;&#33021;&#19981;&#28385;&#36275;&#36825;&#20010;&#26816;&#26597;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20511;&#37492;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#24605;&#24819;&#65292;&#20294;&#23545;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02196</link><description>&lt;p&gt;
&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#26679;&#26412;&#39640;&#25928;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02196
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#32858;&#31867;&#21644;&#24449;&#26381;"&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25171;&#30772;&#26679;&#26412;&#25928;&#29575;&#30340;&#29942;&#39048;&#12290;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;O(p)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#36895;&#24230;&#65292;&#36825;&#26159;&#29702;&#35770;&#19978;&#21487;&#36798;&#21040;&#30340;&#26368;&#20339;&#20943;&#23569;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#22312;&#22266;&#23450;&#39044;&#31639;&#21644;&#22266;&#23450;&#31934;&#24230;&#30340;&#33539;&#24335;&#19979;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21508;&#31181;&#24120;&#35265;&#30340;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#26080;&#38656;&#39640;&#31934;&#30830;&#24230;&#30456;&#20851;&#20272;&#35745;&#21644;&#31934;&#30830;&#32858;&#31867;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25913;&#36827;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#22914;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26080;&#31579;&#36873;&#29256;&#26412;&#30340;&#26041;&#27861;&#24778;&#20154;&#22320;&#36229;&#36807;&#20102;&#23436;&#20840;&#39034;&#24207;&#21270;&#30340;&#22522;&#20934;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#34920;&#26126;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#26159;&#32469;&#36807;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#26465;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&amp;S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&amp;S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02211</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30446;&#26631;&#20998;&#24067;&#19982;&#28304;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#20248;&#21270;&#20851;&#20110;&#30446;&#26631;&#20998;&#24067;&#31867;&#30340;&#21487;&#35299;&#37322;&#26041;&#24046;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#25913;&#21892;&#20102;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26159;&#28304;&#25968;&#25454;&#38598;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#37492;&#21035;&#32467;&#26524;&#26469;&#25552;&#39640;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#26657;&#27491;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;c&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.10299</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65306;&#21033;&#29992;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#32463;&#24120;&#21457;&#29983;&#20998;&#24067;&#21464;&#21270;&#65292;&#24613;&#38656;&#24320;&#21457;&#23545;&#36825;&#31181;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#65292;&#22914;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#35201;&#20040;&#23545;&#26410;&#35265;&#20998;&#24067;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20551;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#21644;&#32467;&#26500;&#30340;&#31283;&#20581;&#39044;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#25152;&#38656;&#30340;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#36825;&#31181;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#24120;&#24120;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#65288;Distributional Robustness via Invariant Gradients&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#65292;&#24182;&#22312;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#25554;&#20540;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DRIG&#20135;&#29983;&#30340;&#39044;&#27979;&#26159;
&lt;/p&gt;
&lt;p&gt;
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.04777</link><description>&lt;p&gt;
&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65292;&#22240;&#20854;&#21487;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#21040;&#20174;&#19994;&#32773;&#30340;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#36825;&#31181;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#20851;&#32852;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#19981;&#33021;&#20445;&#35777;&#25552;&#21462;&#30340;&#26159;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38598;&#35206;&#30422;&#26426;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#20108;&#20540;&#35268;&#21017;&#30340;&#21512;&#21462;/&#26512;&#21462;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#36991;&#20813;&#20102;&#34394;&#20551;&#20851;&#32852;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#32593;&#32476;&#24178;&#25200;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#36890;&#36807;&#23545;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#36830;&#25509;&#26041;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#23398;&#20064;&#25919;&#31574;&#25110;&#27835;&#30103;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#26816;&#39564;&#25919;&#31574;&#26080;&#20851;&#24615;/&#27835;&#30103;&#25928;&#24212;&#65292;&#24182;&#23545;&#24179;&#22343;&#25110;&#20998;&#24067;&#24335;&#25919;&#31574;&#25928;&#24212;/&#27835;&#30103;&#21453;&#24212;&#30340;&#20272;&#35745;&#22120;&#32473;&#20986;&#20102;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2105.03810</link><description>&lt;p&gt;
&#32593;&#32476;&#24178;&#25200;&#26465;&#20214;&#19979;&#22240;&#26524;&#25512;&#26029;&#30340;&#23616;&#37096;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Local Approach to Causal Inference under Network Interference. (arXiv:2105.03810v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03810
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#32593;&#32476;&#24178;&#25200;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#36890;&#36807;&#23545;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#36830;&#25509;&#26041;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#23398;&#20064;&#25919;&#31574;&#25110;&#27835;&#30103;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#26816;&#39564;&#25919;&#31574;&#26080;&#20851;&#24615;/&#27835;&#30103;&#25928;&#24212;&#65292;&#24182;&#23545;&#24179;&#22343;&#25110;&#20998;&#24067;&#24335;&#25919;&#31574;&#25928;&#24212;/&#27835;&#30103;&#21453;&#24212;&#30340;&#20272;&#35745;&#22120;&#32473;&#20986;&#20102;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#31038;&#20132;&#25110;&#32463;&#27982;&#32593;&#32476;&#20013;&#20195;&#29702;&#20154;&#20043;&#38388;&#36830;&#25509;&#26041;&#24335;&#23545;&#32467;&#26524;&#20135;&#29983;&#24433;&#21709;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#12290;&#36825;&#31181;&#32593;&#32476;&#24178;&#25200;&#25551;&#36848;&#20102;&#20851;&#20110;&#27835;&#30103;&#28322;&#20986;&#12289;&#31038;&#20132;&#20114;&#21160;&#12289;&#31038;&#20250;&#23398;&#20064;&#12289;&#20449;&#24687;&#25193;&#25955;&#12289;&#30142;&#30149;&#21644;&#37329;&#34701;&#20256;&#26579;&#12289;&#31038;&#20250;&#36164;&#26412;&#24418;&#25104;&#31561;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#27979;&#37327;&#36335;&#24452;&#36317;&#31163;&#26469;&#25551;&#36848;&#20195;&#29702;&#20154;&#22312;&#32593;&#32476;&#20013;&#30340;&#36830;&#25509;&#26041;&#24335;&#65292;&#28982;&#21518;&#36890;&#36807;&#27719;&#38598;&#20855;&#26377;&#31867;&#20284;&#37197;&#32622;&#30340;&#20195;&#29702;&#20154;&#30340;&#32467;&#26524;&#25968;&#25454;&#26469;&#23398;&#20064;&#25919;&#31574;&#25110;&#27835;&#30103;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#28176;&#36817;&#26377;&#25928;&#30340;&#27979;&#35797;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#35813;&#27979;&#35797;&#29992;&#20110;&#26816;&#39564;&#25919;&#31574;&#26080;&#20851;&#24615;/&#27835;&#30103;&#25928;&#24212;&#30340;&#20551;&#35774;&#65292;&#24182;&#32473;&#20986;&#20102;&#38024;&#23545;&#24179;&#22343;&#25110;&#20998;&#24067;&#24335;&#25919;&#31574;&#25928;&#24212;/&#27835;&#30103;&#21453;&#24212;&#30340;k&#26368;&#36817;&#37051;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new nonparametric modeling framework for causal inference when outcomes depend on how agents are linked in a social or economic network. Such network interference describes a large literature on treatment spillovers, social interactions, social learning, information diffusion, disease and financial contagion, social capital formation, and more. Our approach works by first characterizing how an agent is linked in the network using the configuration of other agents and connections nearby as measured by path distance. The impact of a policy or treatment assignment is then learned by pooling outcome data across similarly configured agents. We demonstrate the approach by proposing an asymptotically valid test for the hypothesis of policy irrelevance/no treatment effects and bounding the mean-squared error of a k-nearest-neighbor estimator for the average or distributional policy effect/treatment response.
&lt;/p&gt;</description></item></channel></rss>