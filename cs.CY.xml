<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item></channel></rss>