<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2401.08103</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#23454;&#26045;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20262;&#29702;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Resolving Ethics Trade-offs in Implementing Responsible AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25226;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#21040;&#23454;&#38469;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22788;&#29702;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#20173;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#31181;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#12289;&#33539;&#22260;&#12289;&#34913;&#37327;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#21644;&#35777;&#26126;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#32452;&#32455;&#12289;&#31995;&#32479;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#31215;&#26497;&#35782;&#21035;&#32039;&#24352;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#20248;&#20808;&#22788;&#29702;&#21644;&#26435;&#34913;&#20262;&#29702;&#26041;&#38754;&#65292;&#65288;iii&#65289;&#35777;&#26126;&#21644;&#35760;&#24405;&#26435;&#34913;&#20915;&#31574;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#26088;&#22312;&#20419;&#36827;&#23454;&#26045;&#31526;&#21512;&#28508;&#22312;&#30417;&#31649;&#35201;&#27714;&#30340;&#20840;&#38754;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
&lt;/p&gt;</description></item></channel></rss>