<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15780</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#65292;&#21253;&#25324;&#37027;&#20123;&#30452;&#25509;&#28041;&#21450;&#20154;&#31867;&#30340;&#39046;&#22495;&#65292;&#24179;&#31561;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#30028;&#24840;&#21457;&#31361;&#20986;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#23548;&#21521;&#26041;&#27861;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#25506;&#35752;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#24615;&#33021;&#20248;&#21270;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q-Learning&#31639;&#27861;&#65292;&#21033;&#29992;&#20854;&#25910;&#25947;&#20445;&#35777;&#26469;&#30830;&#20445;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31449;&#28857;&#31867;&#21035;&#65288;&#20013;&#24515;&#12289;&#36793;&#32536;&#21644;&#36828;&#31243;&#65289;&#20043;&#38388;&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#36890;&#36807;&#22522;&#23612;&#31995;&#25968;&#26469;&#34913;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15780v1 Announce Type: cross  Abstract: As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through stra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2401.07836</link><description>&lt;p&gt;
&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#39118;&#38505;&#65306;&#20915;&#23450;&#24615;&#21644;&#32047;&#31215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Two Types of AI Existential Risk: Decisive and Accumulative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#23545;&#20154;&#24037;&#26234;&#33021;(AI)&#24341;&#36215;&#30340;&#23384;&#22312;&#39118;&#38505;(x-risks)&#30340;&#35752;&#35770;&#36890;&#24120;&#38598;&#20013;&#22312;&#30001;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#24341;&#36215;&#30340;&#31361;&#28982;&#12289;&#20005;&#37325;&#20107;&#20214;&#19978;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#21487;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#20107;&#20214;&#23558;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#35201;&#20040;&#23548;&#33268;&#20154;&#31867;&#28781;&#32477;&#65292;&#35201;&#20040;&#26080;&#27861;&#36870;&#36716;&#22320;&#20351;&#20154;&#31867;&#25991;&#26126;&#38519;&#20837;&#26080;&#27861;&#24674;&#22797;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35752;&#35770;&#32463;&#24120;&#24573;&#35270;AI x-risk&#36880;&#28176;&#36890;&#36807;&#19968;&#31995;&#21015;&#36739;&#23567;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#20013;&#26029;&#36880;&#28176;&#26174;&#29616;&#20986;&#26469;&#30340;&#20005;&#37325;&#21487;&#33021;&#24615;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#36328;&#36234;&#20851;&#38190;&#38408;&#20540;&#12290;&#35813;&#35770;&#25991;&#23558;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#36827;&#34892;&#23545;&#27604;&#12290;&#21069;&#32773;&#25551;&#32472;&#20102;&#19968;&#31181;&#26126;&#26174;&#30340;AI&#25509;&#31649;&#36335;&#24452;&#65292;&#20854;&#29305;&#24449;&#26159;&#26080;&#27861;&#25511;&#21046;&#30340;&#36229;&#32423;&#26234;&#33021;&#31561;&#24773;&#26223;&#65292;&#32780;&#21518;&#32773;&#21017;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#23548;&#33268;&#28781;&#32477;&#24615;&#28798;&#38590;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#36825;&#28041;&#21450;&#21040;&#30001;AI&#24341;&#36215;&#30340;&#20005;&#37325;&#23041;&#32961;&#30340;&#36880;&#28176;&#32047;&#31215;&#65292;&#20363;&#22914;&#20005;&#37325;&#30340;&#28431;&#27934;&#21644;&#31995;&#32479;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic e
&lt;/p&gt;</description></item></channel></rss>