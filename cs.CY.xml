<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item></channel></rss>