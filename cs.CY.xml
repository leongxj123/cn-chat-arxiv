<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13481</link><description>&lt;p&gt;
AI&#24605;&#24819;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#12289;&#22810;&#26679;&#24615;&#21644;&#36827;&#21270;&#65306;&#26469;&#33258;&#19968;&#20010;&#22823;&#35268;&#27169;&#21160;&#24577;&#23454;&#39564;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13481
&lt;/p&gt;
&lt;p&gt;
AI&#24605;&#24819;&#23545;&#20010;&#20307;&#21019;&#36896;&#21147;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25509;&#35302;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#35266;&#30475;&#21040;AI&#29983;&#25104;&#30340;&#24605;&#24819;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24605;&#24819;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65288;800+&#21442;&#19982;&#32773;&#65292;40+&#20010;&#22269;&#23478;&#65289;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#26469;&#33258;ChatGPT&#25110;&#20043;&#21069;&#23454;&#39564;&#21442;&#19982;&#32773;&#30340;&#21019;&#24847;&#24605;&#24819;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#21019;&#24847;&#24605;&#32771;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;AI&#29983;&#25104;&#31034;&#20363;&#30340;&#25968;&#37327;&#65288;&#26080;&#12289;&#20302;&#12289;&#39640;&#26333;&#20809;&#65289;&#20197;&#21450;&#31034;&#20363;&#26159;&#21542;&#26631;&#35760;&#20026;&#8220;AI&#8221;&#65288;&#25259;&#38706;&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23454;&#39564;&#35774;&#35745; - &#22312;&#21516;&#19968;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#20043;&#21069;&#21442;&#19982;&#32773;&#30340;&#24605;&#24819;&#20316;&#20026;&#26410;&#26469;&#21442;&#19982;&#32773;&#30340;&#21050;&#28608; - &#27169;&#25311;&#20102;&#25991;&#21270;&#21019;&#36896;&#30340;&#30456;&#20114;&#20381;&#36182;&#36807;&#31243;&#65306;&#21019;&#36896;&#24615;&#24605;&#24819;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25429;&#25417;&#21040;&#20102;LLM&#8220;&#22312;&#25991;&#21270;&#24490;&#29615;&#20013;&#8221;&#30340;&#22797;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;AI&#26333;&#20809;&#65288;&#20294;&#19981;&#26159;&#20302;AI&#26333;&#20809;&#65289;&#24182;&#27809;&#26377;&#24433;&#21709;&#20010;&#20154;&#24605;&#24819;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#22686;&#21152;&#20102;&#25972;&#20307;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#24179;&#22343;&#25968;&#37327;&#21644;&#21464;&#21270;&#36895;&#29575;&#12290;AI&#20351;&#24605;&#24819;&#22810;&#26679;&#24615;&#30340;&#32047;&#31215;&#25928;&#24212;&#22686;&#24378;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;</title><link>http://arxiv.org/abs/2401.02627</link><description>&lt;p&gt;
&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#29305;&#28857;&#21644;&#26222;&#36941;&#24615;&#65292;&#20351;&#29992;&#30340;&#26159;AI&#29983;&#25104;&#30340;&#38754;&#23380;
&lt;/p&gt;
&lt;p&gt;
Characteristics and prevalence of fake social media profiles with AI-generated faces. (arXiv:2401.02627v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#33021;&#21019;&#24314;&#20986;&#36924;&#30495;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#25285;&#24551;&#65292;&#20294;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;Twitter(X)&#36134;&#25143;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;1,353&#20010;&#27492;&#31867;&#36134;&#25143;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#19968;&#20010;&#29305;&#24449;&#8212;&#8212;&#30524;&#30555;&#30340;&#19968;&#33268;&#20301;&#32622;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#37326;&#22806;&#20013;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#36134;&#25143;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38543;&#26426;&#26679;&#26412;&#30340;&#27963;&#36291;Twitter&#29992;&#25143;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#22810;&#27169;&#24335;&#36134;&#25143;&#23545;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative artificial intelligence (AI) have raised concerns about their potential to create convincing fake social media accounts, but empirical evidence is lacking. In this paper, we present a systematic analysis of Twitter(X) accounts using human faces generated by Generative Adversarial Networks (GANs) for their profile pictures. We present a dataset of 1,353 such accounts and show that they are used to spread scams, spam, and amplify coordinated messages, among other inauthentic activities. Leveraging a feature of GAN-generated faces -- consistent eye placement -- and supplementing it with human annotation, we devise an effective method for identifying GAN-generated profiles in the wild. Applying this method to a random sample of active Twitter users, we estimate a lower bound for the prevalence of profiles using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active accounts. These findings underscore the emerging threats posed by multimod
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item></channel></rss>