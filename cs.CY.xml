<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01620</link><description>&lt;p&gt;
Voice EHR:&#24341;&#20837;&#22810;&#27169;&#24335;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Voice EHR: Introducing Multimodal Audio Data for Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24555;&#36895;&#20998;&#31867;&#24739;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#65292;&#24182;&#21487;&#33021;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#25913;&#21892;&#32467;&#26524;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#39640;&#25910;&#20837;&#12289;&#33521;&#35821;&#22269;&#23478;&#20351;&#29992;&#26114;&#36149;&#35760;&#24405;&#35774;&#22791;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#25216;&#26415;&#38754;&#20020;&#36164;&#28304;&#21463;&#38480;&#12289;&#39640;&#25910;&#20837;&#22330;&#25152;&#30340;&#37096;&#32626;&#25361;&#25112;&#65292;&#38899;&#39057;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#25910;&#38598;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20165;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;/&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#23427;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#20256;&#32479;&#35821;&#38899;/&#21628;&#21560;&#29305;&#24449;&#12289;&#35821;&#38899;&#27169;&#24335;&#21644;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#35821;&#35328;&#30340;&#22797;&#26434;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34917;&#20607;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#20249;&#20276;&#36130;&#22242;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01620v1 Announce Type: cross  Abstract: Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partner
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.17106</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65306;&#22312;&#24744;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#20013;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#23567;&#21270;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#24046;&#24322;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#30340;&#20005;&#37325;&#31243;&#24230;&#22522;&#26412;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#38598;&#30340;&#19981;&#22343;&#34913;&#25110;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#20351;&#29992;&#32479;&#19968;&#30340;&#20844;&#24179;&#24615;&#35201;&#27714;&#20173;&#28982;&#20540;&#24471;&#24576;&#30097;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#25928;&#29992;&#26497;&#20302;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;You-Only-Train-Once&#65288;YOTO&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#22312;&#36924;&#36817;&#26435;&#34913;&#26354;&#32447;&#26102;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35813;&#26354;&#32447;&#21608;&#22260;&#24341;&#20837;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#25105;&#20204;&#36817;&#20284;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
&lt;/p&gt;</description></item><item><title>FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11896</link><description>&lt;p&gt;
&#38024;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11896
&lt;/p&gt;
&lt;p&gt;
FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#24494;&#22937;&#34920;&#36798;&#30340;&#29702;&#35299;&#12290;&#36825;&#26679;&#24494;&#22937;&#32780;&#38544;&#24615;&#30340;&#20167;&#24680;&#32463;&#24120;&#34987;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#38750;&#20167;&#24680;&#12290;&#36890;&#36807;&#22686;&#21152;&#22806;&#37096;&#30340;&#19978;&#19979;&#25991;&#25110;&#36890;&#36807;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#24378;&#21046;&#26631;&#31614;&#20998;&#31163;&#65292;&#24050;&#32463;&#23581;&#35797;&#36807;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#65288;&#38544;&#24615;&#65289;&#20167;&#24680;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#36866;&#24212;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65288;FiADD&#65289;&#12290;FiADD&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26469;&#22686;&#24378;PLM&#24494;&#35843;&#31649;&#36947;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38544;&#24615;&#20167;&#24680;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;FiADD&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20004;&#31867;&#21644;&#19977;&#31867;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;FiADD&#22312;&#19977;&#20010;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363;&#26816;&#27979;&#35773;&#21050;&#12289;&#35773;&#21050;&#21644;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained large language models (PLMs) have achieved state-of-the-art on many NLP tasks, they lack understanding of subtle expressions of implicit hate speech. Such nuanced and implicit hate is often misclassified as non-hate. Various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. We combine these two approaches and introduce FiADD, a novel Focused Inferential Adaptive Density Discrimination framework. FiADD enhances the PLM finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. We test FiADD on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. We further experiment on the generalizability of FiADD on three other tasks, namely detecting sarcasm, irony, and stance, in whic
&lt;/p&gt;</description></item></channel></rss>