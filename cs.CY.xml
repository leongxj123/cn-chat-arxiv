<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.08275</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#65306;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#28389;&#29992;&#21644;&#19981;&#24403;&#20351;&#29992;&#24341;&#36215;&#30340;&#25285;&#24551;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#30340;&#22522;&#26412;&#26041;&#38754;&#21253;&#25324;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#36825;&#32473;&#23547;&#27714;&#36981;&#24490;&#36825;&#20123;&#21407;&#21017;&#30340;AI/ML&#24320;&#21457;&#32773;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#25552;&#39640;AI/ML&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#27719;&#32534;&#21644;&#35752;&#35770;10&#20010;&#31361;&#20986;&#30340;&#32039;&#24352;&#20851;&#31995;&#12289;&#26435;&#34913;&#21644;&#20854;&#20182;&#22522;&#26412;&#26041;&#38754;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20415;&#22312;&#25345;&#32493;&#21162;&#21147;&#23558;&#36825;&#20123;&#21407;&#21017;&#36716;&#21270;&#20026;&#23454;&#36341;&#30340;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#20262;&#29702;&#21407;&#21017;&#26041;&#38754;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#65292;&#24182;&#36890;&#36807;&#22312;&#24191;&#27867;&#25991;&#29486;&#20013;&#30340;&#25903;&#25345;&#36827;&#34892;&#21452;&#38754;&#20114;&#21160;&#30340;&#37325;&#28857;&#35752;&#35770;&#12290;&#36825;&#20010;&#30446;&#24405;&#23545;&#20110;&#25552;&#39640;&#20154;&#20204;&#23545;&#20262;&#29702;&#20934;&#21017;&#26041;&#38754;&#20043;&#38388;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#20197;&#21450;&#20419;&#36827;&#35774;&#35745;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20570;&#20986;&#26377;&#20805;&#20998;&#20381;&#25454;&#30340;&#21028;&#26029;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and develo
&lt;/p&gt;</description></item></channel></rss>