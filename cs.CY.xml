<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#19990;&#30028;&#20013;&#28040;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.12797</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Fair Machine Learning via Rank-Preserving Interventional Distributions. (arXiv:2307.12797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#19990;&#30028;&#20013;&#28040;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#30456;&#21516;&#30340;&#20010;&#20307;&#24471;&#21040;&#30456;&#21516;&#30340;&#23545;&#24453;&#65292;&#32780;&#19981;&#21516;&#30340;&#20010;&#20307;&#24471;&#21040;&#19981;&#21516;&#30340;&#23545;&#24453;&#65292;&#37027;&#20040;&#19968;&#20010;&#20915;&#31574;&#34987;&#23450;&#20041;&#20026;&#20844;&#24179;&#30340;&#12290;&#26681;&#25454;&#36825;&#20010;&#23450;&#20041;&#65292;&#22312;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#20943;&#23569;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#26102;&#65292;&#24517;&#39035;&#24341;&#20837;&#22240;&#26524;&#24605;&#32771;&#26469;&#24341;&#20837;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#23558;&#20010;&#20307;&#23450;&#20041;&#20026;&#22312;&#19968;&#20010;&#20551;&#35774;&#30340;&#12289;&#29702;&#24819;&#30340;&#65288;FiND&#65289;&#19990;&#30028;&#20013;&#26159;&#35268;&#33539;&#19978;&#30456;&#31561;&#30340;&#65292;&#36825;&#20010;&#19990;&#30028;&#20013;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#27809;&#26377;&#65288;&#30452;&#25509;&#25110;&#38388;&#25509;&#65289;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#26469;&#23450;&#20041;&#36825;&#20010;FiND&#19990;&#30028;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#35777;&#25968;&#25454;&#30340;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20215;&#26631;&#20934;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26368;&#21463;&#27495;&#35270;&#30340;&#20010;&#20307;&#24182;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
&lt;/p&gt;</description></item></channel></rss>