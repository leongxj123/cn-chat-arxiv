<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#31038;&#20250;&#31185;&#23398;&#29702;&#35770;&#24314;&#31435;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.02748</link><description>&lt;p&gt;
&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35745;&#31639;&#26694;&#26550;&#23545;&#20110;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A computational framework of human values for ethical AI. (arXiv:2305.02748v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#31038;&#20250;&#31185;&#23398;&#29702;&#35770;&#24314;&#31435;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24515;&#29702;&#23398;&#12289;&#21746;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#22810;&#20803;&#21270;&#25506;&#32034;&#20013;&#65292;&#21487;&#20197;&#24471;&#20986;&#19968;&#20010;&#26126;&#30830;&#30340;&#20849;&#35782;&#65292;&#37027;&#23601;&#26159;&#20215;&#20540;&#35266;&#25351;&#23548;&#30528;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#65292;&#20215;&#20540;&#35266;&#20026;&#24037;&#31243;&#21270;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#27491;&#24335;&#27010;&#24565;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35745;&#31639;&#24418;&#24335;&#23450;&#20041;&#65292;&#20026;&#31995;&#32479;&#12289;&#32508;&#21512;&#21644;&#36328;&#23398;&#31185;&#22320;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. More recently, a recognition that values provide a means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed shifting AI's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. This challenge -- the value alignment problem -- with others including an AI's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. Despite this, no formal, computational definition of values has yet been proposed. We address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical AI.
&lt;/p&gt;</description></item></channel></rss>