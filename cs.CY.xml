<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08946</link><description>&lt;p&gt;
&#21487;&#29992;&#30340;XAI&#65306;&#22312;LLM&#26102;&#20195;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;10&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08946
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25351;&#30340;&#26159;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27934;&#35265;&#65292;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36816;&#20316;&#26041;&#24335;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;XAI&#30340;&#37325;&#28857;&#27491;&#34987;&#25193;&#23637;&#21040;&#24120;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#32780;&#22791;&#21463;&#25209;&#35780;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#19968;&#25299;&#23637;&#38656;&#35201;&#23545;XAI&#26041;&#27861;&#35770;&#36827;&#34892;&#26174;&#33879;&#36716;&#21464;&#65292;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;XAI&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;LLMs&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#34892;&#19994;&#24212;&#29992;&#20013;&#65292;XAI&#30340;&#35282;&#33394;&#20174;&#20165;&#20165;&#25171;&#24320;&#8220;&#40657;&#21283;&#23376;&#8221;&#36716;&#21464;&#20026;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#20316;&#20026;XAI&#27934;&#35265;&#30340;&#34987;&#21160;&#25509;&#21463;&#32773;&#65292;LLMs&#30340;&#29420;&#29305;&#33021;&#21147;&#33021;&#22815;&#30456;&#20114;&#22686;&#24378;XAI&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;1&#65289;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item></channel></rss>