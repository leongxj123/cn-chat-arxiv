<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.12474</link><description>&lt;p&gt;
FairSIN&#65306;&#36890;&#36807;&#25935;&#24863;&#20449;&#24687;&#20013;&#21644;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;GNNs&#20063;&#23481;&#26131;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20570;&#20986;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20844;&#24179;&#32771;&#34385;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#20986;&#20174;&#36755;&#20837;&#25110;&#34920;&#31034;&#20013;&#36807;&#28388;&#25481;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#21024;&#38500;&#36793;&#25110;&#23631;&#34109;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#27492;&#31867;&#36807;&#28388;&#31574;&#30053;&#21487;&#33021;&#20063;&#20250;&#36807;&#28388;&#25481;&#19968;&#20123;&#38750;&#25935;&#24863;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#20135;&#29983;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;&#20013;&#21644;&#22522;&#30784;&#33539;&#24335;&#65292;&#21363;&#22312;&#20449;&#24687;&#20256;&#36882;&#20043;&#21069;&#23558;&#39069;&#22806;&#30340;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#25110;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;F3&#39044;&#26399;&#22312;&#32479;&#35745;&#19978;&#20013;&#21644;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#38750;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12474v1 Announce Type: new  Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nons
&lt;/p&gt;</description></item></channel></rss>