<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;</title><link>http://arxiv.org/abs/2308.10800</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#26080;&#25928;&#19988;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is ineffective and potentially harmful for fact checking. (arXiv:2308.10800v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#20294;&#26159;&#23427;&#22312;&#35268;&#27169;&#19978;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#32593;&#32476;&#19978;&#20449;&#24687;&#36807;&#20110;&#24222;&#22823;&#30340;&#38459;&#30861;&#12290;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#20107;&#23454;&#26680;&#26597;&#20449;&#24687;&#26102;&#30340;&#20316;&#29992;&#26426;&#21046;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19968;&#27454;&#28909;&#38376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20107;&#23454;&#26680;&#26597;&#23545;&#25919;&#27835;&#26032;&#38395;&#20449;&#20208;&#21644;&#20998;&#20139;&#24847;&#22270;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35813;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31359;&#34394;&#20551;&#26631;&#39064;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#24182;&#27809;&#26377;&#23545;&#21442;&#19982;&#32773;&#35782;&#21035;&#26631;&#39064;&#20934;&#30830;&#24615;&#25110;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#30340;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20107;&#23454;&#26680;&#26597;&#22120;&#20855;&#26377;&#21361;&#23475;&#24615;&#65306;&#23558;&#19968;&#20123;&#30495;&#23454;&#26631;&#39064;&#35823;&#26631;&#20026;&#34394;&#20551;&#20250;&#38477;&#20302;&#23545;&#20854;&#30340;&#20449;&#20208;&#65292;&#32780;&#23545;&#20854;&#26410;&#30830;&#23450;&#30340;&#34394;&#20551;&#26631;&#39064;&#21017;&#20250;&#22686;&#21152;&#23545;&#20854;&#30340;&#20449;&#20208;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#27491;&#30830;&#26631;&#23450;&#26631;&#39064;&#30340;&#20998;&#20139;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled t
&lt;/p&gt;</description></item></channel></rss>