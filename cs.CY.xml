<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14791</link><description>&lt;p&gt;
Particip-AI: &#19968;&#31181;&#27665;&#20027;&#35843;&#26597;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14791
&lt;/p&gt;
&lt;p&gt;
Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;ChatGPT&#65292;&#20284;&#20046;&#38477;&#20302;&#20102;&#20844;&#20247;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21450;&#21033;&#29992;&#20854;&#21147;&#37327;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#21644;&#21457;&#23637;&#20173;&#25484;&#25569;&#22312;&#23569;&#25968;&#20154;&#25163;&#20013;&#65292;&#21457;&#23637;&#36895;&#24230;&#21152;&#24555;&#19988;&#32570;&#20047;&#39118;&#38505;&#35780;&#20272;&#12290;&#20316;&#20026;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#27835;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Particip-AI&#65292;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#23558;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#21361;&#23475;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#25910;&#38598;&#20351;&#29992;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#21644;&#35814;&#32454;&#22320;&#30740;&#31350;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#22312;&#22791;&#36873;&#26041;&#26696;&#19979;&#65288;&#21363;&#24320;&#21457;&#21644;&#19981;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#24773;&#20917;&#65289;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#21361;&#23475;&#65292;&#24182;&#36890;&#36807;&#20570;&#20986;&#23545;&#20854;&#21457;&#23637;&#30340;&#32467;&#35770;&#24615;&#36873;&#25321;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#25351;&#23548;&#27665;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;295&#20010;&#20154;&#21475;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07039</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#35843;&#25259;&#38706;&#65306;&#36229;&#36234;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Coordinated Disclosure for AI: Beyond Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07039
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20260;&#23475;&#25253;&#21578;&#22312;&#25259;&#38706;&#25110;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#31181;&#20020;&#26102;&#24615;&#30340;&#25805;&#20316;&#65292;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21327;&#35843;&#28431;&#27934;&#25259;&#38706;&#65288;CVD&#65289;&#30340;&#20262;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#22312;&#36719;&#20214;&#23433;&#20840;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32654;&#22269;&#30340;&#32972;&#26223;&#19979;&#65292;&#20026;&#20102;&#40723;&#21169;&#31177;&#25345;&#21892;&#24847;&#34892;&#20107;&#30340;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#65292;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#38450;&#25252;&#26465;&#27454;&#20197;&#23545;&#25239;&#35745;&#31639;&#26426;&#27450;&#35784;&#21644;&#28389;&#29992;&#27861;&#26696;&#19968;&#30452;&#23384;&#22312;&#38271;&#26399;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#26007;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#32570;&#38519;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#19987;&#38376;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#29305;&#27530;&#22797;&#26434;&#24615;&#30340;&#19987;&#38376;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#30340;&#23454;&#26045;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;ML&#20013;&#30340;&#25259;&#38706;&#21382;&#21490;&#32972;&#26223;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
&lt;/p&gt;</description></item></channel></rss>