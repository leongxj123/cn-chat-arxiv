<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04417</link><description>&lt;p&gt;
&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20844;&#27491;&#24863;&#30693;&#32852;&#37030;&#26497;&#23567;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#33258;&#30001;&#24230;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#20559;&#21521;&#20110;&#25935;&#24863;&#22240;&#32032;&#35832;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#24102;&#26377;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#20844;&#24179;&#32852;&#37030;&#24179;&#22343;&#27861; (FFALM)&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;FL&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#30446;&#26631;&#26045;&#21152;&#20102;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;FFALM&#30340;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;CelebA&#21644;UTKFace&#25968;&#25454;&#38598;&#20013;&#20805;&#20998;&#32771;&#34385;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;FFALM &#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item></channel></rss>