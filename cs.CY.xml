<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12025</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#30528;&#20026;&#22797;&#26434;&#30340;&#20581;&#24247;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#26377;&#21487;&#33021;&#24341;&#20837;&#21361;&#23475;&#24182;&#21152;&#21095;&#20581;&#24247;&#19981;&#24179;&#31561;&#12290;&#21487;&#38752;&#22320;&#35780;&#20272;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#27169;&#22411;&#22833;&#28789;&#26159;&#21457;&#23637;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#21487;&#33021;&#23548;&#33268;LLM&#29983;&#25104;&#30340;&#38271;&#31687;&#31572;&#26696;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#21361;&#23475;&#30340;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Med-PaLM 2&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#35813;&#39046;&#22495;&#36827;&#34892;&#30340;&#26368;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#65292;&#20197;&#21450;EquityMedQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#26032;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#65292;&#20854;&#20013;&#26082;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#21448;&#21253;&#25324;LLM&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20016;&#23500;&#20102;&#23545;&#25239;&#24615;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#35774;&#35745;&#36807;&#31243;&#37117;&#26681;&#26893;&#20110;&#23454;&#38469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.02680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22320;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Geographically Biased
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#22312;&#22320;&#21547;&#26377;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#20260;&#23475;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20559;&#35265;&#23545;&#20110;&#23454;&#29616;&#20844;&#27491;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22320;&#29702;&#35270;&#35282;&#30740;&#31350;LLMs&#23545;&#25105;&#20204;&#25152;&#29983;&#27963;&#30340;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22240;&#20026;&#23545;&#20154;&#31867;&#29983;&#27963;&#20013;&#35832;&#22810;&#19982;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#30340;&#26041;&#38754;&#65288;&#22914;&#25991;&#21270;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#12289;&#25919;&#27835;&#21644;&#23447;&#25945;&#65289;&#26377;&#30528;&#26126;&#26174;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#38382;&#39064;&#22320;&#29702;&#20559;&#35265;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20013;&#30340;&#31995;&#32479;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#33021;&#22815;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#65292;&#20197;&#35780;&#32423;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#21333;&#35843;&#30456;&#20851;&#24615;&#65288;Spearman's &#961;&#26368;&#39640;&#21487;&#36798;0.89&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#20010;&#23458;&#35266;&#21644;&#23376;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#20849;&#21516;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
&lt;/p&gt;</description></item></channel></rss>