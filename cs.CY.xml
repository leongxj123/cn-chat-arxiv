<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13658</link><description>&lt;p&gt;
&#20851;&#20110;AI&#38382;&#36131;&#25919;&#31574;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#20316;&#20986;&#30340;&#22238;&#24212;&#12290;&#22312;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#20851;&#38190;&#21477;&#23376;&#26411;&#23614;&#65292;&#25552;&#20379;&#20102;&#35201;&#27714;&#35780;&#35770;&#30340;&#38382;&#39064;&#32534;&#21495;&#30340;&#19978;&#26631;&#12290;&#35813;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21487;&#33021;&#24102;&#26377;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#33021;&#22815;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#22312;&#28040;&#38500;&#27495;&#35270;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#24182;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/1912.08189</link><description>&lt;p&gt;
&#20174;&#24102;&#26377;&#27495;&#35270;&#24615;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Discriminatory Training Data. (arXiv:1912.08189v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21487;&#33021;&#24102;&#26377;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#33021;&#22815;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#22312;&#28040;&#38500;&#27495;&#35270;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#24182;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#26159;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#30340;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21463;&#21040;&#27495;&#35270;&#24615;&#36136;&#30340;&#24433;&#21709;&#65292;&#37027;&#20040;&#35813;&#31995;&#32479;&#21487;&#33021;&#20250;&#22312;&#20445;&#25252;&#32452;&#20013;&#20135;&#29983;&#27495;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20844;&#24179;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#28508;&#22312;&#30340;&#27495;&#35270;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20063;&#23558;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36716;&#21464;&#20026;&#29305;&#23450;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#29992;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#28040;&#38500;&#30452;&#25509;&#27495;&#35270;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#36716;&#21464;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30450;&#30446;&#35757;&#32451;&#21253;&#21547;&#30452;&#25509;&#21152;&#24615;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#22312;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#35777;&#26126;&#26368;&#23567;&#21270;&#27169;&#22411;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#27861;&#24459;&#20307;&#31995;&#20860;&#23481;&#65292;&#24182;&#36890;&#36807;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26469;&#35299;&#20915;&#24191;&#27867;&#35752;&#35770;&#30340;&#21463;&#20445;&#25252;&#32676;&#20307;&#20132;&#21449;&#30340;&#38382;&#39064;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20102;&#27010;&#29575;&#24178;&#39044;&#65292;&#24182;&#20855;&#26377;&#22240;&#26524;&#21644;&#21453;&#20107;&#23454;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups' intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulat
&lt;/p&gt;</description></item></channel></rss>