<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;</title><link>http://arxiv.org/abs/2401.13408</link><description>&lt;p&gt;
&#22240;&#26524;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Causal Perception. (arXiv:2401.13408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13408
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20004;&#20010;&#20010;&#20307;&#23545;&#30456;&#21516;&#30340;&#20449;&#24687;&#36827;&#34892;&#19981;&#21516;&#35299;&#35835;&#26102;&#65292;&#24863;&#30693;&#20250;&#21457;&#29983;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#20010;&#24050;&#30693;&#29616;&#35937;&#65292;&#23545;&#20915;&#31574;&#20013;&#20559;&#35265;&#26377;&#24433;&#21709;&#65292;&#20294;&#26159;&#24863;&#30693;&#22312;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#24863;&#30693;&#23545;&#20110;ADM&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25110;&#20844;&#24179;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#26412;&#36523;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;&#26412;&#25991;&#23558;&#24863;&#30693;&#22312;&#22240;&#26524;&#25512;&#29702;&#20013;&#24418;&#24335;&#21270;&#65292;&#20197;&#25429;&#25417;&#20010;&#20307;&#30340;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23558;&#20010;&#20307;&#32463;&#39564;&#24418;&#24335;&#21270;&#20026;&#39069;&#22806;&#30340;&#22240;&#26524;&#30693;&#35782;&#65292;&#20010;&#20307;&#20250;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#21644;&#35752;&#35770;&#20102;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#65292;&#21363;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#12290;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#23601;&#26159;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#26126;&#30830;&#31034;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#22240;&#26524;&#21407;&#21017;&#23450;&#20041;&#20102;&#20004;&#31181;&#24863;&#30693;&#65292;&#21363;&#19981;&#24544;&#23454;&#24863;&#30693;&#21644;&#19981;&#19968;&#33268;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item></channel></rss>