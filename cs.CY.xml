<rss version="2.0"><channel><title>Chat Arxiv cs.CY</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CY</description><item><title>&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2311.18460</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#19979;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#25935;&#24863;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18460
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#30001;&#20110;&#27861;&#24459;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#21407;&#22240;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#35201;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#38598;&#20013;&#22312;&#27809;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#35774;&#32622;&#19978;&#65292;&#23613;&#31649;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#36829;&#21453;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19981;&#21516;&#26469;&#28304;&#30340;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#19979;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#26816;&#26597;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22312;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#30340;&#26032;&#22411;&#31070;&#32463;&#26694;&#26550;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#23545;&#22240;&#26524;&#20844;&#24179;&#24615;&#21487;&#33021;&#30001;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#32780;&#21463;&#21040;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18460v2 Announce Type: replace-cross  Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framewor
&lt;/p&gt;</description></item></channel></rss>