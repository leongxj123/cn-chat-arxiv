<rss version="2.0"><channel><title>Chat Arxiv cs.DM</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DM</description><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.08364</link><description>&lt;p&gt;
Plum: &#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plum: Prompt Learning using Metaheuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#20197;&#26469;&#65292;&#25552;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#20248;&#21270;&#21644;&#23450;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#29305;&#27530;&#25552;&#31034;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#29978;&#33267;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#20808;&#21069;&#26410;&#30693;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#26377;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#32531;&#24930;&#65292;&#20419;&#20351;&#20154;&#20204;&#28212;&#26395;&#19968;&#31181;&#36890;&#29992;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24456;&#23569;&#26377;&#28385;&#36275;&#8220;&#36890;&#29992;&#8221;&#30340;&#26631;&#20934;&#65292;&#21363;&#21516;&#26102;&#20855;&#22791;&#33258;&#21160;&#12289;&#31163;&#25955;&#12289;&#40657;&#30418;&#12289;&#26080;&#26799;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20803;&#21551;&#21457;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#31163;&#25955;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#25903;&#65292;&#25317;&#26377;100&#22810;&#31181;&#36873;&#39033;&#12290;&#22312;&#25105;&#20204;&#30340;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20845;&#31181;&#20856;&#22411;&#26041;&#27861;&#65306;&#29228;&#23665;&#12289;&#27169;&#25311;&#36864;&#28779;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;&#24102;/&#19981;&#24102;&#20132;&#21449;&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#21644;&#21644;&#35856;&#25628;&#32034;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30333;&#30418;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.13054</link><description>&lt;p&gt;
&#26080;&#35745;&#31639;&#22256;&#38590;&#30340;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#24403;&#32771;&#34385;&#23454;&#20307;&#38388;&#30340;&#23646;&#24615;&#20849;&#20139;&#26102;&#20250;&#33258;&#28982;&#20135;&#29983;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#23558;&#36229;&#36793;&#25193;&#23637;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23376;&#22270;&#26469;&#23558;&#36229;&#22270;&#36716;&#25442;&#20026;&#22270;&#65292;&#20294;&#36870;&#21521;&#25805;&#20316;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#22797;&#26434;&#19988;&#23646;&#20110;NP-complete&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36229;&#22270;&#21253;&#21547;&#27604;&#22270;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#25805;&#20316;&#36229;&#22270;&#27604;&#23558;&#20854;&#25193;&#23637;&#20026;&#22270;&#26356;&#20026;&#26041;&#20415;&#12290;&#36229;&#22270;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#31934;&#30830;&#39640;&#25928;&#22320;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#20272;&#35745;&#33410;&#28857;&#36317;&#31163;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#33410;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26041;&#27861;&#22312;&#36229;&#22270;&#19978;&#25191;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#25105;&#20204;&#23558;&#33410;&#28857;&#36317;&#31163;&#20272;&#35745;&#20026;&#38543;&#26426;&#28216;&#36208;&#30340;&#39044;&#26399;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#31616;&#21333;&#38543;&#26426;&#28216;&#36208;&#65288;SRW&#65289;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;"frustrated"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
&lt;/p&gt;</description></item></channel></rss>