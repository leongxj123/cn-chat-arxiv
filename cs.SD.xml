<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item></channel></rss>