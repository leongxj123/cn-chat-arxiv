<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10524</link><description>&lt;p&gt;
&#21457;&#25381;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#38646;-shot&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23398;&#20064;&#20013;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#25351;&#23548;&#25110;&#25552;&#31034;&#26469;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#36825;&#31181;&#38646;-shot&#33021;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21435;&#32416;&#27491;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#20013;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#35821;&#35328;&#30693;&#35782;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#28151;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#21644;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20854;&#20013;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;Llama2&#65289;&#34987;&#29992;&#20316;&#35299;&#30721;&#22120;&#30340;&#21069;&#31471;&#12290;&#36890;&#36807;CTC&#35299;&#30721;&#20174;&#32534;&#30721;&#22120;&#33719;&#24471;&#19968;&#20010;&#38656;&#35201;&#32416;&#27491;&#30340;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#25351;&#23548;&#19968;&#36215;&#36755;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#37319;&#21462;...
&lt;/p&gt;
&lt;p&gt;
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently tak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#36827;&#34892;&#21387;&#32553;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#32553;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10788</link><description>&lt;p&gt;
Whisper-KDQ: &#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#23454;&#29616;&#39640;&#25928;ASR&#30340;&#36731;&#22411;Whisper
&lt;/p&gt;
&lt;p&gt;
Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Whisper&#36827;&#34892;&#21387;&#32553;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#32553;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#30828;&#20214;&#36164;&#28304;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#25968;&#25454;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#26377;&#25928;&#25191;&#34892;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#29702;&#12289;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#20445;&#25345;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;Whisper&#12290;&#23398;&#29983;&#27169;&#22411;&#22522;&#20110;&#37327;&#21270;&#25439;&#22833;&#21644;&#33976;&#39311;&#25439;&#22833;&#36873;&#25321;&#33976;&#39311;&#21644;&#37327;&#21270;&#23618;&#12290;&#25105;&#20204;&#23558;$\text{Whisper}_\text{small}$&#21387;&#32553;&#21040;$\text{Whisper}_\text{base}$&#21644;$\text{Whisper}_\text{tiny}$&#32423;&#21035;&#65292;&#20351;$\text{Whisper}_\text{small}$&#20998;&#21035;&#23567;5.18x/10.48x&#12290;&#27492;&#22806;&#65292;&#19982;&#21407;&#22987;$\text{Whisper}_\text{base}$&#21644;$\text{Whisper}_\text{tiny}$&#30456;&#27604;&#65292;&#36824;&#26377;&#30456;&#23545;&#23383;&#31526;&#38169;&#35823;&#29575;&#38477;&#20302;.
&lt;/p&gt;
&lt;p&gt;
Due to the rapid development of computing hardware resources and the dramatic growth of data, pre-trained models in speech recognition, such as Whisper, have significantly improved the performance of speech recognition tasks. However, these models usually have a high computational overhead, making it difficult to execute effectively on resource-constrained devices. To speed up inference and reduce model size while maintaining performance, we propose a novel guided knowledge distillation and quantization for large pre-trained model Whisper. The student model selects distillation and quantization layers based on quantization loss and distillation loss, respectively. We compressed $\text{Whisper}_\text{small}$ to $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$ levels, making $\text{Whisper}_\text{small}$ 5.18x/10.48x smaller, respectively. Moreover, compared to the original $\text{Whisper}_\text{base}$ and $\text{Whisper}_\text{tiny}$, there is also a relative character erro
&lt;/p&gt;</description></item></channel></rss>