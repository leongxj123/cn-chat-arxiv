<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03568</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#30340;&#30690;&#37327;&#37327;&#21270;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#38754;&#30417;&#30563;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#65292;&#24050;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65288;VQ-MAE-AV&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#21407;&#22987;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;MAEs&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;VoxCeleb2&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#38899;&#35270;&#39057;SER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
&lt;/p&gt;</description></item></channel></rss>