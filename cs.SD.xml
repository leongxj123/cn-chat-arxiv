<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13276</link><description>&lt;p&gt;
&#24403;LLMs&#36935;&#21040;&#22768;&#23398;&#26631;&#24535;&#65306;&#19968;&#31181;&#39640;&#25928;&#22320;&#23558;&#35821;&#38899;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#25233;&#37057;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#26159;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#20851;&#20999;&#65292;&#20419;&#20351;&#36827;&#34892;&#22823;&#37327;&#30740;&#31350;&#26469;&#25506;&#35752;&#22522;&#20110;AI&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;AI&#25216;&#26415;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#24515;&#29702;&#21355;&#29983;&#24212;&#29992;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25972;&#20307;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#25233;&#37057;&#29366;&#24577;&#26041;&#38754;&#30340;&#21033;&#29992;&#20173;&#30456;&#23545;&#26410;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#26631;&#24535;&#23558;&#35821;&#38899;&#20449;&#21495;&#38598;&#25104;&#21040;LLMs&#20013;&#30340;&#39640;&#25928;&#25233;&#37057;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#26631;&#24535;&#65292;&#36825;&#20123;&#26631;&#24535;&#26159;&#29305;&#23450;&#20110;&#21475;&#35821;&#21333;&#35789;&#21457;&#38899;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25991;&#26412;&#36716;&#24405;&#28155;&#21152;&#20102;&#20851;&#38190;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13276v1 Announce Type: cross  Abstract: Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in mental healthcare applications. However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities. Furthermore, the utilization of LLMs in identifying and analyzing depressive states is still relatively untapped. In this paper, we present an innovative approach to integrating acoustic speech information into the LLMs framework for multimodal depression detection. We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12858</link><description>&lt;p&gt;
&#38750;&#21018;&#24615;&#25991;&#26412;&#25552;&#31034;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#32534;&#36753;&#27969;&#31243;&#33021;&#22815;&#21019;&#24314;&#19982;&#36755;&#20837;&#38899;&#39057;&#20445;&#25345;&#19968;&#33268;&#30340;&#38899;&#39057;&#32534;&#36753;&#32467;&#26524;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33021;&#22815;&#36827;&#34892;&#28155;&#21152;&#12289;&#39118;&#26684;&#36716;&#25442;&#21644;&#20462;&#22797;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#36825;&#20123;&#32534;&#36753;&#33021;&#22815;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#25991;&#26412;&#25552;&#31034;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;Audio-LDM&#30340;&#32467;&#26524;&#12290;&#23545;&#32467;&#26524;&#30340;&#23450;&#24615;&#26816;&#26597;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#21152;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#21407;&#22987;&#36215;&#22987;&#21644;&#32467;&#26463;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
&lt;/p&gt;</description></item></channel></rss>