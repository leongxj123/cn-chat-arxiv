<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36890;&#36807;&#20998;&#31163;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#20256;&#32479;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#33719;&#24471;21\%&#30340;&#35789;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07369</link><description>&lt;p&gt;
&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation. (arXiv:2309.07369v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07369
&lt;/p&gt;
&lt;p&gt;
&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36890;&#36807;&#20998;&#31163;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#20256;&#32479;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#33719;&#24471;21\%&#30340;&#35789;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#26041;&#24335;&#20013;&#32852;&#21512;&#20248;&#21270;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25991;&#26412;&#36866;&#24212;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#36866;&#24212;&#25991;&#26412;&#24050;&#25104;&#20026;&#22312;&#24037;&#19994;&#20013;&#37096;&#32626;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31995;&#32479;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21363;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#20256;&#32479;&#28151;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#23558;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#20998;&#31163;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#22495;&#22806;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#21322;&#28151;&#21512;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;21\%&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#24120;&#35268;&#27979;&#35797;&#38598;&#19978;&#30340;&#35789;&#38169;&#35823;&#29575;&#21482;&#26377;&#36731;&#24494;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based encoder-decoder (AED) speech recognition model has been widely successful in recent years. However, the joint optimization of acoustic model and language model in end-to-end manner has created challenges for text adaptation. In particular, effectively, quickly and inexpensively adapting text has become a primary concern for deploying AED systems in industry. To address this issue, we propose a novel model, the hybrid attention-based encoder-decoder (HAED) speech recognition model that preserves the modularity of conventional hybrid automatic speech recognition systems. Our HAED model separates the acoustic and language models, allowing for the use of conventional text-based language model adaptation techniques. We demonstrate that the proposed HAED model yields 21\% Word Error Rate (WER) improvements in relative when out-of-domain text data is used for language model adaptation, and with only a minor degradation in WER on a general test set compared with conventional AE
&lt;/p&gt;</description></item></channel></rss>