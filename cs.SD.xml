<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#20026;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#20339;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.12763</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#22797;&#29992;&#12289;&#22238;&#25910;&#65306;&#19982;&#20854;&#20182;&#35821;&#35328;&#22686;&#24378;&#30456;&#27604;&#65292;&#34987;&#25200;&#21160;&#25968;&#25454;&#23545;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models. (arXiv:2309.12763v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12763
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#20026;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#20339;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#24050;&#32463;&#25913;&#21892;&#20102;&#19979;&#28216;&#38899;&#32032;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21463;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;&#35757;&#32451;SSRL&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#36716;&#31227;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#39044;&#35757;&#32451;SSRL&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#19979;&#28216;&#20219;&#21153;&#30340;&#38899;&#32032;&#35782;&#21035;&#12290;&#25105;&#20204;&#23545;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#65292;&#21253;&#25324;&#38899;&#39640;&#21464;&#21270;&#12289;&#22122;&#22768;&#28155;&#21152;&#12289;&#26377;&#37325;&#38899;&#30340;&#30446;&#26631;&#35821;&#38899;&#21644;&#20854;&#20182;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#22909;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25968;&#37327;&#21644;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#22686;&#24378;&#25968;&#25454;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#20197;&#36798;&#21040;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#22495;&#35821;&#38899;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning (SSRL) has improved the performance on downstream phoneme recognition versus supervised models. Training SSRL models requires a large amount of pre-training data and this poses a challenge for low resource languages. A common approach is transferring knowledge from other languages. Instead, we propose to use audio augmentation to pre-train SSRL models in a low resource condition and evaluate phoneme recognition as downstream task. We performed a systematic comparison of augmentation techniques, namely: pitch variation, noise addition, accented target-language speech and other language speech. We found combined augmentations (noise/pitch) was the best augmentation strategy outperforming accent and language knowledge transfer. We compared the performance with various quantities and types of pre-training data. We examined the scaling factor of augmented data to achieve equivalent performance to models pre-trained with target domain speech. Our findi
&lt;/p&gt;</description></item></channel></rss>