<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10652</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#25490;&#21015;&#38382;&#39064;&#65292;&#23427;&#20204;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#35828;&#35805;&#32773;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#20998;&#31163;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#31435;&#24103;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#30340;&#28145;&#24230;&#27169;&#22359;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#65292;&#35828;&#35805;&#20154;&#30340;&#19981;&#21516;&#24103;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#32473;&#23450;&#37027;&#20010;&#35828;&#35805;&#20154;&#30340;&#38544;&#21547;&#26631;&#20934;&#24103;&#30340;&#22686;&#24378;&#29256;&#12290;&#35828;&#35805;&#20154;&#30340;&#24103;&#21253;&#21547;&#36275;&#22815;&#30340;&#38901;&#24459;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#26159;&#35821;&#38899;&#20998;&#31163;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23398;&#20064;&#32553;&#23567;&#24103;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
&lt;/p&gt;</description></item></channel></rss>