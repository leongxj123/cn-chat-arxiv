<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.02342</link><description>&lt;p&gt;
&#27468;&#35789;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Computational Analysis of Lyric Similarity Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#35745;&#31639;&#26041;&#27861;&#23545;&#27169;&#25311;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#22522;&#20110;BERT&#27169;&#22411;&#23884;&#20837;&#12289;&#27468;&#35789;&#38899;&#39057;&#21644;&#38899;&#32032;&#32452;&#20214;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#20855;&#26377;&#25351;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21253;&#21547;&#20154;&#22768;&#30340;&#38899;&#20048;&#20316;&#21697;&#20013;&#65292;&#27468;&#35789;&#23545;&#33402;&#26415;&#34920;&#36798;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#35813;&#31995;&#32479;&#24314;&#35758;&#31867;&#20284;&#20110;&#29992;&#25143;&#21916;&#29233;&#25110;&#20010;&#24615;&#21270;&#20559;&#22909;&#30340;&#27468;&#35789;&#65292;&#26377;&#21161;&#20110;&#22312;&#25968;&#30334;&#19975;&#38899;&#36712;&#20013;&#21457;&#29616;&#27468;&#35789;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31995;&#32479;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#20154;&#31867;&#23545;&#27468;&#35789;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#35745;&#31639;&#26041;&#27861;&#24314;&#27169;&#27468;&#35789;&#30456;&#20284;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12289;&#27468;&#35789;&#26469;&#28304;&#30340;&#38899;&#39057;&#20197;&#21450;&#38899;&#32032;&#32452;&#20214;&#30340;&#35745;&#31639;&#27169;&#22411;&#25351;&#31034;&#20102;&#24863;&#30693;&#19978;&#30340;&#27468;&#35789;&#30456;&#20284;&#24230;&#12290;&#35813;&#21457;&#29616;&#24378;&#35843;&#20102;&#35821;&#20041;&#12289;&#39118;&#26684;&#21644;&#38899;&#38901;&#30456;&#20284;&#24615;&#22312;&#20154;&#31867;&#24863;&#30693;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02342v1 Announce Type: new  Abstract: In musical compositions that include vocals, lyrics significantly contribute to artistic expression. Consequently, previous studies have introduced the concept of a recommendation system that suggests lyrics similar to a user's favorites or personalized preferences, aiding in the discovery of lyrics among millions of tracks. However, many of these systems do not fully consider human perceptions of lyric similarity, primarily due to limited research in this area. To bridge this gap, we conducted a comparative analysis of computational methods for modeling lyric similarity with human perception. Results indicated that computational models based on similarities between embeddings from pre-trained BERT-based models, the audio from which the lyrics are derived, and phonetic components are indicative of perceptual lyric similarity. This finding underscores the importance of semantic, stylistic, and phonetic similarities in human perception abo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12654</link><description>&lt;p&gt;
OWSM-CTC:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24320;&#25918;&#32534;&#30721;&#22120;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#23545;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#38750;&#24120;&#27969;&#34892;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#20250;&#27604;&#36739;&#24930;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#24187;&#35273;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#21516;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#21463;Open Whisper-style Speech Model (OWSM)&#39033;&#30446;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification (CTC)&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;18&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12654v1 Announce Type: new  Abstract: There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and languag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#32479;&#19968;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Speech-Text Pretraining for Spoken Dialog Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#30452;&#25509;&#29702;&#35299;&#21644;&#21512;&#25104;&#35821;&#38899;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35821;&#38899;&#25991;&#26412;LLM&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#65292;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19982;&#32473;&#23450;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#21644;&#26377;&#26426;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;LLM&#25152;&#23637;&#31034;&#30340;&#25512;&#29702;&#38142;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#21644;&#32423;&#32852;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#35814;&#32454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
&lt;/p&gt;</description></item></channel></rss>