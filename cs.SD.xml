<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20116;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TRILLsson&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01579</link><description>&lt;p&gt;
&#22810;&#37325;&#35821;&#35328;&#29615;&#22659;&#19979;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20116;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TRILLsson&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;PTM&#34920;&#31034;&#20316;&#20026;SER&#19979;&#28216;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#38024;&#23545;&#31038;&#20132;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;PTM&#22312;SER&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;PTM&#36824;&#27809;&#26377;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;SER&#35780;&#20272;&#65292;&#19988;&#21482;&#28041;&#21450;&#33521;&#35821;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20116;&#31181;PTM&#65288;TRILLsson&#12289;wav2vec2&#12289;XLS-R&#12289;x-vector&#12289;Whisper&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#35780;&#20272;&#31038;&#20132;&#35821;&#35328;PTM&#65288;TRILLsson&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#24773;&#22659;&#19979;&#23545;SER&#30340;&#25928;&#26524;&#12290;TRILLsson&#30340;&#34920;&#31034;&#22312;&#25152;&#26377;PTM&#20013;&#36798;&#21040;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;TRILLsson&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25968;&#25454;&#20013;&#30340;&#21508;&#31181;&#31038;&#20132;&#35821;&#35328;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;SER&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Models (PTMs) have facilitated substantial progress in the field of Speech Emotion Recognition (SER). SER is an area with applications ranging from HumanComputer Interaction to Healthcare. Recent studies have leveraged various PTM representations as input features for downstream models for SER. PTM specifically pre-trained for paralinguistic tasks have obtained state-of-the-art (SOTA) performance for SER. However, such PTM haven't been evaluated for SER in multilingual settings and experimented only with English. So, we fill this gap, by performing a comprehensive comparative study of five PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages. Representations from TRILLsson achieved the best performance among all the PTMs. This demonstrates that TRILLsson is able to effectively capture the various paralinguistic features from speech data for better SER. We also show that downstre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07421</link><description>&lt;p&gt;
Sumformer: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07421
&lt;/p&gt;
&lt;p&gt;
Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20381;&#36182;&#20110;&#33258;&#27880;&#24847;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#20196;&#29260;&#28151;&#21512;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#35821;&#38899;&#35821;&#21477;&#30340;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#23548;&#33268;&#25512;&#29702;&#12289;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#36895;&#24230;&#21464;&#24930;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#27604;&#33258;&#27880;&#24847;&#21147;&#26356;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#24456;&#38590;&#20445;&#35777;&#36798;&#21040;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#23454;&#38469;&#19978;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#26102;&#38388;&#19978;&#21576;&#20840;&#23616;&#24179;&#22343;&#21270;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#26102;&#38388;&#26367;&#20195;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;&#23427;&#29992;&#25152;&#26377;&#26102;&#38388;&#27493;&#38271;&#30340;&#21521;&#37327;&#30340;&#24179;&#22343;&#20540;&#26469;&#24635;&#32467;&#25972;&#20010;&#35821;&#21477;&#12290;&#28982;&#21518;&#23558;&#36825;&#20010;&#21333;&#19968;&#30340;&#24635;&#32467;&#19982;&#29305;&#23450;&#26102;&#38388;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#24635;&#32467;&#28151;&#21512;&#8221;&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#20013;&#24341;&#20837;&#24635;&#32467;&#28151;&#21512;&#65292;&#21487;&#20197;&#22312;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#22810;&#36798;27%&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25110;&#36229;&#36807;&#20808;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m
&lt;/p&gt;</description></item></channel></rss>