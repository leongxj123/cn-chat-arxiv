<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01172</link><description>&lt;p&gt;
&#27969;&#24335;&#24207;&#21015;&#36716;&#23548;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Sequence Transduction through Dynamic Compression
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01172
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;STAR&#65288;&#24102;&#26377;&#38170;&#23450;&#34920;&#31034;&#30340;&#27969;&#24335;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#12290;STAR&#21160;&#24577;&#22320;&#23545;&#36755;&#20837;&#27969;&#36827;&#34892;&#20998;&#27573;&#65292;&#21019;&#24314;&#21387;&#32553;&#30340;&#38170;&#23450;&#34920;&#31034;&#65292;&#23454;&#29616;&#36817;&#20046;&#26080;&#25439;&#30340;&#21387;&#32553;&#65288;12&#20493;&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;STAR&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#21644;&#24310;&#36831;-&#36136;&#37327;&#25240;&#34935;&#65292;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
&lt;/p&gt;</description></item></channel></rss>