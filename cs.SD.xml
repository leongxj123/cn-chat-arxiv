<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2309.01513</link><description>&lt;p&gt;
RGI-Net&#65306;&#22312;&#27809;&#26377;&#19968;&#38454;&#22238;&#22768;&#30340;&#24773;&#20917;&#19979;&#20174;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#25512;&#26029;3D&#25151;&#38388;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01513
&lt;/p&gt;
&lt;p&gt;
RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#20960;&#20309;&#26159;&#23454;&#29616;&#36924;&#30495;&#30340;3D&#38899;&#39057;&#28210;&#26579;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#21040;&#36798;&#26102;&#38388;&#65288;TOA&#65289;&#25110;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#20449;&#24687;&#21457;&#23637;&#20102;&#21508;&#31181;&#25151;&#38388;&#20960;&#20309;&#25512;&#26029;&#65288;RGI&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;RGI&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#20984;&#25151;&#38388;&#24418;&#29366;&#12289;&#24050;&#30693;&#22681;&#22721;&#25968;&#37327;&#21644;&#19968;&#38454;&#21453;&#23556;&#30340;&#21487;&#35265;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;RGI-Net&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#36848;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#20960;&#20309;&#12290;RGI-Net&#23398;&#20064;&#24182;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;RIRs&#65289;&#20013;&#30340;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#24418;&#29366;&#20026;&#38750;&#20984;&#24418;&#25110;RIRs&#20013;&#32570;&#23569;&#19968;&#38454;&#21453;&#23556;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#24418;&#29366;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20174;&#35013;&#26377;&#22278;&#24418;&#40614;&#20811;&#39118;&#30340;&#32039;&#20945;&#38899;&#39057;&#35774;&#22791;&#27979;&#37327;&#30340;RIRs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01513v2 Announce Type: replace-cross  Abstract: Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphon
&lt;/p&gt;</description></item><item><title>SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2305.08029</link><description>&lt;p&gt;
SongDriver2&#65306;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#19982;&#26580;&#21644;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08029
&lt;/p&gt;
&lt;p&gt;
SongDriver2&#23454;&#29616;&#20102;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#65292;&#24182;&#25552;&#20986;&#20102;&#26580;&#21644;&#36807;&#28193;&#26426;&#21046;&#65292;&#20351;&#38899;&#20048;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#38899;&#20048;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#24341;&#36215;&#29992;&#25143;&#29305;&#23450;&#24773;&#24863;&#20849;&#40483;&#30340;&#38899;&#20048;&#65292;&#22312;&#38899;&#20048;&#30103;&#27861;&#12289;&#28216;&#25103;&#37197;&#20048;&#21644;&#30005;&#24433;&#37197;&#20048;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24773;&#24863;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#24179;&#34913;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#21644;&#26580;&#21644;&#24773;&#24863;&#36716;&#25442;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#29616;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#32780;&#26580;&#21644;&#36807;&#28193;&#30340;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24433;&#21709;&#20102;&#38899;&#20048;&#30340;&#25972;&#20307;&#24773;&#24863;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SongDriver2&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#26368;&#21518;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38899;&#20048;&#24773;&#32490;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#24403;&#21069;&#26102;&#38388;&#27493;&#30340;&#30446;&#26631;&#36755;&#20837;&#24773;&#32490;&#34701;&#21512;&#12290;&#34701;&#21512;&#30340;&#24773;&#24863;&#38543;&#21518;&#20316;&#20026;SongDriver2&#26681;&#25454;&#36755;&#20837;&#26059;&#24459;&#25968;&#25454;&#29983;&#25104;&#21363;&#23558;&#21040;&#26469;&#30340;&#38899;&#20048;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#35843;&#25972;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#24773;&#24863;&#23454;&#26102;&#21305;&#37197;&#65292;&#20197;&#23454;&#29616;&#20004;&#31181;&#19981;&#21516;&#24773;&#24863;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36719;&#36807;&#28193;&#26426;&#21046;&#65292;&#23558;&#25554;&#20540;&#21644;&#24179;&#28369;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SongDriver2&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#24230;&#30495;&#23454;&#24615;&#21644;&#24179;&#28369;&#36807;&#28193;&#30340;&#24773;&#24863;&#38899;&#20048;&#65292;&#36825;&#34920;&#26126;&#20854;&#22312;&#22522;&#20110;&#24773;&#32490;&#30340;&#23454;&#26102;&#38899;&#20048;&#32534;&#25490;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item></channel></rss>