<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.13076</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#26435;&#37325;&#37117;&#26159;&#24179;&#31561;&#30340;: &#22312;&#35774;&#22791;&#19978;&#22686;&#24378;&#33021;&#25928;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13076
&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21442;&#25968;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#24635;&#20307;&#21151;&#32791;&#12290;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#21442;&#25968;&#23545;&#21151;&#32791;&#30340;&#24433;&#21709;&#22240;&#22810;&#31181;&#22240;&#32032;&#32780;&#24322;&#65292;&#21463;&#21040;&#35843;&#29992;&#39057;&#29575;&#21450;&#20854;&#22312;&#20869;&#23384;&#20013;&#30340;&#20301;&#32622;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#20973;&#20511;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#26088;&#22312;&#20248;&#21270;&#35774;&#22791;&#19978;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#20391;&#37325;&#20110;&#22312;&#23613;&#37327;&#19981;&#26174;&#33879;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#21151;&#32791;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#21464;&#21270;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;47%&#30340;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#21892;&#23454;&#26102;&#27969;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13076v1 Announce Type: cross  Abstract: Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08454</link><description>&lt;p&gt;
&#28151;&#21512;&#32534;&#30721;&#22120;&#25903;&#25345;&#36830;&#32493;&#35821;&#38899;&#20998;&#31163;&#29992;&#20110;&#20250;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23558;&#35821;&#38899;&#20998;&#31163;&#25104;&#26080;&#37325;&#21472;&#30340;&#27969;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;ASR&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22312;ASR&#27169;&#22411;&#20013;&#21253;&#21547;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#28151;&#21512;&#32534;&#30721;&#22120;&#21033;&#29992;&#21407;&#22987;&#37325;&#21472;&#30340;&#35821;&#38899;&#26469;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#24341;&#20837;&#30340;&#20266;&#24433;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31163;&#22120;&#65288;&#21253;&#25324;&#24378;&#22823;&#30340;TF-GridNet&#27169;&#22411;&#65289;&#35780;&#20272;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36824;&#23637;&#31034;&#20102;TF-GridNet&#30340;&#24378;&#22823;&#20998;&#31163;&#33021;&#21147;&#65292;&#22823;&#22823;&#32553;&#23567;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A commonmethod involves first separating the speech into overlap-free streams and then performing ASR on the resulting signals. Recently, the inclusion of a mixture encoder in the ASR model has been proposed. This mixture encoder leverages the original overlapped speech to mitigate the effect of artifacts introduced by the speech separation. Previously, however, the method only addressed two-speaker scenarios. In this work, we extend this approach to more natural meeting contexts featuring an arbitrary number of speakers and dynamic overlaps. We evaluate the performance using different speech separators, including the powerful TF-GridNet model. Our experiments show state-of-the-art performance on the LibriCSS dataset and highlight the advantages of the mixture encoder. Furthermore, they demonstrate the strong separation of TF-GridNet which largely closes the gap between previous m
&lt;/p&gt;</description></item></channel></rss>