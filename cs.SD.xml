<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.19333</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#39044;&#35757;&#32451;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compact Speech Translation Models via Discrete Speech Units Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#65292;&#20197;&#33976;&#39311;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#32039;&#20945;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#30701;&#25512;&#29702;&#31649;&#36947;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#31561;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#27169;&#22411;&#21021;&#22987;&#21270;&#22914;&#20170;&#22312;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20013;&#33719;&#24471;&#24378;&#22823;&#32467;&#26524;&#26159;&#24120;&#35265;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20250;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65292;&#38459;&#30861;&#20102;&#35774;&#22791;&#37096;&#32626;&#12290;&#26412;&#25991;&#21033;&#29992;SSL&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#31163;&#25955;&#35821;&#38899;&#21333;&#20803;&#65288;DSU&#65289;&#19978;&#39044;&#35757;&#32451;&#36739;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;1&#65289;Filterbank-to-DSU&#21644;2&#65289;DSU-to-Translation&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#21462;&#33258;1&#65289;&#30340;&#32534;&#30721;&#22120;&#21644;&#26469;&#33258;2&#65289;&#30340;&#35299;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#22312;&#26377;&#38480;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#19978;&#24494;&#35843;&#12290;&#36890;&#36807;&#20351;&#29992;DSU&#39044;&#35757;&#32451;&#26469;&#25552;&#28860;SSL&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#26368;&#32456;&#27169;&#22411;&#21464;&#24471;&#32039;&#20945;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20351;&#29992;DSU&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#27604;&#22914;&#25512;&#29702;&#31649;&#36947;&#26356;&#30701;&#21644;&#23545;&#65288;DSU&#65289;&#26631;&#35760;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;ASR&#39044;&#35757;&#32451;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#36716;&#24405;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#12290;&#22312;CoVoST-2 X-En&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.09361</link><description>&lt;p&gt;
MFAS: &#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;&#23436;&#32654;&#30340;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21508;&#31181;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#29702;&#35299;&#24773;&#24863;&#30340;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#37327;&#21270;&#24314;&#27169;&#30456;&#27604;&#65292;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#38899;&#20869;&#23481;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#20351;&#27169;&#22411;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#26681;&#25454;&#35821;&#38899;&#20013;&#23384;&#22312;&#30340;&#26576;&#20123;&#32447;&#32034;&#35843;&#25972;&#24773;&#24863;&#21333;&#35789;&#30340;&#25991;&#26412;&#35821;&#20041;&#30340;&#24863;&#30693;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#24182;&#25628;&#32034;&#20004;&#31181;&#20449;&#24687;&#30340;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#35843;&#25972;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Multiple perspectives Fusion Architecture Search(MFAS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition aims to identify and analyze emotional states in target speech similar to humans. Perfect emotion recognition can greatly benefit a wide range of human-machine interaction tasks. Inspired by the human process of understanding emotions, we demonstrate that compared to quantized modeling, understanding speech content from a continuous perspective, akin to human-like comprehension, enables the model to capture more comprehensive emotional information. Additionally, considering that humans adjust their perception of emotional words in textual semantic based on certain cues present in speech, we design a novel search space and search for the optimal fusion strategy for the two types of information. Experimental results further validate the significance of this perception adjustment. Building on these observations, we propose a novel framework called Multiple perspectives Fusion Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge to capt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02181</link><description>&lt;p&gt;
&#20851;&#20110;&#22768;&#38899;&#21311;&#21517;&#21270;&#23545;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#26816;&#27979;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection. (arXiv:2304.02181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#21311;&#21517;&#21270;&#22312; COVID-19 &#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#23545;&#35821;&#38899;&#35786;&#26029;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20174;&#20010;&#20154;&#21161;&#29702;&#12289;&#24773;&#24863;&#35745;&#31639;&#21040;&#36828;&#31243;&#30142;&#30149;&#35786;&#26029;&#12290;&#30001;&#20110;&#22768;&#38899;&#21516;&#26102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#29992;&#20449;&#24687;&#65288;&#22914;&#35821;&#38899;&#38899;&#35843;&#12289;&#35821;&#35843;&#12289;&#35821;&#36895;&#12289;&#22768;&#38899;&#22823;&#23567;&#65289;&#65292;&#22240;&#27492;&#20445;&#25252;&#35828;&#35805;&#32773;&#30340;&#38544;&#31169;&#21644;&#36523;&#20221;&#30340;&#22768;&#38899;&#21311;&#21517;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#22768;&#38899;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65292;&#37325;&#28857;&#26159;&#21435;&#38500;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24773;&#24863;&#35745;&#31639;&#21644;&#30142;&#30149;&#30417;&#27979;&#24212;&#29992;&#32780;&#35328;&#65292;&#35821;&#29992;&#20869;&#23481;&#21487;&#33021;&#26356;&#20026;&#20851;&#38190;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#21311;&#21517;&#21270;&#21487;&#33021;&#23545;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#19987;&#27880;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#65306;&#22522;&#20110;&#35821;&#38899;&#30340;COVID-19&#35786;&#26029;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#21450;&#20854;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;COVID-19&#35786;&#26029;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances seen in deep learning, voice-based applications are burgeoning, ranging from personal assistants, affective computing, to remote disease diagnostics. As the voice contains both linguistic and paralinguistic information (e.g., vocal pitch, intonation, speech rate, loudness), there is growing interest in voice anonymization to preserve speaker privacy and identity. Voice privacy challenges have emerged over the last few years and focus has been placed on removing speaker identity while keeping linguistic content intact. For affective computing and disease monitoring applications, however, the paralinguistic content may be more critical. Unfortunately, the effects that anonymization may have on these systems are still largely unknown. In this paper, we fill this gap and focus on one particular health monitoring application: speech-based COVID-19 diagnosis. We test two popular anonymization methods and their impact on five different state-of-the-art COVID-19 diagnostic system
&lt;/p&gt;</description></item></channel></rss>