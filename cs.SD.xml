<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.09949</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#23396;&#31435;&#30340;&#23581;&#35797;&#26469;&#21387;&#32553;Transformer&#65292;&#20294;&#30740;&#31350;&#20013;&#30340;&#35774;&#32622;&#21644;&#25351;&#26631;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#19981;&#21516;&#21387;&#32553;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#21387;&#32553;&#25216;&#26415;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#36825;&#20123;&#23396;&#31435;&#32467;&#26524;&#25552;&#20379;&#32972;&#26223;&#65292;&#30740;&#31350;&#20960;&#31181;&#24120;&#29992;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22681;&#38047;&#26102;&#38388;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#20056;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#21387;&#32553;&#26041;&#27861;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
&lt;/p&gt;</description></item></channel></rss>