<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10524</link><description>&lt;p&gt;
&#21457;&#25381;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#38646;-shot&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23398;&#20064;&#20013;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#25351;&#23548;&#25110;&#25552;&#31034;&#26469;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#36825;&#31181;&#38646;-shot&#33021;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21435;&#32416;&#27491;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#20013;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#35821;&#35328;&#30693;&#35782;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#28151;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#21644;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20854;&#20013;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;Llama2&#65289;&#34987;&#29992;&#20316;&#35299;&#30721;&#22120;&#30340;&#21069;&#31471;&#12290;&#36890;&#36807;CTC&#35299;&#30721;&#20174;&#32534;&#30721;&#22120;&#33719;&#24471;&#19968;&#20010;&#38656;&#35201;&#32416;&#27491;&#30340;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#25351;&#23548;&#19968;&#36215;&#36755;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#37319;&#21462;...
&lt;/p&gt;
&lt;p&gt;
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently tak
&lt;/p&gt;</description></item></channel></rss>