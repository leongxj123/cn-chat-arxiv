<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05010</link><description>&lt;p&gt;
RFWave&#65306;&#29992;&#20110;&#38899;&#39057;&#27874;&#24418;&#37325;&#24314;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05010
&lt;/p&gt;
&lt;p&gt;
RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#20174;&#19981;&#21516;&#34920;&#31034;&#20013;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#20010;&#21035;&#26679;&#26412;&#28857;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#22240;&#27492;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#24310;&#36831;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RFWave&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#23427;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#12290;RFWave&#22312;&#29983;&#25104;&#22797;&#26434;&#39057;&#35889;&#22270;&#24182;&#22312;&#24103;&#32423;&#21035;&#36816;&#34892;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#25152;&#26377;&#23376;&#24102;&#20197;&#22686;&#24378;&#25928;&#29575;&#12290;&#30001;&#20110;&#24076;&#26395;&#33719;&#24471;&#24179;&#32531;&#20256;&#36755;&#36712;&#36857;&#30340;&#25972;&#27969;&#27969;&#21160;&#65292;RFWave&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;RFWave&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05010v1 Announce Type: cross  Abstract: Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although diffusion models have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a spee
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.09508</link><description>&lt;p&gt;
&#25490;&#21015;&#12289;&#20462;&#22797;&#21644;&#25913;&#36827;&#65306;&#36890;&#36807;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#23454;&#29616;&#21487;&#25805;&#25511;&#30340;&#38271;&#26399;&#38899;&#20048;&#38899;&#39057;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#22312;&#20154;&#26426;&#38899;&#20048;&#20849;&#21019;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#38899;&#20048;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#35299;&#20915;&#38899;&#20048;&#20462;&#22797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;PEFT&#26041;&#27861;&#38598;&#25104;&#20102;&#22522;&#20110;&#24103;&#32423;&#20869;&#23481;&#30340;&#25511;&#21046;&#65292;&#20419;&#36827;&#20102;&#36712;&#36947;&#26465;&#20214;&#38899;&#20048;&#30340;&#31934;&#28860;&#21644;&#20998;&#25968;&#26465;&#20214;&#38899;&#20048;&#30340;&#25490;&#21015;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;MusicGen&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#33258;&#22238;&#24402;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;</title><link>http://arxiv.org/abs/2310.17162</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#38899;&#20048;&#38899;&#39057;&#39046;&#22495;&#20986;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#39640;&#36136;&#37327;&#38899;&#20048;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#65292;&#24182;&#19988;&#19968;&#20123;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#22312;&#38899;&#20048;&#19978;&#30340;&#25511;&#21046;&#33021;&#21147;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#36890;&#36807;&#20803;&#25968;&#25454;&#65288;&#22914;&#27468;&#25163;&#21644;&#20048;&#22120;&#65289;&#25110;&#39640;&#32423;&#34920;&#31034;&#65288;&#22914;&#27969;&#27966;&#21644;&#24773;&#24863;&#65289;&#38388;&#25509;&#22320;&#25551;&#36848;&#38899;&#20048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36827;&#19968;&#27493;&#25552;&#20379;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Coco-Mulla&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#20102;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#38899;&#39057;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#65292;&#21442;&#25968;&#35843;&#20248;&#30340;&#27604;&#20363;&#19981;&#21040;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.09505</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#36716;&#25442;&#20013;&#25512;&#36827;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts. (arXiv:2310.09505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35299;&#20915;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22768;&#23398;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#35821;&#38899;&#20998;&#24067;&#36716;&#25442;&#20013;&#38754;&#20020;&#30456;&#20284;&#30340;&#25361;&#25112;&#65292;&#20294;&#38024;&#23545;&#22768;&#23398;&#24314;&#27169;&#22312;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#29615;&#22659;&#19979;&#30340;TTA&#25216;&#26415;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#32771;&#34385;&#21040;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#28857;&#65306;1&#65289;&#23427;&#20204;&#20027;&#35201;&#26159;&#22522;&#20110;&#20855;&#26377;&#23618;&#24402;&#19968;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#26500;&#24314;&#30340;&#65307;2&#65289;&#23427;&#20204;&#20197;&#19968;&#31181;&#38750;&#38745;&#24577;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24230;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#35821;&#38899;&#25968;&#25454;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#24471;&#22312;&#35270;&#35273;&#32858;&#28966;&#30340;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#25209;&#24402;&#19968;&#21270;&#24182;&#20551;&#35774;&#29420;&#31435;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#38754;&#20020;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;TTA&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#12289;&#29109;&#36739;&#39640;&#30340;&#35821;&#38899;&#24103;&#36890;&#24120;&#24102;&#26377;&#20851;&#38190;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#35270;&#35273;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#22312;&#22768;&#23398;&#24314;&#27169;&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Tradit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;</title><link>http://arxiv.org/abs/2310.06930</link><description>&lt;p&gt;
&#12298;&#26377;&#22768;&#20070;&#30340;&#38901;&#24459;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Prosody Analysis of Audiobooks. (arXiv:2310.06930v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20351;&#24471;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#33258;&#28982;&#38899;&#25928;&#30340;&#38899;&#39057;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#22768;&#20070;&#26391;&#35835;&#28041;&#21450;&#21040;&#35835;&#32773;&#30340;&#25103;&#21095;&#24615;&#22768;&#38899;&#21644;&#35821;&#35843;&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#24773;&#24863;&#12289;&#23545;&#35805;&#21644;&#21465;&#36848;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;93&#26412;&#20070;&#19982;&#20854;&#23545;&#24212;&#30340;&#26377;&#22768;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#21465;&#36848;&#25991;&#26412;&#20013;&#39044;&#27979;&#38901;&#24459;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#38899;&#37327;&#21644;&#35821;&#36895;&#65289;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#39044;&#27979;&#30340;&#38901;&#24459;&#23646;&#24615;&#19982;&#20154;&#31867;&#26391;&#35835;&#30340;&#30456;&#20851;&#24615;&#35201;&#36828;&#39640;&#20110;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#30340;&#32467;&#26524;&#65306;&#22312;24&#26412;&#20070;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#39640;&#23545;22&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#20855;&#30456;&#20851;&#24615;&#65292;&#32780;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#37327;&#23646;&#24615;&#23545;23&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#21152;&#30456;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#36824;&#26159;&#21830;&#19994;&#32423;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
&lt;/p&gt;</description></item></channel></rss>