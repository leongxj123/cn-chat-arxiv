<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.08525</link><description>&lt;p&gt;
&#20174;&#24369;&#21040;&#24378;&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#22768;&#38899;&#20107;&#20214;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#65288;A-CPD&#65289;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#24341;&#23548;&#30340;&#38899;&#39057;&#24405;&#21046;&#27573;&#30340;&#24369;&#26631;&#31614;&#27880;&#37322;&#12290;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20851;&#20110;&#30446;&#26631;&#22768;&#38899;&#26102;&#38388;&#28608;&#27963;&#30340;&#20449;&#24687;&#33719;&#21462;&#37327;&#12290;&#23545;&#20110;&#27599;&#20010;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#24405;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#25512;&#23548;&#27010;&#29575;&#26354;&#32447;&#65292;&#29992;&#20110;&#25351;&#23548;&#27880;&#37322;&#12290;&#39044;&#27979;&#27169;&#22411;&#26368;&#21021;&#22312;&#21487;&#29992;&#30340;&#24102;&#26631;&#27880;&#22768;&#38899;&#20107;&#20214;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#31867;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#19981;&#30456;&#20132;&#12290;&#28982;&#21518;&#65292;&#39044;&#27979;&#27169;&#22411;&#36880;&#28176;&#36866;&#24212;&#27880;&#37322;&#32773;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#25552;&#20379;&#30340;&#27880;&#37322;&#12290;&#29992;&#20110;&#24341;&#23548;&#24369;&#26631;&#31614;&#27880;&#37322;&#32773;&#36208;&#21521;&#24378;&#26631;&#31614;&#30340;&#26597;&#35810;&#26159;&#20351;&#29992;&#36825;&#20123;&#27010;&#29575;&#19978;&#30340;&#21464;&#28857;&#26816;&#27979;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08525v1 Announce Type: cross  Abstract: In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favor
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.10305</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#40065;&#26834;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20840;&#23616;&#22768;&#23398;&#29305;&#24449;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#21442;&#32771;&#35821;&#38899;&#20013;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#26080;&#20851;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#22312;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#20013;&#20986;&#29616;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21442;&#32771;&#35821;&#38899;&#32534;&#30721;&#32593;&#32476;&#21644;&#20840;&#23616;&#20449;&#24687;&#20998;&#35299;&#32593;&#32476;&#36880;&#28176;&#20998;&#35299;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#19981;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#19987;&#38376;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#21046;Transformer&#26469;&#30830;&#20445;&#28151;&#21512;&#20449;&#21495;&#30340;&#22768;&#23398;&#34920;&#31034;&#19981;&#21463;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
&lt;/p&gt;</description></item></channel></rss>