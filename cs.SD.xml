<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;</title><link>http://arxiv.org/abs/2401.04152</link><description>&lt;p&gt;
&#36328;&#35828;&#35805;&#20154;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#19968;&#31181;&#30452;&#25509;&#36716;&#24405;&#22810;&#20010;&#35828;&#35805;&#20154;&#37325;&#21472;&#35821;&#38899;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;1&#65289;&#24102;&#26377;&#20998;&#25903;&#32534;&#30721;&#22120;&#30340;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;SIMO&#65289;&#27169;&#22411;&#65292;&#25110;&#32773;2&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#30340;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#26469;&#35299;&#20915;SIMO&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CSE&#27169;&#22411;&#19982;SOT&#30456;&#32467;&#21512;&#65292;&#26082;&#21457;&#25381;&#20102;SIMO&#21644;SISO&#30340;&#20248;&#21183;&#65292;&#21448;&#32531;&#35299;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#24037;&#20316;&#20195;&#34920;&#20102;&#23558;SIMO&#21644;SISO&#38598;&#25104;&#21040;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26089;&#26399;&#24037;&#20316;&#12290;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;LibrispeechMix&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CES&#27169;&#22411;&#30456;&#27604;&#20110;SIMO&#22522;&#20934;&#27169;&#22411;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;8%&#12290;CSE-SOT&#27169;&#22411;&#23558;WER&#38477;&#20302;&#20102;10%
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
&lt;/p&gt;</description></item><item><title>RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00169</link><description>&lt;p&gt;
RepCodec:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#26631;&#35760;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RepCodec: A Speech Representation Codec for Speech Tokenization. (arXiv:2309.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00169
&lt;/p&gt;
&lt;p&gt;
RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#22312;&#23558;&#35821;&#38899;&#27880;&#20837;LLMs&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23548;&#33268;&#20102;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepCodec&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#35821;&#38899;&#26631;&#35760;&#30340;&#26032;&#22411;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#12290;&#19982;&#37325;&#26032;&#26500;&#24314;&#21407;&#22987;&#38899;&#39057;&#30340;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#19981;&#21516;&#65292;RepCodec&#36890;&#36807;&#20174;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#22914;HuBERT&#25110;data2vec&#65289;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#26469;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#12290;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;&#32534;&#35299;&#30721;&#22120;&#21644;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#20849;&#21516;&#26500;&#25104;&#19968;&#20010;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#22686;&#24378;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore
&lt;/p&gt;</description></item></channel></rss>