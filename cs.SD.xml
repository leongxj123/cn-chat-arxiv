<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07937</link><description>&lt;p&gt;
&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Speech Robust Bench: A Robustness Benchmark For Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#30772;&#22351;&#19979;&#36827;&#34892;&#21487;&#38752;&#39044;&#27979;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;ASR&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;SRB&#30001;69&#20010;&#36755;&#20837;&#25200;&#21160;&#32452;&#25104;&#65292;&#26088;&#22312;&#27169;&#25311;ASR&#27169;&#22411;&#21487;&#33021;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#30772;&#22351;&#12290;&#25105;&#20204;&#20351;&#29992;SRB&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#65288;&#22914;&#31163;&#25955;&#34920;&#31034;&#21644;&#33258;&#25105;&#35757;&#32451;&#65289;&#20284;&#20046;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#26512;&#25193;&#23637;&#21040;&#34913;&#37327;ASR&#27169;&#22411;&#22312;&#26469;&#33258;&#21508;&#31181;&#20154;&#21475;&#20122;&#32452;&#30340;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#20197;&#21450;&#30007;&#24615;&#21644;&#22899;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;</title><link>https://arxiv.org/abs/2402.14285</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#25193;&#25955;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14285
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#29983;&#25104;&#38050;&#29748;&#21367;&#35889;&#65289;&#65292;&#25216;&#26415;&#37325;&#28857;&#25918;&#22312;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#19978;&#12290;&#38899;&#20048;&#35268;&#21017;&#36890;&#24120;&#20197;&#31526;&#21495;&#24418;&#24335;&#34920;&#36798;&#22312;&#38899;&#31526;&#29305;&#24449;&#19978;&#65292;&#22914;&#38899;&#31526;&#23494;&#24230;&#25110;&#21644;&#24358;&#36827;&#34892;&#65292;&#35768;&#22810;&#35268;&#21017;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#22312;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#24341;&#23548;&#25193;&#25955;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#25511;&#21046;&#24341;&#23548;&#65288;SCG&#65289;&#65292;&#23427;&#20165;&#38656;&#35201;&#23545;&#35268;&#21017;&#20989;&#25968;&#36827;&#34892;&#21069;&#21521;&#35780;&#20272;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#19968;&#36215;&#24037;&#20316;&#65292;&#20174;&#32780;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#30340;&#26080;&#35757;&#32451;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#21487;&#20197;&#19982;SCG&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#32452;&#21512;&#12290;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26631;&#20934;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.00223</link><description>&lt;p&gt;
FruitShell&#27861;&#35821;&#21512;&#25104;&#31995;&#32479;&#22312;Blizzard 2023&#25361;&#25112;&#36187;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The FruitShell French synthesis system at the Blizzard 2023 Challenge. (arXiv:2309.00223v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#31579;&#36873;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#28155;&#21152;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;Blizzard Challenge 2023&#30340;&#27861;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#20174;&#22899;&#24615;&#28436;&#35762;&#32773;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21644;&#29983;&#25104;&#19982;&#29305;&#23450;&#20010;&#20307;&#30456;&#20284;&#30340;&#35821;&#38899;&#12290;&#20851;&#20110;&#27604;&#36187;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31579;&#36873;&#36807;&#31243;&#65292;&#21435;&#38500;&#20102;&#32570;&#22833;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#38500;&#38899;&#32032;&#20197;&#22806;&#30340;&#25152;&#26377;&#31526;&#21495;&#36827;&#34892;&#20102;&#25972;&#29702;&#65292;&#24182;&#28040;&#38500;&#20102;&#27809;&#26377;&#21457;&#38899;&#25110;&#25345;&#32493;&#26102;&#38388;&#20026;&#38646;&#30340;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#25991;&#26412;&#20013;&#28155;&#21152;&#20102;&#35789;&#36793;&#30028;&#21644;&#36215;&#22987;/&#32467;&#26463;&#31526;&#21495;&#65292;&#26681;&#25454;&#25105;&#20204;&#20043;&#21069;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#36136;&#37327;&#12290;&#23545;&#20110;Spoke&#20219;&#21153;&#65292;&#25105;&#20204;&#26681;&#25454;&#27604;&#36187;&#35268;&#21017;&#36827;&#34892;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;G2P&#27169;&#22411;&#23558;&#27861;&#35821;&#25991;&#26412;&#36716;&#24405;&#20026;&#38899;&#32032;&#12290;&#30001;&#20110;G2P&#27169;&#22411;&#20351;&#29992;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#65292;&#25105;&#20204;&#23545;&#25552;&#20379;&#30340;&#27604;&#36187;&#25968;&#25454;&#24212;&#29992;&#20102;&#30456;&#21516;&#30340;&#36716;&#24405;&#36807;&#31243;&#65292;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32534;&#35793;&#22120;&#23545;&#26576;&#20123;&#25216;&#26415;&#38480;&#21046;&#30340;&#35782;&#21035;&#33021;&#21147;&#26377;&#38480;&#65292;&#25152;&#20197;&#25105;&#20204;&#20026;&#20102;&#20445;&#25345;&#31454;&#20105;&#30340;&#20844;&#27491;&#65292;&#23558;&#25968;&#25454;&#25353;&#38899;&#26631;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#29255;&#27573;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing 
&lt;/p&gt;</description></item></channel></rss>