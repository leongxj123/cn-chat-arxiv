<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13086</link><description>&lt;p&gt;
&#21487;&#21548;&#22270;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Listenable Maps for Audio Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13086
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#22797;&#26434;&#24615;&#32473;&#35299;&#37322;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#38899;&#39057;&#20449;&#21495;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20256;&#36798;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;&#30340;&#21487;&#21548;&#22270;&#65288;Listenable Maps for Audio Classifiers&#65292;L-MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#35299;&#37322;&#30340;&#21518;&#22788;&#29702;&#35299;&#37322;&#26041;&#27861;&#12290;L-MAC&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#20043;&#19978;&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#20108;&#20540;&#25513;&#30721;&#65292;&#31361;&#20986;&#26174;&#31034;&#36755;&#20837;&#38899;&#39057;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#29992;&#19968;&#31181;&#29305;&#27530;&#25439;&#22833;&#26469;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#35813;&#25439;&#22833;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#23545;&#36755;&#20837;&#38899;&#39057;&#30340;&#25513;&#30721;&#37096;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#25513;&#30721;&#37096;&#20998;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;L-MAC&#22987;&#32456;&#20135;&#29983;&#27604;&#20960;&#31181;&#26799;&#24230;&#21644;&#25513;&#30721;&#26041;&#27861;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13086v1 Announce Type: cross  Abstract: Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a special loss that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and maskin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04505</link><description>&lt;p&gt;
COVID-19&#26816;&#27979;&#31995;&#32479;&#65306;&#22522;&#20110;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#22768;&#23398;&#29305;&#24449;&#31995;&#32479;&#24615;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals. (arXiv:2309.04505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21628;&#21560;&#36947;&#30142;&#30149;&#22914;&#24863;&#20882;&#21644;&#27969;&#24863;&#12289;&#21742;&#21912;&#20197;&#21450;COVID-19&#31561;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24433;&#21709;&#30528;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#65292;&#21628;&#21560;&#22768;&#38899;&#34987;&#24191;&#27867;&#29992;&#20110;&#21307;&#30103;&#26381;&#21153;&#20013;&#65292;&#29992;&#20110;&#35786;&#26029;&#21508;&#31181;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#21644;&#32954;&#37096;&#30142;&#30149;&#12290;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#26412;&#39640;&#19988;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#21683;&#22013;&#38899;&#39057;&#35760;&#24405;&#34987;&#29992;&#26469;&#33258;&#21160;&#21270;&#26816;&#27979;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#21508;&#31181;&#22768;&#23398;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21683;&#22013;&#20449;&#21495;&#20013;&#26816;&#27979;COVID-19&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19977;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65288;MFCC&#65292;Chroma&#21644;Spectral Contrast&#29305;&#24449;&#65289;&#22312;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;SVM&#21644;MLP&#65289;&#19978;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a prac
&lt;/p&gt;</description></item></channel></rss>