<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2312.00502</link><description>&lt;p&gt;
&#23545;&#31283;&#20581;&#30340;OOD&#33258;&#30417;&#30563;&#23545;&#27604;&#24515;&#38899;&#22270;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#27963;&#21160;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#22312;&#21307;&#23398;&#31561;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#12290;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30701;&#32570;&#32463;&#24120;&#38459;&#30861;&#20102;&#24320;&#21457;&#31283;&#20581;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#27169;&#22411;&#65292;&#24403;&#38754;&#20020;&#26032;&#25910;&#38598;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#25928;&#26524;&#19979;&#38477;&#32780;&#21463;&#25439;&#12290;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#22686;&#21152;&#27169;&#22411;&#30340;&#25928;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23545;&#27604;SSL&#24212;&#29992;&#20110;&#26816;&#27979;1D&#24515;&#38899;&#22270;&#65288;PCG&#65289;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#65292;&#36890;&#36807;&#23398;&#20064;&#20449;&#21495;&#30340;&#24191;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#31181;&#22522;&#20110;&#38899;&#39057;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00502v2 Announce Type: replace  Abstract: Despite the recent increase in research activity, deep-learning models have not yet been widely accepted in several real-world settings, such as medicine. The shortage of high-quality annotated data often hinders the development of robust and generalizable models, which do not suffer from degraded effectiveness when presented with newly-collected, out-of-distribution (OOD) datasets. Contrastive Self-Supervised Learning (SSL) offers a potential solution to labeled data scarcity, as it takes advantage of unlabeled data to increase model effectiveness and robustness. In this research, we propose applying contrastive SSL for detecting abnormalities in 1D phonocardiogram (PCG) samples by learning a generalized representation of the signal. Specifically, we perform an extensive comparative evaluation of a wide range of audio-based augmentations, evaluate trained classifiers on multiple datasets across different downstream tasks, and finall
&lt;/p&gt;</description></item><item><title>JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.19180</link><description>&lt;p&gt;
JEN-1 Composer: &#19968;&#20010;&#29992;&#20110;&#39640;&#20445;&#30495;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19180
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#21512;&#25104;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#38899;&#36712;&#29983;&#25104;&#30340;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#21407;&#22987;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#21333;&#29420;&#32452;&#25104;&#21644;&#32452;&#21512;&#22810;&#38899;&#36712;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#20316;&#26354;&#23478;&#30340;&#20856;&#22411;&#24037;&#20316;&#27969;&#31243;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEN-1 Composer&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#39640;&#25928;&#22320;&#24314;&#27169;&#22810;&#38899;&#36712;&#38899;&#20048;&#30340;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#12290;JEN-1 Composer&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#22320;&#25972;&#21512;&#20219;&#20309;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#20363;&#22914;Jen-1&#65292;&#22686;&#24378;&#20854;&#22810;&#21151;&#33021;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35838;&#31243;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#36880;&#27493;&#25351;&#23548;&#27169;&#22411;&#20174;&#21333;&#38899;&#36712;&#29983;&#25104;&#21040;&#28789;&#27963;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
&lt;/p&gt;</description></item></channel></rss>