<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;</title><link>http://arxiv.org/abs/2401.17133</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#30340;&#20027;&#21160;&#24615;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65306;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;(SVC)&#36890;&#36807;&#23558;&#19968;&#20010;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#30446;&#26631;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#27468;&#35789;&#21644;&#26059;&#24459;&#65292;&#33258;&#21160;&#21270;&#20102;&#27468;&#26354;&#32763;&#21809;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#29256;&#26435;&#21644;&#20844;&#27665;&#26435;&#21033;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; SongBsAb&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20027;&#21160;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110; SVC &#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;SongBsAb &#22312;&#21457;&#24067;&#27468;&#21809;&#22768;&#38899;&#20043;&#21069;&#24341;&#20837;&#20102;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#36825;&#26679;&#24403;&#23427;&#20204;&#34987;&#20351;&#29992;&#26102;&#65292;SVC &#30340;&#29983;&#25104;&#36807;&#31243;&#23558;&#34987;&#24178;&#25200;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#27468;&#21809;&#22768;&#38899;&#12290; SongBsAb &#20855;&#26377;&#21452;&#37325;&#39044;&#38450;&#25928;&#26524;&#65292;&#24341;&#36215;&#27468;&#25163;&#36523;&#20221;&#21644;&#27468;&#35789;&#30340;&#28151;&#20081;&#65292;&#21363; SVC &#35206;&#30422;&#30340;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;&#20026;&#20102;&#25552;&#39640;&#25200;&#21160;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20197;&#20276;&#22863;&#26354;&#20316;&#20026;&#39069;&#22806;&#25513;&#34109;&#32773;&#30340;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.12850</link><description>&lt;p&gt;
&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization. (arXiv:2401.12850v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#35762;&#32773;&#20998;&#21106;&#26159;&#22522;&#20110;&#35828;&#35805;&#32773;&#36523;&#20221;&#23545;&#38899;&#39057;&#24405;&#38899;&#36827;&#34892;&#20998;&#21106;&#30340;&#37325;&#35201;&#35821;&#38899;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#20998;&#21106;&#26041;&#27861;&#28041;&#21450;&#22810;&#27425;&#23884;&#20837;&#25552;&#21462;&#21644;&#32858;&#31867;&#27493;&#39588;&#65292;&#36890;&#24120;&#20197;&#23396;&#31435;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#34429;&#28982;&#31471;&#21040;&#31471;&#30340;&#20998;&#21106;&#31995;&#32479;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#35757;&#32451;&#22797;&#26434;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;E-SHARC&#12290;E-SHARC&#26041;&#27861;&#20351;&#29992;&#21069;&#31471;mel-filterbank&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#23884;&#20837;&#25552;&#21462;&#22120;&#21644;GNN&#32858;&#31867;&#27169;&#22359;&#65292;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;E-SHARC&#36824;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications. The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion. While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The E-SHARC approach uses front-end mel-filterbank features as input and jointly learns an embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization. Further, with additional inputs from an external overlap detector, the E-SHARC app
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05037</link><description>&lt;p&gt;
&#23558;&#20219;&#20309;&#20320;&#25551;&#36848;&#30340;&#20107;&#29289;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Separate Anything You Describe. (arXiv:2308.05037v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26597;&#35810;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;LASS&#65289;&#26159;&#35745;&#31639;&#21548;&#35273;&#22330;&#26223;&#20998;&#26512;&#65288;CASA&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#33539; Paradigm&#12290;LASS&#26088;&#22312;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20174;&#38899;&#39057;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#30446;&#26631;&#22768;&#38899;&#65292;&#20026;&#25968;&#23383;&#38899;&#39057;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#30340;&#30028;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;LASS&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#31163;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#20048;&#22120;&#65292;&#26377;&#38480;&#31867;&#21035;&#30340;&#38899;&#39057;&#20107;&#20214;&#65289;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#22312;&#24320;&#25918;&#22495;&#20013;&#20998;&#31163;&#38899;&#39057;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudioSep&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;AudioSep&#65292;&#24182;&#23545;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#38899;&#39057;&#20107;&#20214;&#20998;&#31163;&#65292;&#20048;&#22120;&#20998;&#31163;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;AudioSep&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#38899;&#39057;&#26631;&#39064;&#25110;&#25991;&#23383;&#26631;&#31614;&#20316;&#20026;&#26597;&#35810;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperfor
&lt;/p&gt;</description></item></channel></rss>