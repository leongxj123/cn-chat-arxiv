<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;Transformer&#22312;&#35821;&#38899;&#30456;&#20851;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;Transformer&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.11607</link><description>&lt;p&gt;
&#35770;&#25991;&#32763;&#35793;&#65306;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;Transformer&#65306;&#32508;&#36848;&#65288;arXiv:2303.11607v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Transformers in Speech Processing: A Survey. (arXiv:2303.11607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;Transformer&#22312;&#35821;&#38899;&#30456;&#20851;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;Transformer&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#36215;&#20102;&#35821;&#38899;&#22788;&#29702;&#31038;&#21306;&#30340;&#20852;&#36259;&#65292;&#36827;&#32780;&#25506;&#32034;&#20102;&#20854;&#27169;&#25311;&#35821;&#38899;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#65292;Transformer &#22312;&#21508;&#31181;&#28041;&#21450;&#35821;&#38899;&#30340;&#39046;&#22495;&#20013;&#21517;&#22768;&#40522;&#36215;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#22768;&#35843;&#23398;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#21450;&#35768;&#22810;&#22810;&#27169;&#24577;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#19968;&#20221;&#32508;&#21512;&#24615;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#26725;&#25509;&#35821;&#38899;&#25216;&#26415;&#21508;&#23376;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#35821;&#38899;&#25216;&#26415;&#39046;&#22495;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;&#24076;&#26395;&#21033;&#29992;Transformer&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#25351;&#20986;&#20102;Transformer&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issue
&lt;/p&gt;</description></item></channel></rss>