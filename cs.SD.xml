<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07729</link><description>&lt;p&gt;
AIR-Bench: &#36890;&#36807;&#29983;&#25104;&#24615;&#29702;&#35299;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#23548;&#24615;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#23545;&#20154;&#19982;&#38899;&#39057;&#30340;&#20114;&#21160;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#33021;&#22815;&#35780;&#20272;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#20114;&#21160;&#33021;&#21147;&#30340;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#32570;&#20047;&#23545;&#22260;&#32469;&#38899;&#39057;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36861;&#36394;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;LALMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24182;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AIR-Bench&#65288;&#38899;&#39057;&#25351;&#23548;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LALMs&#29702;&#35299;&#21508;&#31181;&#31867;&#22411;&#38899;&#39057;&#20449;&#21495;&#65288;&#21253;&#25324;&#20154;&#31867;&#35821;&#38899;&#12289;&#33258;&#28982;&#22768;&#38899;&#21644;&#38899;&#20048;&#65289;&#20197;&#21450;&#19982;&#20154;&#20197;&#25991;&#26412;&#24418;&#24335;&#36827;&#34892;&#20132;&#20114;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;AIR-Bench&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#65306;&#22522;&#30784;&#21644;&#29983;&#25104;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.08620</link><description>&lt;p&gt;
&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Music Transformer. (arXiv:2306.08620v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;anticipation&#65288;&#39044;&#27979;&#65289;&#65306;&#19968;&#31181;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20107;&#20214;&#36807;&#31243;&#65288;&#26102;&#38388;&#28857;&#36807;&#31243;&#65289;&#30340;&#23454;&#29616;&#65292;&#20197;&#24322;&#27493;&#22320;&#25511;&#21046;&#19982;&#31532;&#20108;&#20010;&#30456;&#20851;&#36807;&#31243;&#65288;&#25511;&#21046;&#36807;&#31243;&#65289;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#38169;&#20107;&#20214;&#21644;&#25511;&#20214;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;&#25511;&#20214;&#20986;&#29616;&#22312;&#20107;&#20214;&#24207;&#21015;&#30340;&#20572;&#27490;&#26102;&#38388;&#20043;&#21518;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21160;&#26426;&#26469;&#33258;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#25511;&#21046;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;infiling&#65288;&#34917;&#20840;&#65289;&#25511;&#21046;&#20219;&#21153;&#65292;&#20854;&#20013;&#25511;&#21046;&#20107;&#20214;&#26159;&#20107;&#20214;&#26412;&#36523;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#26465;&#20214;&#29983;&#25104;&#23436;&#25104;&#32473;&#23450;&#22266;&#23450;&#25511;&#21046;&#20107;&#20214;&#30340;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22810;&#26679;&#30340;Lakh MIDI&#38899;&#20048;&#25968;&#25454;&#38598;&#35757;&#32451;&#39044;&#27979;infiling&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#19982;&#25552;&#31034;&#38899;&#20048;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#25191;&#34892;infilling&#25511;&#21046;&#20219;&#21153;&#30340;&#38468;&#21152;&#33021;&#21147;&#65292;&#21253;&#25324;&#20276;&#22863;&#12290;&#20154;&#24037;&#35780;&#20272;&#21592;&#25253;&#21578;&#35828;&#65292;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#30340;&#20276;&#22863;&#20855;&#26377;&#39640;&#21487;&#36776;&#24615;&#21644;&#20248;&#32654;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with
&lt;/p&gt;</description></item></channel></rss>