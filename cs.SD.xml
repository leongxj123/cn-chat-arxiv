<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2209.03275</link><description>&lt;p&gt;
&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Multimodal Speech Enhancement Using Burst Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MBURST&#30340;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#32771;&#34385;&#20102;&#26377;&#20851;&#21069;&#39069;&#21494;&#30382;&#23618;&#21644;&#20854;&#20182;&#33041;&#21306;&#37329;&#23383;&#22612;&#32454;&#32990;&#30340;&#26368;&#26032;&#31070;&#32463;&#23398;&#21457;&#29616;&#12290;&#25152;&#35859;&#30340;&#29190;&#21457;&#20256;&#25773;&#36890;&#36807;&#21453;&#39304;&#26041;&#24335;&#23454;&#29616;&#20102;&#20960;&#20010;&#20934;&#21017;&#65292;&#20197;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#26041;&#24335;&#35299;&#20915;&#20449;&#20219;&#20998;&#37197;&#38382;&#39064;&#65306;&#36890;&#36807;&#21453;&#39304;&#25511;&#21046;&#22609;&#24615;&#30340;&#31526;&#21495;&#21644;&#24133;&#24230;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26435;&#37325;&#36830;&#25509;&#22312;&#21508;&#23618;&#20043;&#38388;&#22810;&#36335;&#22797;&#29992;&#21453;&#39304;&#21644;&#21069;&#39304;&#20449;&#24687;&#65292;&#36817;&#20284;&#21453;&#39304;&#21644;&#21069;&#39304;&#36830;&#25509;&#65292;&#24182;&#32447;&#24615;&#21270;&#21453;&#39304;&#20449;&#21495;&#12290;MBURST&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36890;&#36807;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#21644;&#25233;&#21046;&#22122;&#22768;&#36171;&#20104;&#35821;&#38899;&#20197;&#21547;&#20041;&#12290;&#22312;Grid Corpus&#21644;&#22522;&#20110;CHiME3&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MBURST&#33021;&#22815;&#22797;&#29616;&#31867;&#20284;&#30340;&#25513;&#27169;&#37325;&#24314;&#65292;&#19982;&#22810;&#27169;&#24577;&#21453;&#21521;&#20256;&#25773;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the MBURST, a novel multimodal solution for audio-visual speech enhancements that consider the most recent neurological discoveries regarding pyramidal cells of the prefrontal cortex and other brain regions. The so-called burst propagation implements several criteria to address the credit assignment problem in a more biologically plausible manner: steering the sign and magnitude of plasticity through feedback, multiplexing the feedback and feedforward information across layers through different weight connections, approximating feedback and feedforward connections, and linearizing the feedback signals. MBURST benefits from such capabilities to learn correlations between the noisy signal and the visual stimuli, thus attributing meaning to the speech by amplifying relevant information and suppressing noise. Experiments conducted over a Grid Corpus and CHiME3-based dataset show that MBURST can reproduce similar mask reconstructions to the multimodal backpropagation-bas
&lt;/p&gt;</description></item></channel></rss>