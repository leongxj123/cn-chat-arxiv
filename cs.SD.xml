<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2309.07566</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#39118;&#26684;&#36716;&#25442;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#39118;&#26684;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38899;&#33394;&#20445;&#30041;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#36890;&#36807;&#31163;&#25955;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#30041;&#28304;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#38899;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39640;&#36136;&#37327;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#23545;&#20110;&#23398;&#20064;&#28304;&#35821;&#38899;&#21644;&#30446;&#26631;&#35821;&#38899;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#31163;&#25955;&#21333;&#20803;&#30340;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#39118;&#26684;&#36716;&#25442;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#30340;S2ST&#26694;&#26550;&#12290;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#19978;&#19979;&#25991;&#23398;&#20064;&#33719;&#24471;&#20102;&#39118;&#26684;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#20381;&#36182;&#20110;&#20219;&#20309;&#35828;&#35805;&#20154;&#24179;&#34892;&#25968;&#25454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#28304;&#35821;&#35328;&#19978;&#23454;&#29616;&#38646;-shot&#36328;&#35821;&#35328;&#39118;&#26684;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#32763;&#35793;&#35821;&#38899;&#20855;&#26377;&#39640;&#24230;&#30340;&#20445;&#30495;&#24230;&#21644;&#39118;&#26684;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.17395</link><description>&lt;p&gt;
WavCaps: &#19968;&#31181;ChatGPT&#36741;&#21161;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35821;&#35328;&#22810;&#27169;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. (arXiv:2303.17395v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#39057;-&#35821;&#35328;&#65288;AL&#65289;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21457;&#23637;&#38750;&#24120;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AL&#25968;&#25454;&#38598;&#25910;&#38598;&#36807;&#31243;&#26114;&#36149;&#36153;&#26102;&#65292;&#35268;&#27169;&#26377;&#38480;&#65292;&#32473;&#30740;&#31350;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WavCaps&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22823;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20174;Web&#36164;&#28304;&#21644;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#38899;&#39057;&#21098;&#36753;&#21450;&#21407;&#22987;&#25551;&#36848;&#12290;&#20294;&#26159;&#65292;&#22312;&#32447;&#25910;&#38598;&#21040;&#30340;&#21407;&#22987;&#25551;&#36848;&#38750;&#24120;&#22024;&#26434;&#65292;&#19981;&#36866;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#36807;&#28388;&#22024;&#26434;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#23383;&#24149;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#33258;&#21160;&#36807;&#28388;&#21644;&#36716;&#25442;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#23545;WavCaps&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps 
&lt;/p&gt;</description></item></channel></rss>