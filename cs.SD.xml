<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14982</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#22312;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#23637;&#29616;&#20986;&#19981;&#21516;&#27169;&#24335;&#65306;&#21021;&#27493;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14982
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#27809;&#26377;&#26174;&#31034;&#20986;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#20043;&#38388;&#30340;&#28165;&#26224;&#19981;&#21516;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#65292;&#36890;&#36807; EEG &#27979;&#37327;&#65292;&#22312;&#20010;&#20307;&#25509;&#35302;&#34394;&#20551;&#19982;&#30495;&#23454;&#38899;&#39057;&#26102;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#21021;&#27493;&#35777;&#25454;&#20026;&#26410;&#26469;&#22312;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10485</link><description>&lt;p&gt;
Grid&#21644;&#33258;&#28982;&#35821;&#21477;&#23545;Normal-to-Lombard&#36716;&#25442;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion. (arXiv:2309.10485v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grid&#21477;&#23376;&#24120;&#29992;&#20110;&#30740;&#31350;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#22522;&#20110;Grid&#21477;&#23376;&#35757;&#32451;&#30340;Normal-to-Lombard&#27169;&#22411;&#26159;&#21542;&#36275;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#38899;&#21487;&#25026;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24179;&#34892;&#30340;Lombard&#35821;&#26009;&#24211;&#65288;&#31216;&#20026;Lombard Chinese TIMIT&#65292;LCT&#65289;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20102;&#20013;&#25991;TIMIT&#30340;&#33258;&#28982;&#21477;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LCT&#21644;Enhanced Mandarin Lombard Grid&#35821;&#26009;&#24211;&#65288;EMALG&#65289;&#27604;&#36739;&#20102;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;Lombard&#25928;&#24212;&#30340;&#21442;&#25968;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#30340;&#21442;&#25968;&#21464;&#21270;&#30456;&#20284;&#65292;&#20294;&#22312;alpha&#27604;&#20363;&#22686;&#21152;&#26041;&#38754;&#65292;Grid&#21477;&#23376;&#30340;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#36328;&#24615;&#21035;&#21644;&#20449;&#22122;&#27604;&#30340;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the mode
&lt;/p&gt;</description></item></channel></rss>