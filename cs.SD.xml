<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;</title><link>https://rss.arxiv.org/abs/2402.01413</link><description>&lt;p&gt;
&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#20013;UDASE&#20219;&#21153;&#20013;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#24037;&#21512;&#25104;&#30340;&#24178;&#20928;&#35821;&#38899;&#21644;&#22122;&#22768;&#20449;&#21495;&#28151;&#21512;&#26469;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#35757;&#32451;&#26465;&#20214;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#22312;&#27979;&#35797;&#22495;&#19982;&#21512;&#25104;&#35757;&#32451;&#22495;&#26174;&#33879;&#19981;&#21516;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#26088;&#22312;&#21033;&#29992;&#27979;&#35797;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#22122;&#22768;&#35821;&#38899;&#24405;&#38899;&#26469;&#23545;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20010;&#27979;&#35797;&#22495;&#23545;&#24212;&#20110;CHiME-5&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#22312;&#22024;&#26434;&#21644;&#28151;&#21709;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#30495;&#23454;&#22810;&#35828;&#35805;&#20154;&#23545;&#35805;&#24405;&#38899;&#32452;&#25104;&#65292;&#26080;&#27861;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#24178;&#20928;&#35821;&#38899;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#20132;&#21040;CHiME-7 UDASE&#20219;&#21153;&#30340;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals. However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing. This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain. To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models. Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available. In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the resul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;</title><link>https://arxiv.org/abs/2404.01616</link><description>&lt;p&gt;
&#23558;LLMs&#36716;&#21270;&#20026;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#20165;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#36229;&#20986;&#20102;&#20855;&#26377;&#37197;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#65288;DE&#65289;&#30340;&#26816;&#32034;&#31995;&#32479;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#25237;&#24433;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#22312;&#26816;&#32034;&#21644;&#21452;&#35821;&#25991;&#26412;&#25366;&#25496;&#20013;&#23637;&#31034;&#20102;&#25104;&#21151;&#12290;&#20026;&#20102;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;LLM&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#26469;&#21305;&#37197;&#26816;&#32034;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;LLM-based&#26816;&#32034;&#31995;&#32479;&#33021;&#22815;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#23613;&#31649;&#21482;&#22312;21&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20808;&#21069;&#19987;&#38376;&#22312;&#25152;&#26377;102&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#25105;&#20204;&#22312;Recall@1&#19978;&#23454;&#29616;&#20102;10&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
&lt;/p&gt;</description></item><item><title>DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.17000</link><description>&lt;p&gt;
DistriBlock: &#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.17000
&lt;/p&gt;
&lt;p&gt;
DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#35823;&#23548;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20351;&#20854;&#39044;&#27979;&#20219;&#24847;&#30446;&#26631;&#25991;&#26412;&#65292;&#20174;&#32780;&#26500;&#25104;&#26126;&#26174;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistriBlock&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;ASR&#31995;&#32479;&#30340;&#39640;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#39044;&#27979;&#36755;&#20986;&#26631;&#35760;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#35813;&#20998;&#24067;&#30340;&#19968;&#32452;&#29305;&#24449;&#36827;&#34892;&#27979;&#37327;&#65306;&#36755;&#20986;&#27010;&#29575;&#30340;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#65292;&#20998;&#24067;&#30340;&#29109;&#65292;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#21644;Jensen-Shannon&#25955;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#20998;&#31867;&#12289;&#36825;&#31181;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14824</link><description>&lt;p&gt;
&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20083;&#21046;&#21697;&#24066;&#22330;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#20892;&#27665;&#24517;&#39035;&#19981;&#26029;&#25913;&#36827;&#20182;&#20204;&#30340;&#30044;&#29287;&#29983;&#20135;&#31995;&#32479;&#12290;&#31934;&#30830;&#30044;&#29287;&#19994;&#25216;&#26415;&#25552;&#20379;&#20102;&#21830;&#19994;&#20892;&#22330;&#21160;&#29289;&#20010;&#20307;&#21270;&#30417;&#27979;&#65292;&#20248;&#21270;&#30044;&#29287;&#29983;&#20135;&#12290;&#36830;&#32493;&#30340;&#22768;&#23398;&#30417;&#27979;&#26159;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#24863;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#33258;&#30001;&#25918;&#29287;&#29275;&#30340;&#26085;&#21453;&#21005;&#21644;&#21507;&#33609;&#26102;&#38388;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#29287;&#22330;&#19978;&#30340;&#20856;&#22411;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26126;&#26174;&#24433;&#21709;&#24403;&#21069;&#22768;&#23398;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#26041;&#27861;&#65292;&#31216;&#20026;&#25239;&#22122;&#22768;&#35269;&#39135;&#27963;&#21160;&#35782;&#21035;&#22120; (NRFAR)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#30830;&#23450;&#35269;&#39135;&#27963;&#21160;&#30340;&#31361;&#21457;&#12290;NRFAR &#30340;&#21152;&#24615;&#22122;&#22768;&#40065;&#26834;&#24615;&#20351;&#29992;&#38745;&#24577;&#39640;&#26031;&#30333;&#22122;&#22768;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#38750;&#38745;&#24577;&#33258;&#28982;&#22122;&#22768;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
&lt;/p&gt;</description></item></channel></rss>