<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item></channel></rss>