<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#32479;&#19968;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Speech-Text Pretraining for Spoken Dialog Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#30452;&#25509;&#29702;&#35299;&#21644;&#21512;&#25104;&#35821;&#38899;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35821;&#38899;&#25991;&#26412;LLM&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#65292;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19982;&#32473;&#23450;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#21644;&#26377;&#26426;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;LLM&#25152;&#23637;&#31034;&#30340;&#25512;&#29702;&#38142;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#21644;&#32423;&#32852;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#35814;&#32454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
&lt;/p&gt;</description></item></channel></rss>