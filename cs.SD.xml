<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06267</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26377;&#21161;&#20110;&#21333;&#27169;&#24577;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#30340;&#20132;&#21449;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20063;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#20351;&#29992;&#26469;&#33258;&#21333;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#36275;&#20197;&#25551;&#36848;&#25972;&#20010;&#27010;&#24565;&#31867;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38405;&#35835;&#20851;&#20110;&#29399;&#24182;&#21548;&#23427;&#20204;&#21536;&#21483;&#30340;&#22768;&#38899;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#35270;&#35273;&#29399;&#20998;&#31867;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26159;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#30340;&#29305;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#36328;&#36234;&#19981;&#21516;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#31867;&#21517;&#37325;&#26032;&#29992;&#20316;&#39069;&#22806;&#30340;&#19968;&#27425;&#24615;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
&lt;/p&gt;</description></item></channel></rss>