<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.08297</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;Transformer&#32467;&#26500;&#24182;&#21033;&#29992;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures. (arXiv:2206.08297v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#21363;&#20351;&#22312;&#23567;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#20063;&#20250;&#20135;&#29983;&#25968;&#21313;&#19975;&#20010;&#26679;&#26412;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;Transformer&#30340;&#20986;&#29616;&#65292;&#31070;&#32463;&#32467;&#26500;&#21464;&#24471;&#25797;&#38271;&#20110;&#23545;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20108;&#27425;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#33258;&#22238;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#30456;&#24403;&#22823;&#30340;&#19978;&#19979;&#25991;&#36229;&#36807;500,000&#20010;&#26679;&#26412;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;CNN&#21069;&#31471;&#26469;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#22312;&#36825;&#20123;&#34920;&#31034;&#20043;&#19978;&#23398;&#20064;&#20381;&#36182;&#39033;&#65292;&#23436;&#20840;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#20102;&#35757;&#32451;&#65306;&#20174;&#32780;&#20801;&#35768;&#23427;&#26681;&#25454;&#19979;&#19968;&#20010;&#26679;&#26412;&#33258;&#34892;&#23398;&#20064;&#34920;&#31034;&#12290;&#19982;&#20197;&#21069;&#29992;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#27604;&#36739;&#20197;&#23637;&#31034;&#25913;&#36827;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#25968;&#30446;&#30340;&#21442;&#25968;/&#19978;&#19979;&#25991;&#26174;&#31034;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-term dependencies for audio signals is a particularly challenging problem, as even small-time scales yield on the order of a hundred thousand samples. With the recent advent of Transformers, neural architectures became good at modeling dependencies over longer time scales, but they suffered from quadratic constraints to scale them. We propose a generative auto-regressive architecture that can model audio waveforms over quite a large context, greater than 500,000 samples. Our work is adapted to learn time dependencies by learning a latent representation by a CNN front-end, and then learning dependencies over these representations using Transformer encoders, fully trained end-to-end: thereby allowing to learn representations as it deems fit for the next sample. Unlike previous works that compared different time scales to show improvement, we use a standard dataset, with the same number of parameters/context to show improvements. We achieve a state-of-the-art performance as 
&lt;/p&gt;</description></item></channel></rss>