<rss version="2.0"><channel><title>Chat Arxiv q-fin</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-fin</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#21512;&#38669;&#20811;&#26031;&#36827;&#31243;&#24314;&#27169;&#38480;&#20215;&#21333;&#31807;&#21160;&#24577;&#21644;&#35746;&#21333;&#23610;&#23544;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26657;&#20934;&#20998;&#24067;&#25277;&#21462;&#27599;&#20010;&#20107;&#20214;&#30340;&#35746;&#21333;&#23610;&#23544;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#27491;&#30340;&#20215;&#24046;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#21442;&#25968;&#25903;&#25345;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26657;&#20934;&#38669;&#20811;&#26031;&#26680;&#20989;&#25968;&#21644;&#25233;&#21046;&#24615;&#20132;&#21449;&#28608;&#21457;&#26680;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2312.08927</link><description>&lt;p&gt;
&#38480;&#20215;&#21333;&#31807;&#21160;&#24577;&#19982;&#35746;&#21333;&#23610;&#23544;&#24314;&#27169;&#65306;&#22797;&#21512;&#38669;&#20811;&#26031;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#21512;&#38669;&#20811;&#26031;&#36827;&#31243;&#24314;&#27169;&#38480;&#20215;&#21333;&#31807;&#21160;&#24577;&#21644;&#35746;&#21333;&#23610;&#23544;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26657;&#20934;&#20998;&#24067;&#25277;&#21462;&#27599;&#20010;&#20107;&#20214;&#30340;&#35746;&#21333;&#23610;&#23544;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#27491;&#30340;&#20215;&#24046;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#21442;&#25968;&#25903;&#25345;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26657;&#20934;&#38669;&#20811;&#26031;&#26680;&#20989;&#25968;&#21644;&#25233;&#21046;&#24615;&#20132;&#21449;&#28608;&#21457;&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38669;&#20811;&#26031;&#36827;&#31243;&#24050;&#22312;&#25991;&#29486;&#20013;&#22810;&#31181;&#26041;&#24335;&#34987;&#29992;&#20110;&#27169;&#25311;&#38480;&#20215;&#21333;&#31807;&#21160;&#24577;&#65292;&#20294;&#24448;&#24448;&#20165;&#20851;&#27880;&#20107;&#20214;&#38388;&#38548;&#65292;&#32780;&#35746;&#21333;&#23610;&#23544;&#36890;&#24120;&#34987;&#20551;&#35774;&#20026;&#24120;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22797;&#21512;&#38669;&#20811;&#26031;&#36827;&#31243;&#26469;&#27169;&#25311;&#38480;&#20215;&#21333;&#31807;&#65292;&#20854;&#20013;&#27599;&#20010;&#20107;&#20214;&#30340;&#35746;&#21333;&#23610;&#23544;&#26469;&#33258;&#26657;&#20934;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26500;&#24314;&#65292;&#20351;&#36827;&#31243;&#30340;&#20215;&#24046;&#22987;&#32456;&#20445;&#25345;&#27491;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#26102;&#38388;&#26465;&#20214;&#27169;&#22411;&#21442;&#25968;&#20197;&#25903;&#25345;&#32463;&#39564;&#35266;&#23519;&#12290;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#26657;&#20934;&#38669;&#20811;&#26031;&#26680;&#20989;&#25968;&#65292;&#24182;&#20801;&#35768;&#25233;&#21046;&#24615;&#20132;&#21449;&#28608;&#21457;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32435;&#26031;&#36798;&#20811;&#20132;&#26131;&#25152;&#20013;&#19968;&#21482;&#32929;&#31080;&#30340;&#38480;&#20215;&#21333;&#31807;&#19978;&#30340;&#32467;&#26524;&#21644;&#36866;&#24230;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock's LOB in the NASDAQ exchange.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07393</link><description>&lt;p&gt;
&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#22312;&#20107;&#20214;&#26102;&#38388;&#19979;&#30340;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#24066;&#22330;&#29983;&#24577;&#31995;&#32479;&#65292;&#30001;&#19977;&#20010;&#33829;&#20859;&#32423;&#21035;&#20195;&#34920;&#65306;&#26368;&#20248;&#25191;&#34892;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#26368;&#23567;&#26234;&#33021;&#30340;&#27969;&#21160;&#24615;&#38656;&#35201;&#32773;&#21644;&#24555;&#36895;&#30340;&#30005;&#23376;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#12290;&#26368;&#20248;&#25191;&#34892;&#20195;&#29702;&#31867;&#21035;&#21253;&#25324;&#20080;&#20837;&#21644;&#21334;&#20986;&#20195;&#29702;&#65292;&#21487;&#20197;&#20351;&#29992;&#38480;&#20215;&#21333;&#21644;&#24066;&#20215;&#21333;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#24066;&#20215;&#21333;&#36827;&#34892;&#20132;&#26131;&#12290;&#22870;&#21169;&#20989;&#25968;&#26126;&#30830;&#24179;&#34913;&#20102;&#20132;&#26131;&#25191;&#34892;&#24046;&#20215;&#19982;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#31454;&#20105;&#23398;&#20064;&#26234;&#33021;&#20307;&#22914;&#20309;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#30340;&#22823;&#23567;&#21644;&#29992;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#20989;&#25968;&#24433;&#21709;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#31354;&#38388;&#22270;&#26469;&#30740;&#31350;ABM&#30340;&#21160;&#24577;&#65292;&#24403;&#29305;&#23450;&#35268;&#33539;&#34987;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
&lt;/p&gt;</description></item></channel></rss>