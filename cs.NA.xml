<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.02490</link><description>&lt;p&gt;
&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#25910;&#25947;&#29575;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02490
&lt;/p&gt;
&lt;p&gt;
&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#65288;AA&#65289;&#31639;&#27861;&#29992;&#20110;&#19981;&#21160;&#28857;&#26041;&#27861;&#65292;$x^{(k+1)}=q(x^{(k)})$&#12290;&#23427;&#39318;&#27425;&#35777;&#26126;&#20102;&#24403;&#31639;&#23376;$q$&#26159;&#32447;&#24615;&#19988;&#23545;&#31216;&#26102;&#65292;&#20351;&#29992;&#20808;&#21069;&#36845;&#20195;&#30340;&#28369;&#21160;&#31383;&#21475;&#30340;&#31383;&#21475;&#24335;AA&#31639;&#27861;&#33021;&#22815;&#25913;&#36827;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#65292;&#36229;&#36807;&#19981;&#21160;&#28857;&#36845;&#20195;&#12290;&#24403;$q$&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#22266;&#23450;&#28857;&#22788;&#20855;&#26377;&#23545;&#31216;&#38597;&#21487;&#27604;&#30697;&#38453;&#26102;&#65292;&#32463;&#36807;&#30053;&#24494;&#20462;&#25913;&#30340;AA&#31639;&#27861;&#34987;&#35777;&#26126;&#23545;&#27604;&#19981;&#21160;&#28857;&#36845;&#20195;&#20855;&#26377;&#31867;&#20284;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#25913;&#36827;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;Tyler&#30340;M&#20272;&#35745;&#20013;&#65292;AA&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#30340;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02490v2 Announce Type: replace-cross  Abstract: This paper studies the commonly utilized windowed Anderson acceleration (AA) algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the first proof that when the operator $q$ is linear and symmetric the windowed AA, which uses a sliding window of prior iterates, improves the root-linear convergence factor over the fixed-point iterations. When $q$ is nonlinear, yet has a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is proved to have an analogous root-linear convergence factor improvement over fixed-point iterations. Simulations verify our observations. Furthermore, experiments with different data models demonstrate AA is significantly superior to the standard fixed-point methods for Tyler's M-estimation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.14057</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Weighted least-squares approximation with determinantal point processes and generalized volume sampling. (arXiv:2312.14057v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#32473;&#23450;&#30340;m&#32500;&#31354;&#38388;V_m&#20013;&#30340;&#20803;&#32032;&#65292;&#20511;&#21161;&#20110;&#19968;&#20123;&#29305;&#24449;&#26144;&#23556;&#966;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#28857;x_1&#65292;...&#65292;x_n&#22788;&#30340;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#36924;&#36817;&#20989;&#25968;&#20174;L^2&#21040;&#20989;&#25968;&#12290;&#22312;&#22238;&#39038;&#19968;&#20123;&#20851;&#20110;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#28857;&#30340;&#26368;&#20248;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#30340;&#32467;&#26524;&#20043;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25237;&#24433;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#25110;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#12290;&#36825;&#20123;&#20998;&#24067;&#22312;&#36873;&#23450;&#30340;&#29305;&#24449;&#966;(x_i)&#20013;&#24341;&#20837;&#20102;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#65292;&#20351;&#29992;&#26679;&#26412;&#25968;n = O(mlog(m))&#24471;&#21040;&#20102;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#26399;&#26395;&#30340;L^2&#35823;&#24046;&#21463;&#21040;&#19968;&#20010;&#24120;&#25968;&#20056;&#20197;&#22312;L^2&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36827;&#19968;&#27493;&#20551;&#35774;&#20989;&#25968;&#22312;&#26576;&#20010;&#23884;&#20837;&#22312;L^2&#20013;&#30340;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;H&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36924;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is
&lt;/p&gt;</description></item></channel></rss>