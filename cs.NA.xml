<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.04937</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradient-free neural topology optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26799;&#24230;&#20248;&#21270;&#22120;&#21487;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#26080;&#35770;&#20854;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#25110;&#21487;&#24494;&#24615;&#22914;&#20309;&#65292;&#20294;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#30340;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#25299;&#25169;&#20248;&#21270;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#38382;&#39064;&#30340;&#32500;&#24230;&#20063;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#24403;&#22312;&#28508;&#22312;&#31354;&#38388;&#20248;&#21270;&#35774;&#35745;&#26102;&#65292;&#36845;&#20195;&#27425;&#25968;&#33267;&#23569;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#20351;&#29992;&#28508;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24191;&#27867;&#30340;&#35745;&#31639;&#23454;&#39564;&#65292;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#22522;&#20110;&#26799;&#24230;&#30340;&#25299;&#25169;&#20248;&#21270;&#23545;&#20110;&#21487;&#24494;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#32467;&#26500;&#30340;&#21512;&#35268;&#24615;&#20248;&#21270;&#65292;&#20173;&#28982;&#26356;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#37027;&#20123;&#38656;&#35201;&#26080;&#26799;&#24230;&#26041;&#27861;&#30340;&#38382;&#39064;&#24320;&#36767;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04937v1 Announce Type: new  Abstract: Gradient-free optimizers allow for tackling problems regardless of the smoothness or differentiability of their objective function, but they require many more iterations to converge when compared to gradient-based algorithms. This has made them unviable for topology optimization due to the high computational cost per iteration and high dimensionality of these problems. We propose a pre-trained neural reparameterization strategy that leads to at least one order of magnitude decrease in iteration count when optimizing the designs in latent space, as opposed to the conventional approach without latent reparameterization. We demonstrate this via extensive computational experiments in- and out-of-distribution with the training data. Although gradient-based topology optimization is still more efficient for differentiable problems, such as compliance optimization of structures, we believe this work will open up a new path for problems where gra
&lt;/p&gt;</description></item></channel></rss>