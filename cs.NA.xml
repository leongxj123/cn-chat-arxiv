<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10710</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification with neural networks with quadratic decision functions. (arXiv:2401.10710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#20316;&#20026;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#19968;&#31181;&#20248;&#21183;&#65292;&#24403;&#38656;&#35201;&#35782;&#21035;&#30340;&#23545;&#35937;&#20855;&#26377;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#65288;&#22914;&#22278;&#24418;&#12289;&#26925;&#22278;&#24418;&#31561;&#65289;&#26102;&#65292;&#36825;&#31181;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#31867;&#38382;&#39064;&#19978;&#20351;&#29992;&#36825;&#31181;&#20551;&#35774;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#20102;&#35813;&#31639;&#27861;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;Tensorflow&#21644;Keras&#36719;&#20214;&#20013;&#21487;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one. They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.02736</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#24179;&#28369;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65306;MaxPool&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the numerical reliability of nonsmooth autodiff: a MaxPool case study. (arXiv:2401.02736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#31934;&#24230;&#32423;&#21035;&#65288;16&#20301;&#12289;32&#20301;&#12289;64&#20301;&#65289;&#21644;&#21367;&#31215;&#26550;&#26500;&#65288;LeNet&#12289;VGG&#21644;ResNet&#65289;&#20197;&#21450;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#21644;ImageNet&#65289;&#19978;&#30340;AD&#34892;&#20026;&#12290;&#23613;&#31649;AD&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#20960;&#20046;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#65288;&#22914;MaxPool&#21644;ReLU&#65289;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65288;&#32780;&#19981;&#26159;&#23454;&#25968;&#65289;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;AD&#21487;&#33021;&#22312;&#25968;&#20540;&#19978;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#21253;&#25324;&#20998;&#27495;&#21306;&#65288;AD&#22312;&#23454;&#25968;&#19978;&#19981;&#27491;&#30830;&#65289;&#21644;&#34917;&#20607;&#21306;&#65288;AD&#22312;&#28014;&#28857;&#25968;&#19978;&#19981;&#27491;&#30830;&#20294;&#22312;&#23454;&#25968;&#19978;&#27491;&#30830;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;SGD&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#30740;&#31350;&#20102;MaxPool&#38750;&#24179;&#28369;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#19981;&#21516;&#36873;&#25321;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19809</link><description>&lt;p&gt;
MgNO:&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
MgNO: Efficient Parameterization of Linear Operators via Multigrid. (arXiv:2310.19809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#26469;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#27604;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#31639;&#23376;&#23618;&#20013;&#31532;$i$&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#65292;&#35760;&#20316;$\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$&#12290;&#20854;&#20013;&#65292;$\mathcal W_{ij}$&#34920;&#31034;&#36830;&#25509;&#31532;$j$&#20010;&#36755;&#20837;&#31070;&#32463;&#20803;&#21644;&#31532;$i$&#20010;&#36755;&#20986;&#31070;&#32463;&#20803;&#30340;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#32780;&#20559;&#24046;$\mathcal B_{ij}$&#37319;&#29992;&#20989;&#25968;&#24418;&#24335;&#32780;&#38750;&#26631;&#37327;&#24418;&#24335;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31070;&#32463;&#20803;&#65288;Banach&#31354;&#38388;&#65289;&#20043;&#38388;&#26377;&#25928;&#21442;&#25968;&#21270;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;MgNO&#24341;&#20837;&#20102;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#26082;&#20855;&#22791;&#20102;&#25968;&#23398;&#20005;&#23494;&#24615;&#65292;&#21448;&#20855;&#22791;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;MgNO&#28040;&#38500;&#20102;&#23545;&#20256;&#32479;&#30340;lifting&#21644;projecting&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting ope
&lt;/p&gt;</description></item></channel></rss>