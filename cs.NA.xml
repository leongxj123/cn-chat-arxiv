<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.00181</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#29699;&#30340;&#23481;&#37327;&#38543;&#21322;&#24452;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#33021;&#22815;&#23558;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#23884;&#20837;&#20854;&#20013;&#32780;&#19981;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#25968;&#22686;&#38271;&#30340;&#24615;&#36136;&#24120;&#24120;&#23548;&#33268;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#36229;&#20960;&#20309;&#23398;&#20064;&#27169;&#22411;&#26377;&#26102;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;NaN&#38382;&#39064;&#21644;&#28014;&#28857;&#31639;&#26415;&#20013;&#36935;&#21040;&#26080;&#27861;&#34920;&#31034;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;&#8212;&#8212;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#65292;&#22312;64&#20301;&#31639;&#26415;&#31995;&#32479;&#19979;&#65292;Poincar\'e&#29699;&#30456;&#23545;&#20110;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#33021;&#21147;&#26469;&#27491;&#30830;&#34920;&#31034;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;Lorentz&#27169;&#22411;&#20248;&#20110;Poincar\'e&#29699;&#30340;&#20248;&#36234;&#24615;&#12290;&#37492;&#20110;&#20004;&#31181;&#27169;&#22411;&#30340;&#25968;&#20540;&#38480;&#21046;&#65292;&#25105;&#20204;&#30830;&#23450;&#19968;&#31181;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#20043;&#22806;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
&lt;/p&gt;</description></item></channel></rss>