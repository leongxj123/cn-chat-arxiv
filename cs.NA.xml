<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.20708</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26399;&#26395;&#25913;&#36827;&#30340;&#24847;&#22806;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Unexpected Improvements to Expected Improvement for Bayesian Optimization. (arXiv:2310.20708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#21487;&#20197;&#35828;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#27969;&#34892;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24456;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;EI&#30340;&#24615;&#33021;&#24448;&#24448;&#34987;&#19968;&#20123;&#26032;&#26041;&#27861;&#36229;&#36234;&#12290;&#23588;&#20854;&#26159;&#65292;EI&#21450;&#20854;&#21464;&#31181;&#22312;&#24182;&#34892;&#21644;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#33719;&#24471;&#20540;&#22312;&#35768;&#22810;&#21306;&#22495;&#20013;&#25968;&#20540;&#19978;&#21464;&#20026;&#38646;&#12290;&#24403;&#35266;&#27979;&#27425;&#25968;&#22686;&#21152;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#22256;&#38590;&#36890;&#24120;&#20250;&#22686;&#21152;&#65292;&#23548;&#33268;&#24615;&#33021;&#22312;&#25991;&#29486;&#20013;&#19981;&#19968;&#33268;&#19988;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20122;&#20248;&#21270;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogEI&#65292;&#36825;&#26159;&#19968;&#31867;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#12290;&#19982;&#26631;&#20934;EI&#30456;&#27604;&#65292;&#36825;&#20123;LogEI&#20989;&#25968;&#30340;&#25104;&#21592;&#35201;&#20040;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#35201;&#20040;&#20855;&#26377;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#20540;&#30149;&#24577;&#22312;&#8220;&#32463;&#20856;&#8221;&#20998;&#26512;EI&#12289;&#26399;&#26395;&#36229;&#20307;&#31215;&#25913;&#36827;&#65288;EHVI&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their
&lt;/p&gt;</description></item></channel></rss>