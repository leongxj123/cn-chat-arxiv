<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;</title><link>https://arxiv.org/abs/2403.12278</link><description>&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12278
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#38543;&#26426;&#33293;&#20837;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27969;&#34892;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#30697;&#38453;$\mathbf{A}$&#30340;&#38543;&#26426;&#36817;&#20284;&#33293;&#20837;&#65292;&#20854;&#20013;&#34892;&#25968;&#36828;&#36828;&#22810;&#20110;&#21015;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35777;&#25454;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#25903;&#25345;&#65292;&#39640;&#27010;&#29575;&#19979;&#65292;&#38543;&#26426;&#33293;&#20837;&#30697;&#38453;&#30340;&#26368;&#23567;&#22855;&#24322;&#20540;&#36828;&#31163;&#38646;--&#26080;&#35770;$\mathbf{A}$&#25509;&#36817;&#22855;&#24322;&#36824;&#26159;$\mathbf{A}$&#22855;&#24322;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#26426;&#33293;&#20837;\textit{&#38544;&#24335;&#27491;&#21017;&#21270;}&#39640;&#30246;&#30697;&#38453;$\mathbf{A}$&#65292;&#20351;&#24471;&#33293;&#20837;&#21518;&#30340;&#29256;&#26412;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#26377;&#21147;&#32467;&#26524;&#65292;&#20197;&#21450;&#38543;&#26426;&#33293;&#20837;&#35823;&#24046;&#19981;&#38598;&#20013;&#22312;&#20302;&#32500;&#21015;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12481</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;DeepFool&#65306;&#27867;&#21270;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revisiting DeepFool: generalization and improvement. (arXiv:2303.12481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#36755;&#20837;&#31245;&#21152;&#20462;&#25913;&#20415;&#20250;&#23548;&#33268;&#32593;&#32476;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#23545;&#27492;&#31867;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#26368;&#23567;l2&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#26159;&#19968;&#31181;&#29305;&#21035;&#37325;&#35201;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#19981;&#22826;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;&#28145;&#24230;&#27450;&#39575;&#65288;DeepFool&#65289;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26131;&#20110;&#29702;&#35299;&#21644;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#20063;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal l2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large
&lt;/p&gt;</description></item></channel></rss>