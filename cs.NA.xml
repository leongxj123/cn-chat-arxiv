<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21521;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#20013;&#28155;&#21152;&#20960;&#20309;&#24179;&#28369;&#21160;&#37327;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#20851;&#20110;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#26041;&#21521;&#19978;&#30340;&#26399;&#26395;&#35823;&#24046;&#12290;&#25968;&#20540;&#31034;&#20363;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09415</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20309;&#24179;&#28369;&#21160;&#37327;&#30340;&#38543;&#26426;Kaczmarz&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Kaczmarz with geometrically smoothed momentum. (arXiv:2401.09415v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21521;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#20013;&#28155;&#21152;&#20960;&#20309;&#24179;&#28369;&#21160;&#37327;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#20851;&#20110;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#26041;&#21521;&#19978;&#30340;&#26399;&#26395;&#35823;&#24046;&#12290;&#25968;&#20540;&#31034;&#20363;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21521;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#20013;&#28155;&#21152;&#20960;&#20309;&#24179;&#28369;&#21160;&#37327;&#30340;&#25928;&#26524;&#65292;&#35813;&#31639;&#27861;&#26159;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#19978;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#23450;&#20041;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30340;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#26041;&#21521;&#19978;&#26399;&#26395;&#35823;&#24046;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#35828;&#26126;&#25105;&#20204;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the effect of adding geometrically smoothed momentum to the randomized Kaczmarz algorithm, which is an instance of stochastic gradient descent on a linear least squares loss function. We prove a result about the expected error in the direction of singular vectors of the matrix defining the least squares loss. We present several numerical examples illustrating the utility of our result and pose several questions.
&lt;/p&gt;</description></item></channel></rss>