<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38543;&#26426;&#21270;&#25311;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#26469;&#25552;&#39640;&#20613;&#37324;&#21494;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#22659;&#19979;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#35299;&#20915;&#39640;&#25928;&#23450;&#20215;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02832</link><description>&lt;p&gt;
&#39640;&#25928;&#20613;&#37324;&#21494;&#23450;&#20215;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#25311;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quasi-Monte Carlo for Efficient Fourier Pricing of Multi-Asset Options
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38543;&#26426;&#21270;&#25311;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#26469;&#25552;&#39640;&#20613;&#37324;&#21494;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#22659;&#19979;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#35299;&#20915;&#39640;&#25928;&#23450;&#20215;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23450;&#37327;&#37329;&#34701;&#20013;&#65292;&#39640;&#25928;&#23450;&#20215;&#22810;&#36164;&#20135;&#26399;&#26435;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;&#26041;&#27861;&#20173;&#28982;&#26159;&#23450;&#20215;&#24341;&#25806;&#30340;&#20027;&#35201;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#24930;&#38459;&#30861;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20613;&#37324;&#21494;&#26041;&#27861;&#21033;&#29992;&#29305;&#24449;&#20989;&#25968;&#30340;&#30693;&#35782;&#65292;&#20934;&#30830;&#24555;&#36895;&#22320;&#20272;&#20540;&#22810;&#36798;&#20004;&#20010;&#36164;&#20135;&#30340;&#26399;&#26435;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#24120;&#29992;&#30340;&#31215;&#20998;&#25216;&#26415;&#20855;&#26377;&#24352;&#37327;&#31215;&#65288;TP&#65289;&#32467;&#26500;&#65292;&#23427;&#20204;&#38754;&#20020;&#38556;&#30861;&#12290;&#26412;&#25991;&#20027;&#24352;&#20351;&#29992;&#38543;&#26426;&#21270;&#25311;&#33945;&#29305;&#21345;&#27931;&#65288;RQMC&#65289;&#31215;&#20998;&#26469;&#25913;&#21892;&#39640;&#32500;&#20613;&#37324;&#21494;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;RQMC&#25216;&#26415;&#21463;&#30410;&#20110;&#34987;&#31215;&#20989;&#25968;&#30340;&#20809;&#28369;&#24615;&#65292;&#32531;&#35299;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;RQMC&#22312;&#26080;&#30028;&#22495;$\mathbb{R}^d$&#19978;&#30340;&#36866;&#29992;&#24615;&#38656;&#35201;&#23558;&#22495;&#36716;&#25442;&#20026;$[0,1]^d$&#65292;&#36825;&#21487;&#33021;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02832v1 Announce Type: new  Abstract: Efficiently pricing multi-asset options poses a significant challenge in quantitative finance. The Monte Carlo (MC) method remains the prevalent choice for pricing engines; however, its slow convergence rate impedes its practical application. Fourier methods leverage the knowledge of the characteristic function to accurately and rapidly value options with up to two assets. Nevertheless, they face hurdles in the high-dimensional settings due to the tensor product (TP) structure of commonly employed quadrature techniques. This work advocates using the randomized quasi-MC (RQMC) quadrature to improve the scalability of Fourier methods with high dimensions. The RQMC technique benefits from the smoothness of the integrand and alleviates the curse of dimensionality while providing practical error estimates. Nonetheless, the applicability of RQMC on the unbounded domain, $\mathbb{R}^d$, requires a domain transformation to $[0,1]^d$, which may r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2303.03984</link><description>&lt;p&gt;
&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#22686;&#24378;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization. (arXiv:2303.03984v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a class of enhanced momentum-based gradient descent ascent methods (MSGDA and AdaMSGDA) to solve nonconvex-PL minimax problems, where the AdaMSGDA algorithm can use various adaptive learning rates to update variables x and y without relying on any global and coordinate-wise adaptive learning rates. Theoretical analysis shows that MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of O(&#949;&#8722;3) in finding an &#949;-stationary solution.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;&#21363;$\min_x\max_y f(x,y)$&#65289;&#65292;&#20854;&#20013;$f(x,y)$&#22312;$x$&#19978;&#21487;&#33021;&#26159;&#38750;&#20984;&#30340;&#65292;&#22312;$y$&#19978;&#26159;&#38750;&#20985;&#30340;&#65292;&#24182;&#28385;&#36275;Polyak-Lojasiewicz&#65288;PL&#65289;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#65288;&#21363;$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$&#65292;&#20854;&#20013;$F(x)=\max_y f(x,y)$&#65289;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we study a class of nonconvex nonconcave minimax optimization problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition in $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any global and coordinate-wise adaptive learning rates. Theoretically, we present an effective convergence analysis framework for our methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring one sample at each loop in finding an $\epsilon$-stationary solution (i.e., $\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This manuscript 
&lt;/p&gt;</description></item></channel></rss>