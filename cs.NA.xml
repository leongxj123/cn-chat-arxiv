<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16368</link><description>&lt;p&gt;
&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65306;&#23398;&#20064;&#20849;&#36717;&#26799;&#24230;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31185;&#23398;&#35745;&#31639;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#26367;&#25442;&#19982;&#20849;&#36717;&#26799;&#24230;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65289;&#26174;&#30528;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#21463;&#31232;&#30095;&#30697;&#38453;&#29702;&#35770;&#21551;&#21457;&#30340;&#26032;&#22411;&#28040;&#24687;&#20256;&#36882;&#22359;&#65292;&#23427;&#19982;&#23547;&#25214;&#30697;&#38453;&#30340;&#31232;&#30095;&#20998;&#35299;&#30340;&#30446;&#26631;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#38382;&#39064;&#21644;&#26469;&#33258;&#31185;&#23398;&#35745;&#31639;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#22987;&#32456;&#20248;&#20110;&#26368;&#24120;&#35265;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#22120;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#30340;Cholesky&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
&lt;/p&gt;</description></item></channel></rss>