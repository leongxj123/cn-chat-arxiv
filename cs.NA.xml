<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01047</link><description>&lt;p&gt;
Tensor PCA&#30340;&#21151;&#29575;&#36845;&#20195;&#30340;&#23574;&#38160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sharp Analysis of Power Iteration for Tensor PCA. (arXiv:2401.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;Richard&#21644;Montanari&#65288;2014&#65289;&#24341;&#20837;&#30340;Tensor PCA&#27169;&#22411;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#12290;&#20043;&#21069;&#30740;&#31350;Tensor&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#30340;&#24037;&#20316;&#35201;&#20040;&#20165;&#38480;&#20110;&#22266;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Tensor&#21151;&#29575;&#36845;&#20195;&#30340;&#21160;&#24577;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#25968;&#37327;&#32423;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#19979;&#65292;&#21151;&#29575;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#26893;&#20449;&#21495;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23454;&#38469;&#30340;&#31639;&#27861;&#38408;&#20540;&#27604;&#25991;&#29486;&#20013;&#29468;&#27979;&#30340;&#35201;&#23567;&#19968;&#20010;polylog(n)&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#26159;&#29615;&#22659;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21151;&#29575;&#36845;&#20195;&#20572;&#27490;&#20934;&#21017;&#65292;&#21487;&#20197;&#20445;&#35777;&#36755;&#20986;&#19982;&#30495;&#23454;&#20449;&#21495;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the power iteration algorithm for the tensor PCA model introduced in Richard and Montanari (2014). Previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. In this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. Our contributions are threefold: First, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. Second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. Finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.15285</link><description>&lt;p&gt;
Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#19968;&#20010;&#20219;&#24847;&#30340;&#22312;$\mathbb{R}^{d+1}$&#31354;&#38388;&#20013;&#30340;Zonoid&#21487;&#20197;&#36890;&#36807;$n$&#20010;&#32447;&#27573;&#30340;Hausdorff&#36317;&#31163;&#26469;&#36924;&#36817;&#30340;&#35823;&#24046;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#27973;&#23618;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#21464;&#20998;&#31354;&#38388;&#20013;&#30340;&#22343;&#21248;&#33539;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;&#29575;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#24050;&#32463;&#22312;$d \neq 2, 3$&#26102;&#24471;&#21040;&#35299;&#20915;&#65292;&#20294;&#24403;$d = 2, 3$&#26102;&#65292;&#26368;&#20248;&#19978;&#30028;&#21644;&#26368;&#20248;&#19979;&#30028;&#20043;&#38388;&#20173;&#23384;&#22312;&#19968;&#20010;&#23545;&#25968;&#24046;&#36317;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#24046;&#36317;&#65292;&#23436;&#25104;&#20102;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#22343;&#21248;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
&lt;/p&gt;</description></item></channel></rss>