<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.18517</link><description>&lt;p&gt;
&#38024;&#23545;&#27491;&#21017;&#21270;&#38750;&#36127;&#23610;&#24230;&#19981;&#21464;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18517
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#29702;&#35299;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#24182;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#38750;&#36127;&#20302;&#31209;&#36924;&#36817;&#65292;&#22914;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#25110;&#31232;&#30095;&#30340;&#38750;&#36127;Tucker&#20998;&#35299;&#65292;&#26159;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#38477;&#32500;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#22240;&#32032;&#29305;&#24615;&#20197;&#21450;&#32570;&#20047;&#25903;&#25345;&#36825;&#20123;&#36873;&#25321;&#30340;&#29702;&#35770;&#65292;&#27491;&#21017;&#21270;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#39640;&#25928;&#31639;&#27861;&#30340;&#35774;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#31216;&#20026;&#22343;&#21248;&#27491;&#21017;&#21270;&#23610;&#24230;&#19981;&#21464;&#30340;&#26356;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#23548;&#33268;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#30410;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#20013;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#25351;&#23548;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18517v1 Announce Type: new  Abstract: Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;GAROM&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15881</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Reduced Order Modelling. (arXiv:2305.15881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;GAROM&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;GAROM&#12290;GAN&#22312;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#31616;&#21270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;GAN&#21644;ROM&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23558;&#37492;&#21035;&#22120;&#32593;&#32476;&#24314;&#27169;&#20026;&#33258;&#32534;&#30721;&#22120;&#65292;&#25552;&#21462;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#23558;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20316;&#20026;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#30340;&#36755;&#20837;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present GAROM, a new approach for reduced order modelling (ROM) based on generative adversarial networks (GANs). GANs have the potential to learn data distribution and generate more realistic data. While widely applied in many areas of deep learning, little research is done on their application for ROM, i.e. approximating a high-fidelity model with a simpler one. In this work, we combine the GAN and ROM framework, by introducing a data-driven generative adversarial model able to learn solutions to parametric differential equations. The latter is achieved by modelling the discriminator network as an autoencoder, extracting relevant features of the input, and applying a conditioning mechanism to the generator and discriminator networks specifying the differential equation parameters. We show how to apply our methodology for inference, provide experimental evidence of the model generalisation, and perform a convergence study of the method.
&lt;/p&gt;</description></item></channel></rss>