<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.02438</link><description>&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02438
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#22312;&#25955;&#20081;&#25968;&#25454;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36890;&#24120;&#38656;&#35201;&#22788;&#29702;&#35768;&#22810;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#19977;&#35282;&#20989;&#25968;&#25110;&#23567;&#27874;&#30340;&#29305;&#24449;&#26144;&#23556;&#26469;&#35299;&#20915;SVM&#30340;&#21407;&#22987;&#24418;&#24335;&#12290;&#22312;&#23567;&#32500;&#24230;&#35774;&#32622;&#20013;&#65292;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#21644;&#30456;&#20851;&#26041;&#27861;&#26159;&#22788;&#29702;&#25152;&#32771;&#34385;&#22522;&#20989;&#25968;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;&#32500;&#25968;&#28798;&#38590;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#21464;&#24471;&#20302;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38480;&#21046;&#33258;&#24049;&#20351;&#29992;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#65292;&#27599;&#20010;&#22522;&#20989;&#25968;&#21482;&#20381;&#36182;&#20110;&#23569;&#25968;&#20960;&#20010;&#32500;&#24230;&#12290;&#36825;&#26159;&#30001;&#20110;&#25928;&#24212;&#30340;&#31232;&#30095;&#24615;&#21644;&#26368;&#36817;&#20851;&#20110;&#20989;&#25968;&#20174;&#25955;&#20081;&#25968;&#25454;&#20013;&#30340;&#25130;&#26029;&#26041;&#24046;&#20998;&#35299;&#30340;&#37325;&#24314;&#30340;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#21160;&#26426;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#32806;&#21512;&#26041;&#38754;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machines (SVMs) are an important tool for performing classification on scattered data, where one usually has to deal with many data points in high-dimensional spaces. We propose solving SVMs in primal form using feature maps based on trigonometric functions or wavelets. In small dimensional settings the Fast Fourier Transform (FFT) and related methods are a powerful tool in order to deal with the considered basis functions. For growing dimensions the classical FFT-based methods become inefficient due to the curse of dimensionality. Therefore, we restrict ourselves to multivariate basis functions, each one of them depends only on a small number of dimensions. This is motivated by the well-known sparsity of effects and recent results regarding the reconstruction of functions from scattered data in terms of truncated analysis of variance (ANOVA) decomposition, which makes the resulting model even interpretable in terms of importance of the features as well as their coupling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#36873;&#25321;&#33258;&#36866;&#24212;&#28857;&#23545;&#39044;&#35757;&#32451;&#30340;&#36817;&#20284;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36880;&#28176;&#20943;&#23569;&#24314;&#27169;&#35823;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#35299;&#20915;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17844</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25805;&#20316;&#21592;&#23398;&#20064;&#29992;&#20110;&#26080;&#38480;&#32500;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adaptive operator learning for infinite-dimensional Bayesian inverse problems. (arXiv:2310.17844v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#36873;&#25321;&#33258;&#36866;&#24212;&#28857;&#23545;&#39044;&#35757;&#32451;&#30340;&#36817;&#20284;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36880;&#28176;&#20943;&#23569;&#24314;&#27169;&#35823;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#35299;&#20915;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;(BIPs)&#20013;&#30340;&#22522;&#26412;&#35745;&#31639;&#38382;&#39064;&#28304;&#20110;&#38656;&#35201;&#37325;&#22797;&#36827;&#34892;&#27491;&#21521;&#27169;&#22411;&#35780;&#20272;&#30340;&#35201;&#27714;&#12290;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#30340;&#19968;&#31181;&#24120;&#35265;&#31574;&#30053;&#26159;&#36890;&#36807;&#25805;&#20316;&#21592;&#23398;&#20064;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#20195;&#26114;&#36149;&#30340;&#27169;&#22411;&#27169;&#25311;&#65292;&#36825;&#21463;&#21040;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#36817;&#20284;&#27169;&#22411;&#21487;&#33021;&#24341;&#20837;&#24314;&#27169;&#35823;&#24046;&#65292;&#21152;&#21095;&#20102;&#36870;&#38382;&#39064;&#24050;&#32463;&#23384;&#22312;&#30340;&#30149;&#24577;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26377;&#25928;&#23454;&#26045;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#22312;&#23616;&#37096;&#21306;&#22495;&#20013;&#20934;&#30830;&#25311;&#21512;&#30340;&#20195;&#29702;&#36880;&#28176;&#20943;&#23569;&#24314;&#27169;&#35823;&#24046;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#36873;&#25321;&#30340;&#33258;&#36866;&#24212;&#28857;&#22312;&#21453;&#28436;&#36807;&#31243;&#20013;&#23545;&#39044;&#35757;&#32451;&#30340;&#36817;&#20284;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#31639;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#27491;&#21521;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental computational issues in Bayesian inverse problems (BIPs) governed by partial differential equations (PDEs) stem from the requirement of repeated forward model evaluations. A popular strategy to reduce such cost is to replace expensive model simulations by computationally efficient approximations using operator learning, motivated by recent progresses in deep learning. However, using the approximated model directly may introduce a modeling error, exacerbating the already ill-posedness of inverse problems. Thus, balancing between accuracy and efficiency is essential for the effective implementation of such approaches. To this end, we develop an adaptive operator learning framework that can reduce modeling error gradually by forcing the surrogate to be accurate in local areas. This is accomplished by fine-tuning the pre-trained approximate model during the inversion process with adaptive points selected by a greedy algorithm, which requires only a few forward model evaluat
&lt;/p&gt;</description></item></channel></rss>