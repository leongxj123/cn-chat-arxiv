<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13572</link><description>&lt;p&gt;
&#35770;&#19968;&#31181;&#21464;&#31181;Looped Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of a Variant of the Looped Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13572
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22312;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#31185;&#23398;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#26041;&#38754;&#65292;Transformer&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#20174;&#34920;&#36798;&#33021;&#21147;&#21644;&#21151;&#33021;&#24615;&#35282;&#24230;&#35299;&#37322;&#65292;&#26631;&#20934;&#30340;Transformer&#33021;&#22815;&#25191;&#34892;&#19968;&#20123;&#31639;&#27861;&#12290;&#20026;&#20102;&#36171;&#20104;Transformer&#31639;&#27861;&#33021;&#21147;&#65292;&#24182;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;Looped Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#22359;&#65292;&#21517;&#20026;Algorithm Transformer&#65288;&#31616;&#31216;AlgoFormer&#65289;&#12290;&#19982;&#26631;&#20934;Transformer&#21644;&#32431;&#31929;&#30340;Looped Transformer&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AlgoFormer&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#31034;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21463;&#20154;&#31867;&#35774;&#35745;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;Transformer&#22359;&#21253;&#25324;&#19968;&#20010;&#36127;&#36131;&#36827;&#34892;ta
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2308.13840</link><description>&lt;p&gt;
&#21463;&#26368;&#20248;&#20256;&#36755;&#21551;&#21457;&#30340;&#24930;&#34928;&#20943;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#21033;&#29992;Sinkhorn&#25439;&#22833;&#21644;Wasserstein&#26680;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel. (arXiv:2308.13840v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#27169;&#22411;&#65288;ROMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#20013;&#20197;&#22788;&#29702;&#39640;&#32500;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ROM&#26041;&#27861;&#21487;&#33021;&#21482;&#33021;&#37096;&#20998;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#21253;&#25324;&#24213;&#23618;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23545;&#31934;&#30830;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;ROM&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;Wasserstein&#36317;&#31163;&#20026;&#33258;&#23450;&#20041;&#26680;&#30340;&#26680;Proper&#27491;&#20132;&#20998;&#35299;&#65288;kPOD&#65289;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#39640;&#25928;&#22320;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;OT&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#30340;&#22343;&#26041;&#35823;&#24046;&#25110;&#20132;&#21449;&#29109;&#31561;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Reduced order models (ROMs) are widely used in scientific computing to tackle high-dimensional systems. However, traditional ROM methods may only partially capture the intrinsic geometric characteristics of the data. These characteristics encompass the underlying structure, relationships, and essential features crucial for accurate modeling.  To overcome this limitation, we propose a novel ROM framework that integrates optimal transport (OT) theory and neural network-based methods. Specifically, we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method exploiting the Wasserstein distance as the custom kernel, and we efficiently train the resulting neural network (NN) employing the Sinkhorn algorithm. By leveraging an OT-based nonlinear reduction, the presented framework can capture the geometric structure of the data, which is crucial for accurate learning of the reduced solution manifold. When compared with traditional metrics such as mean squared error or cross-entropy,
&lt;/p&gt;</description></item></channel></rss>