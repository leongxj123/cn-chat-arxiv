<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.13704</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#22120;&#36890;&#24120;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#23545;&#24212;&#20110;&#22312;&#38750;&#24120;&#23567;&#30340;&#23398;&#20064;&#36895;&#29575;&#38480;&#21046;&#19979;&#30340;&#22522;&#26412;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#32463;&#20856;Adam&#31639;&#27861;&#26159;&#24213;&#23618;ODE&#30340;&#19968;&#38454;&#38544;&#24335;&#26174;&#24335;(IMEX) Euler&#31163;&#25955;&#21270;&#12290;&#20174;&#26102;&#38388;&#31163;&#25955;&#21270;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#38454;IMEX&#26041;&#27861;&#26469;&#35299;&#20915;ODE&#30340;Adam&#26041;&#26696;&#30340;&#26032;&#25193;&#23637;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#27604;&#32463;&#20856;Adam&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13704v1 Announce Type: cross  Abstract: The Adam optimizer, often used in Machine Learning for neural network training, corresponds to an underlying ordinary differential equation (ODE) in the limit of very small learning rates. This work shows that the classical Adam algorithm is a first order implicit-explicit (IMEX) Euler discretization of the underlying ODE. Employing the time discretization point of view, we propose new extensions of the Adam scheme obtained by using higher order IMEX methods to solve the ODE. Based on this approach, we derive a new optimization algorithm for neural network training that performs better than classical Adam on several regression and classification problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03890</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#32593;&#32476;&#36924;&#36817;&#29992;&#20110;&#36328;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20247;&#22810;&#36807;&#31243;&#65288;&#22914;&#65306;&#27973;&#23618;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#21644;&#21508;&#31181;&#20869;&#26680;&#26041;&#27861;&#65289;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#34920;&#36798;&#33021;&#21147;&#65289;&#30740;&#31350;&#20013;&#20419;&#36827;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21464;&#23398;&#20064;&#12289;&#20256;&#36882;&#23398;&#20064;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#31561;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#26469;&#30740;&#31350;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#32452;&#20869;&#26680;&#30340;&#26356;&#19968;&#33324;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#24179;&#31227;&#32593;&#32476;&#65288;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#31227;&#19981;&#21464;&#26680;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65289;&#21644;&#26059;&#36716;&#21306;&#20989;&#25968;&#26680;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#35201;&#27714;&#20869;&#26680;&#26159;&#27491;&#23450;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#21487;&#33021;&#22312;&#20998;&#24067;&#19978;&#19981;&#21516;&#30340;&#36328;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
&lt;/p&gt;</description></item></channel></rss>