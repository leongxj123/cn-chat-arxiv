<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27714;&#35299;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#24555;&#19988;&#31616;&#21333;&#26131;&#34892;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.09046</link><description>&lt;p&gt;
&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convex optimization over a probability simplex. (arXiv:2305.09046v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09046
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27714;&#35299;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#24555;&#19988;&#31616;&#21333;&#26131;&#34892;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#8212;&#8212;&#26607;&#35199;&#21333;&#32431;&#24418;&#26469;&#20248;&#21270;&#20984;&#38382;&#39064;&#65292;&#20351;&#20854;&#28385;&#36275;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#21363;$w\in\mathbb{R}^n$&#20013;$\sum_i w_i=1$&#65292;$w_i\geq0$&#12290;&#25105;&#20204;&#23558;&#21333;&#32431;&#24418;&#26144;&#23556;&#21040;&#21333;&#20301;&#29699;&#30340;&#27491;&#22235;&#38754;&#20307;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#33719;&#24471;&#38544;&#21464;&#37327;&#30340;&#35299;&#65292;&#24182;&#23558;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#21464;&#37327;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#65292;&#27599;&#27425;&#36845;&#20195;&#30001;&#31616;&#21333;&#30340;&#25805;&#20316;&#32452;&#25104;&#65292;&#19988;&#38024;&#23545;&#20984;&#20989;&#25968;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;${O}(1/T)$&#12290;&#21516;&#26102;&#26412;&#25991;&#20851;&#27880;&#20102;&#20449;&#24687;&#29702;&#35770;&#65288;&#22914;&#20132;&#21449;&#29109;&#21644;KL&#25955;&#24230;&#65289;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\ \textrm{and}\ w_i\geq0\}$. Other works have taken steps to enforce positivity or unit normalization automatically but never simultaneously within a unified setting. This paper presents a natural framework for manifestly requiring the probability condition. Specifically, we map the simplex to the positive quadrant of a unit sphere, envisage gradient descent in latent variables, and map the result back in a way that only depends on the simplex variable. Moreover, proving rigorous convergence results in this formulation leads inherently to tools from information theory (e.g. cross entropy and KL divergence). Each iteration of the Cauchy-Simplex consists of simple operations, making it well-suited for high-dimensional problems. We prove that it has a convergence rate of ${O}(1/T)$ for convex functions, and numerical experiments of projection 
&lt;/p&gt;</description></item></channel></rss>