<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05809</link><description>&lt;p&gt;
&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;
&lt;/p&gt;
&lt;p&gt;
Shallow ReLU neural networks and finite elements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05809
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25351;&#20986;&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#21487;&#20197;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#24369;&#24847;&#20041;&#19979;&#34920;&#31034;&#65288;&#36830;&#32493;&#25110;&#19981;&#36830;&#32493;&#30340;&#65289;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28041;&#21450;&#21040;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#65292;&#20934;&#30830;&#32473;&#20986;&#20102;&#24369;&#34920;&#31034;&#25152;&#38656;&#30340;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#20803;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#24120;&#25968;&#21644;&#32447;&#24615;&#26377;&#38480;&#20803;&#20989;&#25968;&#12290;&#36825;&#31181;&#24369;&#34920;&#31034;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#20026;&#36890;&#36807;&#26377;&#38480;&#20803;&#20989;&#25968;&#20998;&#26512;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;$L^p$&#33539;&#25968;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#25552;&#20379;&#20102;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23545;&#24352;&#37327;&#26377;&#38480;&#20803;&#20989;&#25968;&#30340;&#20005;&#26684;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05809v1 Announce Type: cross  Abstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item></channel></rss>