<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13869</link><description>&lt;p&gt;
&#20248;&#33391;&#26684;&#35757;&#32451;: &#20511;&#21161;&#25968;&#35770;&#21152;&#36895;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory. (arXiv:2307.13869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#26041;&#27861;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#20110;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#28385;&#36275;&#32473;&#23450;&#28857;&#19978;&#30340;PDE&#65292;&#24182;&#23545;&#35299;&#36827;&#34892;&#36924;&#36817;&#12290;&#28982;&#32780;&#65292;PDE&#30340;&#35299;&#22312;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#24182;&#19988;&#36755;&#20986;&#19982;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#23450;&#20041;&#22312;&#25972;&#20010;&#22495;&#19978;&#30340;&#31215;&#20998;&#12290;&#22240;&#27492;&#65292;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#20165;&#25552;&#20379;&#26377;&#38480;&#30340;&#36924;&#36817;&#12290;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#25554;&#20540;&#28857;&#26041;&#38754;&#21017;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23613;&#31649;&#36825;&#19968;&#26041;&#38754;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#20248;&#33391;&#26684;&#35757;&#32451;(GLT)&#65292;&#29992;&#20110;PINNs&#65292;&#21463;&#25968;&#20540;&#20998;&#26512;&#20013;&#30340;&#25968;&#35770;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;GLT&#25552;&#20379;&#20102;&#19968;&#32452;&#21363;&#20351;&#22312;&#23569;&#37327;&#28857;&#21644;&#22810;&#32500;&#31354;&#38388;&#20013;&#20063;&#38750;&#24120;&#26377;&#25928;&#30340;&#25554;&#20540;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GLT&#21482;&#38656;&#35201;2-20&#20493;&#30340;&#28857;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07882</link><description>&lt;p&gt;
&#22522;&#20110;&#27425;&#27969;&#24418;&#20551;&#35774;&#19979;&#25193;&#25955;&#27169;&#22411;&#22855;&#24322;&#24615;&#30340;&#25968;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical analysis of singularities in the diffusion model under the submanifold assumption. (arXiv:2301.07882v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#20197;&#26465;&#20214;&#26399;&#26395;&#34920;&#31034;&#21453;&#21521;&#37319;&#26679;&#27969;&#31243;&#30340;&#28418;&#31227;&#39033;&#65292;&#20854;&#20013;&#28041;&#21450;&#25968;&#25454;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#12290;&#35757;&#32451;&#36807;&#31243;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#26465;&#20214;&#26399;&#26395;&#30456;&#20851;&#30340;&#22343;&#26041;&#27531;&#24046;&#26469;&#23547;&#25214;&#27492;&#31867;&#28418;&#31227;&#20989;&#25968;&#12290;&#20351;&#29992;&#21069;&#21521;&#25193;&#25955;&#30340;Green&#20989;&#25968;&#30340;&#23567;&#26102;&#38388;&#36817;&#20284;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DDPM&#20013;&#30340;&#35299;&#26512;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;SGM&#20013;&#30340;&#24471;&#20998;&#20989;&#25968;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#23545;&#20110;&#20687;&#37027;&#20123;&#38598;&#20013;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#32780;&#35328;&#65292;&#28176;&#36817;&#22320;&#21457;&#25955;&#65292;&#22240;&#27492;&#38590;&#20197;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#22788;&#29702;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#26469;&#35828;&#26126;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provide several mathematical analyses of the diffusion model in machine learning. The drift term of the backwards sampling process is represented as a conditional expectation involving the data distribution and the forward diffusion. The training process aims to find such a drift function by minimizing the mean-squared residue related to the conditional expectation. Using small-time approximations of the Green's function of the forward diffusion, we show that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds, and is therefore difficult to approximate by a network. To overcome this difficulty, we derive a new target function and associated loss, which remains bounded even for singular data distributions. We illustrate the theoretical findings with several numerical examples.
&lt;/p&gt;</description></item></channel></rss>