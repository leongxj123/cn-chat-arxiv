<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#30340;&#21464;&#31181;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22312;&#38663;&#33633;&#38468;&#36817;&#34920;&#29616;&#19981;&#20339;&#30340;WENO&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14848</link><description>&lt;p&gt;
&#23398;&#20064;WENO&#29992;&#20110;&#29109;&#31283;&#23450;&#26041;&#26696;&#20197;&#35299;&#20915;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Learning WENO for entropy stable schemes to solve conservation laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#30340;&#21464;&#31181;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22312;&#38663;&#33633;&#38468;&#36817;&#34920;&#29616;&#19981;&#20339;&#30340;WENO&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#26465;&#20214;&#22312;&#25552;&#21462;&#31995;&#32479;&#23432;&#24658;&#24459;&#30340;&#29289;&#29702;&#30456;&#20851;&#35299;&#26102;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#27492;&#20419;&#20351;&#26500;&#24314;&#28385;&#36275;&#31163;&#25955;&#26465;&#20214;&#30340;&#29109;&#31283;&#23450;&#26041;&#26696;&#12290; TeCNO&#26041;&#26696;&#65288;Fjordholm&#31561;&#65292;2012&#65289;&#24418;&#25104;&#20102;&#19968;&#31867;&#20219;&#24847;&#39640;&#38454;&#29109;&#31283;&#23450;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#22120;&#65292;&#23427;&#20204;&#38656;&#35201;&#28385;&#36275;&#27599;&#20010;&#21333;&#20803;&#26684;&#30028;&#38754;&#30340;&#31526;&#21495;&#29305;&#24615;&#30340;&#19987;&#19994;&#37325;&#26500;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35774;&#35745;&#20102;&#28385;&#36275;&#31526;&#21495;&#29305;&#24615;&#30340;&#31532;&#19977;&#38454;WENO&#26041;&#26696;&#65292;&#31216;&#20026;SP-WENO&#65288;Fjordholm&#21644;Ray&#65292;2016&#65289;&#21644;SP-WENOc&#65288;Ray&#65292;2018&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;WENO&#31639;&#27861;&#22312;&#38663;&#33633;&#38468;&#36817;&#30340;&#24615;&#33021;&#21487;&#33021;&#24456;&#24046;&#65292;&#25968;&#20540;&#35299;&#34920;&#29616;&#20986;&#22823;&#30340;&#20154;&#24037;&#25391;&#33633;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SP-WENO&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31216;&#20026;Deep Sign-Preserving WENO&#65288;DSP-WENO&#65289;&#65292;&#22312;&#20854;&#20013;&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;WENO&#21152;&#26435;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14848v1 Announce Type: cross  Abstract: Entropy conditions play a crucial role in the extraction of a physically relevant solution for a system of conservation laws, thus motivating the construction of entropy stable schemes that satisfy a discrete analogue of such conditions. TeCNO schemes (Fjordholm et al. 2012) form a class of arbitrary high-order entropy stable finite difference solvers, which require specialized reconstruction algorithms satisfying the sign property at each cell interface. Recently, third-order WENO schemes called SP-WENO (Fjordholm and Ray, 2016) and SP-WENOc (Ray, 2018) have been designed to satisfy the sign property. However, these WENO algorithms can perform poorly near shocks, with the numerical solutions exhibiting large spurious oscillations. In the present work, we propose a variant of the SP-WENO, termed as Deep Sign-Preserving WENO (DSP-WENO), where a neural network is trained to learn the WENO weighting strategy. The sign property and third-o
&lt;/p&gt;</description></item><item><title>&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05809</link><description>&lt;p&gt;
&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;
&lt;/p&gt;
&lt;p&gt;
Shallow ReLU neural networks and finite elements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05809
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25351;&#20986;&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#21487;&#20197;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#24369;&#24847;&#20041;&#19979;&#34920;&#31034;&#65288;&#36830;&#32493;&#25110;&#19981;&#36830;&#32493;&#30340;&#65289;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28041;&#21450;&#21040;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#65292;&#20934;&#30830;&#32473;&#20986;&#20102;&#24369;&#34920;&#31034;&#25152;&#38656;&#30340;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#20803;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#24120;&#25968;&#21644;&#32447;&#24615;&#26377;&#38480;&#20803;&#20989;&#25968;&#12290;&#36825;&#31181;&#24369;&#34920;&#31034;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#20026;&#36890;&#36807;&#26377;&#38480;&#20803;&#20989;&#25968;&#20998;&#26512;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;$L^p$&#33539;&#25968;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#25552;&#20379;&#20102;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23545;&#24352;&#37327;&#26377;&#38480;&#20803;&#20989;&#25968;&#30340;&#20005;&#26684;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05809v1 Announce Type: cross  Abstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2306.17301</link><description>&lt;p&gt;
&#27973;&#23618;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65306;&#19968;&#20010;&#25968;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Why Shallow Networks Struggle with Approximating and Learning High Frequency: A Numerical Study. (arXiv:2306.17301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#25506;&#35752;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#21644;&#23398;&#20064;&#39640;&#39057;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#30340;&#35889;&#20998;&#26512;&#26469;&#29702;&#35299;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20998;&#26512;&#21644;&#23454;&#39564;&#30340;&#32508;&#21512;&#25968;&#20540;&#30740;&#31350;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#23454;&#38469;&#22240;&#32032;&#20013;&#65292;&#22788;&#29702;&#39640;&#39057;&#29575;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20102;&#20197;&#19979;&#22522;&#26412;&#35745;&#31639;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#19979;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#31934;&#24230;&#65292;&#65288;2&#65289;&#23454;&#29616;&#32473;&#23450;&#31934;&#24230;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23545;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#30456;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#30340;&#35889;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#23646;&#24615;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a comprehensive numerical study involving analysis and experiments shows why a two-layer neural network has difficulties handling high frequencies in approximation and learning when machine precision and computation cost are important factors in real practice. In particular, the following fundamental computational issues are investigated: (1) the best accuracy one can achieve given a finite machine precision, (2) the computation cost to achieve a given accuracy, and (3) stability with respect to perturbations. The key to the study is the spectral analysis of the corresponding Gram matrix of the activation functions which also shows how the properties of the activation function play a role in the picture.
&lt;/p&gt;</description></item></channel></rss>