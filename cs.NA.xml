<rss version="2.0"><channel><title>Chat Arxiv cs.NA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NA</description><item><title>&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.20360</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#65306;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#24102;&#26377;&#25209;&#24402;&#19968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#22522;&#26412;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#12289;&#21152;&#36895;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20960;&#20010;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#31215;&#20998;&#65289;&#12289;&#20248;&#21270;&#29702;&#35770;&#65288;&#21253;&#25324;Kurdyka-Lojasiewicz&#19981;&#31561;&#24335;&#65289;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#20070;&#30340;&#26368;&#21518;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#19968;&#20123;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Galerkin&#26041;&#27861;&#12290;&#24076;&#26395;&#26412;&#20070;&#33021;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
&lt;/p&gt;</description></item></channel></rss>