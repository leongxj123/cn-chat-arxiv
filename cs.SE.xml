<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17988</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#22312;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#31243;&#24207;&#21512;&#25104;&#21644;&#39640;&#32423;&#33258;&#21160;&#23436;&#25104;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#20854;&#36755;&#20986;&#20195;&#30721;&#22312;&#35821;&#27861;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#20801;&#35768;&#26089;&#26399;&#25298;&#32477;&#35821;&#27861;&#19978;&#19981;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#29992;&#20110;&#22635;&#20805;&#20219;&#21153;&#30340;&#23436;&#25972;&#31243;&#24207;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33021;&#22815;&#22312;&#20219;&#24847;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#30340;&#24038;&#21491;&#21830;&#19978;&#25805;&#20316;&#30340;Earley&#24335;&#35299;&#26512;&#22120;&#65292;&#24182;&#23558;&#22686;&#37327;&#35299;&#26512;&#21644;&#21830;&#25805;&#20316;&#25193;&#23637;&#21040;&#35768;&#22810;&#24120;&#35265;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#19978;&#19979;&#25991;&#25935;&#24863;&#29305;&#24615;&#12290;&#36825;&#20123;&#36129;&#29486;&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#36890;&#29992;&#21644;&#25166;&#23454;&#30340;&#24038;&#21491;&#21830;&#35299;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17988v1 Announce Type: cross  Abstract: Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2311.00889</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#39564;&#35777;&#65306;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GitHub Copilot&#65292;ChatGPT&#31561;&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#30830;&#20445;&#36825;&#20123;&#24037;&#20855;&#29983;&#25104;&#30340;&#20195;&#30721;&#19981;&#20165;&#21151;&#33021;&#27491;&#30830;&#65292;&#32780;&#19988;&#27809;&#26377;&#28431;&#27934;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;LLM&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#20294;&#20043;&#21069;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;LLM&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#23384;&#22312;&#20004;&#20010;&#23548;&#33268;&#19981;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#27809;&#26377;&#20805;&#20998;&#22320;&#20195;&#34920;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#31454;&#25216;&#32534;&#31243;&#25361;&#25112;&#25110;&#20197;&#35838;&#22530;&#24418;&#24335;&#20026;&#22522;&#30784;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#23558;&#34987;&#38598;&#25104;&#21040;&#26356;&#22823;&#30340;&#20195;&#30721;&#24211;&#20013;&#65292;&#24341;&#20837;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#32570;&#20047;&#19987;&#27880;&#20110;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#23433;&#20840;&#24615;&#30340;&#22522;&#20934;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#20391;&#37325;&#20110;&#21151;&#33021;&#24615;&#32780;&#24573;&#35270;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func
&lt;/p&gt;</description></item></channel></rss>