<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item></channel></rss>