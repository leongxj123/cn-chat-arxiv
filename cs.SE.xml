<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16437</link><description>&lt;p&gt;
&#20351;&#29992;&#31243;&#24207;&#25191;&#34892;&#36816;&#34892;&#26102;&#34892;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with Runtime Behavior of Program Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#20195;&#30721;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#20195;&#30721;LLMs&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#65288;&#22914;HumanEval&#21644;ClassEval&#65289;&#12290;&#20195;&#30721;&#25512;&#29702;&#26159;&#20195;&#30721;LLMs&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#29616;&#26377;&#30340;&#20195;&#30721;&#25512;&#29702;&#22522;&#20934;&#19981;&#36275;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#37325;&#28857;&#39044;&#27979;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24573;&#30053;&#20102;&#31243;&#24207;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#34892;&#20026;&#35780;&#20272;&#65292;&#20197;&#21450;&#36923;&#36753;&#19968;&#33268;&#24615;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25191;&#34892;&#36335;&#24452;&#39044;&#27979;&#38169;&#35823;&#65292;&#21017;&#27169;&#22411;&#19981;&#24212;&#35813;&#32473;&#20986;&#27491;&#30830;&#30340;&#36755;&#20986;&#65289;&#22312;&#25191;&#34892;&#25512;&#29702;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REval&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#19982;&#31243;&#24207;&#25191;&#34892;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20195;&#30721;&#22522;&#20934;&#65292;&#24182;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#26032;&#22522;&#20934;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16437v1 Announce Type: cross  Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs, but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely REval, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framew
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.15020</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Automated Approaches to Detect Self-Admitted Technical Debt: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15020
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#28304;&#33258;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#26435;&#34913;&#65292;&#22312;&#24433;&#21709;&#36719;&#20214;&#21487;&#32500;&#25252;&#24615;&#21644;&#38459;&#30861;&#26410;&#26469;&#24320;&#21457;&#24037;&#20316;&#26041;&#38754;&#36215;&#21040;&#20316;&#29992;&#12290;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#25351;&#30340;&#26159;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#25215;&#35748;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#30340;&#20195;&#30721;&#36136;&#37327;&#25110;&#35774;&#35745;&#32570;&#38519;&#12290;&#33258;&#21160;&#26816;&#27979;SATD&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#25216;&#26415;&#20538;&#21153;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;NLP&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31639;&#27861;&#31181;&#31867;&#22810;&#26679;&#21270;&#24120;&#24120;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#25152;&#32771;&#23519;&#30740;&#31350;&#20013;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15020v2 Announce Type: replace-cross  Abstract: Technical debt is a pervasive issue in software development, often arising from trade-offs made during development, which can impede software maintainability and hinder future development efforts. Self-admitted technical debt (SATD) refers to instances where developers explicitly acknowledge suboptimal code quality or design flaws in the codebase. Automated detection of SATD has emerged as a critical area of research, aiming to assist developers in identifying and addressing technical debt efficiently. However, the enormous variety of feature extraction approaches of NLP and algorithms employed in the literature often hinder researchers from trying to improve their performance. In light of this, this systematic literature review proposes a taxonomy of feature extraction techniques and ML/DL algorithms used in technical debt detection: its objective is to compare and benchmark their performance in the examined studies. We select
&lt;/p&gt;</description></item></channel></rss>