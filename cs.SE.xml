<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02806</link><description>&lt;p&gt;
RealHumanEval: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02806
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;HumanEval&#65288;Chen&#31561;&#65292;2021&#65289;&#65292;&#36825;&#20123;&#22522;&#20934;&#29992;&#20110;&#34913;&#37327;LLMs&#29983;&#25104;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#23436;&#25972;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#31243;&#24207;&#21592;&#21161;&#25163;&#65292;&#25105;&#20204;&#30740;&#31350;&#29616;&#26377;&#22522;&#20934;&#19978;&#30340;&#22686;&#30410;&#26159;&#21542;&#33021;&#36716;&#21270;&#20026;&#20351;&#29992;LLMs&#32534;&#30721;&#26102;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#32534;&#30721;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#38500;&#20102;&#38745;&#24577;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21487;&#33021;&#29992;&#20316;&#24230;&#37327;LLM&#24110;&#21161;&#24615;&#20195;&#29702;&#30340;&#20559;&#22909;&#24230;&#37327;&#30340;&#23454;&#29992;&#24615;&#65292;&#20363;&#22914;&#20195;&#30721;&#25509;&#21463;&#25110;&#22797;&#21046;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RealHumanEval&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;LLMs&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#23436;&#25104;&#25110;&#32842;&#22825;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65288;N = 213&#65289;&#65292;&#20351;&#29992;RealHumanEval&#65292;&#20854;&#20013;&#29992;&#25143;&#19982;&#20845;&#20010;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#21508;&#24322;&#30340;LLMs&#36827;&#34892;&#20132;&#20114;&#12290;&#23613;&#31649;&#38745;&#24577;&#22522;&#20934;&#27809;&#26377;&#21253;&#21547;&#20154;&#20026;&#24178;&#39044;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we 
&lt;/p&gt;</description></item></channel></rss>