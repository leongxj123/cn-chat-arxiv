<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02368</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#27979;&#35797;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21019;&#24314;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;&#32500;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#21253;&#25324;&#33258;&#21160;&#21019;&#24314;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#36890;&#24120;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#19981;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#29978;&#33267;&#21253;&#21547;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#65288;&#21453;&#27169;&#24335;&#65289;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#29983;&#25104;&#30340;&#21453;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;LLM&#21487;&#20197;&#29983;&#25104;&#19981;&#33391;&#30340;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#35757;&#32451;&#20102;&#19987;&#38376;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#21033;&#29992;Proximal Policy Optimization &#65288;PPO&#65289;&#26469;&#35757;&#32451;&#36880;&#20010;&#20248;&#21270;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22870;&#21169;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item></channel></rss>