<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.15960</link><description>&lt;p&gt;
MoTCoder: &#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15960
&lt;/p&gt;
&lt;p&gt;
MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#31616;&#21333;&#30340;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#20316;&#20026;&#21333;&#19968;&#20195;&#30721;&#22359;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modular-of-Thought Coder (MoTCoder)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;MoT&#25351;&#20196;&#35843;&#25972;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22521;&#20859;&#21644;&#21033;&#29992;&#23376;&#27169;&#22359;&#65292;MoTCoder&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27169;&#22359;&#21270;&#21644;&#27491;&#30830;&#24615;&#65292;&#23548;&#33268;&#22312;APPS&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;12.9%&#65292;&#22312;CodeContests&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;9.43%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/dvlab-research/MoTCoder&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
&lt;/p&gt;</description></item></channel></rss>