<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03268</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#30456;&#27604;&#19968;&#38149;&#31909;&#24335;&#27169;&#22411;&#65292;&#21315;&#19975;&#19981;&#35201;&#35753;&#39046;&#22495;&#30340;&#20379;&#32473;&#19981;&#36275;&#21463;&#21040;&#27874;&#21450;
&lt;/p&gt;
&lt;p&gt;
Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models. (arXiv:2306.03268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#65289;&#20026;NLP&#21644;&#36719;&#20214;&#24037;&#31243;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36861;&#27714;&#22823;&#32780;&#20840;&#30340;&#28526;&#27969;&#24212;&#35813;&#19982;&#38024;&#23545;&#29305;&#23450;&#30446;&#30340;&#12289;&#35268;&#27169;&#36866;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#20197;StackOverflow&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#39564;&#35777;&#22256;&#24785;&#24230;&#21644;&#36801;&#31227;&#23398;&#20064;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained neural language models have brought immense progress to both NLP and software engineering. Models in OpenAI's GPT series now dwarf Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide range of NLP applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take StackOverflow (SO) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens
&lt;/p&gt;</description></item></channel></rss>