<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bias Assessment and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#32534;&#30721;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;LLM&#22312;&#36719;&#20214;&#32534;&#30721;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65306;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65311;&#36825;&#20010;&#38382;&#39064;&#20851;&#31995;&#21040;&#20381;&#36182;&#20110;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#36719;&#20214;&#24212;&#29992;&#30340;&#23436;&#25972;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36947;&#24503;&#22522;&#30784;&#65292;&#28982;&#32780;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#39062;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;9.68\%&#21040;37.37\%&#30340;&#20195;&#30721;&#20989;&#25968;&#30340;&#21151;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' funct
&lt;/p&gt;</description></item></channel></rss>