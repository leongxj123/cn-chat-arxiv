<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05319</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#24211;&#19978;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;LM&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#24182;&#32463;&#24120;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#26412;&#30740;&#31350;&#27839;&#30528;&#20004;&#20010;&#37325;&#35201;&#26041;&#21521;&#30740;&#31350;&#20102;LM&#30340;&#23433;&#20840;&#24615;:(i)&#23433;&#20840;&#21152;&#22266;&#65292;&#26088;&#22312;&#22686;&#24378;LM&#22312;&#29983;&#25104;&#23433;&#20840;&#20195;&#30721;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;;(ii)&#23545;&#25239;&#27979;&#35797;&#65292;&#26088;&#22312;&#22312;&#23545;&#25239;&#24615;&#31435;&#22330;&#35780;&#20272;LM&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#39033;&#31216;&#20026;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#23433;&#20840;&#20219;&#21153;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#23558;&#19968;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25351;&#23548;LM&#29983;&#25104;&#23433;&#20840;&#25110;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#30041;LM&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SVEN&#30340;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;SVEN&#21033;&#29992;&#23646;&#24615;&#29305;&#23450;&#30340;&#36830;&#32493;&#21521;&#37327;&#26469;&#24341;&#23548;&#31243;&#24207;&#29983;&#25104;&#36798;&#21040;&#32473;&#23450;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#20462;&#25913;LM&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#25237;&#24433;&#25439;&#22833;&#26469;&#20248;&#21270;&#36825;&#20123;&#36830;&#32493;&#21521;&#37327;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SVEN&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#24403;&#21069;&#30340;LM&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22312;&#27979;&#35797;&#26102;&#20462;&#25913;&#23427;&#20204;&#30340;&#36755;&#20837;&#32780;&#20445;&#30041;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#38656;&#35201;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
&lt;/p&gt;</description></item></channel></rss>