<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10754</link><description>&lt;p&gt;
&#24403;&#25968;&#25454;&#27969;&#20998;&#26512;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
When Dataflow Analysis Meets Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#20998;&#26512;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20195;&#30721;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#25512;&#26029;&#31243;&#24207;&#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20195;&#30721;&#20248;&#21270;&#12289;&#31243;&#24207;&#29702;&#35299;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLMDFA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#12290;LLMDFA&#21463;&#22522;&#20110;&#25688;&#35201;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20960;&#31181;&#20851;&#38190;&#31574;&#30053;&#26377;&#25928;&#35299;&#20915;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21644;&#24037;&#20855;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#35774;&#35745;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#24182;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10754v1 Announce Type: cross  Abstract: Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark
&lt;/p&gt;</description></item><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item></channel></rss>