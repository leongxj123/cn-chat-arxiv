<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;</title><link>http://arxiv.org/abs/2401.16445</link><description>&lt;p&gt;
OMPGPT: &#19968;&#31181;&#29992;&#20110;OpenMP&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16445
&lt;/p&gt;
&lt;p&gt;
OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#38543;&#30528;&#36825;&#19968;&#36235;&#21183;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;StarCoder&#12289;WizardCoder&#21644;CodeLlama&#31561;&#65292;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#65292;&#22312;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#22266;&#26377;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#27880;&#37322;&#29983;&#25104;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#21450;&#23545;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#19968;&#33324;&#25903;&#25345;&#12290;&#34429;&#28982;&#20195;&#30721;LLMs&#30340;&#36890;&#29992;&#33021;&#21147;&#23545;&#35768;&#22810;&#31243;&#24207;&#21592;&#26469;&#35828;&#24456;&#26377;&#29992;&#65292;&#20294;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20855;&#26377;&#26356;&#31364;&#30340;&#38656;&#27714;&#38598;&#65292;&#20351;&#24471;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#30340;LM&#25104;&#20026;&#19968;&#20010;&#26356;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OMPGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;OpenMP pragma&#29983;&#25104;&#26041;&#38754;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24182;&#25913;&#36827;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#38142;&#24335;OMP&#65288;chain-of-OMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item></channel></rss>