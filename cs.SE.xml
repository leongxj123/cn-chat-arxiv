<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09299</link><description>&lt;p&gt;
&#26410;&#32463;&#26412;&#20154;&#21516;&#24847;&#30340;&#35757;&#32451;&#65306;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#35745;&#36890;&#36807;&#39564;&#35777;&#24320;&#21457;&#30340;&#20195;&#30721;&#26159;&#21542;&#31526;&#21512;&#26631;&#20934;&#12289;&#27861;&#35268;&#21644;&#29256;&#26435;&#20445;&#25252;&#65292;&#30830;&#20445;&#20854;&#19981;&#21253;&#21547;&#26469;&#33258;&#21463;&#20445;&#25252;&#26469;&#28304;&#30340;&#20195;&#30721;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#20986;&#29616;&#32473;&#20195;&#30721;&#23457;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#12290;&#36825;&#24341;&#21457;&#20102;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#20026;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#24050;&#21253;&#21547;&#22312;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#24320;&#21457;&#30340;&#20195;&#30721;&#23457;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;LLM&#26159;&#21542;&#24050;&#32463;&#22312;&#29305;&#23450;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#37492;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#23494;&#24615;&#65292;&#20256;&#32479;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#31561;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#29256;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
&lt;/p&gt;</description></item></channel></rss>