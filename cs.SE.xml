<rss version="2.0"><channel><title>Chat Arxiv cs.SE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SE</description><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DQNLog&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#24322;&#24120;&#29615;&#22659;&#20132;&#20114;&#21644;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#24182;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.03151</link><description>&lt;p&gt;
&#36890;&#36807;DQN&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning via DQN for log anomaly detection. (arXiv:2401.03151v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DQNLog&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#24322;&#24120;&#29615;&#22659;&#20132;&#20114;&#21644;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#24182;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#20445;&#38556;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#32500;&#25252;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#26816;&#27979;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#30563;&#24335;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#31216;&#20026;DQNLog&#12290;DQNLog&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#19982;&#20559;&#21521;&#20110;&#24322;&#24120;&#30340;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#65292;&#36824;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;DQNLog&#36824;&#24341;&#20837;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#39033;&#65292;&#38450;&#27490;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#27169;&#22411;&#36807;&#39640;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log anomaly detection plays a critical role in ensuring the security and maintenance of modern software systems. At present, the primary approach for detecting anomalies in log data is through supervised anomaly detection. Nonetheless, existing supervised methods heavily rely on labeled data, which can be frequently limited in real-world scenarios. In this paper, we propose a semi-supervised log anomaly detection method that combines the DQN algorithm from deep reinforcement learning, which is called DQNLog. DQNLog leverages a small amount of labeled data and a large-scale unlabeled dataset, effectively addressing the challenges of imbalanced data and limited labeling. This approach not only learns known anomalies by interacting with an environment biased towards anomalies but also discovers unknown anomalies by actively exploring the unlabeled dataset. Additionally, DQNLog incorporates a cross-entropy loss term to prevent model overestimation during Deep Reinforcement Learning (DRL). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item></channel></rss>