# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^2] | [The role of the metaverse in calibrating an embodied artificial general intelligence](https://arxiv.org/abs/2402.06660) | 本文研究了具有肉身的人工通用智能(AGI)的概念及其与人类意识的关系，强调了元宇宙在促进这一关系中的关键作用。通过结合不同理论框架和技术工具，论文总结出实现具有肉身的AGI的关键要素和发展阶段。 |
| [^3] | [GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents.](http://arxiv.org/abs/2310.12821) | GestureGPT是一个零样本交互手势理解和对接框架，利用大语言模型代理解读手势描述并根据交互环境提供上下文信息，能够将用户意图对接到交互功能上。 |

# 详细

[^1]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^2]: 元宇宙在校准具有肉身的人工通用智能中的作用

    The role of the metaverse in calibrating an embodied artificial general intelligence

    [https://arxiv.org/abs/2402.06660](https://arxiv.org/abs/2402.06660)

    本文研究了具有肉身的人工通用智能(AGI)的概念及其与人类意识的关系，强调了元宇宙在促进这一关系中的关键作用。通过结合不同理论框架和技术工具，论文总结出实现具有肉身的AGI的关键要素和发展阶段。

    

    本文探讨了具有肉身的人工通用智能(AGI)的概念，它与人类意识的关系，以及元宇宙在促进这种关系中的关键作用。本文利用融入认知、Michael Levin的计算边界"Self"、Donald D. Hoffman的感知界面理论以及Bernardo Kastrup的分析唯心主义等理论框架来构建实现具有肉身的AGI的论证。它认为我们所感知的外部现实是一种内在存在的交替状态的象征性表示，而AGI可以具有更大计算边界的更高意识。本文进一步讨论了AGI的发展阶段、实现具有肉身的AGI的要求、为AGI校准象征性界面的重要性，以及元宇宙、去中心化系统、开源区块链技术以及开源人工智能研究所扮演的关键角色。它还探讨了新的沟通机制和用于加强对元宇宙的理解的技术工具，以帮助实现具有肉身的AGI。

    This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
    
[^3]: GestureGPT: 零样本交互手势理解与基于大语言模型代理的对接

    GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])

    [http://arxiv.org/abs/2310.12821](http://arxiv.org/abs/2310.12821)

    GestureGPT是一个零样本交互手势理解和对接框架，利用大语言模型代理解读手势描述并根据交互环境提供上下文信息，能够将用户意图对接到交互功能上。

    

    当前的手势识别系统主要关注识别预定义集合中的手势，未能将这些手势与交互式图形用户界面元素或系统功能相连接（例如，将“竖起大拇指”手势与“喜欢”按钮关联起来）。我们引入了GestureGPT，这是一个新颖的零样本手势理解和对接框架，利用大语言模型（LLM）。手势描述根据手势视频中的手部关键点坐标进行形式化，并输入到我们的双代理对话系统中。一个手势代理解读这些描述，并询问有关交互环境的信息（例如，界面、历史记录、凝视数据），一个上下文代理负责组织并提供这些信息。经过迭代的交流，手势代理能够理解用户意图，并将其对接到一个交互功能上。我们使用公开的第一视角和第三视角手势数据集验证了手势描述模块，并在视频流和智能家居物联网控制的两个真实场景中测试了整个系统。

    Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. T
    

