# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Investigating Labeler Bias in Face Annotation for Machine Learning.](http://arxiv.org/abs/2301.09902) | 面部注释标注者的刻板印象和个人特征会影响其数据标注的公正性，强调需要对整个人工智能培训过程保持高度透明以尽早识别和纠正偏见。 |

# 详细

[^1]: 探究面部注释在机器学习中的标注者偏见

    Investigating Labeler Bias in Face Annotation for Machine Learning. (arXiv:2301.09902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09902](http://arxiv.org/abs/2301.09902)

    面部注释标注者的刻板印象和个人特征会影响其数据标注的公正性，强调需要对整个人工智能培训过程保持高度透明以尽早识别和纠正偏见。

    

    在一个越来越依赖人工智能的世界中，考虑人工智能对人类的伦理影响变得比以往任何时候都更加重要。一个尚未充分探讨的关键挑战是标注者偏见，这可能会为训练创建本质上带有偏见的数据集，并随后导致在医疗保健、就业、教育和执法等领域中出现不准确或不公平的决策。因此，我们进行了一项研究，使用来自不同种族和性别的人的图像进行标记任务，以调查和衡量标注者偏见的存在。我们的结果表明，参与者拥有影响其决策过程的刻板印象，并且标注者人口统计数据对所分配的注释标签产生影响。我们还讨论了标注者偏见如何影响数据集，随后影响所训练的模型。总体而言，在整个人工智能培训过程中必须保持高度透明，尽早识别和纠正数据中的偏见。

    In a world increasingly reliant on artificial intelligence, it is more important than ever to consider the ethical implications of artificial intelligence on humanity. One key under-explored challenge is labeler bias, which can create inherently biased datasets for training and subsequently lead to inaccurate or unfair decisions in healthcare, employment, education, and law enforcement. Hence, we conducted a study to investigate and measure the existence of labeler bias using images of people from different ethnicities and sexes in a labeling task. Our results show that participants possess stereotypes that influence their decision-making process and that labeler demographics impact assigned labels. We also discuss how labeler bias influences datasets and, subsequently, the models trained on them. Overall, a high degree of transparency must be maintained throughout the entire artificial intelligence training process to identify and correct biases in the data as early as possible.
    

