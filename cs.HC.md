# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions.](http://arxiv.org/abs/2401.13324) | 本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。 |
| [^2] | [AI Alignment in the Design of Interactive AI: Specification Alignment, Process Alignment, and Evaluation Support.](http://arxiv.org/abs/2311.00710) | 本文关注AI在界面设计和评估中的对齐问题，提出了规范对齐、过程对齐和评估支持等三个对齐目标，并介绍了代理过程和过程海湾的概念。 |

# 详细

[^1]: 有关算法决策的信息：探索受到算法决策影响的人的信息需求。

    Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])

    [http://arxiv.org/abs/2401.13324](http://arxiv.org/abs/2401.13324)

    本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。

    

    AI系统的解释很少涉及到受算法决策影响的人的信息需求。这种传达信息与受影响利益相关者所关心的信息之间的差距可能阻碍对监管框架（如AI法案）的理解和遵守。为了解决这个差距，我们提出了“XAI初学者问题库”：这是一个涵盖两个算法决策应用领域（就业预测和健康监测）中受影响利益相关者信息需求的目录，包括数据、系统背景、系统使用和系统规范等类别。信息需求是通过访谈研究收集的，参与者根据自己的问题获得解释。参与者还报告了他们的理解和决策信心，结果显示，尽管在接受解释后信心倾向于增加，但参与者也面临着理解上的挑战，如无法解释为什么自己的理解感觉不完整。解释还对理解产生了影响。

    Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
    
[^2]: AI互动中的AI对齐：规范对齐，过程对齐和评估支持

    AI Alignment in the Design of Interactive AI: Specification Alignment, Process Alignment, and Evaluation Support. (arXiv:2311.00710v1 [cs.HC])

    [http://arxiv.org/abs/2311.00710](http://arxiv.org/abs/2311.00710)

    本文关注AI在界面设计和评估中的对齐问题，提出了规范对齐、过程对齐和评估支持等三个对齐目标，并介绍了代理过程和过程海湾的概念。

    

    AI对齐是确保AI产生期望结果而避免不良副作用的整体问题。虽然通常从安全和人类价值的角度考虑AI对齐，但也可以在设计和评估交互式AI系统的界面的背景下考虑AI对齐。本文将AI对齐的概念映射到基本的三步交互循环中，得出相应的对齐目标：1）规范对齐：确保用户能够高效可靠地将目标传达给AI；2）过程对齐：提供验证和可选择控制AI执行过程的能力；3）评估支持：确保用户能够验证和理解AI的输出。我们还介绍了代理过程的概念，它被定义为AI实际过程的简化、分离派生但可控制的表示；以及过程海湾的概念，它突显人类和AI过程之间的差异。

    AI alignment considers the overall problem of ensuring an AI produces desired outcomes, without undesirable side effects. While often considered from the perspectives of safety and human values, AI alignment can also be considered in the context of designing and evaluating interfaces for interactive AI systems. This paper maps concepts from AI alignment onto a basic, three step interaction cycle, yielding a corresponding set of alignment objectives: 1) specification alignment: ensuring the user can efficiently and reliably communicate objectives to the AI, 2) process alignment: providing the ability to verify and optionally control the AI's execution process, and 3) evaluation support: ensuring the user can verify and understand the AI's output. We also introduce the concepts of a surrogate process, defined as a simplified, separately derived, but controllable representation of the AI's actual process; and the notion of a Process Gulf, which highlights how differences between human and
    

