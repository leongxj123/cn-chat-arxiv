# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting](https://arxiv.org/abs/2402.08658) | 本研究探索了使用大型语言模型（LLMs）实现即时自适应干预（JITAIs）的可行性。通过测试GPT-4模型以促进门诊心脏康复中心的心脏健康体育活动的使用案例，我们提出了450个JITAI决策和信息。 |
| [^2] | [Confounding-Robust Policy Improvement with Human-AI Teams.](http://arxiv.org/abs/2310.08824) | 本文提出了一种通过采用边际灵敏度模型来解决人工智能与人类合作中未被观察到的混淆问题的新方法。该方法结合了领域专业知识和基于人工智能的统计建模，以控制潜在的混淆因素的影响，并通过推迟合作系统来利用不同决策者的专业知识。 |
| [^3] | [Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses.](http://arxiv.org/abs/2302.01241) | 本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。 |

# 详细

[^1]: 最后的JITAI？大型语言模型在发放及时自适应干预中的不合理有效性：在前瞻性心脏康复环境中促进体育活动

    The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting

    [https://arxiv.org/abs/2402.08658](https://arxiv.org/abs/2402.08658)

    本研究探索了使用大型语言模型（LLMs）实现即时自适应干预（JITAIs）的可行性。通过测试GPT-4模型以促进门诊心脏康复中心的心脏健康体育活动的使用案例，我们提出了450个JITAI决策和信息。

    

    我们探索了大型语言模型（LLMs）在数字健康中触发和个性化即时自适应干预（JITAIs）内容的可行性。JITAIs被视为可持续行为改变的关键机制，将干预措施根据个体的当前情境和需求进行调整。然而，传统的基于规则和机器学习模型在JITAI实施中面临可扩展性和可靠性的限制，例如缺乏个性化、管理多参数系统困难以及数据稀疏性等问题。为了研究通过LLMs实现JITAI，我们使用基于在门诊心脏康复中促进心脏健康体育活动的使用案例的现代最高性能模型“GPT-4”的实例作为触发和个性化JITAIs的基础。随后，我们生成了总共450个建议的JITAI决策和信息。

    We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message c
    
[^2]: 带有人工智能团队的混淆鲁棒的策略改进

    Confounding-Robust Policy Improvement with Human-AI Teams. (arXiv:2310.08824v1 [cs.HC])

    [http://arxiv.org/abs/2310.08824](http://arxiv.org/abs/2310.08824)

    本文提出了一种通过采用边际灵敏度模型来解决人工智能与人类合作中未被观察到的混淆问题的新方法。该方法结合了领域专业知识和基于人工智能的统计建模，以控制潜在的混淆因素的影响，并通过推迟合作系统来利用不同决策者的专业知识。

    

    人工智能与人类的合作有可能通过充分发挥人类专家和人工智能系统的相互补充优势来改变各个领域。然而，未被观察到的混淆可能会破坏这种合作的有效性，导致偏见和不可靠的结果。本文提出了一种解决人工智能与人类合作中未被观察到的混淆问题的新方法，即采用边际灵敏度模型（MSM）。我们的方法将领域专业知识与基于人工智能的统计建模相结合，以考虑潜在的可能会隐藏的混淆因素。我们提出了一个推迟合作框架，将边际灵敏度模型纳入观测数据中的策略学习，使系统能够控制未被观察到的混淆因素的影响。此外，我们提出了一个个性化的推迟合作系统，以利用不同人类决策者的多样化专业知识。通过调整潜在的偏见，我们的解决方案能够提高合作结果的可靠性。

    Human-AI collaboration has the potential to transform various domains by leveraging the complementary strengths of human experts and Artificial Intelligence (AI) systems. However, unobserved confounding can undermine the effectiveness of this collaboration, leading to biased and unreliable outcomes. In this paper, we propose a novel solution to address unobserved confounding in human-AI collaboration by employing the marginal sensitivity model (MSM). Our approach combines domain expertise with AI-driven statistical modeling to account for potential confounders that may otherwise remain hidden. We present a deferral collaboration framework for incorporating the MSM into policy learning from observational data, enabling the system to control for the influence of unobserved confounding factors. In addition, we propose a personalized deferral collaboration system to leverage the diverse expertise of different human decision-makers. By adjusting for potential biases, our proposed solution e
    
[^3]: 图解化：利用图解型AI解释对假设性演绎推理的理性化

    Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.01241](http://arxiv.org/abs/2302.01241)

    本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。

    

    许多可解释的人工智能（XAI）可视化工具已经被开发出来，但它们通常需要用户进一步推理来解释。我们认为，XAI应该支持图解型和假设性推理，以便AI能够进行假设生成和评估，从而减少可解释性差距。我们提出了图解化方法，以i)进行Peircean推导-演绎推理，ii)遵循领域惯例，和iii)用图示或语言进行解释。我们在临床应用领域实现了DiagramNet，以预测心脏听诊中的心脏诊断，并用基于形状的杂音图解进行解释。在建模研究中，我们发现DiagramNet不仅提供了忠实的杂音形状解释，而且比基线模型具有更好的预测性能。我们进一步通过医学生的定性用户研究展示了图解型解释的可理解性和可信度，并表明在临床相关的情况下，图解式解释比其他方式更受推崇。

    Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
    

