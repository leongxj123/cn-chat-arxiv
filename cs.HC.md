# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers](https://arxiv.org/abs/2404.02806) | 评估了大型语言模型在支持程序员方面的能力，引入了RealHumanEval作为衡量其帮助性的界面，并探讨了其对程序员生产力的影响。 |
| [^2] | [Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study.](http://arxiv.org/abs/2401.09637) | 通过大型语言模型辅助阅读临床笔记，患者可以获得更好的理解和自信。这项研究开发了一个工具，利用语言模型简化和增加上下文，使临床笔记更易读。研究结果表明，这些增强对患者有益。 |
| [^3] | [Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI.](http://arxiv.org/abs/2308.07213) | 本研究使用AI的匹配设计方法，通过与专业事实核查员的合作设计，发现并解决事实核查员与技术之间的差距。合作设计会议产生了11个新的设计思路，包括提高效率和个性化的事实核查工具，帮助事实核查员准备未来的虚假信息，监测偏见，以及支持内部组织。 |

# 详细

[^1]: RealHumanEval: 评估大型语言模型支持程序员的能力

    The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers

    [https://arxiv.org/abs/2404.02806](https://arxiv.org/abs/2404.02806)

    评估了大型语言模型在支持程序员方面的能力，引入了RealHumanEval作为衡量其帮助性的界面，并探讨了其对程序员生产力的影响。

    

    大型语言模型（LLMs）的评估主要依赖于静态基准，包括HumanEval（Chen等，2021），这些基准用于衡量LLMs生成通过单元测试的完整代码的能力。随着LLMs越来越多地被用作程序员助手，我们研究现有基准上的增益是否能转化为使用LLMs编码时程序员生产力的提升，包括编码所花费的时间。除了静态基准，我们还研究了可能用作度量LLM帮助性代理的偏好度量的实用性，例如代码接受或复制率。为此，我们引入了RealHumanEval，这是一个用于衡量LLMs辅助程序员的能力的网络界面，可以通过自动完成或聊天支持。我们进行了一个用户研究（N = 213），使用RealHumanEval，其中用户与六个基础模型性能各异的LLMs进行交互。尽管静态基准没有包含人为干预，我们...

    arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we 
    
[^2]: 大型语言模型辅助对患者阅读临床笔记的影响：一个混合方法研究

    Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])

    [http://arxiv.org/abs/2401.09637](http://arxiv.org/abs/2401.09637)

    通过大型语言模型辅助阅读临床笔记，患者可以获得更好的理解和自信。这项研究开发了一个工具，利用语言模型简化和增加上下文，使临床笔记更易读。研究结果表明，这些增强对患者有益。

    

    患者通过阅读他们的临床笔记获得了许多好处，包括增加对自身健康的控制感和对护理计划的理解提高。然而，在临床笔记中复杂的医学概念和术语阻碍了患者的理解，并可能导致焦虑。我们开发了一个面向患者的工具，利用大型语言模型（LLMs）简化笔记、从中提取信息并增加上下文，以使临床笔记更易读。我们使用我们的工具提示改进的GPT-4对由乳腺癌幸存者捐赠的真实临床笔记和临床医生生成的合成临床笔记进行这些增强任务。共有12条笔记，3868个字。2023年6月，我们随机分配了200名美国女性参与者，并向他们分发了三个具有不同程度增强的临床笔记。参与者回答了有关每个笔记的问题，评估了他们对后续行动的理解和自我报告的自信心。我们发现增强对阅读理解和自信心友好。

    Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were ass
    
[^3]: 人本自然语言处理事实核查：使用AI的匹配设计与事实核查员合作

    Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. (arXiv:2308.07213v1 [cs.HC] CROSS LISTED)

    [http://arxiv.org/abs/2308.07213](http://arxiv.org/abs/2308.07213)

    本研究使用AI的匹配设计方法，通过与专业事实核查员的合作设计，发现并解决事实核查员与技术之间的差距。合作设计会议产生了11个新的设计思路，包括提高效率和个性化的事实核查工具，帮助事实核查员准备未来的虚假信息，监测偏见，以及支持内部组织。

    

    专业事实核查在应对大量虚假信息方面存在可扩展性有限的挑战。虽然提出了许多自然语言处理工具来增强事实核查的效率和可扩展性，但学术研究和事实核查组织均报告了对此类工具的有限采用，因为这些工具不足以与事实核查员的实践、价值观和需求保持一致。为了弥补这一差距，我们研究了一种合作设计方法，即AI的匹配设计，该方法促进事实核查员、设计师和自然语言处理研究人员共同发现应以何种方式解决事实核查员的需求。我们与22名专业事实核查员进行的合作设计会议产生了11个新的设计思路。这些思路有助于提高信息搜索、处理和撰写效率以及个性化的事实核查；帮助事实核查员主动准备未来的虚假信息；监测潜在的偏见；并支持内部组织。

    A key challenge in professional fact-checking is its limited scalability in relation to the magnitude of false information. While many Natural Language Processing (NLP) tools have been proposed to enhance fact-checking efficiency and scalability, both academic research and fact-checking organizations report limited adoption of such tooling due to insufficient alignment with fact-checker practices, values, and needs. To address this gap, we investigate a co-design method, Matchmaking for AI, which facilitates fact-checkers, designers, and NLP researchers to collaboratively discover what fact-checker needs should be addressed by technology and how. Our co-design sessions with 22 professional fact-checkers yielded a set of 11 novel design ideas. They assist in information searching, processing, and writing tasks for efficient and personalized fact-checking; help fact-checkers proactively prepare for future misinformation; monitor their potential biases; and support internal organization c
    

