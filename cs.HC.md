# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas](https://arxiv.org/abs/2402.17270) | 调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。 |
| [^2] | [GigaPevt: Multimodal Medical Assistant](https://arxiv.org/abs/2402.16654) | GigaPevt是第一个结合大型语言模型和专业医疗模型的多模态医疗助手，在对话质量和度量性能方面表现出明显优势，并在问答任务中提高了1.18\%的准确率。 |
| [^3] | [ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots](https://arxiv.org/abs/2209.08199) | ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。 |

# 详细

[^1]: 多智能体、人智能体及其进展：合作在社会困境中的调查

    Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas

    [https://arxiv.org/abs/2402.17270](https://arxiv.org/abs/2402.17270)

    调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。

    

    在社会困境中研究合作长期以来一直是各种学科的基本课题，包括计算机科学和社会科学。人工智能领域的最新进展显著重塑了这一领域，为理解和增强合作提供了新的见解。本调查考察了人工智能和社会困境合作交汇处的三个关键领域。首先，着重于多智能体合作，我们审查了支持理性智能体之间合作的内在和外在动机，以及用于制定有效策略对抗不同对手的方法。其次，探讨了人智能体合作，我们讨论了当前用于与人类合作的人工智能算法，以及人类对人工智能智能体的偏见。第三，我们审查了利用人工智能智能体增强人类合作的新兴领域。最后，我们讨论了未来研究方向，例如 u

    arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
    
[^2]: GigaPevt：多模态医疗助手

    GigaPevt: Multimodal Medical Assistant

    [https://arxiv.org/abs/2402.16654](https://arxiv.org/abs/2402.16654)

    GigaPevt是第一个结合大型语言模型和专业医疗模型的多模态医疗助手，在对话质量和度量性能方面表现出明显优势，并在问答任务中提高了1.18\%的准确率。

    

    建立一个智能高效的医疗助手仍然是一个具有挑战性的人工智能问题。主要限制来自数据模态的稀缺性，降低了全面的患者感知。本演示论文介绍了GigaPevt，这是第一个结合了大型语言模型的对话功能和专业医疗模型的多模态医疗助手。这种方法在对话质量和度量性能方面具有明显优势，使得在问答任务中准确率提高了1.18\%。

    arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
    
[^3]: ScreenQA: 移动应用截图上的大规模问答对

    ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots

    [https://arxiv.org/abs/2209.08199](https://arxiv.org/abs/2209.08199)

    ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。

    

    我们提出了一个新的任务和数据集ScreenQA，用于通过问答来理解屏幕内容。现有的屏幕数据集要么侧重于结构和组件级别的理解，要么侧重于像导航和任务完成之类的更高级别的组合任务。我们试图通过在RICO数据集上注释86K个问答对来弥合这两者之间的差距，希望能够基准化屏幕阅读理解能力。

    arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
    

