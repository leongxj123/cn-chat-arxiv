# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios.](http://arxiv.org/abs/2309.04421) | SynthoGestures是一种使用虚幻引擎合成逼真手势的新框架，可以用于驾驶场景下的动态人机界面。该框架通过生成多种变体和模拟不同摄像机类型，提高了手势识别的准确性并节省了数据集创建的时间和精力。 |
| [^2] | [Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition.](http://arxiv.org/abs/2308.11635) | 本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。 |

# 详细

[^1]: SynthoGestures：一种用于驾驶场景的合成动态手势生成的新框架

    SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios. (arXiv:2309.04421v1 [cs.CV])

    [http://arxiv.org/abs/2309.04421](http://arxiv.org/abs/2309.04421)

    SynthoGestures是一种使用虚幻引擎合成逼真手势的新框架，可以用于驾驶场景下的动态人机界面。该框架通过生成多种变体和模拟不同摄像机类型，提高了手势识别的准确性并节省了数据集创建的时间和精力。

    

    在汽车领域中，为动态人机界面创建多样化和全面的手势数据集可能具有挑战性且耗时。为了克服这一挑战，我们提出使用虚拟3D模型生成合成手势数据集。我们的框架利用虚幻引擎合成逼真的手势，提供定制选项并降低过拟合风险。生成多种变体，包括手势速度、性能和手形，以提高泛化能力。此外，我们模拟不同的摄像机位置和类型，如RGB、红外和深度摄像机，而无需额外的时间和费用获取这些摄像机。实验结果表明，我们的提议框架SynthoGestures提高了手势识别准确率，可以替代或增强真手数据集。通过节省数据集创建的时间和精力，我们的工具促进了研究的进展。

    Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\footnote{\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool acc
    
[^2]: 半监督双流自注意对抗图对比学习在基于跨主体脑电情绪识别中的应用

    Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])

    [http://arxiv.org/abs/2308.11635](http://arxiv.org/abs/2308.11635)

    本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。

    

    脑电图 (EEG) 是一种有着广泛应用前景的客观情绪识别工具。然而，标记数据的稀缺性仍然是该领域的主要挑战，限制了基于脑电的情绪识别的广泛应用。本文提出了一种半监督双流自注意对抗图对比学习框架 (简称为 DS-AGC)，用于解决基于跨主体脑电情绪识别中有限标记数据的挑战。DS-AGC 框架包括两个并行流，用于提取非结构化和结构化脑电特征。非结构化流采用半监督多领域适应方法，以缓解标记源域、未标记源域和未知目标域之间的分布差异。结构化流则开发了一种图对比学习方法，以半监督方式从多个脑电通道中提取有效的基于图的特征表示。此外，一种自注意

    Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
    

