# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars](https://arxiv.org/abs/2402.17988) | 本文提出了一种增量解析器，通过对上下文敏感语法进行高效左右商，实现了对语法正确性的早期拒绝和对完整程序的有效检测。 |
| [^2] | [Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code.](http://arxiv.org/abs/2311.00889) | 该论文研究了使用SALLMS评估LLM生成代码的安全性，指出现有数据集和评估指标未能充分考虑到与安全相关的真实软件工程任务，从而导致不安全的代码生成。 |

# 详细

[^1]: 通过对上下文敏感语法进行高效左右商，在代码语言模型的约束解码

    Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars

    [https://arxiv.org/abs/2402.17988](https://arxiv.org/abs/2402.17988)

    本文提出了一种增量解析器，通过对上下文敏感语法进行高效左右商，实现了对语法正确性的早期拒绝和对完整程序的有效检测。

    

    大型语言模型是程序合成和高级自动完成的强大工具，但不能保证其输出代码在语法上是正确的。本文提出了一种增量解析器，允许早期拒绝语法上不正确的代码，并且能够有效检测用于填充任务的完整程序。我们开发了能够在任意上下文无关语法的左右商上操作的Earley式解析器，并将增量解析和商操作扩展到许多常见编程语言的语法中存在的几个上下文敏感特性。这些贡献的结果是一种高效、通用和扎实的左右商解析方法。

    arXiv:2402.17988v1 Announce Type: cross  Abstract: Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for
    
[^2]: 生成和验证：使用SALLMS评估LLM生成的代码的安全性

    Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])

    [http://arxiv.org/abs/2311.00889](http://arxiv.org/abs/2311.00889)

    该论文研究了使用SALLMS评估LLM生成代码的安全性，指出现有数据集和评估指标未能充分考虑到与安全相关的真实软件工程任务，从而导致不安全的代码生成。

    

    随着大型语言模型（例如GitHub Copilot，ChatGPT等）在软件工程师的日常实践中越来越受欢迎，确保这些工具生成的代码不仅功能正确，而且没有漏洞变得非常重要。尽管LLM可以帮助开发人员提高生产力，但之前的实证研究表明LLM可能会生成不安全的代码。存在两个导致不安全代码生成的因素。首先，用于评估大型语言模型（LLM）的现有数据集没有充分地代表与安全相关的真实软件工程任务。相反，它们通常基于竞技编程挑战或以课堂形式为基础的编码任务。在真实世界的应用中，生成的代码将被集成到更大的代码库中，引入潜在的安全风险。目前缺乏专注于评估生成代码安全性的基准。其次，现有的评估指标主要侧重于功能性而忽视安全性。

    With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func
    

