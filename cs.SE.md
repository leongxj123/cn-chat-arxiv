# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Universal Fuzzing via Large Language Models.](http://arxiv.org/abs/2308.04748) | 本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。 |
| [^2] | [Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT.](http://arxiv.org/abs/2304.00385) | 提出了一个名为ChatRepair的新型自动程序修复方法，与传统的“生成和验证”范式不同，它能够通过对话风格实现即时反馈，从而显着提高漏洞修复的效率和补丁的准确性。 |
| [^3] | [Revisiting the Plastic Surgery Hypothesis via Large Language Models.](http://arxiv.org/abs/2303.10494) | 本论文重新审视了自动程序修复中的整形手术假设，并提出使用大型语言模型进行APR的新方法，主要解决了传统APR工具在不同项目中无法产生多样化修补程序的问题。 |

# 详细

[^1]: 通过大型语言模型实现通用模糊测试

    Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])

    [http://arxiv.org/abs/2308.04748](http://arxiv.org/abs/2308.04748)

    本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。

    

    模糊测试在发现各种软件系统中的漏洞和脆弱性方面取得了巨大成功。接受编程或形式语言作为输入的测试系统（SUTs），如编译器，运行时引擎，约束求解器和具有可访问API的软件库，尤其重要，因为它们是软件开发的基本构建块。然而，针对这些系统的现有模糊测试工具通常针对特定语言，因此无法轻易应用于其他语言甚至同一语言的其他版本。此外，现有模糊测试工具生成的输入通常局限于输入语言的特定功能，因此很难揭示与其他功能相关的漏洞或新功能。本文提出了Fuzz4All，这是第一个通用的模糊测试工具，它可以针对许多不同的输入语言和这些语言的许多不同功能进行测试。Fuzz4All的关键思想是利用大型语言模型（LLMs）作为输入生成器。

    Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
    
[^2]: 让对话继续：使用ChatGPT仅以0.42美元的价格修复了337个漏洞中的162个

    Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. (arXiv:2304.00385v1 [cs.SE])

    [http://arxiv.org/abs/2304.00385](http://arxiv.org/abs/2304.00385)

    提出了一个名为ChatRepair的新型自动程序修复方法，与传统的“生成和验证”范式不同，它能够通过对话风格实现即时反馈，从而显着提高漏洞修复的效率和补丁的准确性。

    

    自动程序修复（APR）旨在自动生成有关有漏洞程序的修补程序。最近的APR工作集中于利用现代的大型语言模型（LLMs）直接生成APR的补丁。这种基于LLM的APR工具的工作方法是首先构建一个由原始有漏洞代码构建的输入提示，然后查询LLM生成补丁。虽然基于LLM的APR工具能够实现最先进的结果，但它仍然遵循“生成和验证”修复范式，即首先生成大量的补丁，然后逐个验证每个补丁。这不仅会导致许多重复的不正确的补丁，而且还会错过测试失败中的关键信息以及可行的补丁信息。为了解决这些局限性，我们提出了ChatRepair，这是第一种完全自动化的对话驱动的APR方法，它将补丁生成与即时反馈交替进行，以以对话风格执行APR。ChatRepair首先将相关的测试失败信息馈入LLM中，然后在补丁生成过程中使用交互式对话，以集中方式生成补丁。此外，ChatRepair还利用了测试结果中的关键信息，以生成更好的补丁。

    Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.  To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failur
    
[^3]: 通过大型语言模型重新审视整形手术假设

    Revisiting the Plastic Surgery Hypothesis via Large Language Models. (arXiv:2303.10494v1 [cs.SE])

    [http://arxiv.org/abs/2303.10494](http://arxiv.org/abs/2303.10494)

    本论文重新审视了自动程序修复中的整形手术假设，并提出使用大型语言模型进行APR的新方法，主要解决了传统APR工具在不同项目中无法产生多样化修补程序的问题。

    

    自动化程序修复（APR）旨在自动生成输入错误程序的补丁。传统APR工具通常专注于特定的错误类型和修复方式，通过使用模板、启发式和正式规范。然而，这些技术在错误类型和修补程序的多样化方面存在限制。因此，研究人员设计了各种基于学习的APR工具，最近的工作集中在直接使用大型语言模型（LLMs）进行APR。虽然基于LLM的APR工具能够在许多修复数据集上实现最先进的性能，但用于直接修复的LLMs并没有完全了解项目特定信息，如独特的变量或方法名称。整形手术假设是APR的一个著名的见解，它指出修复错误的代码部分通常已经存在于同一项目中。传统的APR工具主要通过设计手动或基于启发的方法来利用整形手术假设。

    Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.  The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based a
    

