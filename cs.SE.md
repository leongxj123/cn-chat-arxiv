# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A systematic evaluation of large language models for generating programming code](https://arxiv.org/abs/2403.00894) | GPT-4在生成编程代码方面表现优异，特别是在选择最佳提示策略时，超过了其他大型语言模型和85%的人类参与者。 |
| [^2] | [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://arxiv.org/abs/2402.02037) | 本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。 |

# 详细

[^1]: 对于生成编程代码的大型语言模型进行系统评估

    A systematic evaluation of large language models for generating programming code

    [https://arxiv.org/abs/2403.00894](https://arxiv.org/abs/2403.00894)

    GPT-4在生成编程代码方面表现优异，特别是在选择最佳提示策略时，超过了其他大型语言模型和85%的人类参与者。

    

    我们系统评估了七个大型语言模型在使用不同提示策略、编程语言和任务难度生成编程代码时的性能。GPT-4在很大程度上优于其他大型语言模型，包括Gemini Ultra和Claude 2。GPT-4的编码性能随不同提示策略而变化。在本研究中评估的大多数LeetCode和GeeksforGeeks编程比赛中，采用最佳提示策略的GPT-4胜过85%的人类参与者。此外，GPT-4表现出在不同编程语言之间翻译代码和从过去错误中学习的强大能力。由GPT-4生成的代码的计算效率与人类程序员相当。这些结果表明，GPT-4有潜力成为在编程代码生成和软件开发中的可靠助手。

    arXiv:2403.00894v1 Announce Type: cross  Abstract: We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.
    
[^2]: EffiBench:评估自动生成代码的效率的基准测试

    EffiBench: Benchmarking the Efficiency of Automatically Generated Code

    [https://arxiv.org/abs/2402.02037](https://arxiv.org/abs/2402.02037)

    本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。

    

    代码生成模型在辅助软件开发方面变得越来越重要，可以帮助完成代码补全、调试和代码转换等任务。尽管当前的研究已经深入研究了代码生成模型生成的正确性，但生成代码的效率这一重要方面常常被忽视。本文提出了EffiBench，一个包含1,000个效率关键的编码问题的基准测试，用于评估代码生成模型生成的代码的效率。EffiBench包含了一系列多样化的LeetCode编码问题，每个问题都与一个可执行的人工编写的典型解决方案配对。通过EffiBench，我们在实践中考察了21种大型语言模型（其中13种是开源的，8种是闭源的）在生成高效代码方面的能力。结果表明，GPT-4-turbo生成的代码最高效，明显优于Palm-2-chat-bison、Claude-instant-1、Gemini-pro、GPT-4和GPT-3.5。

    Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne
    

