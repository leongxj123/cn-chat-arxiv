<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;kNN&#31639;&#27861;&#36827;&#34892;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01635</link><description>&lt;p&gt;
kNN&#31639;&#27861;&#29992;&#20110;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#65292;&#20855;&#26377;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;kNN&#31639;&#27861;&#36827;&#34892;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#21160;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;kNN&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;kNN&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#19982;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#36873;&#25321;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#30446;&#26631;&#26159;&#20934;&#30830;&#20272;&#35745;&#38543;&#26426;&#21709;&#24212;&#21464;&#37327;&#30340;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25551;&#36848;&#21508;&#31181;&#24773;&#26223;&#19979;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26426;&#21046;&#65292;&#21033;&#29992;&#25105;&#20204;&#20043;&#21069;&#20851;&#20110;&#26465;&#20214;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#20272;&#35745;&#24037;&#20316;&#12290; kNN&#30340;&#24212;&#29992;&#30830;&#20445;&#20102;&#22312;&#39044;&#27979;&#21306;&#38388;&#26102;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#19982;&#26368;&#20248;&#38750;&#21442;&#25968;&#36895;&#29575;&#30456;&#19968;&#33268;&#30340;&#32479;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;kNN&#21322;&#21442;&#25968;&#31639;&#27861;&#26469;&#20272;&#35745;&#32771;&#34385;&#21327;&#21464;&#37327;&#30340;ROC&#26354;&#32447;&#12290;&#23545;&#20110;&#36873;&#25321;&#24179;&#28369;&#21442;&#25968;k&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#21464;&#37327;&#36873;&#25321;&#30340;&#24341;&#20837;&#26174;&#33879;&#25552;&#39640;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique. This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates. For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.Incorporation of variable selection enhances the performance of the method significantly over convention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#31070;&#32463;&#20803;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#20943;&#23569;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01342</link><description>&lt;p&gt;
&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#21644;&#27169;&#22411;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#31070;&#32463;&#20803;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32622;&#25442;&#23376;&#31354;&#38388;&#20943;&#23569;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#21021;&#22987;&#21270;&#26465;&#20214;&#19979;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#32463;&#24120;&#20135;&#29983;&#20855;&#26377;&#21151;&#33021;&#30456;&#20284;&#20294;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#20998;&#25955;&#30340;&#35299;&#65292;&#36825;&#23548;&#33268;&#20102;&#32447;&#24615;&#27169;&#22359;&#36830;&#36890;&#24615;&#65288;LMC&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#21644;&#25552;&#39640;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#32622;&#25442;&#23545;&#31216;&#24615;&#22312;&#36890;&#36807;&#32593;&#32476;&#32622;&#25442;&#20943;&#23569;&#35757;&#32451;&#21518;&#30340;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20107;&#21518;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#22312;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65288;&#22914;ViT&#65292;LLM&#65289;&#19978;&#25928;&#26524;&#36739;&#24046;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#37327;&#30340;&#32622;&#25442;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32622;&#25442;&#23376;&#31354;&#38388;&#21487;&#20197;&#20813;&#36153;&#20943;&#23569;LMC&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21021;&#22987;&#21270;&#26102;&#36827;&#34892;&#20462;&#21098;&#21487;&#20197;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;&#38500;&#20102;&#20462;&#21098;&#20043;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TNA-PFN&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26080;&#25439;&#30340;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#37096;&#20998;&#26799;&#24230;&#25513;&#30721;&#12290;TNA-PFN&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#37117;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10028</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#19982;&#22823;&#21160;&#20316;&#31354;&#38388;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models Meet Contextual Bandits with Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#65292;&#26377;&#25928;&#30340;&#25506;&#32034;&#26159;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35774;&#35745;&#20102;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;dTS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;dTS&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07176</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#20844;&#24179;&#30340;&#40723;&#21169;&#25919;&#31574;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#24378;&#21046;&#20010;&#20307;&#25509;&#21463;&#27835;&#30103;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#27492;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25509;&#21463;&#27835;&#30103;&#30340;&#20010;&#20307;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#65292;&#27835;&#30103;&#25928;&#26524;&#20063;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#34429;&#28982;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#21487;&#20197;&#26368;&#22823;&#21270;&#25972;&#20010;&#20154;&#32676;&#30340;&#22240;&#26524;&#32467;&#26524;&#65292;&#20294;&#22312;&#40723;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#35775;&#38382;&#24179;&#31561;&#38480;&#21046;&#25110;&#20854;&#20182;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#38590;&#39064;&#26159;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#20174;&#20013;&#21463;&#30410;&#30340;&#20154;&#20013;&#37027;&#20123;&#33719;&#30410;&#26381;&#21153;&#30340;&#20351;&#29992;&#24046;&#36317;&#12290;&#24403;&#20915;&#31574;&#32773;&#23545;&#35775;&#38382;&#21644;&#24179;&#22343;&#32467;&#26524;&#37117;&#26377;&#20998;&#37197;&#20559;&#22909;&#26102;&#65292;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#35782;&#21035;&#12289;&#32479;&#35745;&#26041;&#24046;&#20943;&#23569;&#20272;&#35745;&#21644;&#31283;&#20581;&#20272;&#35745;&#30340;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#65292;&#21253;&#25324;&#22312;&#36829;&#21453;&#38451;&#24615;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.03720</link><description>&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#19982;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#65306;&#22522;&#20110;&#20960;&#20309;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Representer Theorems for Metric and Preference Learning: A Geometric Perspective. (arXiv:2304.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#32467;&#26500;&#20869;&#22312;&#30340;&#20869;&#31215;&#25152;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19977;&#20803;&#32452;&#27604;&#36739;&#30340;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#23427;&#23548;&#33268;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#33258;&#21253;&#21547;&#30340;&#35813;&#20219;&#21153;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#32463;&#20856;&#34920;&#29616;&#23450;&#29702;&#30340;&#26680;&#26415;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the metric and preference learning problem in Hilbert spaces. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem can be formulated with respect to the norm induced by the inner product inherent in the problem structure. Additionally, we demonstrate how our framework can be applied to the task of metric learning from triplet comparisons and show that it leads to a simple and self-contained representer theorem for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that the solution to the learning problem can be expressed using kernel terms, akin to classical representer theorems.
&lt;/p&gt;</description></item></channel></rss>