<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#21306;&#32463;&#39564;Bayes ECM&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12285</link><description>&lt;p&gt;
&#31232;&#30095;&#39640;&#32500;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#21306;&#32463;&#39564;Bayes ECM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse high-dimensional linear mixed modeling with a partitioned empirical Bayes ECM algorithm. (arXiv:2310.12285v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#21306;&#32463;&#39564;Bayes ECM&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#32437;&#21521;&#25968;&#25454;&#22312;&#21508;&#31181;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#32500;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;(LMMs)&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#32479;&#35745;&#26041;&#27861;&#21487;&#29992;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#36125;&#21494;&#26031;&#21464;&#37327;&#36873;&#25321;&#25110;&#32602;&#20989;&#25968;&#26041;&#27861;&#26159;&#38024;&#23545;&#29420;&#31435;&#35266;&#27979;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23569;&#25968;&#21487;&#29992;&#30340;&#39640;&#32500;LMMs&#36719;&#20214;&#21253;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#39640;&#32500;LMMs&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#39564;Bayes&#20272;&#35745;&#22120;&#30340;&#36229;&#21442;&#25968;&#26469;&#22686;&#21152;&#28789;&#27963;&#24615;&#65292;&#24182;&#20351;&#29992;Expectation-Conditional-Minimization (ECM)&#31639;&#27861;&#26469;&#35745;&#31639;&#21442;&#25968;&#30340;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;(MAP)&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;&#20998;&#21306;&#21644;&#21442;&#25968;&#25193;&#23637;&#65292;&#20197;&#21450;&#20854;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#20013;&#30340;&#22266;&#23450;&#25928;&#24212;&#21644;&#38543;&#26426;&#25928;&#24212;&#20272;&#35745;&#23637;&#31034;&#20102;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#32467;&#21512;&#20998;&#21306;&#32463;&#39564;Bayes ECM (LMM-PROBE)&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional longitudinal data is increasingly used in a wide range of scientific studies. However, there are few statistical methods for high-dimensional linear mixed models (LMMs), as most Bayesian variable selection or penalization methods are designed for independent observations. Additionally, the few available software packages for high-dimensional LMMs suffer from scalability issues. This work presents an efficient and accurate Bayesian framework for high-dimensional LMMs. We use empirical Bayes estimators of hyperparameters for increased flexibility and an Expectation-Conditional-Minimization (ECM) algorithm for computationally efficient maximum a posteriori probability (MAP) estimation of parameters. The novelty of the approach lies in its partitioning and parameter expansion as well as its fast and scalable computation. We illustrate Linear Mixed Modeling with PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies evaluating fixed and random effects estimation 
&lt;/p&gt;</description></item><item><title>TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11567</link><description>&lt;p&gt;
TSGM&#65306;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11567
&lt;/p&gt;
&lt;p&gt;
TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#20063;&#24456;&#26377;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#25110;&#39640;&#24230;&#25935;&#24863;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#22312;&#30740;&#31350;&#32773;&#21644;&#24037;&#19994;&#32452;&#32455;&#20043;&#38388;&#30340;&#20849;&#20139;&#20197;&#21450;&#29616;&#26377;&#21644;&#26032;&#30340;&#25968;&#25454;&#23494;&#38598;&#22411; ML &#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#30340;&#21487;&#33021;&#26041;&#27861;&#26159;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65288;TSGM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;TSGM&#21253;&#25324;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#29983;&#25104;&#27169;&#22411;&#12289;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#65306;&#30456;&#20284;&#24615;&#12289;&#19979;&#28216;&#25928;&#26524;&#12289;&#39044;&#27979;&#19968;&#33268;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#38544;&#31169;&#12290;&#35813;&#26694;&#26550;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;TSGM&#23558;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporally indexed data are essential in a wide range of fields and of interest to machine learning researchers. Time series data, however, are often scarce or highly sensitive, which precludes the sharing of data between researchers and industrial organizations and the application of existing and new data-intensive ML methods. A possible solution to this bottleneck is to generate synthetic data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling of synthetic time series. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, and simulator-based approaches. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, and privacy. The framework is extensible, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. TSGM w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02444</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26657;&#20934;Transformer
&lt;/p&gt;
&lt;p&gt;
Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23558;Transformer&#30340;&#25104;&#21151;&#25193;&#23637;&#21040;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#27880;&#24847;&#21147;&#65288;SGPA&#65289;&#65292;&#23427;&#30452;&#25509;&#22312;Transformer&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#65288;MHA&#65289;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20197;&#26657;&#20934;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#31216;&#26680;&#26367;&#20195;&#20102;&#32553;&#25918;&#28857;&#31215;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#25216;&#26415;&#26469;&#36817;&#20284;MHA&#36755;&#20986;&#30340;&#21518;&#39564;&#36807;&#31243;&#12290;&#32463;&#39564;&#19978;&#65292;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#19968;&#31995;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;SGPA&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
&lt;/p&gt;</description></item></channel></rss>