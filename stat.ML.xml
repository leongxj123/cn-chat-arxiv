<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30693;&#36947;&#28304;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23558;&#26679;&#26412;&#20174;&#28304;&#27169;&#22411;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#24402;&#32422;&#26041;&#27861;&#12290;&#36825;&#20123;&#24402;&#32422;&#26041;&#27861;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.07717</link><description>&lt;p&gt;
&#19968;&#20123;&#32479;&#35745;&#27169;&#22411;&#20043;&#38388;&#30340;&#39640;&#25928;&#24402;&#32422;
&lt;/p&gt;
&lt;p&gt;
Efficient reductions between some statistical models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30693;&#36947;&#28304;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23558;&#26679;&#26412;&#20174;&#28304;&#27169;&#22411;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#24402;&#32422;&#26041;&#27861;&#12290;&#36825;&#20123;&#24402;&#32422;&#26041;&#27861;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#21363;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#30693;&#36947;&#28304;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;&#22320;&#23558;&#26469;&#33258;&#28304;&#32479;&#35745;&#27169;&#22411;&#30340;&#26679;&#26412;&#36716;&#25442;&#20026;&#30446;&#26631;&#32479;&#35745;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#36896;&#20102;&#20960;&#20010;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#36825;&#31181;&#32479;&#35745;&#23454;&#39564;&#20043;&#38388;&#30340;&#24402;&#32422;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31243;&#24207;&#65292;&#21487;&#20197;&#36817;&#20284;&#23558;&#22343;&#21248;&#20998;&#24067;&#12289;Erlang&#20998;&#24067;&#21644;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;&#20301;&#32622;&#27169;&#22411;&#24402;&#32422;&#21040;&#19968;&#33324;&#30340;&#30446;&#26631;&#26063;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20123;&#32463;&#20856;&#30340;&#39640;&#32500;&#38382;&#39064;&#20043;&#38388;&#30340;&#38750;&#28176;&#36817;&#24402;&#32422;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20449;&#21495;&#38477;&#22122;&#31561;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24402;&#32422;&#20445;&#25345;&#20102;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#23558;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#26426;&#21046;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12708</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#23545;&#21487;&#38752;&#21644;&#21487;&#20449;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#39640;&#38169;&#35823;&#39118;&#38505;&#26102;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#20026;&#27169;&#22411;&#28155;&#21152;&#36873;&#25321;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36873;&#25321;&#27169;&#22411;&#23558;&#25552;&#20379;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#34987;&#25298;&#32477;&#39044;&#27979;&#27604;&#20363;&#65288;&#21363;&#27169;&#22411;&#19981;&#36827;&#34892;&#39044;&#27979;&#30340;&#20363;&#23376;&#27604;&#20363;&#65289;&#19982;&#22312;&#25152;&#36873;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#23384;&#22312;&#22810;&#20010;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#20173;&#23616;&#38480;&#20110;&#37096;&#20998;&#26041;&#27861;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#32473;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00809</link><description>&lt;p&gt;
&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#65306;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#26576;&#20123;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#22312;&#35843;&#33410;&#21518;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#29978;&#33267;&#22312;&#19981;&#23384;&#22312;&#26174;&#24335;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#27169;&#22411;&#23558;&#25152;&#26377;&#39044;&#27979;&#37117;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#31216;&#20026;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#65288;Initial Guessing Bias&#65292;IGB&#65289;&#65292;&#36825;&#21462;&#20915;&#20110;&#26550;&#26500;&#36873;&#25321;&#65292;&#20363;&#22914;&#28608;&#27963;&#20989;&#25968;&#12289;&#26368;&#22823;&#27744;&#21270;&#23618;&#21644;&#32593;&#32476;&#28145;&#24230;&#12290;&#25105;&#20204;&#23545;IGB&#36827;&#34892;&#30340;&#20998;&#26512;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#21487;&#20197;&#25351;&#23548;&#26550;&#26500;&#30340;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#12289;&#33258;&#24179;&#22343;&#30340;&#30772;&#22351;&#12289;&#26576;&#20123;&#22343;&#22330;&#36817;&#20284;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18779</link><description>&lt;p&gt;
&#20174;&#20960;&#20309;&#35282;&#24230;&#30475;&#24453;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
It begins with a boundary: A geometric view on probabilistically robust learning. (arXiv:2305.18779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#23558;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#37325;&#26500;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#28857;&#24050;&#32463;&#36716;&#21521;&#20102;&#20171;&#20110;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#21644;ERM&#25552;&#20379;&#30340;&#26356;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#65288;Robey&#31561;&#20154;&#65292;ICML&#65292;2022&#65289;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#26694;&#26550;&#26469;&#29702;&#35299;PRL&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#20854;&#21407;&#22987;&#34920;&#36848;&#20013;&#30340;&#24494;&#22937;&#32570;&#38519;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#39062;&#30340;&#26494;&#24347;&#26041;&#27861;&#35777;&#26126;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#30740;&#31350;&#20102;&#24341;&#20837;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#30340;&#29305;&#24615;&#20197;&#21450;&#23616;&#37096;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method -- Probabilistically Robust Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.00922</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#21494;&#38754;&#65306;&#40065;&#26834;&#24615;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Canonical foliations of neural networks: application to robustness. (arXiv:2203.00922v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#32780;&#23545;&#25239;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#37319;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#12290;&#36890;&#36807;&#21019;&#24314;&#32771;&#34385;&#25968;&#25454;&#31354;&#38388;&#26354;&#29575;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363; two-step spectral attack&#65292;&#26469;&#35828;&#26126;&#36825;&#20010;&#24819;&#27861;&#12290;&#25968;&#25454;&#31354;&#38388;&#34987;&#35270;&#20026;&#19968;&#20010;&#37197;&#22791;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340; Fisher &#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#25289;&#22238;&#30340;&#65288;&#36864;&#21270;&#30340;&#65289;&#40654;&#26364;&#27969;&#24418;&#12290;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#35813;&#24230;&#37327;&#20165;&#20026;&#21322;&#27491;&#23450;&#65292;&#20854;&#20869;&#26680;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#23545;&#35937;&#12290;&#20174;&#35813;&#26680;&#20013;&#23548;&#20986;&#19968;&#20010;&#35268;&#33539;&#21494;&#38754;&#12290;&#27178;&#21521;&#21494;&#30340;&#26354;&#29575;&#32473;&#20986;&#20102;&#36866;&#24403;&#30340;&#20462;&#27491;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20004;&#27493;&#36817;&#20284;&#30340;&#27979;&#22320;&#32447;&#21644;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#25239;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#19968;&#20010; 2D &#29609;&#20855;&#31034;&#20363;&#20013;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are known to be vulnerable to adversarial attacks. Adversarial learning is therefore becoming a crucial task. We propose a new vision on neural network robustness using Riemannian geometry and foliation theory. The idea is illustrated by creating a new adversarial attack that takes into account the curvature of the data space. This new adversarial attack called the two-step spectral attack is a piece-wise linear approximation of a geodesic in the data space. The data space is treated as a (degenerate) Riemannian manifold equipped with the pullback of the Fisher Information Metric (FIM) of the neural network. In most cases, this metric is only semi-definite and its kernel becomes a central object to study. A canonical foliation is derived from this kernel. The curvature of transverse leaves gives the appropriate correction to get a two-step approximation of the geodesic and hence a new efficient adversarial attack. The method is first illustrated on a 2D toy example
&lt;/p&gt;</description></item></channel></rss>