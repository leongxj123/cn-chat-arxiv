<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.16158</link><description>&lt;p&gt;
&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#19982;&#23567;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Fair Federated Learning with Small Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#36328;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29992;&#20110;&#30830;&#20445;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#20026;&#38598;&#20013;&#21270;&#25968;&#25454;&#29615;&#22659;&#35774;&#35745;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#21644;&#20998;&#24067;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#38024;&#23545;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#20998;&#24067;&#26080;&#20851;&#20445;&#35777;&#30340;&#21435;&#20013;&#24515;&#21270;&#21644;&#24322;&#26500;&#31995;&#32479;&#36827;&#34892;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;FedFaiREE&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23567;&#26679;&#26412;&#30340;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#23458;&#25143;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#23567;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#20026;bot&#25552;&#20379;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16158v1 Announce Type: cross  Abstract: As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for bot
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#19988;&#21487;&#20197;&#22312;&#22122;&#22768;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#21152;&#36895;&#33267;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13794</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;AdaGrad&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Convergence of AdaGrad with Relaxed Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13794
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#19988;&#21487;&#20197;&#22312;&#22122;&#22768;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#21152;&#36895;&#33267;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;AdaGrad&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;&#22122;&#22768;&#30340;&#22823;&#23567;&#30001;&#20989;&#25968;&#20540;&#24046;&#21644;&#26799;&#24230;&#22823;&#23567;&#25511;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#28085;&#30422;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#26377;&#30028;&#22122;&#22768;&#12289;&#27425;&#39640;&#26031;&#22122;&#22768;&#12289;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#21644;&#39044;&#26399;&#20809;&#28369;&#24230;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26356;&#21152;&#29616;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26681;&#25454;&#36890;&#29992;&#22122;&#22768;&#65292;&#21487;&#20197;&#36798;&#21040;( \tilde{\mathcal{O}}(1/\sqrt{T}))&#12290;&#36825;&#20010;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#23545;&#38382;&#39064;&#21442;&#25968;&#30340;&#20102;&#35299;&#65292;&#24403;&#19982;&#20989;&#25968;&#20540;&#24046;&#21644;&#22122;&#22768;&#27700;&#24179;&#30456;&#20851;&#30340;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#21040;(\tilde{\mathcal{O}}(1/T))&#65292;&#20854;&#20013;(T)&#34920;&#31034;&#24635;&#36845;&#20195;&#27425;&#25968;&#12290;&#25910;&#25947;&#36895;&#24230;&#22240;&#27492;&#21305;&#37197;&#20102;&#19979;&#38480;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13794v1 Announce Type: cross  Abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17744</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes image restoration with compressive autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17744
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#22312;&#35745;&#31639;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#25928;&#22270;&#20687;&#34920;&#31034;&#30340;&#33021;&#21147;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27491;&#21017;&#21270;&#22120;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#34987;&#30475;&#20316;&#20855;&#26377;&#28789;&#27963;&#28508;&#22312;&#20808;&#39564;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#27604;&#36215;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26356;&#23567;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17744v2 Announce Type: replace-cross  Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.14483</link><description>&lt;p&gt;
&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#65306;&#26657;&#20934;&#12289;&#39044;&#27979;&#24615;&#12289;&#38543;&#26426;&#24615;&#21644;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret. (arXiv:2401.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20851;&#20110;&#39044;&#27979;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#21482;&#26377;&#32463;&#36807;&#35780;&#20272;&#21518;&#25165;&#20855;&#26377;&#20854;&#26377;&#29992;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#20256;&#32479;&#19978;&#20851;&#27880;&#25439;&#22833;&#31867;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#36951;&#25022;&#12290;&#30446;&#21069;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#23545;&#26657;&#20934;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#36890;&#36807;&#23545;&#36172;&#24466;&#21644;&#39044;&#27979;&#32773;&#26045;&#21152;&#30452;&#35266;&#30340;&#38480;&#21046;&#65292;&#26657;&#20934;&#21644;&#36951;&#25022;&#33258;&#28982;&#22320;&#25104;&#20026;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#21338;&#24328;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#30456;&#23545;&#20110;&#39044;&#27979;&#32780;&#35328;&#65292;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#31561;&#21516;&#20110;&#20851;&#20110;&#32467;&#26524;&#30340;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#36825;&#20004;&#20010;&#26041;&#38754;&#20026;&#26657;&#20934;&#21644;&#36951;&#25022;&#12289;&#39044;&#27979;&#24615;&#21644;&#38543;&#26426;&#24615;&#65292;&#21363;&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is about forecasting. Forecasts, however, obtain their usefulness only through their evaluation. Machine learning has traditionally focused on types of losses and their corresponding regret. Currently, the machine learning community regained interest in calibration. In this work, we show the conceptual equivalence of calibration and regret in evaluating forecasts. We frame the evaluation problem as a game between a forecaster, a gambler and nature. Putting intuitive restrictions on gambler and forecaster, calibration and regret naturally fall out of the framework. In addition, this game links evaluation of forecasts to randomness of outcomes. Random outcomes with respect to forecasts are equivalent to good forecasts with respect to outcomes. We call those dual aspects, calibration and regret, predictiveness and randomness, the four facets of forecast felicity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#26799;&#24230;&#24773;&#20917;&#19979;&#26410;&#35843;&#25972;&#30340;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;&#20013;&#30340;&#21453;&#23556;&#32806;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein 1&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#39640;&#26031;&#38598;&#20013;&#30028;&#38480;&#65292;&#21516;&#26102;&#36824;&#32473;&#20986;&#20102;Wasserstein 2&#36317;&#31163;&#12289;&#24635;&#21464;&#24046;&#21644;&#30456;&#23545;&#29109;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18774</link><description>&lt;p&gt;
&#38750;&#20984;&#38543;&#26426;&#26799;&#24230;&#24773;&#20917;&#19979;&#26410;&#35843;&#25972;&#30340;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;&#20013;&#30340;&#21453;&#23556;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Reflection coupling for unadjusted generalized Hamiltonian Monte Carlo in the nonconvex stochastic gradient case. (arXiv:2310.18774v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#26799;&#24230;&#24773;&#20917;&#19979;&#26410;&#35843;&#25972;&#30340;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;&#20013;&#30340;&#21453;&#23556;&#32806;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein 1&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#39640;&#26031;&#38598;&#20013;&#30028;&#38480;&#65292;&#21516;&#26102;&#36824;&#32473;&#20986;&#20102;Wasserstein 2&#36317;&#31163;&#12289;&#24635;&#21464;&#24046;&#21644;&#30456;&#23545;&#29109;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#33021;&#38750;&#20984;&#30340;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20855;&#26377;&#38543;&#26426;&#26799;&#24230;&#30340;&#24191;&#20041;&#21704;&#23494;&#23572;&#39039;&#33945;&#29305;&#21345;&#32599;&#30340;Wasserstein 1&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#21147;&#23398;Langevin&#25193;&#25955;&#30340;&#20998;&#35010;&#26041;&#26696;&#31639;&#27861;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#32463;&#39564;&#24179;&#22343;&#20540;&#30340;&#23450;&#37327;&#39640;&#26031;&#38598;&#20013;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;Wasserstein 2&#36317;&#31163;&#12289;&#24635;&#21464;&#24046;&#21644;&#30456;&#23545;&#29109;&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#25968;&#20540;&#20559;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contraction in Wasserstein 1-distance with explicit rates is established for generalized Hamiltonian Monte Carlo with stochastic gradients under possibly nonconvex conditions. The algorithms considered include splitting schemes of kinetic Langevin diffusion. As consequence, quantitative Gaussian concentration bounds are provided for empirical averages. Convergence in Wasserstein 2-distance, total variation and relative entropy are also given, together with numerical bias estimates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;</title><link>http://arxiv.org/abs/2301.13088</link><description>&lt;p&gt;
Lie &#32676;&#21644;&#23427;&#20204;&#30340;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#38745;&#27490;&#26680;&#21644;&#39640;&#26031;&#36807;&#31243; II&#65306;&#38750;&#32039;&#23545;&#31216;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. (arXiv:2301.13088v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26102;&#31354;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#32534;&#30721;&#26377;&#20851;&#24314;&#27169;&#20989;&#25968;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21487;&#29992;&#20110;&#31934;&#30830;&#25110;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#21450;&#22320;&#36136;&#32479;&#35745;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#23545;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#26159;&#21487;&#20197;&#32771;&#34385;&#30340;&#26368;&#22522;&#26412;&#24418;&#24335;&#20043;&#19968;&#12290;&#39640;&#26031;&#36807;&#31243;&#21327;&#26041;&#24046;&#23545;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#31354;&#38388;&#30340;&#24179;&#31283;&#24615;&#27010;&#24565;&#30340;&#26368;&#33258;&#28982;&#30340;&#25512;&#24191;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#24314;&#31435;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#26500;&#36896;&#24615;&#21644;&#23454;&#29992;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#24615;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#38750;&#24120;&#22823;&#30340;&#31867;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20351;&#24471;&#33021;&#22815;&#65288;i&#65289;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#21644;&#65288;ii&#65289;&#20174;&#36825;&#20123;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#20013;&#23454;&#38469;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is 
&lt;/p&gt;</description></item></channel></rss>