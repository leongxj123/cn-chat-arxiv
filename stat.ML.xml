<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.18540</link><description>&lt;p&gt;
skscope&#65306;Python&#20013;&#30340;&#24555;&#36895;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
skscope: Fast Sparsity-Constrained Optimization in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18540
&lt;/p&gt;
&lt;p&gt;
skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#65288;SCO&#65289;&#19978;&#24212;&#29992;&#36845;&#20195;&#27714;&#35299;&#22120;&#38656;&#35201;&#32321;&#29712;&#30340;&#25968;&#23398;&#25512;&#23548;&#21644;&#20180;&#32454;&#30340;&#32534;&#31243;/&#35843;&#35797;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24211;skscope&#65292;&#20197;&#20811;&#26381;&#27492;&#38556;&#30861;&#12290;&#20511;&#21161;skscope&#65292;&#29992;&#25143;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#21363;&#21487;&#35299;&#20915;SCO&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#28436;&#31034;&#20102;skscope&#30340;&#26041;&#20415;&#20043;&#22788;&#65292;&#20854;&#20013;&#21482;&#38656;&#22235;&#34892;&#20195;&#30721;&#23601;&#21487;&#20197;&#35299;&#20915;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#36235;&#21183;&#36807;&#28388;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;skscope&#30340;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;skscope&#20013;&#30340;&#21487;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#23454;&#29616;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#31454;&#20105;&#26494;&#24347;&#35299;&#39640;&#36798;80&#20493;&#30340;&#21152;&#36895;&#24230;&#12290;skscope&#24050;&#32463;&#21457;&#24067;&#22312;Python&#36719;&#20214;&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#21644;Conda&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18540v1 Announce Type: cross  Abstract: Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.07511</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21355;&#26143;&#38477;&#27700;&#25554;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in satellite precipitation interpolation with machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#25968;&#25454;&#38598;&#65292;&#20294;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#32570;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20845;&#31181;&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#26159;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#37327;&#21270;&#31354;&#38388;&#25554;&#20540;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36830;&#32493;&#32654;&#22269;&#30340;15&#24180;&#26376;&#24230;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65288;QRF&#65289;&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#65288;GRF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#12289;&#36731;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#12290;&#23427;&#20204;&#33021;&#22815;&#22312;&#20061;&#20010;&#20998;&#20301;&#27700;&#24179;&#65288;0.025&#12289;0.050&#12289;0.100&#12289;0.250&#12289;0.500&#12289;0.750&#12289;0.900&#12289;0.950&#12289;0.975&#65289;&#19978;&#21457;&#24067;&#39044;&#27979;&#38477;&#27700;&#20998;&#20301;&#25968;&#65292;&#20197;&#36817;&#20284;&#23436;&#25972;&#27010;&#29575;&#20998;&#24067;&#65292;&#35780;&#20272;&#26102;&#37319;&#29992;&#20998;&#20301;&#25968;&#35780;&#20998;&#20989;&#25968;&#21644;&#20998;&#20301;&#25968;&#35780;&#20998;&#35268;&#21017;&#12290;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#21355;&#26143;&#38477;&#27700;&#65288;PERSIA
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07511v2 Announce Type: replace-cross  Abstract: Merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. We address this gap by benchmarking six algorithms, mostly novel for this task, for quantifying predictive uncertainty in spatial interpolation. On 15 years of monthly data over the contiguous United States (CONUS), we compared quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). Their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. Feature importance analysis revealed satellite precipitation (PERSIA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2302.09193</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#30340;&#21487;&#36716;&#31227;&#27169;&#22411;&#29992;&#20110;&#21512;&#25104;&#20154;&#21475;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Copula-based transferable models for synthetic population generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.09193
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32508;&#21512;&#28041;&#21450;&#29983;&#25104;&#24494;&#35266;&#20195;&#29702;&#30446;&#26631;&#20154;&#21475;&#30340;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#34892;&#20026;&#24314;&#27169;&#21644;&#27169;&#25311;&#12290; &#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#30446;&#26631;&#20154;&#21475;&#26679;&#26412;&#65292;&#22914;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#25110;&#26053;&#34892;&#35843;&#26597;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#36739;&#23567;&#30340;&#26679;&#26412;&#37327;&#65292;&#22312;&#36739;&#23567;&#30340;&#22320;&#29702;&#23610;&#24230;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20165;&#24050;&#30693;&#32463;&#39564;&#36793;&#38469;&#20998;&#24067;&#30340;&#30446;&#26631;&#20154;&#21475;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290; &#35813;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#30340;&#19981;&#21516;&#20154;&#21475;&#30340;&#26679;&#26412;&#65292;&#23558;&#31354;&#38388;&#32452;&#20214;&#24341;&#20837;&#21040;&#20154;&#21475;&#32508;&#21512;&#20013;&#65292;&#24182;&#32771;&#34385;&#21508;&#31181;&#20449;&#24687;&#28304;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#29983;&#25104;&#22120;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36807;&#31243;&#28041;&#21450;&#23558;&#25968;&#25454;&#26631;&#20934;&#21270;&#24182;&#23558;&#20854;&#35270;&#20026;&#32473;&#23450;Copula&#30340;&#23454;&#29616;&#65292;&#28982;&#21518;&#22312;&#34701;&#20837;&#20851;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.09193v2 Announce Type: replace-cross  Abstract: Population synthesis involves generating synthetic yet realistic representations of a target population of micro-agents for behavioral modeling and simulation. Traditional methods, often reliant on target population samples, such as census data or travel surveys, face limitations due to high costs and small sample sizes, particularly at smaller geographical scales. We propose a novel framework based on copulas to generate synthetic data for target populations where only empirical marginal distributions are known. This method utilizes samples from different populations with similar marginal dependencies, introduces a spatial component into population synthesis, and considers various information sources for more realistic generators. Concretely, the process involves normalizing the data and treat it as realizations of a given copula, and then training a generative model before incorporating the information on the marginals of the
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29616;&#20195;&#25968;&#25454;&#38598;&#20013;&#22823;&#37327;&#20013;&#38388;&#32467;&#26524;&#30340;&#20107;&#23454;&#65292;&#21363;&#20351;&#27809;&#26377;&#21333;&#20010;&#26367;&#20195;&#25351;&#26631;&#28385;&#36275;&#32479;&#35745;&#26367;&#20195;&#26465;&#20214;&#65292;&#20351;&#29992;&#22810;&#20010;&#26367;&#20195;&#25351;&#26631;&#20063;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/1603.09326</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#20010;&#26367;&#20195;&#25351;&#26631;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#65306;&#26367;&#20195;&#20998;&#25968;&#21644;&#26367;&#20195;&#25351;&#25968;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1603.09326
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29616;&#20195;&#25968;&#25454;&#38598;&#20013;&#22823;&#37327;&#20013;&#38388;&#32467;&#26524;&#30340;&#20107;&#23454;&#65292;&#21363;&#20351;&#27809;&#26377;&#21333;&#20010;&#26367;&#20195;&#25351;&#26631;&#28385;&#36275;&#32479;&#35745;&#26367;&#20195;&#26465;&#20214;&#65292;&#20351;&#29992;&#22810;&#20010;&#26367;&#20195;&#25351;&#26631;&#20063;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#38271;&#26399;&#20316;&#29992;&#26159;&#35768;&#22810;&#39046;&#22495;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290; &#20272;&#35745;&#27492;&#31867;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#20010;&#24120;&#35265;&#25361;&#25112;&#22312;&#20110;&#38271;&#26399;&#32467;&#26524;&#22312;&#38656;&#35201;&#20570;&#20986;&#25919;&#31574;&#20915;&#31574;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26159;&#26410;&#35266;&#23519;&#21040;&#30340;&#12290; &#20811;&#26381;&#36825;&#31181;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20998;&#26512;&#27835;&#30103;&#25928;&#26524;&#23545;&#20013;&#38388;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#31216;&#20026;&#32479;&#35745;&#26367;&#20195;&#25351;&#26631;&#65292;&#22914;&#26524;&#28385;&#36275;&#26465;&#20214;&#65306;&#22312;&#32479;&#35745;&#26367;&#20195;&#25351;&#26631;&#30340;&#26465;&#20214;&#19979;&#65292;&#27835;&#30103;&#21644;&#32467;&#26524;&#26159;&#29420;&#31435;&#30340;&#12290;  &#26367;&#20195;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#32463;&#24120;&#26159;&#26377;&#20105;&#35758;&#30340;&#12290; &#22312;&#29616;&#20195;&#25968;&#25454;&#38598;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#35266;&#23519;&#21040;&#22823;&#37327;&#20013;&#38388;&#32467;&#26524;&#65292;&#21487;&#33021;&#26159;&#25968;&#30334;&#20010;&#25110;&#25968;&#21315;&#20010;&#65292;&#34987;&#35748;&#20026;&#20301;&#20110;&#27835;&#30103;&#21644;&#38271;&#26399;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#38142;&#19978;&#25110;&#38468;&#36817;&#12290; &#21363;&#20351;&#27809;&#26377;&#20010;&#21035;&#20195;&#29702;&#28385;&#36275;&#32479;&#35745;&#26367;&#20195;&#26465;&#20214;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#20063;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1603.09326v4 Announce Type: replace-cross  Abstract: Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#26469;&#33719;&#24471;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#20248;&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#32771;&#34385;&#21644;&#19977;&#31181;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.05352</link><description>&lt;p&gt;
&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#21442;&#25968;&#21270;MMSE&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging Variational Autoencoders for Parameterized MMSE Channel Estimation. (arXiv:2307.05352v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#26469;&#33719;&#24471;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#20248;&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#32771;&#34385;&#21644;&#19977;&#31181;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#23558;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#20449;&#36947;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#12290;&#25152;&#24471;&#21040;&#30340;&#20449;&#36947;&#20272;&#35745;&#22120;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20869;&#37096;&#32467;&#26500;&#23545;&#26469;&#33258;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#20272;&#35745;&#22120;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20160;&#20040;&#26465;&#20214;&#19979;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#26159;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#23454;&#29992;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#38454;&#27573;&#23545;&#20449;&#36947;&#30693;&#35782;&#30340;&#33719;&#21462;&#26041;&#24335;&#19981;&#21516;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#22522;&#20110;&#22122;&#22768;&#23548;&#39057;&#35266;&#27979;&#36827;&#34892;&#35757;&#32451;&#30340;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#33719;&#21462;&#20449;&#36947;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this manuscript, we propose to utilize the generative neural network-based variational autoencoder for channel estimation. The variational autoencoder models the underlying true but unknown channel distribution as a conditional Gaussian distribution in a novel way. The derived channel estimator exploits the internal structure of the variational autoencoder to parameterize an approximation of the mean squared error optimal estimator resulting from the conditional Gaussian channel models. We provide a rigorous analysis under which conditions a variational autoencoder-based estimator is mean squared error optimal. We then present considerations that make the variational autoencoder-based estimator practical and propose three different estimator variants that differ in their access to channel knowledge during the training and evaluation phase. In particular, the proposed estimator variant trained solely on noisy pilot observations is particularly noteworthy as it does not require access
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01497</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19968;&#33324;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31867;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#35266;&#27979;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#19982;&#31639;&#27861;&#20135;&#29983;&#30340;&#36817;&#20284;&#35299;&#30340;"&#20122;&#26368;&#20248;&#24615;" &#30456;&#20851;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#32479;&#35745;&#23398;&#20013;&#30340;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#8212;&#8212;&#38543;&#26426;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;SAGD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#22806;&#25512;&#65288;SGE&#65289;&#8212;&#8212;&#23427;&#20204;&#20855;&#26377;&#19968;&#31181;&#29305;&#27530;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
&lt;/p&gt;</description></item></channel></rss>