<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13929</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21457;&#29616;&#20915;&#31574;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics. (arXiv:2401.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#65292;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#30340;&#35777;&#25454;&#34920;&#26126;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#21487;&#33021;&#20316;&#20026;MDD&#30340;&#34892;&#20026;&#26631;&#35760;&#12290;&#20026;&#20102;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#65292;&#24739;&#32773;&#25191;&#34892;&#28041;&#21450;&#20570;&#20986;&#36873;&#25321;&#25110;&#23545;&#19982;&#19981;&#21516;&#32467;&#26524;&#30456;&#20851;&#32852;&#30340;&#21050;&#28608;&#20316;&#20986;&#21453;&#24212;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#34987;&#25311;&#21512;&#20197;&#25552;&#21462;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#21508;&#20010;&#26041;&#38754;&#30340;&#21442;&#25968;&#65292;&#20197;&#34920;&#24449;&#24739;&#32773;&#22312;&#34892;&#20026;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#22522;&#20110;&#21333;&#20010;RL&#27169;&#22411;&#30340;&#22870;&#21169;&#23398;&#20064;&#34920;&#24449;&#19981;&#36275;; &#30456;&#21453;&#65292;&#20915;&#31574;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#20043;&#38388;&#30340;&#20999;&#25442;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31185;&#23398;&#38382;&#39064;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#21160;&#24577;&#22914;&#20309;&#24433;&#21709;MDD&#24739;&#32773;&#30340;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#27010;&#29575;&#22870;&#21169;&#20219;&#21153;(PRT)&#25152;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.08620</link><description>&lt;p&gt;
&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Music Transformer. (arXiv:2306.08620v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;anticipation&#65288;&#39044;&#27979;&#65289;&#65306;&#19968;&#31181;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20107;&#20214;&#36807;&#31243;&#65288;&#26102;&#38388;&#28857;&#36807;&#31243;&#65289;&#30340;&#23454;&#29616;&#65292;&#20197;&#24322;&#27493;&#22320;&#25511;&#21046;&#19982;&#31532;&#20108;&#20010;&#30456;&#20851;&#36807;&#31243;&#65288;&#25511;&#21046;&#36807;&#31243;&#65289;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#38169;&#20107;&#20214;&#21644;&#25511;&#20214;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;&#25511;&#20214;&#20986;&#29616;&#22312;&#20107;&#20214;&#24207;&#21015;&#30340;&#20572;&#27490;&#26102;&#38388;&#20043;&#21518;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21160;&#26426;&#26469;&#33258;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#25511;&#21046;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;infiling&#65288;&#34917;&#20840;&#65289;&#25511;&#21046;&#20219;&#21153;&#65292;&#20854;&#20013;&#25511;&#21046;&#20107;&#20214;&#26159;&#20107;&#20214;&#26412;&#36523;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#26465;&#20214;&#29983;&#25104;&#23436;&#25104;&#32473;&#23450;&#22266;&#23450;&#25511;&#21046;&#20107;&#20214;&#30340;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22810;&#26679;&#30340;Lakh MIDI&#38899;&#20048;&#25968;&#25454;&#38598;&#35757;&#32451;&#39044;&#27979;infiling&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#19982;&#25552;&#31034;&#38899;&#20048;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#25191;&#34892;infilling&#25511;&#21046;&#20219;&#21153;&#30340;&#38468;&#21152;&#33021;&#21147;&#65292;&#21253;&#25324;&#20276;&#22863;&#12290;&#20154;&#24037;&#35780;&#20272;&#21592;&#25253;&#21578;&#35828;&#65292;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#30340;&#20276;&#22863;&#20855;&#26377;&#39640;&#21487;&#36776;&#24615;&#21644;&#20248;&#32654;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with
&lt;/p&gt;</description></item></channel></rss>