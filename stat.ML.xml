<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19629</link><description>&lt;p&gt;
&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Metric Learning from Limited Pairwise Preference Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19629
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29702;&#24819;&#28857;&#27169;&#22411;&#19979;&#30340;&#20559;&#22909;&#27604;&#36739;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#65292;&#20854;&#20013;&#29992;&#25143;&#22914;&#26524;&#19968;&#20010;&#39033;&#30446;&#27604;&#20854;&#28508;&#22312;&#29702;&#24819;&#39033;&#30446;&#26356;&#25509;&#36817;&#65292;&#21017;&#26356;&#21916;&#27426;&#35813;&#39033;&#30446;&#12290;&#36825;&#20123;&#39033;&#30446;&#23884;&#20837;&#21040;&#20855;&#26377;&#26410;&#30693;&#39532;&#27663;&#36317;&#31163;&#30340;$\mathbb{R}^d$&#20013;&#65292;&#35813;&#36317;&#31163;&#22312;&#29992;&#25143;&#38388;&#20849;&#20139;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;$\mathcal{O}(d)$&#20010;&#25104;&#23545;&#27604;&#36739;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#24230;&#37327;&#21644;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#26377;$o(d)$&#30340;&#26377;&#38480;&#27604;&#36739;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21363;&#20351;&#24050;&#30693;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#29616;&#22312;&#19981;&#20877;&#21487;&#33021;&#65292;&#24230;&#37327;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#33324;&#26469;&#35828;&#65292;$o(d)$&#27604;&#36739;&#19981;&#20250;&#25581;&#31034;&#26377;&#20851;&#24230;&#37327;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#25143;&#25968;&#37327;&#26080;&#38480;&#12290;&#28982;&#32780;&#65292;&#24403;&#27604;&#36739;&#30340;&#39033;&#30446;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#21487;&#20197;&#26377;&#21161;&#20110;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#65292;&#36825;&#26679;&#24230;&#37327;&#23601;&#21487;&#20197;&#34987;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.19449</link><description>&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;Adam&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#19982;&#19981;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#27604;&#19982;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#24930;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26679;&#26412;&#26469;&#33258;&#30456;&#23545;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65292;&#24179;&#22343;&#25439;&#22833;&#20540;&#22312;&#26799;&#24230;&#19979;&#38477;&#26102;&#19979;&#38477;&#36895;&#24230;&#36739;&#24930;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#21364;&#19981;&#21463;&#27492;&#38382;&#39064;&#24433;&#21709;&#65292;&#24182;&#25913;&#21892;&#20102;&#25152;&#26377;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#34892;&#20026;&#30830;&#23454;&#26159;&#30001;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19449v1 Announce Type: cross  Abstract: Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear clas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09373</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Loss Shaping Constraints for Long-Term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#26377;&#22823;&#37327;&#30340;&#25991;&#29486;&#65292;&#20294;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#31383;&#21475;&#19978;&#30340;&#24615;&#33021;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#27979;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#38169;&#35823;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#24120;&#35265;&#39044;&#27979;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#26368;&#36817;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#38169;&#35823;&#36807;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20445;&#25345;&#29992;&#25143;&#23450;&#20041;&#30340;&#25439;&#22833;&#19978;&#38480;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;&#65292;&#22240;&#20026;&#23427;&#23545;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#23545;&#20598;&#24615;&#32467;&#26524;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26657;&#20934;&#38477;&#32500;&#36229;&#21442;&#25968;&#65292;&#25506;&#32034;&#20102;&#22256;&#24785;&#24230;&#21644;&#32500;&#24230;&#25968;&#37327;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2312.02946</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26657;&#20934;&#38477;&#32500;&#36229;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Calibrating dimension reduction hyperparameters in the presence of noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26657;&#20934;&#38477;&#32500;&#36229;&#21442;&#25968;&#65292;&#25506;&#32034;&#20102;&#22256;&#24785;&#24230;&#21644;&#32500;&#24230;&#25968;&#37327;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#24037;&#20855;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#36825;&#20123;&#24037;&#20855;&#34987;&#29992;&#20110;&#22122;&#22768;&#38477;&#20302;&#12289;&#21487;&#35270;&#21270;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#31561;&#21508;&#31181;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#22312;&#38477;&#32500;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#35752;&#35770;&#36807;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36807;&#25311;&#21512;&#65292;&#32780;&#22312;&#20854;&#20182;&#24314;&#27169;&#38382;&#39064;&#20013;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#35752;&#35770;&#12290;&#22914;&#26524;&#25105;&#20204;&#23558;&#25968;&#25454;&#35299;&#37322;&#20026;&#20449;&#21495;&#21644;&#22122;&#22768;&#30340;&#32452;&#21512;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23545;&#38477;&#32500;&#25216;&#26415;&#30340;&#35780;&#21028;&#26159;&#20854;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#20840;&#37096;&#20869;&#23481;&#65292;&#21363;&#20449;&#21495;&#21644;&#22122;&#22768;&#12290;&#22312;&#20854;&#20182;&#24314;&#27169;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20250;&#37319;&#29992;&#29305;&#24449;&#36873;&#25321;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#27491;&#21017;&#21270;&#31561;&#25216;&#26415;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20294;&#22312;&#36827;&#34892;&#38477;&#32500;&#26102;&#21364;&#27809;&#26377;&#37319;&#21462;&#31867;&#20284;&#30340;&#39044;&#38450;&#25514;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#38477;&#32500;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#35813;&#26694;&#26550;&#25506;&#32034;&#20102;&#22256;&#24785;&#24230;&#21644;&#32500;&#24230;&#25968;&#37327;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of dimension reduction tools is to construct a low-dimensional representation of high-dimensional data. These tools are employed for a variety of reasons such as noise reduction, visualization, and to lower computational costs. However, there is a fundamental issue that is highly discussed in other modeling problems, but almost entirely ignored in the dimension reduction literature: overfitting. If we interpret data as a combination of signal and noise, prior works judge dimension reduction techniques on their ability to capture the entirety of the data, i.e. both the signal and the noise. In the context of other modeling problems, techniques such as feature-selection, cross-validation, and regularization are employed to combat overfitting, but no such precautions are taken when performing dimension reduction. In this paper, we present a framework that models dimension reduction problems in the presence of noise and use this framework to explore the role perplexity and number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04647</link><description>&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65306;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#28385;&#36275;&#19968;&#32452;&#32447;&#24615;&#27979;&#37327;&#65292;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#30340;&#25968;&#20540;&#23481;&#38480;&#12290;&#21387;&#32553;&#24863;&#30693;&#26159;&#32479;&#35745;&#23398;&#12289;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#25968;&#25454;&#21387;&#32553;&#21644;&#22270;&#20687;&#37325;&#24314;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;$\ell_2$&#27491;&#21017;&#21270;&#30340;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#38181;&#35268;&#21010;&#26469;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#27492;&#38382;&#39064;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#28201;&#21644;&#38480;&#21046;&#19979;&#65292;&#24471;&#21040;&#30340;&#26494;&#24347;&#31561;&#20215;&#20110;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#30784;&#36861;&#36394;&#21435;&#22122;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#26494;&#24347;&#26469;&#21152;&#24378;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#20197;&#30830;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#31934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
&lt;/p&gt;</description></item></channel></rss>