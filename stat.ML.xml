<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#19979;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;&#65292;&#25581;&#31034;&#20102;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10946</link><description>&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;
&lt;/p&gt;
&lt;p&gt;
The Fallacy of Minimizing Local Regret in the Sequential Task Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10946
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#24207;&#21015;&#20219;&#21153;&#35774;&#32622;&#19979;&#26368;&#23567;&#21270;&#23616;&#37096;&#36951;&#25022;&#30340;&#35884;&#35823;&#65292;&#25581;&#31034;&#20102;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#32463;&#24120;&#34987;&#27010;&#24565;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#31639;&#27861;&#19982;&#26410;&#30693;&#29615;&#22659;&#20132;&#20114;&#20197;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#22312;&#38745;&#24577;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#22914;&#27425;&#32447;&#24615;&#65288;$\sqrt{T}$&#65289;&#36951;&#25022;&#30028;&#38480;&#65292;&#36890;&#24120;&#24847;&#21619;&#30528;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#24182;&#20572;&#27490;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29702;&#35770;&#35774;&#32622;&#36890;&#24120;&#36807;&#20998;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#20219;&#21153;&#25353;&#39034;&#24207;&#21040;&#36798;&#65292;&#20219;&#21153;&#20043;&#38388;&#26377;&#37325;&#22823;&#21464;&#21270;&#65292;&#24182;&#19988;&#31639;&#27861;&#21487;&#33021;&#19981;&#20801;&#35768;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#36229;&#20986;&#32467;&#26524;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#28085;&#30422;&#22870;&#21169;&#35774;&#35745;&#65288;&#20174;&#32467;&#26524;&#21040;&#22870;&#21169;&#30340;&#26144;&#23556;&#65289;&#21644;&#20801;&#35768;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#36817;&#35270;&#22320;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#35884;&#35823;&#65306;&#33719;&#24471;&#26368;&#20248;&#36951;&#25022;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10946v1 Announce Type: cross  Abstract: In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#24212;&#23545;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#31163;&#25955;&#20998;&#24067;&#65292;&#21253;&#25324;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#12289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#21644;&#33258;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.17699</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#24212;&#23545;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#31163;&#25955;&#20998;&#24067;&#65292;&#21253;&#25324;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#12289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#21644;&#33258;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#36890;&#24120;&#30001;&#20110;&#22266;&#26377;&#30340;&#19981;&#36830;&#32493;&#24615;&#32780;&#21576;&#29616;&#39640;&#24230;&#22810;&#27169;&#24577;&#12290;&#34429;&#28982;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#37319;&#26679;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#20449;&#24687;&#65292;&#23427;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24490;&#29615;&#35843;&#24230;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#31163;&#25955;&#20998;&#24067;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#65306;&#65288;1&#65289;&#24490;&#29615;&#27493;&#38271;&#35843;&#24230;&#65292;&#20854;&#20013;&#22823;&#27493;&#38271;&#21457;&#29616;&#26032;&#27169;&#24335;&#65292;&#23567;&#27493;&#38271;&#21033;&#29992;&#27599;&#20010;&#27169;&#24335;&#65307;&#65288;2&#65289;&#24490;&#29615;&#24179;&#34913;&#35843;&#24230;&#65292;&#30830;&#20445;&#32473;&#23450;&#27493;&#38271;&#30340;&#8220;&#24179;&#34913;&#8221;&#25552;&#26696;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#39640;&#25928;&#29575;&#65307;&#20197;&#21450;&#65288;3&#65289;&#33258;&#21160;&#35843;&#25972;&#26041;&#26696;&#65292;&#29992;&#20110;&#35843;&#25972;&#24490;&#29615;&#35843;&#24230;&#20013;&#30340;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#33258;&#36866;&#24212;&#24615;&#19988;&#38656;&#26368;&#23567;&#35843;&#25972;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#21644;&#25512;&#26029;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17699v1 Announce Type: new  Abstract: Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16877</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#22635;&#20805;&#21644;&#36890;&#36807;&#26631;&#31614;&#22635;&#20805;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22635;&#20805;&#26041;&#27861;&#26469;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#37117;&#26377;&#26631;&#31614;&#65292;&#20294;&#24120;&#35265;&#30340;&#22635;&#20805;&#26041;&#27861;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#36755;&#20837;&#32780;&#24573;&#30053;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23558;&#26631;&#31614;&#22534;&#21472;&#21040;&#36755;&#20837;&#20013;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#36755;&#20837;&#30340;&#22635;&#20805;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#39044;&#27979;&#30340;&#27979;&#35797;&#26631;&#31614;&#21021;&#22987;&#21270;&#20026;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#22312;&#19968;&#36215;&#36827;&#34892;&#22635;&#20805;&#12290;&#36825;&#26679;&#21487;&#20197;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#32780;&#19988;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#22635;&#20805;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36830;&#32493;&#22411;&#12289;&#20998;&#31867;&#22411;&#25110;&#28151;&#21512;&#22411;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14689</link><description>&lt;p&gt;
&#22522;&#20110;&#23725;&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#30340;&#27424;&#21442;&#25968;&#21270;&#21452;&#35895;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising of Data on a Line. (arXiv:2305.14689v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#23725;&#27491;&#21017;&#21270;&#30340;&#21435;&#22122;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#28857;&#25968;&#12289;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#25968;&#21644;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24050;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#33021;&#20986;&#29616;&#21452;&#23792;&#35895;&#29616;&#35937;&#65292;&#32780;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21017;&#26222;&#36941;&#23384;&#22312;&#26631;&#20934;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#21487;&#20197;&#35777;&#26126;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#21487;&#20197;&#21457;&#29983;&#21452;&#23792;&#35895;&#29616;&#35937;&#12290;&#32771;&#34385;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25968;&#25454;&#26368;&#23567;&#20108;&#20056;&#21435;&#22122;&#38382;&#39064;&#20013;&#30340;&#23725;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#28176;&#36817;&#20934;&#30830;&#30340;&#24191;&#20041;&#35823;&#24046;&#20844;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26679;&#26412;&#21644;&#21442;&#25968;&#30340;&#21452;&#35895;&#25928;&#24212;&#65292;&#21452;&#23792;&#35895;&#20301;&#20110;&#25554;&#20540;&#28857;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#21306;&#22495;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#21452;&#35895;&#26354;&#32447;&#30340;&#39640;&#23792;&#23545;&#24212;&#20110;&#20272;&#35745;&#37327;&#30340;&#33539;&#25968;&#26354;&#32447;&#30340;&#39640;&#23792;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between the number of training data points, the number of parameters in a statistical model, and the generalization capabilities of the model has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime, and believe that the standard bias-variance trade-off holds in the under-parameterized regime. In this paper, we present a simple example that provably exhibits double descent in the under-parameterized regime. For simplicity, we look at the ridge regularized least squares denoising problem with data on a line embedded in high-dimension space. By deriving an asymptotically accurate formula for the generalization error, we observe sample-wise and parameter-wise double descent with the peak in the under-parameterized regime rather than at the interpolation point or in the over-parameterized regime.  Further, the peak of the sample-wise double descent curve corresponds to a peak in the curve for the norm of the estimator,
&lt;/p&gt;</description></item></channel></rss>