<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15776</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;MDP&#20013;&#30340;&#30495;&#27491;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Truly No-Regret Learning in Constrained MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24314;&#27169;&#23433;&#20840;&#32422;&#26463;&#30340;&#24120;&#35265;&#26041;&#24335;&#12290;&#30446;&#21069;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;CMDPs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#65292;&#25152;&#26377;&#24403;&#21069;&#24050;&#30693;&#30340;&#21518;&#24724;&#30028;&#37117;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#8212;&#8212;&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#29992;&#20005;&#26684;&#30340;&#32422;&#26463;&#28385;&#36275;&#22312;&#21478;&#19968;&#20010;&#22238;&#21512;&#20013;&#12290;&#36825;&#20351;&#24471;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#19981;&#23433;&#20840;&#65292;&#22240;&#20026;&#23427;&#20165;&#20445;&#35777;&#26368;&#32456;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#27491;&#22914;Efroni&#31561;&#20154;&#65288;2020&#24180;&#65289;&#25351;&#20986;&#30340;&#65292;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#21487;&#35777;&#26126;&#22320;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#20110;&#27491;&#21017;&#21270;&#21407;&#22987;-&#23545;&#20598;&#26041;&#26696;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36890;&#29992;&#21270;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21407;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10232</link><description>&lt;p&gt;
Johnson-Lindenstrauss&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple, unified analysis of Johnson-Lindenstrauss with applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#36825;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#38477;&#32500;&#39046;&#22495;&#20013;&#30340;&#22522;&#30707;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#29702;&#35299;&#65292;&#36824;&#23558;&#21508;&#31181;&#26500;&#36896;&#32479;&#19968;&#21040;JL&#26694;&#26550;&#19979;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#12290;&#36825;&#31181;&#31616;&#21270;&#21644;&#32479;&#19968;&#22312;&#20445;&#25345;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23545;&#20174;&#27969;&#31639;&#27861;&#21040;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#31616;&#21270;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#29699;&#24418;&#26500;&#36896;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#35777;&#26126;&#12290;&#25105;&#20204;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#24120;&#25968;&#65292;&#36825;&#26631;&#24535;&#30528;&#25991;&#29486;&#20013;&#36136;&#30340;&#39134;&#36291;&#12290;&#36890;&#36807;&#36816;&#29992;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.07613</link><description>&lt;p&gt;
&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global optimality under amenable symmetry constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#28385;&#36275;&#21487;&#25509;&#21463;&#21464;&#25442;&#32676;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#21363;&#21516;&#26102;&#28385;&#36275;&#20197;&#19979;&#20004;&#20010;&#26465;&#20214;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#32473;&#23450;&#30340;&#20984;&#24615;&#27867;&#20989;&#25110;&#39118;&#38505;&#65292;&#65288;2&#65289;&#28385;&#36275;&#21487;&#23481;&#24525;&#23545;&#31216;&#32422;&#26463;&#12290;&#36825;&#31181;&#23545;&#31216;&#24615;&#36136;&#30340;&#20363;&#23376;&#21253;&#25324;&#19981;&#21464;&#24615;&#12289;&#21487;&#21464;&#24615;&#25110;&#20934;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;Stein&#21644;Le Cam&#30340;&#32769;&#24605;&#24819;&#65292;&#20197;&#21450;&#22312;&#21487;&#25509;&#21463;&#32676;&#30340;&#36941;&#21382;&#23450;&#29702;&#20013;&#20986;&#29616;&#30340;&#36817;&#20284;&#32676;&#24179;&#22343;&#20540;&#12290;&#22312;&#20984;&#20998;&#26512;&#20013;&#65292;&#19968;&#31867;&#31216;&#20026;&#36712;&#36947;&#20984;&#20307;&#30340;&#20984;&#38598;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#20013;&#30830;&#23450;&#20102;&#36825;&#31867;&#36712;&#36947;&#20984;&#20307;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#31216;&#20026;coycle&#30340;&#31616;&#21333;&#35013;&#32622;&#22914;&#20309;&#23558;&#19981;&#21516;&#24418;&#24335;&#30340;&#23545;&#31216;&#24615;&#36716;&#21270;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22312;&#23545;&#31216;&#32422;&#26463;&#19979;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#30340;Monge-Kantorovich&#23450;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.04082</link><description>&lt;p&gt;
&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#25913;&#36827;&#20102;&#22270;&#26696;&#25903;&#26550;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved motif-scaffolding with SE(3) flow matching. (arXiv:2401.04082v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35774;&#35745;&#36890;&#24120;&#20174;&#19968;&#20010;&#22270;&#26696;&#30340;&#26399;&#26395;&#21151;&#33021;&#24320;&#22987;&#65292;&#22270;&#26696;&#25903;&#26550;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21151;&#33021;&#24615;&#34507;&#30333;&#36136;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#35774;&#35745;&#21508;&#31181;&#22270;&#26696;&#30340;&#25903;&#26550;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25903;&#26550;&#24448;&#24448;&#32570;&#20047;&#32467;&#26500;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#28287;&#23454;&#39564;&#39564;&#35777;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;FrameFlow&#65292;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#30340;SE(3)&#27969;&#21305;&#37197;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#20004;&#31181;&#20114;&#34917;&#30340;&#26041;&#27861;&#36827;&#34892;&#22270;&#26696;&#25903;&#26550;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#25674;&#38144;&#65292;&#21363;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;FrameFlow&#35757;&#32451;&#20026;&#20197;&#22270;&#26696;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;FrameFlow&#30340;&#26465;&#20214;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#25903;&#26550;&#26500;&#24314;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20135;&#29983;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;2.5&#20493;&#30340;&#25903;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein design often begins with knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a diverse range of motifs. However, the generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow, and requires no additional training. Both approaches achieve an equivalent or higher success rate than previous state-of-the-art methods, with 2.5 times more structurally diverse scaffolds. Code: https://github.com/ mi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16975</link><description>&lt;p&gt;
&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference. (arXiv:2310.16975v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20998;&#21035;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#34920;&#31034;&#20026;&#21487;&#22788;&#29702;&#30340;&#21442;&#32771;&#20998;&#24067;&#30340;&#36716;&#25442;&#65292;&#22240;&#27492;&#23646;&#20110;&#27979;&#24230;&#20256;&#36755;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;COT&#26144;&#23556;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#36873;&#25321;&#65292;&#20855;&#26377;&#21807;&#19968;&#24615;&#21644;&#21333;&#35843;&#24615;&#31561;&#21487;&#21462;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#30340;COT&#38382;&#39064;&#22312;&#20013;&#31561;&#32500;&#24230;&#19979;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;COT&#26144;&#23556;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;COT&#38382;&#39064;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#34920;&#36798;&#24418;&#24335;&#30340;&#32467;&#26500;&#12290;PCP-Map&#23558;&#26465;&#20214;&#20256;&#36755;&#26144;&#23556;&#24314;&#27169;&#20026;&#37096;&#20998;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems, respectively. Both approaches enable sampling and density estimation of conditional probability distributions, which are core tasks in Bayesian inference. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. COT maps are a canonical choice within this framework, with desirable properties such as uniqueness and monotonicity. However, the associated COT problems are computationally challenging, even in moderate dimensions. To improve the scalability, our numerical algorithms leverage neural networks to parameterize COT maps. Our methods exploit the structure of the static and dynamic formulations of the COT problem. PCP-Map models conditional transport maps as the gradient of a partially input convex neural network (PICNN) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14543</link><description>&lt;p&gt;
DF2M&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#29992;&#20110;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DF2M: An Explainable Deep Bayesian Nonparametric Model for High-Dimensional Functional Time Series. (arXiv:2305.14543v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Deep Functional Factor Model(DF2M)&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#12290;DF2M&#21033;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#26469;&#25429;&#25417;&#38750;&#39532;&#23572;&#31185;&#22827;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#21160;&#24577;&#12290;&#19982;&#35768;&#22810;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;DF2M&#36890;&#36807;&#26500;&#24314;&#22240;&#23376;&#27169;&#22411;&#24182;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#26680;&#20989;&#25968;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#26469;&#25512;&#26029;DF2M&#12290;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Deep Functional Factor Model (DF2M), a Bayesian nonparametric model for analyzing high-dimensional functional time series. The DF2M makes use of the Indian Buffet Process and the multi-task Gaussian Process with a deep kernel function to capture non-Markovian and nonlinear temporal dynamics. Unlike many black-box deep learning models, the DF2M provides an explainable way to use neural networks by constructing a factor model and incorporating deep neural networks within the kernel function. Additionally, we develop a computationally efficient variational inference algorithm for inferring the DF2M. Empirical results from four real-world datasets demonstrate that the DF2M offers better explainability and superior predictive accuracy compared to conventional deep learning models for high-dimensional functional time series.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2211.09619</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09619
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#19982;&#21487;&#24494;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#21644;&#20984;&#26494;&#24347;&#25216;&#26415;&#24471;&#21040;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#26368;&#20339;&#21644;&#40065;&#26834;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#19982;&#20854;&#20182;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#26080;&#27861;&#39044;&#27979;&#25200;&#21160;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
&lt;/p&gt;</description></item></channel></rss>