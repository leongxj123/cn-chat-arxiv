<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13751</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#20855;&#26377;&#36234;&#26469;&#36234;&#22810;&#21487;&#35843;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27169;&#22411;&#25439;&#22833;&#25110;&#21019;&#24314;&#26356;&#20855;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#30456;&#20114;&#30683;&#30462;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#26356;&#22823;&#27169;&#22411;&#33021;&#21542;&#25512;&#24191;&#21040;&#21463;&#25511;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#30340;&#30097;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ResNet&#27169;&#22411;&#20013;&#38544;&#34255;&#23618;&#30340;&#25968;&#37327;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#65292;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.09157</link><description>&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Finding Stationary Points in Non-Convex Optimization. (arXiv:2310.09157v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20294;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;$f$&#22312;&#26080;&#38480;&#21046;&#30340;$d$&#32500;&#22495;&#19978;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21363;&#26799;&#24230;&#36817;&#20284;&#20026;&#38646;&#30340;&#28857;&#65292;&#26159;&#32463;&#20856;&#38750;&#20984;&#20248;&#21270;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#38382;&#39064;&#30340;&#32500;&#24230;$d$&#19982;&#36817;&#20284;&#35823;&#24046;&#29420;&#31435;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#20173;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#32467;&#26524;&#65306;1.&#22312;&#26080;&#38480;&#21046;&#30340;&#22495;&#20013;&#23547;&#25214;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#38382;&#39064;&#26159;PLS&#23436;&#20840;&#38382;&#39064;&#12290;2.&#23545;&#20110;$d=2$&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#26368;&#22810;$O(1/\varepsilon)$&#27425;&#20989;&#25968;&#20540;&#26597;&#35810;&#12290;3.&#25105;&#20204;&#35777;&#26126;&#24403;$d=2$&#26102;&#65292;&#20219;&#20309;&#31639;&#27861;&#33267;&#23569;&#38656;&#35201;$\Omega(1/\varepsilon)$&#27425;&#23545;&#30446;&#26631;&#20989;&#25968;&#21644;/&#25110;&#26799;&#24230;&#30340;&#26597;&#35810;&#26469;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions $f$ over unrestricted $d$-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension $d$ of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:  1. The problem of finding approximate stationary points over unrestricted domains is PLS-complete.  2. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-approximate stationary points that requires at most $O(1/\varepsilon)$ value queries to the objective function.  3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries to the objective function and/or its gradient to find $\varepsilon$-approximate stationary points when $d=2$.
&lt;/p&gt;</description></item><item><title>fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;</title><link>http://arxiv.org/abs/2310.02008</link><description>&lt;p&gt;
fmeffects: &#19968;&#20010;&#29992;&#20110;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
fmeffects: An R Package for Forward Marginal Effects. (arXiv:2310.02008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02008
&lt;/p&gt;
&lt;p&gt;
fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#26377;&#25928;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#35299;&#37322;&#26041;&#27861;&#26368;&#36817;&#34987;&#24341;&#20837;&#12290;&#23427;&#20204;&#20197;&#8220;&#22914;&#26524;&#25105;&#20204;&#23558;$x$&#25913;&#21464;$h$&#65292;&#37027;&#20040;&#39044;&#27979;&#32467;&#26524;$\widehat{y}$&#20250;&#21457;&#29983;&#20160;&#20040;&#21464;&#21270;&#65311;&#8221;&#30340;&#24418;&#24335;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;fmeffects&#36719;&#20214;&#21253;&#65292;&#36825;&#26159;FMEs&#30340;&#31532;&#19968;&#20010;&#36719;&#20214;&#23454;&#29616;&#12290;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#32972;&#26223;&#12289;&#36719;&#20214;&#21253;&#21151;&#33021;&#21644;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#21450;&#36719;&#20214;&#35774;&#35745;&#21644;&#26410;&#26469;&#25193;&#23637;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forward marginal effects (FMEs) have recently been introduced as a versatile and effective model-agnostic interpretation method. They provide comprehensible and actionable model explanations in the form of: If we change $x$ by an amount $h$, what is the change in predicted outcome $\widehat{y}$? We present the R package fmeffects, the first software implementation of FMEs. The relevant theoretical background, package functionality and handling, as well as the software design and options for future extensions are discussed in this paper.
&lt;/p&gt;</description></item><item><title>&#26080;&#26089;&#20572;&#30340;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#19981;&#38656;&#35201;&#24471;&#20998;&#20989;&#25968;&#30340;Lipschitz&#22343;&#21248;&#26465;&#20214;&#65292;&#21482;&#38656;&#35201;&#26377;&#38480;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.12240</link><description>&lt;p&gt;
&#26080;&#26089;&#20572;&#30340;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#65306;&#26377;&#38480;&#36153;&#33293;&#23572;&#20449;&#24687;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Score diffusion models without early stopping: finite Fisher information is all you need. (arXiv:2308.12240v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12240
&lt;/p&gt;
&lt;p&gt;
&#26080;&#26089;&#20572;&#30340;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#19981;&#38656;&#35201;&#24471;&#20998;&#20989;&#25968;&#30340;Lipschitz&#22343;&#21248;&#26465;&#20214;&#65292;&#21482;&#38656;&#35201;&#26377;&#38480;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#22260;&#32469;&#30528;&#19982;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#33719;&#24471;&#36817;&#20284;&#30340;&#24471;&#20998;&#20989;&#25968;&#20043;&#21518;&#65292;&#21033;&#29992;&#23427;&#26469;&#27169;&#25311;&#30456;&#24212;&#30340;&#26102;&#38388;&#36870;&#36807;&#31243;&#65292;&#26368;&#32456;&#23454;&#29616;&#36817;&#20284;&#25968;&#25454;&#26679;&#26412;&#30340;&#29983;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#23454;&#38469;&#24847;&#20041;&#65292;&#20294;&#22312;&#28041;&#21450;&#38750;&#24120;&#35268;&#24471;&#20998;&#21644;&#20272;&#35745;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#21363;&#32570;&#20047;&#20840;&#38754;&#30340;&#23450;&#37327;&#32467;&#26524;&#12290;&#22312;&#20960;&#20046;&#25152;&#26377;&#30340;Kullback Leibler&#25955;&#24230;&#30340;&#30456;&#20851;&#32467;&#26524;&#20013;&#65292;&#37117;&#20551;&#35774;&#24471;&#20998;&#20989;&#25968;&#25110;&#20854;&#36817;&#20284;&#22312;&#26102;&#38388;&#19978;&#26159;Lipschitz&#22343;&#21248;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20010;&#26465;&#20214;&#38750;&#24120;&#20005;&#26684;&#65292;&#25110;&#32773;&#24456;&#38590;&#24314;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#26159;&#20851;&#27880;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#26089;&#20572;&#29256;&#26412;&#22312;KL&#25955;&#24230;&#19978;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a new class of generative models that revolve around the estimation of the score function associated with a stochastic differential equation. Subsequent to its acquisition, the approximated score function is then harnessed to simulate the corresponding time-reversal process, ultimately enabling the generation of approximate data samples. Despite their evident practical significance these models carry, a notable challenge persists in the form of a lack of comprehensive quantitative results, especially in scenarios involving non-regular scores and estimators. In almost all reported bounds in Kullback Leibler (KL) divergence, it is assumed that either the score function or its approximation is Lipschitz uniformly in time. However, this condition is very restrictive in practice or appears to be difficult to establish.  To circumvent this issue, previous works mainly focused on establishing convergence bounds in KL for an early stopped version of the diffusion model and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.10614</link><description>&lt;p&gt;
&#24102;&#26377;&#22024;&#26434;&#27835;&#30103;&#21644;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#30340;&#21487;&#35782;&#21035;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Identifiable causal inference with noisy treatment and no side information. (arXiv:2306.10614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#22240;&#26524;&#25512;&#26029;&#22330;&#26223;&#20013;&#65292;&#27835;&#30103;&#65288;&#21363;&#21407;&#22240;&#65289;&#21464;&#37327;&#30340;&#27979;&#37327;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#23398;&#25110;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#12290;&#26410;&#33021;&#32416;&#27491;&#27979;&#37327;&#35823;&#24046;&#30340;&#24433;&#21709;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#20174;&#22240;&#26524;&#35270;&#35282;&#30740;&#31350;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#24182;&#19988;&#19981;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20391;&#38754;&#20449;&#24687;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#22330;&#26223;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#27835;&#30103;&#21464;&#37327;&#65292;&#35813;&#21464;&#37327;&#27979;&#37327;&#19981;&#20934;&#30830;&#12290;&#24314;&#31435;&#22312;&#29616;&#26377;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#21363;&#20351;&#27809;&#26377;&#27979;&#37327;&#35823;&#24046;&#26041;&#24046;&#25110;&#20854;&#20182;&#20391;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#39640;&#26031;&#26465;&#20214;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#26469;&#35757;&#32451;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some causal inference scenarios, the treatment (i.e. cause) variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such as scenario, this paper proposes a model that assumes a continuous treatment variable which is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model where Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27835;&#30103;&#19981;&#26381;&#20174;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#26694;&#26550;&#26469;&#35780;&#20272;&#22240;&#26524;&#20013;&#20171;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20551;&#35774;&#26469;&#35782;&#21035;&#33258;&#28982;&#20013;&#20171;&#25928;&#24212;&#24182;&#25512;&#23548;&#20986;&#25104;&#20493;&#31283;&#20581;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.10025</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#20013;&#20855;&#26377;&#27835;&#30103;&#19981;&#26381;&#20174;&#24615;&#30340;&#35782;&#21035;&#21644;&#20493;&#22686;&#31283;&#20581;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Identification and multiply robust estimation in causal mediation analysis with treatment noncompliance. (arXiv:2304.10025v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27835;&#30103;&#19981;&#26381;&#20174;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#26694;&#26550;&#26469;&#35780;&#20272;&#22240;&#26524;&#20013;&#20171;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20551;&#35774;&#26469;&#35782;&#21035;&#33258;&#28982;&#20013;&#20171;&#25928;&#24212;&#24182;&#25512;&#23548;&#20986;&#25104;&#20493;&#31283;&#20581;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#21644;&#35266;&#23519;&#30740;&#31350;&#20013;&#65292;&#20154;&#20204;&#36890;&#24120;&#23545;&#20102;&#35299;&#24178;&#39044;&#26041;&#26696;&#22914;&#20309;&#25913;&#21892;&#26368;&#32456;&#32467;&#26524;&#30340;&#28508;&#22312;&#26426;&#21046;&#24863;&#20852;&#36259;&#12290;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26088;&#22312;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#20294;&#20027;&#35201;&#38480;&#20110;&#27835;&#30103;&#23436;&#20840;&#26381;&#20174;&#30340;&#24773;&#20917;&#65292;&#21482;&#26377;&#23569;&#25968;&#24773;&#20917;&#38656;&#35201;&#25490;&#38500;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21322;&#21442;&#25968;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#38656;&#25490;&#38500;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20855;&#26377;&#27835;&#30103;&#19981;&#26381;&#20174;&#24615;&#30340;&#22240;&#26524;&#20013;&#20171;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20551;&#35774;&#26469;&#35782;&#21035;&#25972;&#20010;&#30740;&#31350;&#20154;&#32676;&#30340;&#33258;&#28982;&#20013;&#20171;&#25928;&#24212;&#65292;&#24182;&#36827;&#19968;&#27493;&#38024;&#23545;&#30001;&#28508;&#22312;&#26381;&#20174;&#34892;&#20026;&#29305;&#24449;&#21270;&#30340;&#20122;&#20154;&#32676;&#20013;&#30340;&#20027;&#35201;&#33258;&#28982;&#20013;&#20171;&#25928;&#24212;&#36827;&#34892;&#35782;&#21035;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20027;&#35201;&#33258;&#28982;&#20013;&#20171;&#25928;&#24212;&#20272;&#35745;&#37327;&#30340;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#36825;&#28608;&#21169;&#20102;&#19968;&#32452;&#20493;&#22686;&#31283;&#20581;&#20272;&#35745;&#22120;&#36827;&#34892;&#25512;&#35770;&#12290;&#36825;&#20123;&#34987;&#35782;&#21035;&#20272;&#35745;&#37327;&#30340;&#21322;&#21442;&#25968;&#25928;&#29575;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In experimental and observational studies, there is often interest in understanding the potential mechanism by which an intervention program improves the final outcome. Causal mediation analyses have been developed for this purpose but are primarily restricted to the case of perfect treatment compliance, with a few exceptions that require exclusion restriction. In this article, we establish a semiparametric framework for assessing causal mediation in the presence of treatment noncompliance without exclusion restriction. We propose a set of assumptions to identify the natural mediation effects for the entire study population and further, for the principal natural mediation effects within subpopulations characterized by the potential compliance behaviour. We derive the efficient influence functions for the principal natural mediation effect estimands, which motivate a set of multiply robust estimators for inference. The semiparametric efficiency theory for the identified estimands is der
&lt;/p&gt;</description></item></channel></rss>