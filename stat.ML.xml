<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06925</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#20559;&#20506;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#36827;&#34892;&#26465;&#20214;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#25805;&#20316;&#23558;&#20855;&#26377;&#26126;&#30830;&#28508;&#22312;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#36716;&#25442;&#20026;&#27809;&#26377;&#27492;&#31867;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#26681;&#25454;&#21407;&#22987;SCM&#36873;&#25321;&#30340;&#20122;&#24635;&#20307;&#30340;&#22240;&#26524;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26465;&#20214;&#25805;&#20316;&#20445;&#25345;SCMs&#30340;&#31616;&#27905;&#24615;&#65292;&#26080;&#29615;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#24182;&#19982;&#36793;&#38469;&#21270;&#25805;&#20316;&#30456;&#31526;&#21512;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#19982;&#36793;&#38469;&#21270;&#21644;&#24178;&#39044;&#32467;&#21512;&#36215;&#26469;&#65292;&#26465;&#20214;&#25805;&#20316;&#20026;&#22312;&#28508;&#22312;&#32454;&#33410;&#24050;&#32463;&#21435;&#38500;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#20197;&#21253;&#25324;&#36873;&#25321;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20102;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;&#26469;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.18615</link><description>&lt;p&gt;
&#26410;&#30693;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporally Disentangled Representation Learning under Unknown Nonstationarity. (arXiv:2310.18615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20102;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;&#26469;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26102;&#24310;&#28508;&#22312;&#22240;&#26524;&#24433;&#21709;&#30340;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#32467;&#26500;&#22312;&#31283;&#24577;&#24773;&#20917;&#19979;&#24050;&#32463;&#24314;&#31435;&#20102;&#26377;&#20851;&#22240;&#26524;&#30456;&#20851;&#28508;&#22312;&#21464;&#37327;&#35299;&#32544;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#21482;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#36741;&#21161;&#21464;&#37327;&#65288;&#22914;&#31867;&#21035;&#26631;&#31614;&#21644;/&#25110;&#22495;&#32034;&#24341;&#65289;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#35774;&#31616;&#21270;&#30340;&#28508;&#22312;&#22240;&#26524;&#21160;&#21147;&#23398;&#12290;&#36825;&#20004;&#32773;&#38480;&#21046;&#20102;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#26102;&#38388;&#24310;&#36831;&#30456;&#20851;&#36807;&#31243;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#19981;&#35266;&#23519;&#36741;&#21161;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#20174;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#25490;&#21015;&#21644;&#20998;&#37327;&#32423;&#36716;&#25442;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20272;&#35745;&#26694;&#26550;NCTRL&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.13124</link><description>&lt;p&gt;
&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#23558;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#25193;&#23637;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#22522;&#30784;&#20005;&#37325;&#24615;&#27169;&#22411;&#26159;&#38543;&#26426;&#26862;&#26519;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#38454;&#27573;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#30340;&#27169;&#24335;&#24674;&#22797;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#27169;&#24335;&#21644;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#21644;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#36215;&#21040;&#20102;&#30456;&#21516;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10158</link><description>&lt;p&gt;
&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#20013;&#30340;&#27169;&#24335;&#24674;&#22797;&#21450;&#20854;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Pattern Recovery in Penalized and Thresholded Estimation and its Geometry. (arXiv:2307.10158v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10158
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#30340;&#27169;&#24335;&#24674;&#22797;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#27169;&#24335;&#21644;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#21644;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#36215;&#21040;&#20102;&#30456;&#21516;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24809;&#32602;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#24809;&#32602;&#39033;&#30001;&#23454;&#20540;&#30340;&#22810;&#38754;&#20307;&#35268;&#33539;&#32473;&#20986;&#65292;&#20854;&#20013;&#21253;&#25324;&#35832;&#22914;LASSO&#65288;&#20197;&#21450;&#20854;&#35768;&#22810;&#21464;&#20307;&#22914;&#24191;&#20041;LASSO&#65289;&#12289;SLOPE&#12289;OSCAR&#12289;PACS&#31561;&#26041;&#27861;&#12290;&#27599;&#20010;&#20272;&#35745;&#22120;&#21487;&#20197;&#25581;&#31034;&#26410;&#30693;&#21442;&#25968;&#21521;&#37327;&#30340;&#19981;&#21516;&#32467;&#26500;&#25110;&#8220;&#27169;&#24335;&#8221;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#27425;&#24494;&#20998;&#30340;&#27169;&#24335;&#30340;&#19968;&#33324;&#27010;&#24565;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#34913;&#37327;&#20854;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#27169;&#24335;&#24674;&#22797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#24335;&#20197;&#27491;&#27010;&#29575;&#34987;&#35813;&#36807;&#31243;&#26816;&#27979;&#21040;&#30340;&#26368;&#23567;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;&#21487;&#36798;&#24615;&#26465;&#20214;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26356;&#24378;&#30340;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#26159;&#20351;&#27169;&#24335;&#24674;&#22797;&#30340;&#27010;&#29575;&#22823;&#20110;1/2&#25152;&#24517;&#38656;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#22122;&#22768;&#24674;&#22797;&#36215;&#21040;&#20102;&#23436;&#20840;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25193;&#23637;&#21644;&#32479;&#19968;&#20102;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the framework of penalized estimation where the penalty term is given by a real-valued polyhedral gauge, which encompasses methods such as LASSO (and many variants thereof such as the generalized LASSO), SLOPE, OSCAR, PACS and others. Each of these estimators can uncover a different structure or ``pattern'' of the unknown parameter vector. We define a general notion of patterns based on subdifferentials and formalize an approach to measure their complexity. For pattern recovery, we provide a minimal condition for a particular pattern to be detected by the procedure with positive probability, the so-called accessibility condition. Using our approach, we also introduce the stronger noiseless recovery condition. For the LASSO, it is well known that the irrepresentability condition is necessary for pattern recovery with probability larger than $1/2$ and we show that the noiseless recovery plays exactly the same role, thereby extending and unifying the irrepresentability conditi
&lt;/p&gt;</description></item></channel></rss>