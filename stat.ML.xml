<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#25324;&#37325;&#23614;&#22122;&#22768;&#22312;&#20869;&#30340;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#35813;&#26041;&#27861;&#30340;&#29305;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12828</link><description>&lt;p&gt;
SGD&#26799;&#24230;&#21098;&#20999;&#26041;&#27861;&#26263;&#20013;&#20272;&#35745;&#20013;&#20540;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
SGD with Clipping is Secretly Estimating the Median Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#25324;&#37325;&#23614;&#22122;&#22768;&#22312;&#20869;&#30340;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#35813;&#26041;&#27861;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20960;&#31181;&#38543;&#26426;&#20248;&#21270;&#30340;&#24212;&#29992;&#22330;&#26223;&#21487;&#20197;&#21463;&#30410;&#20110;&#23545;&#26799;&#24230;&#30340;&#31283;&#20581;&#20272;&#35745;&#12290;&#20363;&#22914;&#65292;&#22312;&#20855;&#26377;&#25439;&#22351;&#33410;&#28857;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#39046;&#22495;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#22823;&#30340;&#24322;&#24120;&#20540;&#12289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#29978;&#33267;&#30001;&#20110;&#31639;&#27861;&#21160;&#24577;&#26412;&#36523;&#30340;&#37325;&#23614;&#22122;&#22768;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#30340;SGD&#12290;&#39318;&#20808;&#32771;&#34385;&#36328;&#26679;&#26412;&#35745;&#31639;&#20013;&#20540;&#26799;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#37325;&#23614;&#12289;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#25910;&#25947;&#12290;&#28982;&#21518;&#25105;&#20204;&#25512;&#23548;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#31471;&#28857;&#26041;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20960;&#20309;&#20013;&#20540;&#21644;&#20854;&#25512;&#24191;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#36845;&#20195;&#38388;&#30340;&#20013;&#20540;&#26799;&#24230;&#65292;&#24182;&#21457;&#29616;&#20960;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861; - &#29305;&#21035;&#26159;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999; - &#26159;&#36825;&#19968;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12828v1 Announce Type: cross  Abstract: There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;&#32479;&#35745;&#24615;&#36136;&#21644;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13966</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#26368;&#20248;&#26497;&#23567;&#21270;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression. (arXiv:2310.13966v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;&#32479;&#35745;&#24615;&#36136;&#21644;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20256;&#36882;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#21463;&#21040;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#23427;&#33021;&#22815;&#21033;&#29992;&#30456;&#20851;&#30740;&#31350;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#30740;&#31350;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#21560;&#24341;&#21147;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#20256;&#36882;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#30340;&#26159;&#32553;&#23567;&#23454;&#38469;&#25928;&#26524;&#19982;&#29702;&#35770;&#20445;&#35777;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#24050;&#30693;&#21487;&#20256;&#36882;&#30340;&#26469;&#28304;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#24050;&#30693;&#21487;&#20256;&#36882;&#30340;&#26469;&#28304;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#26680;&#20272;&#35745;&#22120;&#65292;&#20165;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#12290;&#23545;&#20110;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#32858;&#21512;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#24182;&#20943;&#36731;&#36127;&#38754;&#26469;&#28304;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25152;&#38656;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#24182;&#28155;&#21152;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#30340;&#20559;&#24046;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2302.00662</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;Fitted-Q&#35780;&#20272;&#21644;&#36845;&#20195;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;
&lt;/p&gt;
&lt;p&gt;
Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders. (arXiv:2302.00662v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#24182;&#28155;&#21152;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#30340;&#20559;&#24046;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#12289;&#32463;&#27982;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#32447;&#23454;&#39564;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12289;&#21361;&#38505;&#25110;&#19981;&#36947;&#24503;&#65292;&#24182;&#19988;&#30495;&#23454;&#27169;&#22411;&#26410;&#30693;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#34892;&#20026;&#31574;&#30053;&#30340;&#25152;&#26377;&#21327;&#21464;&#37327;&#37117;&#26159;&#24050;&#35266;&#23519;&#21040;&#30340;&#12290;&#23613;&#31649;&#36825;&#20010;&#20551;&#35774;"&#39034;&#24207;&#21487;&#24573;&#30053;&#24615;"&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#19981;&#22826;&#21487;&#33021;&#25104;&#31435;&#65292;&#20294;&#22823;&#37096;&#20998;&#32771;&#34385;&#36827;&#20837;&#27835;&#30103;&#22240;&#32032;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#35266;&#23519;&#21040;&#30340;&#65292;&#36825;&#28608;&#21169;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25935;&#24863;&#24615;&#27169;&#22411;&#19979;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24378;&#20581;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#26469;&#23548;&#20986;&#24378;&#20581;Q&#20989;&#25968;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23545;&#20998;&#20301;&#25968;&#20272;&#35745;&#21152;&#20837;&#20559;&#24046;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20860;&#20855;Fitted-Q&#36845;&#20195;&#30340;&#35745;&#31639;&#31616;&#20415;&#24615;&#21644;&#32479;&#35745;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning is important in domains such as medicine, economics, and e-commerce where online experimentation is costly, dangerous or unethical, and where the true model is unknown. However, most methods assume all covariates used in the behavior policy's action decisions are observed. Though this assumption, sequential ignorability/unconfoundedness, likely does not hold in observational data, most of the data that accounts for selection into treatment may be observed, motivating sensitivity analysis. We study robust policy evaluation and policy optimization in the presence of sequentially-exogenous unobserved confounders under a sensitivity model. We propose and analyze orthogonalized robust fitted-Q-iteration that uses closed-form solutions of the robust Bellman operator to derive a loss minimization problem for the robust Q function, and adds a bias-correction to quantile estimation. Our algorithm enjoys the computational ease of fitted-Q-iteration and statistical 
&lt;/p&gt;</description></item></channel></rss>