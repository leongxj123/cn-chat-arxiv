<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12808</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#40784;&#27425;&#26102;&#38388;&#27850;&#26494;&#36807;&#31243;&#30340;&#27867;&#21270;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12808
&lt;/p&gt;
&lt;p&gt;
&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27850;&#26494;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#38750;&#40784;&#27425;&#27850;&#26494;&#36807;&#31243;(NHPP)&#65292;&#26159;&#19968;&#31181;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#25968;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20351;&#29992;&#38750;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#23545;&#20855;&#26377;&#26080;&#31351;&#25968;&#25454;&#30340;NHPP&#36827;&#34892;&#20272;&#35745;&#12290;&#26412;&#25991;&#23558;&#26377;&#38480;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;NHPP&#20272;&#35745;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#65292;&#23613;&#31649;&#20998;&#31665;&#26041;&#27861;&#23545;&#20110;&#20272;&#35745;NHPPs&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#20250;&#24102;&#26469;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#24110;&#21161;&#28040;&#38500;&#20998;&#31665;&#21442;&#25968;&#30340;&#21363;&#20852;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12808v1 Announce Type: new  Abstract: The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.20609</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#35299;&#20915;&#22270;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Matching via convex relaxation to the simplex. (arXiv:2310.20609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#21305;&#37197;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#21253;&#25324;&#22312;&#20004;&#20010;&#36755;&#20837;&#22270;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#23545;&#40784;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#32593;&#32476;&#21435;&#21311;&#21517;&#21270;&#21644;&#34507;&#30333;&#36136;&#23545;&#40784;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;NP&#38590;&#38382;&#39064;&#8220;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#8221;&#65288;QAP&#65289;&#36827;&#34892;&#20984;&#26494;&#24347;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#21363;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#38381;&#21512;&#36845;&#20195;&#24418;&#24335;&#30340;&#39640;&#25928;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#21807;&#19968;&#35299;&#12290;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#36825;&#34987;&#35777;&#26126;&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#30697;&#38453;&#20551;&#35774;&#26465;&#20214;&#65292;&#29992;&#20110;&#26631;&#20934;&#36138;&#24515;&#21462;&#25972;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20010;&#26465;&#20214;&#27604;&#24120;&#29992;&#30340;&#8220;&#23545;&#35282;&#32447;&#20248;&#21183;&#8221;&#26465;&#20214;&#26356;&#23485;&#26494;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26465;&#20214;&#35777;&#26126;&#20102;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#30340;&#31934;&#30830;&#19968;&#27493;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).  Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.00736</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#35780;&#20272;&#20102;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#22312;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24314;&#31435;&#30340;&#21021;&#27493;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#24120;&#35265;&#30340;&#21508;&#31181;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#22312;&#30495;&#23454;&#35823;&#24046;&#29575;&#21644;&#26399;&#26395;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#24179;&#22343;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#12290;&#19982;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#20132;&#21449;&#39564;&#35777;&#12289;&#33258;&#20030;&#21644;&#25968;&#25454;&#21010;&#20998;&#31561;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#12290;&#22312;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21046;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#21644;&#32467;&#26500;&#20449;&#24687;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Distributionally robust risk evaluation with a causality constraint and structural information. (arXiv:2203.10571v3 [q-fin.MF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#12290;&#22312;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21046;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25968;&#25454;&#30340;&#26399;&#26395;&#20989;&#25968;&#20540;&#30340;&#20998;&#24067;&#40065;&#26834;&#35780;&#20272;&#12290;&#19968;&#32452;&#22791;&#36873;&#24230;&#37327;&#36890;&#36807;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#23545;&#20598;&#24615;&#65292;&#24182;&#23558;&#22240;&#26524;&#32422;&#26463;&#37325;&#26500;&#20026;&#26080;&#38480;&#32500;&#27979;&#35797;&#20989;&#25968;&#31354;&#38388;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#29992;Rademacher&#22797;&#26434;&#24230;&#35777;&#26126;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#32467;&#26500;&#20449;&#24687;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#38480;&#21046;&#27169;&#31946;&#38598;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20598;&#24418;&#24335;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#23545;&#23454;&#29616;&#27874;&#21160;&#29575;&#21644;&#32929;&#25351;&#30340;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#32463;&#20856;&#26368;&#20248;&#20256;&#36755;&#20844;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies distributionally robust evaluation of expected function values over temporal data. A set of alternative measures is characterized by the causal optimal transport. We prove the strong duality and recast the causality constraint as minimization over an infinite-dimensional test function space. We approximate test functions by neural networks and prove the sample complexity with Rademacher complexity. Moreover, when structural information is available to further restrict the ambiguity set, we prove the dual formulation and provide efficient optimization methods. Empirical analysis of realized volatility and stock indices demonstrates that our framework offers an attractive alternative to the classic optimal transport formulation.
&lt;/p&gt;</description></item></channel></rss>