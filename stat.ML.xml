<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.10923</link><description>&lt;p&gt;
TabPFN&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10923
&lt;/p&gt;
&lt;p&gt;
TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Prior-Data Fitted Networks&#65288;PFNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#32467;&#26524;&#12290;TabPFN&#27169;&#22411;&#26159;PFN&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;TabPFN&#22240;&#27492;&#25104;&#20026;&#20102;&#35768;&#22810;&#39046;&#22495;&#24212;&#29992;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;TabPFN&#19987;&#38376;&#35774;&#35745;&#30340;&#27969;&#34892;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#20801;&#35768;&#27604;&#29616;&#26377;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36991;&#20813;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15602</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65306;&#36229;&#36234;&#23494;&#24230;&#19979;&#30028;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#32479;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22823;&#26679;&#26412;&#22330;&#26223;&#19979;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#25277;&#26679;&#30340;&#28176;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545; $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ &#30340;&#24471;&#20998;&#20989;&#25968;&#30340;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#20026; $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$&#65292;&#20854;&#20013; $n$ &#21644; $d$ &#20998;&#21035;&#20195;&#34920;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#65292;$t$ &#22312;&#19978;&#19979;&#21463;&#21040; $n$ &#30340;&#22810;&#39033;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988; $p_0$ &#26159;&#20219;&#24847;&#27425;&#20122;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#36827;&#34892;&#27425;&#39640;&#26031;&#20551;&#35774;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;&#22914;&#26524;&#27492;&#22806;&#65292;$p_0$ &#23646;&#20110; $\beta\le 2$ &#30340; $\beta$-Sobolev&#31354;&#38388;&#30340;&#38750;&#21442;&#25968;&#26063;&#65292;&#36890;&#36807;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#25105;&#20204;&#24471;&#21040;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11991</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#28040;&#38500;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#38169;&#35823;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation. (arXiv:2310.11991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#32463;&#24120;&#20250;&#24433;&#21709;&#21040;&#27169;&#22411;&#22312;&#26679;&#26412;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#38169;&#35823;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#28608;&#36827;&#65292;&#19981;&#32463;&#24847;&#38388;&#20250;&#28040;&#38500;&#19982;&#27169;&#22411;&#20027;&#35201;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#35782;&#21035;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#20004;&#20010;&#20302;&#32500;&#27491;&#20132;&#23376;&#31354;&#38388;&#26469;&#20998;&#31163;&#38169;&#35823;&#21644;&#20027;&#35201;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;Waterbirds&#65292;CelebA&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;MultiNLI&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
&lt;/p&gt;</description></item><item><title>&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09379</link><description>&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#22312;&#20998;&#24067;&#20869;&#20855;&#26377;&#24191;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09379
&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#37327;&#39044;&#27979;&#22120;&#30340;&#24191;&#20041;&#24615;&#36136;&#65292;&#21363;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#23567;&#32452;&#65288;&#25110;&#25209;&#37327;&#65289;&#31034;&#20363;&#30340;&#22343;&#20540;&#26631;&#31614;&#30340;&#27169;&#22411;&#12290;&#25209;&#37327;&#39044;&#27979;&#33539;&#24335;&#23545;&#20110;&#37096;&#32626;&#22312;&#31163;&#32447;&#27979;&#35797;&#21069;&#30830;&#23450;&#19968;&#32452;&#21270;&#21512;&#29289;&#30340;&#36136;&#37327;&#30340;&#27169;&#22411;&#23588;&#20026;&#30456;&#20851;&#12290;&#36890;&#36807;&#21033;&#29992;&#36866;&#24403;&#30340;Rademacher&#22797;&#26434;&#24615;&#30340;&#24191;&#20041;&#21270;&#65292;&#25105;&#20204;&#35777;&#26126;&#25209;&#37327;&#39044;&#27979;&#22120;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#19982;&#26631;&#20934;&#30340;&#36880;&#20010;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#25552;&#35758;&#30340;&#19978;&#30028;&#29420;&#31435;&#20110;&#36807;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#26550;&#26500;&#21644;&#24212;&#29992;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.07679</link><description>&lt;p&gt;
&#21305;&#37197;&#36861;&#36394;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21363;&#36890;&#36807;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#36817;&#20284;&#30446;&#26631;&#20989;&#25968;&#30340;&#32431;&#36138;&#23146;&#31639;&#27861;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21253;&#21547;&#22312;&#23545;&#24212;&#20110;&#23383;&#20856;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#26102;&#65292;&#35768;&#22810;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#30740;&#31350;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#33719;&#24471;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#33719;&#24471;&#21305;&#37197;&#36861;&#36394;&#24615;&#33021;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#19979;&#30028;&#20197;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19978;&#30028;&#19981;&#33021;&#25913;&#36827;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#36138;&#23146;&#31639;&#27861;&#21464;&#20307;&#19981;&#21516;&#65292;&#25910;&#25947;&#36895;&#24230;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#30001;&#35299;&#26576;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20915;&#23450;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20219;&#24847;&#31243;&#24230;&#30340;&#25910;&#32553;&#37117;&#20250;&#25913;&#21892;&#21305;&#37197;&#36861;&#36394;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#36136;&#24615;&#21151;&#33021;&#25968;&#25454;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;EM&#31639;&#27861;&#36817;&#20284;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#26041;&#27861;&#22312;&#27169;&#25311;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.10712</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#21151;&#33021;&#25968;&#25454;&#30340;&#28151;&#21512;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture of segmentation for heterogeneous functional data. (arXiv:2303.10712v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#36136;&#24615;&#21151;&#33021;&#25968;&#25454;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;EM&#31639;&#27861;&#36817;&#20284;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#26041;&#27861;&#22312;&#27169;&#25311;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26102;&#38388;&#21644;&#20154;&#21475;&#24322;&#36136;&#24615;&#30340;&#21151;&#33021;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20998;&#21106;&#27169;&#22411;&#65292;&#26088;&#22312;&#20445;&#25345;&#21151;&#33021;&#32467;&#26500;&#30340;&#21516;&#26102;&#34920;&#31034;&#24322;&#36136;&#24615;&#12290; &#35752;&#35770;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#21487;&#36776;&#35782;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;EM&#31639;&#27861;&#26469;&#36817;&#20284;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#12290; &#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#24182;&#22312;&#29992;&#30005;&#37327;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider functional data with heterogeneity in time and in population. We propose a mixture model with segmentation of time to represent this heterogeneity while keeping the functional structure. Maximum likelihood estimator is considered, proved to be identifiable and consistent. In practice, an EM algorithm is used, combined with dynamic programming for the maximization step, to approximate the maximum likelihood estimator. The method is illustrated on a simulated dataset, and used on a real dataset of electricity consumption.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.01335</link><description>&lt;p&gt;
&#21021;&#38454;ANIL&#22312;&#23384;&#22312;&#35823;&#25351;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#24773;&#20917;&#19979;&#23398;&#20064;&#32447;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
First-order ANIL learns linear representations despite misspecified latent dimension. (arXiv:2303.01335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#20803;&#23398;&#20064;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20803;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#23547;&#25214;&#36215;&#22987;&#28857;&#65292;&#20174;&#35813;&#36215;&#22987;&#28857;&#24320;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36805;&#36895;&#36866;&#24212;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#32463;&#39564;&#19978;&#24314;&#35758;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#34892;&#20026;&#30340;&#29702;&#35770;&#35777;&#25454;&#26377;&#38480;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24182;&#27809;&#26377;&#20005;&#26684;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#20173;&#20250;&#23398;&#20064;&#21040;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#22312;&#26080;&#38480;&#25968;&#37327;&#30340;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#65292;&#20351;&#29992;&#32447;&#24615;&#21452;&#23618;&#32593;&#32476;&#32467;&#26500;&#30340;&#21021;&#38454;ANIL&#25104;&#21151;&#22320;&#23398;&#20064;&#21040;&#20102;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#21363;&#20351;&#26159;&#22312;&#21442;&#25968;&#21270;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#65292;&#21363;&#32593;&#32476;&#30340;&#23485;&#24230;&#22823;&#20110;
&lt;/p&gt;
&lt;p&gt;
Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been rigorously shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with a misspecified network parameterisation; having a width larger than 
&lt;/p&gt;</description></item></channel></rss>