<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.17704</link><description>&lt;p&gt;
&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#36716;&#31227;&#23398;&#20064;&#20197;&#35774;&#35745;&#29992;&#20110;&#35786;&#26029;&#27979;&#23450;&#30340;&#31454;&#20105;&#23545;&#25163;DNA&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#31243;&#29983;&#29289;&#20998;&#23376;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#23450;&#21046;&#29983;&#29289;&#24207;&#21015;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#29305;&#23450;&#24212;&#29992;&#38656;&#35201;&#21046;&#20316;&#35768;&#22810;&#31867;&#20284;&#30340;&#29983;&#29289;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#29978;&#33267;&#26114;&#36149;&#30340;&#23454;&#39564;&#26469;&#20248;&#21270;&#36825;&#20123;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36716;&#31227;&#23398;&#20064;&#35774;&#35745;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#36825;&#31181;&#24320;&#21457;&#21464;&#24471;&#21487;&#34892;&#12290;&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#20013;&#20351;&#29992;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#24320;&#21457;&#25968;&#25454;&#26469;&#20943;&#23569;&#23454;&#39564;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#27604;&#36739;&#19981;&#21516;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#28982;&#21518;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#22312;&#21333;&#19968;&#30446;&#26631;&#21644;&#24809;&#32602;&#20248;&#21270;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17704v1 Announce Type: cross  Abstract: With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.00691</link><description>&lt;p&gt;
&#28155;&#21152;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent for Additive Nonparametric Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#32452;&#20214;&#20989;&#25968;&#30340;&#25130;&#26029;&#22522;&#25193;&#23637;&#30340;&#31995;&#25968;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#28385;&#36275;&#19968;&#20010;&#22885;&#25289;&#20811;&#19981;&#31561;&#24335;&#65292;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19977;&#20010;&#19981;&#21516;&#38454;&#27573;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#39118;&#38505;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20381;&#36182;&#26041;&#38754;&#26159;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23558;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#25311;&#21512;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17712</link><description>&lt;p&gt;
&#20351;&#29992;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#26377;&#21508;&#31181;&#24037;&#20855;&#21487;&#29992;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20316;&#31038;&#21306;&#26816;&#27979;/&#33410;&#28857;&#32858;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#38500;&#20102;&#35889;&#32858;&#31867;&#26041;&#27861;&#20043;&#22806;&#65292;&#23545;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#23398;&#20064;&#23884;&#20837;&#26041;&#27861;&#65292;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#30001;node2vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;node2vec&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#24212;&#29992;k-means&#32858;&#31867;&#21487;&#20197;&#23545;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#24369;&#19968;&#33268;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#32593;&#32476;&#25968;&#25454;&#30340;&#20854;&#20182;&#23884;&#20837;&#24037;&#20855;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65288;SoftAD&#65289;&#26469;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.10006</link><description>&lt;p&gt;
&#36890;&#36807;&#36719;&#19978;&#21319;-&#19979;&#38477;&#23454;&#29616;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization via soft ascent-descent. (arXiv:2310.10006v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65288;SoftAD&#65289;&#26469;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#21644;&#22797;&#26434;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#35797;&#38169;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#31163;&#32447;&#27867;&#21270;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#38752;&#24615;&#21644;&#32463;&#27982;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#23547;&#27714;&#8220;&#24179;&#22374;&#8221;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20195;&#29702;&#65292;&#26799;&#24230;&#27491;&#21017;&#21270;&#26159;&#19968;&#26465;&#33258;&#28982;&#30340;&#36884;&#24452;&#65292;&#19968;&#38454;&#36817;&#20284;&#26041;&#27861;&#22914;Floding&#21644;Sharpness-Aware Minimization (SAM) &#24050;&#32463;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65288;&#27946;&#27700;&#38408;&#20540;&#21644;&#37051;&#22495;&#21322;&#24452;&#65289;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#19981;&#23481;&#26131;&#20107;&#20808;&#30830;&#23450;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#23545;&#38169;&#35823;&#36229;&#21442;&#25968;&#26356;&#20855;&#38887;&#24615;&#30340;&#36807;&#31243;&#65292;&#21463;Flooding&#20013;&#20351;&#29992;&#30340;&#30828;&#38408;&#20540;&#8220;&#19978;&#21319;-&#19979;&#38477;&#8221;&#24320;&#20851;&#35013;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65292;&#31216;&#20026;SoftAD&#65292;&#23427;&#23545;&#36793;&#30028;&#19978;&#30340;&#28857;&#36827;&#34892;&#38477;&#26435;&#65292;&#38480;&#21046;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#24418;&#24335;&#30340;&#24179;&#31283;&#24615;&#20445;&#35777;&#19982;Flooding&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking "flat" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold "ascent-descent" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.03372</link><description>&lt;p&gt;
&#22312;&#32447;&#24352;&#37327;&#23398;&#20064;&#65306;&#35745;&#31639;&#21644;&#32479;&#35745;&#26435;&#34913;&#65292;&#36866;&#24212;&#24615;&#21644;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65306;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#20013;&#37117;&#21487;&#20197;&#26681;&#25454;&#36866;&#24403;&#30340;&#26465;&#20214;&#32447;&#24615;&#25910;&#25947;&#24182;&#24674;&#22797;&#20302;&#31209;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#24314;&#31435;&#20102;&#31934;&#30830;&#30340;&#36880;&#20010;&#26465;&#30446;&#38169;&#35823;&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#34920;&#20102;&#39318;&#27425;&#23581;&#35797;&#22312;&#22312;&#32447;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20219;&#21153;&#20013;&#32435;&#20837;&#22122;&#22768;&#30340;&#21162;&#21147;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;&#22686;&#21152;&#27493;&#38271;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#65292;&#20294;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.09656</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;, &#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33021;&#22815;&#34987;&#35780;&#20272;&#65292;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;IBNNs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#27010;&#25324;&#21644;&#20811;&#26381;&#26631;&#20934;BNNs&#30340;&#26576;&#20123;&#32570;&#28857;&#12290;&#26631;&#20934;BNNs&#20351;&#29992;&#21333;&#19968;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;IBNNs&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#20204;&#20801;&#35768;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#27604;&#26631;&#20934;BNNs&#26356;&#21152;&#40065;&#26834;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PAC&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#38598;&#12290;&#25105;&#20204;&#23558;IBNNs&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#20026;&#20102;&#20154;&#24037;&#33008;&#33146;&#25511;&#21046;&#27169;&#25311;&#34880;&#31958;&#21644;&#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item></channel></rss>