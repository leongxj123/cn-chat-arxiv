<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.06903</link><description>&lt;p&gt;
&#20855;&#26377;&#36866;&#24230;&#36755;&#20837;&#32500;&#24230;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting in leaky ReLU networks with moderate input dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06903
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#25506;&#35752;&#20102;&#19968;&#20010;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#32654;&#22320;&#25311;&#21512;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20108;&#23618;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38024;&#23545;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#21516;&#20449;&#21495;&#21644;&#19968;&#20010;&#38543;&#26426;&#22122;&#22768;&#25104;&#20998;&#30340;&#24635;&#21644;&#65292;&#36825;&#20004;&#32773;&#30456;&#20114;&#27491;&#20132;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#65292;&#23548;&#33268;&#20102;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#65288;&#26377;&#23475;&#65289;&#36807;&#25311;&#21512;&#65306;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;SNR&#24456;&#39640;&#65292;&#21017;&#21457;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#30456;&#21453;&#65292;&#22914;&#26524;SNR&#24456;&#20302;&#65292;&#21017;&#21457;&#29983;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23558;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#24402;&#22240;&#20110;&#19968;&#20010;&#36817;&#20284;&#36793;&#30028;&#26368;&#22823;&#21270;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38128;&#38142;&#25439;&#22833;&#19979;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#28385;&#36275;&#36825;&#19968;&#24615;&#36136;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;nea
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Lie&#32676;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#32039;&#33268;&#38750;&#38463;&#36125;&#23572;&#30340;Lie&#32676;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;$\text{GL}^{+}(n, \mathbb{R})$&#21644;$\text{SL}(n, \mathbb{R})$&#36825;&#20004;&#20010;Lie&#32676;&#12290;</title><link>http://arxiv.org/abs/2310.11366</link><description>&lt;p&gt;
Lie Group Decompositions for Equivariant Neural Networks. (arXiv:2310.11366v1 [cs.LG]) (&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;Lie&#32676;&#20998;&#35299;)
&lt;/p&gt;
&lt;p&gt;
Lie Group Decompositions for Equivariant Neural Networks. (arXiv:2310.11366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Lie&#32676;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#32039;&#33268;&#38750;&#38463;&#36125;&#23572;&#30340;Lie&#32676;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;$\text{GL}^{+}(n, \mathbb{R})$&#21644;$\text{SL}(n, \mathbb{R})$&#36825;&#20004;&#20010;Lie&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#65288;&#21367;&#31215;&#65289;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#65292;&#23545;&#20960;&#20309;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#30340;&#23545;&#31216;&#32676;&#20026;&#32039;&#33268;&#25110;&#38463;&#36125;&#23572;&#32676;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26159;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25299;&#23637;&#20102;&#20351;&#29992;&#30340;&#21464;&#25442;&#31867;&#21035;&#21040;Lie&#32676;&#30340;&#24773;&#20917;&#65292;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#20854;Lie&#20195;&#25968;&#20197;&#21450;&#32676;&#30340;&#25351;&#25968;&#21644;&#23545;&#25968;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#21464;&#25442;&#32676;&#26102;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#26681;&#25454;&#25152;&#20851;&#24515;&#30340;&#32676;$G$&#30340;&#19981;&#21516;&#65292;&#25351;&#25968;&#26144;&#23556;&#21487;&#33021;&#19981;&#28385;&#23556;&#12290;&#24403;$G$&#26082;&#19981;&#26159;&#32039;&#33268;&#32676;&#20063;&#19981;&#26159;&#38463;&#36125;&#23572;&#32676;&#26102;&#65292;&#36824;&#20250;&#36935;&#21040;&#36827;&#19968;&#27493;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;Lie&#32676;&#21450;&#20854;&#40784;&#27425;&#31354;&#38388;&#30340;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22788;&#29702;&#36825;&#31867;&#32676;&#30340;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;Lie&#32676;$G = \text{GL}^{+}(n, \mathbb{R})$&#21644;$G = \text{SL}(n, \mathbb{R}$&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \text{GL}^{+}(n, \mathbb{R})$ and $G = \text{SL}(n, \mathbb{R}
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.05833</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models. (arXiv:2310.05833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#27867;&#21270;&#34892;&#20026;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#20197;&#19968;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;&#20020;&#26102;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#26041;&#27861;&#19981;&#33021;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#29992;&#20110;&#26680;&#35780;&#20998;&#21450;&#20854;&#30456;&#20851;&#29109;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#65292;&#21482;&#38656;&#35201;&#29983;&#25104;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#24213;&#23618;&#27169;&#22411;&#26412;&#36523;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#29983;&#25104;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc manner and task dependent. For example, natural language approaches cannot be transferred to image generation. In this paper we introduce the first bias-variance-covariance decomposition for kernel scores and their associated entropy. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. As an application, we offer a generalization evaluation of diffusion models and discover how mode collapse of minority groups is a contrary phenomenon to overfitting. Further, we demonstrate that variance and predictive kernel entropy are viable measures of uncertainty for image, audio, and language generation. Specifically, our approach f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2307.14012</link><description>&lt;p&gt;
MCMC-&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
MCMC-Correction of Score-Based Diffusion Models for Model Composition. (arXiv:2307.14012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29992;&#24471;&#20998;&#25110;&#33021;&#37327;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#12290;&#33021;&#37327;&#21442;&#25968;&#21270;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#20027;&#35201;&#26159;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#25552;&#35758;&#26679;&#26412;&#20013;&#24635;&#33021;&#37327;&#30340;&#21464;&#21270;&#22522;&#20110;Metropolis-Hastings&#20462;&#27491;&#27493;&#39588;&#26469;&#36827;&#34892;&#25193;&#23637;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20284;&#20046;&#20135;&#29983;&#20102;&#31245;&#24494;&#36739;&#24046;&#30340;&#24615;&#33021;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27969;&#34892;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#33021;&#37327;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21066;&#24369;&#20102;&#27169;&#22411;&#32452;&#21512;&#30340;&#30446;&#30340;&#65292;&#21363;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#21512;&#36215;&#26469;&#20174;&#26032;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#24314;&#35758;&#20445;&#30041;&#24471;&#20998;&#21442;&#25968;&#21270;&#65292;&#32780;&#26159;&#36890;&#36807;&#23545;&#24471;&#20998;&#20989;&#25968;&#36827;&#34892;&#32447;&#31215;&#20998;&#26469;&#35745;&#31639;&#22522;&#20110;&#33021;&#37327;&#30340;&#25509;&#21463;&#27010;&#29575;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#21508;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13498</link><description>&lt;p&gt;
&#29992;&#20110;&#24102;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise. (arXiv:2305.13498v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#23545;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20056;&#24615;&#22122;&#22768;&#21644;&#28909;&#22122;&#22768;&#23545;&#20449;&#21495;&#20998;&#31163;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#21306;&#20998;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12289;&#25913;&#21892;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#30340;&#31639;&#27861;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20056;&#24615;&#21644;&#28909;&#22122;&#22768;&#23545;&#23454;&#38469;&#20449;&#21495;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#20998;&#31163;&#28909;&#22122;&#22768;&#30340;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;Hamilton Monte Carlo (HMC)&#30456;&#23218;&#32654;&#65292;&#20294;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#35777;&#26126;&#20102;HMC&#26080;&#27861;&#38548;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#39069;&#22806;&#20102;&#35299;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#20043;&#38388;&#27604;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#20272;&#35745;&#21442;&#25968;&#21644;&#20998;&#31163;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to investigate the impact of noise on parameter fitting for an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and thermal noise on the accuracy of signal separation. To address these issues, we propose algorithms and methods that can effectively distinguish between thermal and multiplicative noise and improve the precision of parameter estimation for optimal data analysis. Specifically, we explore the impact of both multiplicative and thermal noise on the obfuscation of the actual signal and propose methods to resolve them. Firstly, we present an algorithm that can effectively separate thermal noise with comparable performance to Hamilton Monte Carlo (HMC) but with significantly improved speed. Subsequently, we analyze multiplicative noise and demonstrate that HMC is insufficient for isolating thermal and multiplicative noise. However, we show that, with additional knowledge of the ratio between thermal and multiplicative noise, we can accuratel
&lt;/p&gt;</description></item></channel></rss>