<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;</title><link>https://arxiv.org/abs/2402.13728</link><description>&lt;p&gt;
&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#22349;&#22604;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Average gradient outer product as a mechanism for deep neural collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Collapse (DNC)&#25351;&#30340;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26368;&#21518;&#20960;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#24778;&#20154;&#21018;&#24615;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#37117;&#24471;&#21040;&#20102;&#27979;&#37327;&#65292;&#20294;&#20854;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20805;&#20998;&#35777;&#25454;&#65292;&#34920;&#26126;DNC&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;(AGOP)&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#21457;&#29983;&#30340;&#12290;&#30456;&#27604;&#20110;&#35299;&#37322;&#31070;&#32463;&#22349;&#22604;&#30340;&#29305;&#24449;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#22914;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65292;&#36825;&#19968;&#36827;&#23637;&#26356;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#32487;&#32493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#26435;&#37325;&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#20540;&#26159;DNN&#20013;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36825;&#31181;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#30340;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AGOP&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#21457;&#31070;&#32463;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02368</link><description>&lt;p&gt;
&#35745;&#26102;&#22120;: &#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Timer: Transformers for Time Series Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#27169;&#22411;&#21487;&#33021;&#36935;&#21040;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#20013;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#39281;&#21644;&#32780;&#38544;&#34109;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;&#22823;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#22312;&#23569;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#26041;&#38754;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#25913;&#21464;&#30446;&#21069;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#27169;&#22411;&#30340;&#20570;&#27861;&#65292;&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#21253;&#21547;10&#20159;&#20010;&#26102;&#38388;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#32479;&#19968;&#20026;&#21333;&#24207;&#21015;&#24207;&#21015;&#65288;S3&#65289;&#26684;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#38754;&#21521;LTSM&#30340;GPT&#39118;&#26684;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23454;&#29616;&#20272;&#35745;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#25968;&#20540;&#24615;&#33021;&#65292;&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21518;&#20063;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16571</link><description>&lt;p&gt;
&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#20272;&#35745;&#20010;&#20307;&#21270;&#22810;&#27835;&#30103;&#21453;&#24212;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons. (arXiv:2401.16571v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16571
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23454;&#29616;&#20272;&#35745;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#25968;&#20540;&#24615;&#33021;&#65292;&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21518;&#20063;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26159;&#31934;&#30830;&#21307;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20852;&#36259;&#22312;&#20110;&#22522;&#20110;&#19968;&#20123;&#22806;&#37096;&#21327;&#21464;&#37327;&#65292;&#30830;&#23450;&#19981;&#21516;&#27835;&#30103;&#26041;&#24335;&#30340;&#24046;&#24322;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#25105;&#20204;&#23545;&#21709;&#24212;&#26354;&#32447;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#20381;&#36182;&#20110;&#24102;&#26377;&#20849;&#20139;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#12290;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#36866;&#24403;&#22320;&#22788;&#29702;&#20102;&#20998;&#26512;&#21508;&#20010;&#26041;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;MIMIC&#25968;&#25454;&#21518;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;&#19981;&#21516;&#27835;&#30103;&#31574;&#30053;&#23545;ICU&#20303;&#38498;&#26102;&#38388;&#21644;12&#23567;&#26102;SOFA&#35780;&#20998;&#30340;&#24433;&#21709;&#30340;&#19968;&#20123;&#26377;&#36259;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA sc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.03546</link><description>&lt;p&gt;
&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#22312;&#19981;&#21305;&#37197;&#27979;&#37327;&#21644;&#20808;&#39564;&#27169;&#22411;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models. (arXiv:2310.03546v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25554;&#25300;&#24335;&#26410;&#35843;&#25972;&#26391;&#20043;&#19975;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#20351;&#29992;&#22270;&#20687;&#21435;&#22122;&#22120;&#25351;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#19982;&#19981;&#21305;&#37197;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#21644;&#21435;&#22122;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#23578;&#26410;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21518;&#39564;-L2&#25311;&#24230;&#37327;&#24182;&#21033;&#29992;&#23427;&#26469;&#37327;&#21270;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#30340;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#22914;&#20174;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#22270;&#20687;&#21435;&#27169;&#31946;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#21487;&#20197;&#31934;&#30830;&#22320;&#25551;&#36848;&#20854;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#21644;No U-Turn Sampler (NUTS) &#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;NUTS&#20316;&#20026;&#21160;&#24577;HMC&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#20855;&#26377;&#36941;&#21382;&#24615;&#21644;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;HMC&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;HMC&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.03460</link><description>&lt;p&gt;
&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo&#21644;No U-Turn Samplers&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of dynamic implementations of Hamiltonian Monte Carlo and No U-Turn Samplers. (arXiv:2307.03460v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#21644;No U-Turn Sampler (NUTS) &#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;NUTS&#20316;&#20026;&#21160;&#24577;HMC&#30340;&#29305;&#20363;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#20855;&#26377;&#36941;&#21382;&#24615;&#21644;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;HMC&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;HMC&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21160;&#24577;&#23454;&#29616;&#30340;Hamiltonian Monte Carlo (HMC)&#31639;&#27861;&#65292;&#20363;&#22914;No U-Turn Sampler (NUTS)&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#20855;&#26377;&#25104;&#21151;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20294;&#20851;&#20110;&#23427;&#20204;&#34892;&#20026;&#30340;&#29702;&#35770;&#32467;&#26524;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31216;&#20026;&#21160;&#24577;HMC&#30340;&#36890;&#29992;MCMC&#31639;&#27861;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#36890;&#29992;&#26694;&#26550;&#28085;&#30422;&#20102;NUTS&#20316;&#20026;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#20010;&#38468;&#24102;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#30446;&#26631;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;NUTS&#19981;&#21487;&#32422;&#21644;&#38750;&#21608;&#26399;&#30340;&#26465;&#20214;&#65292;&#24182;&#20316;&#20026;&#25512;&#35770;&#32780;&#35777;&#26126;&#20102;&#36941;&#21382;&#24615;&#12290;&#22312;&#31867;&#20284;&#20110;HMC&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;NUTS&#20855;&#26377;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;HMC&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#24494;&#25200;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#23545;&#27493;&#38271;&#21644;leapfrog&#27493;&#25968;&#36827;&#34892;&#20219;&#20309;&#26377;&#30028;&#26465;&#20214;&#65292;&#20063;&#26159;&#36941;&#21382;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is substantial empirical evidence about the success of dynamic implementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler (NUTS), in many challenging inference problems but theoretical results about their behavior are scarce. The aim of this paper is to fill this gap. More precisely, we consider a general class of MCMC algorithms we call dynamic HMC. We show that this general framework encompasses NUTS as a particular case, implying the invariance of the target distribution as a by-product. Second, we establish conditions under which NUTS is irreducible and aperiodic and as a corrolary ergodic. Under conditions similar to the ones existing for HMC, we also show that NUTS is geometrically ergodic. Finally, we improve existing convergence results for HMC showing that this method is ergodic without any boundedness condition on the stepsize and the number of leapfrog steps, in the case where the target is a perturbation of a Gaussian distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13580</link><description>&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816; (EOT) &#26159;&#20248;&#21270;&#36755;&#36816; (OT) &#30340;&#19968;&#31181;&#26377;&#25928;&#19988;&#35745;&#31639;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25512;&#23548;&#20986;&#20102; EOT &#25104;&#26412;&#30340;&#26032;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#29109;&#27491;&#21017;&#21270;&#21442;&#25968; $\epsilon$ &#21644;&#26679;&#26412;&#22823;&#23567; $n$ &#30340;&#32479;&#35745;&#24615;&#33021;&#20165;&#21462;&#20915;&#20110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#20043;&#20013;&#36739;&#31616;&#21333;&#30340;&#37027;&#20010;&#12290;&#20363;&#22914;&#65292;&#22312;&#20805;&#20998;&#24179;&#28369;&#30340;&#25104;&#26412;&#19979;&#65292;&#36825;&#20250;&#20135;&#29983;&#20855;&#26377;$\epsilon^{-d/2}$&#22240;&#23376;&#30340;&#21442;&#25968;&#21270;&#36895;&#29575;$n^{-1/2}$&#65292;&#20854;&#20013;$d$&#26159;&#20004;&#20010;&#24635;&#20307;&#27979;&#24230;&#30340;&#26368;&#23567;&#32500;&#24230;&#12290;&#36825;&#30830;&#35748;&#20102;&#32463;&#39564;EOT&#20063;&#36981;&#24490;&#20102;&#26368;&#36817;&#25165;&#20026;&#26410;&#35268;&#21017;&#21270;OT&#30830;&#35748;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#30340;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#32463;&#39564;&#29109;Gromov-Wasserstein&#36317;&#31163;&#21450;&#20854;&#26410;&#35268;&#21017;&#21270;&#29256;&#26412;&#20063;&#36981;&#24490;&#27492;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip
&lt;/p&gt;</description></item></channel></rss>