<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19455</link><description>&lt;p&gt;
&#21548;&#22122;&#22768;&#65306;&#20351;&#29992;Gibbs&#25193;&#25955;&#36827;&#34892;&#30450;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Listening to the Noise: Blind Denoising with Gibbs Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19455
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#38382;&#39064;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#23494;&#19981;&#21487;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#21435;&#22122;&#22120;&#65292;&#23427;&#20204;&#25152;&#24314;&#27169;&#30340;&#20998;&#24067;&#19982;&#36125;&#21494;&#26031;&#22270;&#20687;&#20013;&#30340;&#21435;&#22122;&#20808;&#39564;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#21518;&#39564;&#37319;&#26679;&#36827;&#34892;&#21435;&#22122;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#65292;&#36825;&#38459;&#30861;&#20102;&#30450;&#21435;&#22122;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#22788;&#29702;&#20449;&#21495;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#20551;&#35774;&#20219;&#24847;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;Gibbs&#31639;&#27861;&#65292;&#20132;&#26367;&#22320;&#20174;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#23558;&#20449;&#21495;&#20808;&#39564;&#26144;&#23556;&#21040;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#19968;&#20010;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#32570;&#38519;&#65292;&#25351;&#23548;&#20102;&#35786;&#26029;&#30340;&#20351;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;Gibbs s&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19455v1 Announce Type: cross  Abstract: In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.11039</link><description>&lt;p&gt;
&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#20122;&#32676;&#20307;&#36716;&#31227;&#40065;&#26834;&#24615;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#38024;&#23545;&#26368;&#20248;&#32452;&#20934;&#30830;&#24615;(WGA)&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#20110;&#20381;&#36182;&#20110;&#33391;&#22909;&#26631;&#27880;&#30340;&#32452;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#20013;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27880;&#37322;&#30340;&#25968;&#25454;&#22686;&#24378;&#20351;&#29992;&#19979;&#37319;&#26679;&#25110;&#19978;&#21152;&#26435;&#29992;&#20110;WGA&#26159;&#23481;&#26131;&#21463;&#21040;&#39046;&#22495;&#26631;&#27880;&#22122;&#22768;&#24178;&#25200;&#65292;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#25509;&#36817;&#20351;&#29992;&#21407;&#22987;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;WGA&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#26631;&#27880;&#27491;&#21017;&#21270;(RAD)&#26469;&#35757;&#32451;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#39046;&#22495;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RAD&#19982;&#20854;&#20182;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#39046;&#22495;&#26631;&#27880;&#25216;&#26415;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20165;&#26377;5%&#30340;&#22122;&#22768;&#65292;RAD&#20063;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#20381;&#36182;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06121</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#20174;&#29627;&#23572;&#20857;&#26364;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Iterated Denoising Energy Matching for Sampling from Boltzmann Densities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06121
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#27604;&#22914;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#34913;&#26679;&#26412;&#65292;&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#65288;iDEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#65292;&#20165;&#20351;&#29992;&#33021;&#37327;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230; - &#32780;&#19981;&#26159;&#25968;&#25454;&#26679;&#26412; - &#26469;&#35757;&#32451;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;iDEM&#22312;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65306;&#65288;I&#65289;&#20174;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#20013;&#37319;&#26679;&#39640;&#27169;&#22411;&#23494;&#24230;&#30340;&#21306;&#22495;&#65292;&#21644;&#65288;II&#65289;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22312;&#25105;&#20204;&#30340;&#38543;&#26426;&#21305;&#37197;&#30446;&#26631;&#20013;&#36827;&#19968;&#27493;&#25913;&#36827;&#37319;&#26679;&#22120;&#12290;iDEM&#22312;&#39640;&#32500;&#24230;&#19978;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#20869;&#37096;&#21305;&#37197;&#30446;&#26631;&#26159;&#26080;&#38656;&#27169;&#25311;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;MCMC&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;iDEM&#24179;&#28369;&#20102;&#33021;&#37327;&#32972;&#26223;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20998;&#25674;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#20219;&#21153;&#36827;&#34892;&#20102;iDEM&#30340;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks rang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#20851;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#22240;&#26524;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#20869;&#37096;&#26495;&#26469;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#27169;&#22411;&#12290;&#21457;&#29616;&#20998;&#23618;&#25968;&#25454;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#65292;&#21363;&#20351;&#20351;&#29992;&#38750;&#20998;&#23618;&#25968;&#25454;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#24320;&#21457;&#20102;&#29992;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#20272;&#35745;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.05330</link><description>&lt;p&gt;
&#20998;&#23618;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Causal Models. (arXiv:2401.05330v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#20851;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#22240;&#26524;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#20869;&#37096;&#26495;&#26469;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#27169;&#22411;&#12290;&#21457;&#29616;&#20998;&#23618;&#25968;&#25454;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#65292;&#21363;&#20351;&#20351;&#29992;&#38750;&#20998;&#23618;&#25968;&#25454;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#24320;&#21457;&#20102;&#29992;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#20272;&#35745;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#20204;&#32463;&#24120;&#24819;&#35201;&#20174;&#20998;&#23618;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#20174;&#23884;&#22871;&#22312;&#21333;&#20301;&#20869;&#37096;&#30340;&#23376;&#21333;&#20803;&#25910;&#38598;&#30340;&#12290;&#27604;&#22914;&#23398;&#26657;&#20013;&#30340;&#23398;&#29983;&#12289;&#30149;&#20154;&#30340;&#32454;&#32990;&#25110;&#24030;&#20013;&#30340;&#22478;&#24066;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#20301;&#32423;&#21464;&#37327;&#65288;&#20363;&#22914;&#27599;&#20010;&#23398;&#26657;&#30340;&#39044;&#31639;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;&#23376;&#21333;&#20301;&#32423;&#21464;&#37327;&#65288;&#20363;&#22914;&#27599;&#20010;&#23398;&#26657;&#27599;&#20010;&#23398;&#29983;&#30340;&#32771;&#35797;&#25104;&#32489;&#65289;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#22240;&#26524;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#22240;&#26524;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#20869;&#37096;&#26495;&#26469;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23618;&#22240;&#26524;&#27169;&#22411;&#30340;&#36890;&#29992;&#22270;&#24418;&#35782;&#21035;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25193;&#23637;&#20102;do-&#35745;&#31639;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#20351;&#29992;&#38750;&#20998;&#23618;&#25968;&#25454;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20998;&#23618;&#25968;&#25454;&#20063;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;&#25105;&#20204;&#21482;&#26377;&#23376;&#21333;&#20301;&#32423;&#21464;&#37327;&#30340;&#21333;&#20301;&#32423;&#27719;&#24635;&#65288;&#20363;&#22914;&#23398;&#26657;&#30340;&#24179;&#22343;&#32771;&#35797;&#25104;&#32489;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#23398;&#29983;&#30340;&#25104;&#32489;&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#20998;&#23618;&#25968;&#25454;&#30340;&#20272;&#35745;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists often want to learn about cause and effect from hierarchical data, collected from subunits nested inside units. Consider students in schools, cells in patients, or cities in states. In such settings, unit-level variables (e.g. each school's budget) may affect subunit-level variables (e.g. the test scores of each student in each school) and vice versa. To address causal questions with hierarchical data, we propose hierarchical causal models, which extend structural causal models and causal graphical models by adding inner plates. We develop a general graphical identification technique for hierarchical causal models that extends do-calculus. We find many situations in which hierarchical data can enable causal identification even when it would be impossible with non-hierarchical data, that is, if we had only unit-level summaries of subunit-level variables (e.g. the school's average test score, rather than each student's score). We develop estimation techniques for hierarchical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.04055</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23450;&#20215;&#26159;&#20225;&#19994;&#24120;&#29992;&#30340;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#21046;&#23450;&#20215;&#26684;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20080;&#23478;&#20063;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#25968;&#25454;&#26469;&#33719;&#21462;&#26356;&#20302;&#30340;&#20215;&#26684;&#65292;&#20294;&#36825;&#20063;&#20250;&#23548;&#33268;&#29305;&#23450;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#20250;&#38459;&#30861;&#20225;&#19994;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#12290;&#21334;&#26041;&#26080;&#27861;&#35266;&#23519;&#21040;&#20080;&#23478;&#30340;&#30495;&#23454;&#29305;&#24449;&#65292;&#32780;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#26681;&#25454;&#31574;&#30053;&#34892;&#20026;&#25805;&#32437;&#21518;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21334;&#26041;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#20272;&#20540;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#20855;&#20307;&#25968;&#20540;&#65292;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#20108;&#36827;&#21046;&#30340;&#21709;&#24212;&#65292;&#34920;&#31034;&#26159;&#21542;&#21457;&#29983;&#38144;&#21806;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19981;&#32771;&#34385;&#31574;&#30053;&#24615;&#30340;&#23450;&#20215;&#31574;&#30053;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#65292;&#20197;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#29305;&#24449;&#26500;&#24314;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16297</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26102;&#21464;&#35843;&#33410;&#22240;&#32032;&#30340;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#20272;&#35745;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Method for Estimation of Causal Excursion Effects to Assess Time-Varying Moderation. (arXiv:2306.16297v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16297
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#65292;&#20197;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#29305;&#24449;&#26500;&#24314;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#30340;&#25968;&#23383;&#21270;&#20581;&#24247;&#24178;&#39044;&#30340;&#21452;&#37325;&#38761;&#21629;&#26174;&#33879;&#22686;&#21152;&#20102;&#31227;&#21160;&#20581;&#24247;&#65288;mHealth&#65289;&#24178;&#39044;&#22312;&#21508;&#20010;&#20581;&#24247;&#31185;&#23398;&#39046;&#22495;&#30340;&#21487;&#21450;&#24615;&#21644;&#37319;&#32435;&#29575;&#12290;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#31216;&#20026;&#24494;&#38543;&#26426;&#35797;&#39564;&#65288;MRTs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#23454;&#35777;&#35780;&#20272;&#36825;&#20123;mHealth&#24178;&#39044;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#24615;&#12290;MRTs&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;&#37327;&#65292;&#31216;&#20026;&#8220;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#8221;&#65292;&#20351;&#20581;&#24247;&#31185;&#23398;&#23478;&#33021;&#22815;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#29305;&#24449;&#26500;&#24314;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23548;&#33268;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twin revolutions in wearable technologies and smartphone-delivered digital health interventions have significantly expanded the accessibility and uptake of mobile health (mHealth) interventions across various health science domains. Sequentially randomized experiments called micro-randomized trials (MRTs) have grown in popularity to empirically evaluate the effectiveness of these mHealth intervention components. MRTs have given rise to a new class of causal estimands known as "causal excursion effects", which enable health scientists to assess how intervention effectiveness changes over time or is moderated by individual characteristics, context, or responses in the past. However, current data analysis methods for estimating causal excursion effects require pre-specified features of the observed high-dimensional history to construct a working model of an important nuisance parameter. While machine learning algorithms are ideal for automatic feature construction, their naive application
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15598</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20419;&#36827;&#23398;&#20064;&#21333;&#25351;&#25968;&#21644;&#22810;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#22823;&#20110;&#20004;&#23618;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#31867;&#28145;&#24230;&#19981;&#21516;&#20294;&#23481;&#37327;&#30456;&#21516;&#30340;&#32593;&#32476;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#26174;&#24335;&#23450;&#20041;&#30340;&#34920;&#31034;&#25104;&#26412;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35825;&#23548;&#30340;&#20989;&#25968;&#30340;&#34920;&#31034;&#25104;&#26412;&#26159;&#32593;&#32476;&#34920;&#31034;&#35813;&#20989;&#25968;&#25152;&#38656;&#30340;&#24179;&#26041;&#26435;&#37325;&#20043;&#21644;&#30340;&#26368;&#23567;&#20540;&#65307;&#23427;&#21453;&#26144;&#20102;&#19982;&#35813;&#26550;&#26500;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32447;&#24615;&#23618;&#28155;&#21152;&#21040;ReLU&#32593;&#32476;&#20250;&#20135;&#29983;&#19968;&#20010;&#34920;&#31034;&#25104;&#26412;&#65292;&#36825;&#26377;&#21033;&#20110;&#20351;&#29992;&#20004;&#23618;&#32593;&#32476;&#26469;&#36924;&#36817;&#30001;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20855;&#26377;&#20302;&#34920;&#31034;&#25104;&#26412;&#30340;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26368;&#23567;&#30340;&#34920;&#31034;&#25104;&#26412;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#20250;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13152</link><description>&lt;p&gt;
STEEL: &#22855;&#24322;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#24635;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#30446;&#26631;&#31574;&#30053;&#35825;&#23548;&#30340;&#20998;&#24067;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#20197;&#20415;&#36890;&#36807;&#21464;&#25442;&#27979;&#24230;&#20351;&#29992;&#25209;&#37327;&#25968;&#25454;&#26469;&#26657;&#20934;&#30446;&#26631;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32477;&#23545;&#36830;&#32493;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31639;&#27861;&#20026;STEEL&#65306;SingulariTy-awarE rEinforcement Learning&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;&#20851;&#20110;&#31163;&#32447;&#35780;&#20272;&#30340;&#26032;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#65292;&#20197;&#21450;&#24102;&#26377;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31574;&#30053;&#23450;&#21521;&#35823;&#24046;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22855;&#24322;&#24773;&#20917;&#30340;&#23450;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
&lt;/p&gt;</description></item></channel></rss>