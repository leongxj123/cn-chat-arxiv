<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03655</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23884;&#22871;&#20302;&#31209;&#36817;&#20284;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Operator SVD with Neural Networks via Nested Low-Rank Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#65292;&#35745;&#31639;&#32473;&#23450;&#32447;&#24615;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#65288;EVD&#65289;&#25110;&#25214;&#21040;&#20854;&#20027;&#35201;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#29305;&#24449;&#20989;&#25968;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#22522;&#20110;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36817;&#20284;&#34920;&#24449;&#65292;&#24182;&#20276;&#38543;&#30528;&#31216;&#20026;&#23884;&#22871;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#27491;&#30830;&#30340;&#39034;&#24207;&#23398;&#20064;&#21069;L&#20010;&#22855;&#24322;&#20540;&#21644;&#22855;&#24322;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20419;&#36827;&#20102;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#65292;&#36825;&#20010;&#20844;&#24335;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#27714;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#22312;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2311.12163</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantum Inception Score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12163
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24320;&#22987;&#20102;&#23545;&#23427;&#20204;&#37327;&#23376;&#29256;&#26412;&#30340;&#28909;&#20999;&#25506;&#32034;&#12290;&#20026;&#20102;&#24320;&#22987;&#36825;&#19968;&#25506;&#32034;&#20043;&#26053;&#65292;&#24320;&#21457;&#19968;&#20010;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#24456;&#37325;&#35201;&#30340;&#65307;&#22312;&#32463;&#20856;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#20415;&#26159;&#21551;&#33945;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#23427;&#23558;&#36136;&#37327;&#19982;&#29992;&#20110;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#30340;&#37327;&#23376;&#36890;&#36947;&#30340;Holevo&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#65292;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#27604;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#30001;&#19981;&#23545;&#31216;&#24615;&#30340;&#36164;&#28304;&#29702;&#35770;&#21644;&#32416;&#32544;&#25152;&#34920;&#24449;&#30340;&#37327;&#23376;&#30456;&#24178;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#26469;&#34920;&#24449;&#38480;&#21046;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06715</link><description>&lt;p&gt;
S4Sleep: &#35299;&#26512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models. (arXiv:2310.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#36890;&#36947;&#30561;&#30496;&#33041;&#30005;&#22270;&#35760;&#24405;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#25171;&#20998;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#35780;&#20998;&#20154;&#21592;&#20043;&#38388;&#24046;&#24322;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24102;&#26469;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#34429;&#28982;&#24050;&#32463;&#20026;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20294;&#26576;&#20123;&#20851;&#38190;&#30340;&#26550;&#26500;&#20915;&#31574;&#24182;&#26410;&#24471;&#21040;&#31995;&#32479;&#24615;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35843;&#26597;&#20102;&#24191;&#27867;&#30340;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#26550;&#26500;&#33539;&#30068;&#20869;&#30340;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#22768;&#35889;&#22270;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#24191;&#27867;&#30340;SHHS&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#25913;&#36827;&#36890;&#36807;&#32479;&#35745;&#21644;&#31995;&#32479;&#35823;&#24046;&#20272;&#35745;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#20174;&#26412;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#26550;&#26500;&#27934;&#23519;&#19981;&#20165;&#23545;&#26410;&#26469;&#30340;&#30561;&#30496;&#20998;&#26399;&#30740;&#31350;&#26377;&#20215;&#20540;&#65292;&#32780;&#19988;&#23545;&#25972;&#20307;&#30561;&#30496;&#30740;&#31350;&#37117;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.09104</link><description>&lt;p&gt;
&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32553;&#20943;&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks. (arXiv:2308.09104v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20943;&#23569;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#24674;&#22797;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32467;&#26500;&#31232;&#30095;&#65288;&#22914;&#33410;&#28857;&#31232;&#30095;&#65289;&#21387;&#32553;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#25512;&#29702;&#12289;&#26356;&#39640;&#30340;&#25968;&#25454;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32553;&#20943;&#25216;&#26415;&#65292;Lasso&#21644;Horseshoe&#65292;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso (SS-GL)&#21644;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe (SS-GHS)&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#21253;&#25324;&#23545;&#20271;&#21162;&#21033;&#21464;&#37327;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#32553;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational 
&lt;/p&gt;</description></item><item><title>&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16156</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Optimal Transport for Machine Learning. (arXiv:2306.16156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16156
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26368;&#20248;&#36755;&#36816;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#27604;&#36739;&#21644;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#29575;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#28304;&#20110;&#20854;&#20016;&#23500;&#30340;&#21382;&#21490;&#21644;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;2012&#24180;&#33267;2022&#24180;&#26399;&#38388;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#36129;&#29486;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#30340;&#22235;&#20010;&#23376;&#39046;&#22495;&#65306;&#26377;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#31361;&#20986;&#20102;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 -- 2022, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport, and its interplay with Machine Learning practice.
&lt;/p&gt;</description></item></channel></rss>