<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#30340;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#34892;&#19978;&#36880;&#27493;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.10545</link><description>&lt;p&gt;
&#20248;&#21270;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#19982;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;
&lt;/p&gt;
&lt;p&gt;
Optimal vintage factor analysis with deflation varimax. (arXiv:2310.10545v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#30340;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#34892;&#19978;&#36880;&#27493;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39318;&#20808;&#25214;&#21040;&#21407;&#22987;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#23547;&#27714;&#26059;&#36716;&#65292;&#20351;&#26059;&#36716;&#21518;&#30340;&#20302;&#32500;&#34920;&#31034;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#12290;&#23613;&#31649;Principal Component Analysis (PCA) followed by the varimax rotation&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#65292;&#20294;&#30001;&#20110;varimax rotation&#38656;&#35201;&#22312;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#19978;&#35299;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#24456;&#38590;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#30340;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#36807;&#31243;&#12290;&#38500;&#20102;&#22312;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#21644;&#28789;&#27963;&#24615;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#33021;&#22312;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#23545;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#36827;&#34892;&#23436;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;PCA&#20043;&#21518;&#37319;&#29992;&#36825;&#31181;&#26032;&#30340;varimax&#26041;&#27861;&#20316;&#20026;&#31532;&#20108;&#27493;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#20004;&#27493;&#36807;&#31243;&#22312;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#22240;&#23376;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.  In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.  Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our resul
&lt;/p&gt;</description></item><item><title>&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07430</link><description>&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07430
&lt;/p&gt;
&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#26356;&#26032;&#20801;&#35768;&#20351;&#29992;&#26412;&#22320;&#21644;&#35745;&#31639;&#19978;&#21487;&#36319;&#36394;&#30340;&#26356;&#26032;&#26469;&#34920;&#31034;&#22823;&#35268;&#27169;&#22270;&#12290;&#28982;&#32780;&#65292;&#26412;&#22320;&#26356;&#26032;&#21463;&#21040;&#22238;&#28335;&#30340;&#24433;&#21709;&#65292;&#21363;&#28040;&#24687;&#36890;&#36807;&#21516;&#19968;&#26465;&#36793;&#20004;&#27425;&#27969;&#21160;&#24182;&#37325;&#35775;&#20808;&#21069;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#30001;&#20110;&#28040;&#24687;&#27969;&#30340;&#25968;&#37327;&#38543;&#30528;&#26356;&#26032;&#30340;&#27425;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38459;&#30861;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#28040;&#24687;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#22238;&#28335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;NBA-GNN&#65289;&#35299;&#20915;&#20102;&#36825;&#31181;&#20887;&#20313;&#65292;&#35813;&#32593;&#32476;&#22312;&#26356;&#26032;&#28040;&#24687;&#26102;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;NBA-GNN&#22914;&#20309;&#32531;&#35299;GNN&#30340;&#36807;&#24230;&#21387;&#32553;&#65292;&#24182;&#24314;&#31435;&#20102;NBA-GNN&#21644;&#38750;&#22238;&#28335;&#26356;&#26032;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#20986;&#33394;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;NBA-
&lt;/p&gt;
&lt;p&gt;
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00646</link><description>&lt;p&gt;
WASA&#65306;&#22522;&#20110;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;&#28304;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data. (arXiv:2310.00646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#20854;&#21830;&#19994;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#24341;&#21457;&#20102;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#21487;&#33021;&#20405;&#29359;&#34987;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;a&#65289;&#36890;&#36807;&#27700;&#21360;&#35782;&#21035;&#20986;&#23545;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#20570;&#20986;&#36129;&#29486;&#30340;&#25968;&#25454;&#25552;&#20379;&#32773;&#65288;&#28304;&#24402;&#23646;&#65289;&#65307;&#20197;&#21450;&#65288;b&#65289;&#39564;&#35777;&#25991;&#26412;&#25968;&#25454;&#26159;&#21542;&#26469;&#33258;&#20110;&#26576;&#20010;&#25968;&#25454;&#25552;&#20379;&#32773;&#23545;LLM&#36827;&#34892;&#20102;&#35757;&#32451;&#65288;&#25968;&#25454;&#26469;&#28304;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27700;&#21360;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#35753;LLM&#29983;&#25104;&#20855;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#24615;&#65288;&#20363;&#22914;&#28304;&#24402;&#23646;&#20934;&#30830;&#24615;&#12289;&#25269;&#25239;&#23545;&#25163;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;WAtermarking for Source Attribution&#65288;WASA&#65289;&#26694;&#26550;.
&lt;/p&gt;
&lt;p&gt;
The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies
&lt;/p&gt;</description></item><item><title>&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06442</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#21327;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06442
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32771;&#34385;&#36164;&#28304;&#32422;&#26463;&#21644;&#19981;&#21516;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25910;&#38598;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#20256;&#24863;&#22120;/&#20195;&#29702;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;/&#20195;&#29702;&#26679;&#26412;&#26469;&#33258;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#38416;&#36848;&#20026;&#36153;&#33293;&#23572;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;&#25110;Cramer-Rao&#30028;&#26368;&#23567;&#21270;&#65289;&#38382;&#39064;&#12290;&#24403;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#22320;&#35782;&#21035;&#20986;&#20004;&#20010;&#29305;&#23450;&#24773;&#20917;&#65306;&#65288;1&#65289;&#19981;&#33021;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#36827;&#34892;&#21327;&#20316;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#65288;2&#65289;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#28041;&#21450;&#25237;&#36164;&#26377;&#38480;&#36164;&#28304;&#20197;&#21327;&#20316;&#37319;&#26679;&#21644;&#36716;&#31227;&#24050;&#30693;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;(PPGD)&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#19968;&#31867;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#24555;&#36895;&#25910;&#25947;&#65292;&#24403;&#36845;&#20195;&#27425;&#25968; $k \geq k_0$ &#26102;&#65292;PPGD &#21487;&#20197;&#20197; $\cO(1/k^2)$ &#30340;&#24555;&#36895;&#25910;&#25947;&#29575;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.10499</link><description>&lt;p&gt;
&#19968;&#31867;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#25237;&#24433;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65306;&#19981;&#38656;&#35201; Kurdyka-Lojasiewicz&#65288;KL&#65289;&#24615;&#36136;&#20063;&#33021;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Projective Proximal Gradient Descent for A Class of Nonconvex Nonsmooth Optimization Problems: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property. (arXiv:2304.10499v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;(PPGD)&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#19968;&#31867;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#24555;&#36895;&#25910;&#25947;&#65292;&#24403;&#36845;&#20195;&#27425;&#25968; $k \geq k_0$ &#26102;&#65292;PPGD &#21487;&#20197;&#20197; $\cO(1/k^2)$ &#30340;&#24555;&#36895;&#25910;&#25947;&#29575;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#19968;&#31867;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#25237;&#24433;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;(PPGD)&#65292;&#20854;&#20013;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#28304;&#33258;&#19968;&#20010;&#38750;&#20984;&#20294;&#20998;&#27573;&#20984;&#30340;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110; Kurdyka-\L{}ojasiewicz (K\L{}) &#24615;&#36136;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#36827;&#34892;&#21152;&#36895; PGD &#26041;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102; PPGD &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#22312;&#19968;&#31867;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#23616;&#37096;&#25910;&#25947;&#12290;&#35777;&#26126;&#20102;&#24403;&#36845;&#20195;&#27425;&#25968; $k \geq k_0$ &#26102;&#65292;PPGD &#21487;&#20197;&#20197; $\cO(1/k^2)$ &#30340;&#24555;&#36895;&#25910;&#25947;&#29575;&#25910;&#25947;&#65292;&#20854;&#20013; $k_0$ &#26159;&#19968;&#20010;&#26377;&#38480;&#30340;&#24120;&#25968;&#12290;&#35813;&#31639;&#27861;&#22312;&#20809;&#28369;&#19988;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#19968;&#38454;&#26041;&#27861;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23616;&#37096; Nesterov &#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;......&#65288;&#27492;&#22788;&#30465;&#30053;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex and nonsmooth optimization problems are important and challenging for statistics and machine learning. In this paper, we propose Projected Proximal Gradient Descent (PPGD) which solves a class of nonconvex and nonsmooth optimization problems, where the nonconvexity and nonsmoothness come from a nonsmooth regularization term which is nonconvex but piecewise convex. In contrast with existing convergence analysis of accelerated PGD methods for nonconvex and nonsmooth problems based on the Kurdyka-\L{}ojasiewicz (K\L{}) property, we provide a new theoretical analysis showing local fast convergence of PPGD. It is proved that PPGD achieves a fast convergence rate of $\cO(1/k^2)$ when the iteration number $k \ge k_0$ for a finite $k_0$ on a class of nonconvex and nonsmooth problems under mild assumptions, which is locally Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Experimental results demonst
&lt;/p&gt;</description></item></channel></rss>