<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;</title><link>https://arxiv.org/abs/2403.13724</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#21644;F\"ollmer&#36807;&#31243;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Stochastic Interpolants and F\"ollmer Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#12290;&#22312;&#32473;&#23450;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#29366;&#24577;&#35266;&#27979;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#27979;&#38382;&#39064;&#26500;&#24314;&#20026;&#20174;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#24471;&#21040;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#26500;&#24314;&#22312;&#20219;&#24847;&#22522;&#30784;&#20998;&#24067;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34394;&#26500;&#30340;&#12289;&#38750;&#29289;&#29702;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20854;&#20197;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#20316;&#20026;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#19968;&#20010;&#26469;&#33258;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#36807;&#31243;&#23558;&#20197;&#24403;&#21069;&#29366;&#24577;&#20026;&#20013;&#24515;&#30340;&#28857;&#29366;&#36136;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#20013;&#30340;&#28418;&#31227;&#31995;&#25968;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13724v1 Announce Type: new  Abstract: We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be lear
&lt;/p&gt;</description></item><item><title>CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11960</link><description>&lt;p&gt;
CASPER&#65306;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11960
&lt;/p&gt;
&lt;p&gt;
CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#26159;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#24433;&#21709;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#36890;&#36807;&#25918;&#32622;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#25910;&#38598;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#21508;&#31181;&#25925;&#38556;&#32780;&#23548;&#33268;&#30340;&#32570;&#22833;&#20540;&#65292;&#36825;&#23545;&#25968;&#25454;&#20998;&#26512;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#22312;&#24674;&#22797;&#29305;&#23450;&#25968;&#25454;&#28857;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#32771;&#34385;&#19982;&#35813;&#28857;&#30456;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#19968;&#20123;&#26410;&#30693;&#28151;&#26434;&#22240;&#32032;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#26500;&#24314;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#38750;&#22240;&#26524;&#24555;&#25463;&#36793;&#12290;&#36825;&#20123;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24320;&#36767;&#21453;&#21521;&#36335;&#24452;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#24314;&#31435;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.00645</link><description>&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#21644;&#26434;&#36136;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20248;&#34880;&#28165;&#20998;&#31867;&#30340;&#26368;&#23567;&#20551;&#35774;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimal Assumptions for Optimal Serology Classification: Theory and Implications for Multidimensional Settings and Impure Training Data. (arXiv:2309.00645v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34880;&#28165;&#23398;&#20013;&#65292;&#20943;&#23569;&#20559;&#24046;&#20272;&#35745;&#21644;&#35786;&#26029;&#20998;&#31867;&#22120;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;&#24314;&#27169;&#27979;&#37327;&#32467;&#26524;&#30340;&#31867;&#21035;-&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDFs&#65289;&#65292;&#23427;&#20204;&#25511;&#21046;&#25152;&#26377;&#21518;&#32493;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20165;&#20855;&#26377;&#23569;&#25968;&#32500;&#24230;&#65288;&#20363;&#22914;&#30446;&#26631;&#25239;&#21407;&#65289;&#30340;&#27979;&#37327;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#20063;&#24456;&#24555;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32463;&#39564;&#35757;&#32451;&#25968;&#25454;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#20998;&#31867;&#26679;&#26412;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;PDFs&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24341;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#26041;&#27861;&#65292;&#35813;&#24341;&#29702;&#23558;&#30456;&#23545;&#26465;&#20214;&#27010;&#29575;&#19982;&#26368;&#23567;&#35823;&#24046;&#20998;&#31867;&#36793;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#65288;i&#65289;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65307;&#65288;ii&#65289;&#26681;&#25454;&#26679;&#26412;&#30456;&#23545;&#20110;&#22352;&#26631;&#36724;&#30340;&#20301;&#32622;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Minimizing error in prevalence estimates and diagnostic classifiers remains a challenging task in serology. In theory, these problems can be reduced to modeling class-conditional probability densities (PDFs) of measurement outcomes, which control all downstream analyses. However, this task quickly succumbs to the curse of dimensionality, even for assay outputs with only a few dimensions (e.g. target antigens). To address this problem, we propose a technique that uses empirical training data to classify samples and estimate prevalence in arbitrary dimension without direct access to the conditional PDFs. We motivate this method via a lemma that relates relative conditional probabilities to minimum-error classification boundaries. This leads us to formulate an optimization problem that: (i) embeds the data in a parameterized, curved space; (ii) classifies samples based on their position relative to a coordinate axis; and (iii) subsequently optimizes the space by minimizing the empirical c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.13139</link><description>&lt;p&gt;
&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#31934;&#30830;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exact recovery for the non-uniform Hypergraph Stochastic Block Model. (arXiv:2304.13139v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#22312;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#19979;&#30340;&#38543;&#26426;&#36229;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#36793;&#29420;&#31435;&#22320;&#20197;&#26576;&#20123;&#32473;&#23450;&#27010;&#29575;&#20986;&#29616;&#65292;&#35813;&#27010;&#29575;&#20165;&#21462;&#20915;&#20110;&#20854;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#39318;&#27425;&#24314;&#31435;&#20102;&#22312;&#36825;&#31181;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#21463;&#21040;&#27425;&#35201;&#32422;&#26463;&#65307;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;K&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#23545;&#31216;&#20108;&#36827;&#21046;&#27169;&#22411;&#65288;K=2&#65289;&#12290;&#20851;&#38190;&#28857;&#26159;&#36890;&#36807;&#32858;&#21512;&#25152;&#26377;&#22343;&#21248;&#23618;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#32771;&#34385;&#27599;&#20010;&#23618;&#26102;&#20284;&#20046;&#19981;&#21487;&#33021;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#33719;&#24471;&#31934;&#30830;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#22312;&#38408;&#20540;&#20197;&#19978;&#23454;&#29616;&#20102;&#31934;&#30830;&#24674;&#22797;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#20381;&#36182;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#36229;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#38598;&#20013;&#21644;&#27491;&#21017;&#21270;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Consider the community detection problem in random hypergraphs under the non-uniform hypergraph stochastic block model (HSBM), where each hyperedge appears independently with some given probability depending only on the labels of its vertices. We establish, for the first time in the literature, a sharp threshold for exact recovery under this non-uniform case, subject to minor constraints; in particular, we consider the model with $K$ classes as well as the symmetric binary model ($K=2$). One crucial point here is that by aggregating information from all the uniform layers, we may obtain exact recovery even in cases when this may appear impossible if each layer were considered alone. Two efficient algorithms that successfully achieve exact recovery above the threshold are provided. The theoretical analysis of our algorithms relies on the concentration and regularization of the adjacency matrix for non-uniform random hypergraphs, which could be of independent interest. We also address so
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17230</link><description>&lt;p&gt;
Lipschitz&#27491;&#21017;&#21270;&#26799;&#24230;&#27969;&#21644;&#39640;&#32500;&#31232;&#32570;&#25968;&#25454;&#30340;&#29983;&#25104;&#31890;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data. (arXiv:2210.17230v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17230
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21487;&#33021;&#31232;&#32570;&#12289;&#39640;&#32500;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#29983;&#25104;&#31639;&#27861;&#26159;&#22522;&#20110;&#31890;&#23376;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;Kullback-Leibler&#25110;&#20854;&#20182;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#26469;&#26500;&#36896;&#30340;&#65292;&#20854;&#20013;&#26469;&#33258;&#28304;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#31283;&#23450;&#22320;&#20316;&#20026;&#31890;&#23376;&#20256;&#36755;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#38468;&#36817;&#12290;&#20316;&#20026;&#25968;&#25454;&#25972;&#21512;&#30340;&#19968;&#20010;&#31361;&#20986;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#20256;&#36755;&#32500;&#25968;&#36229;&#36807;54K&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#28857;&#65292;&#32780;&#26679;&#26412;&#37327;&#36890;&#24120;&#21482;&#26377;&#20960;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a new class of generative algorithms capable of efficiently learning an arbitrary target distribution from possibly scarce, high-dimensional data and subsequently generate new samples. These generative algorithms are particle-based and are constructed as gradient flows of Lipschitz-regularized Kullback-Leibler or other $f$-divergences, where data from a source distribution can be stably transported as particles, towards the vicinity of the target distribution. As a highlighted result in data integration, we demonstrate that the proposed algorithms correctly transport gene expression data points with dimension exceeding 54K, while the sample size is typically only in the hundreds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.08847</link><description>&lt;p&gt;
&#26377;&#38480;&#32500;&#19979;&#30340;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#39564;&#27979;&#24230;&#21253;&#21547;&#22312;&#19968;&#20010;&#33258;&#28982;&#30340;&#20984;&#38598;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#20250;&#23548;&#33268;&#25968;&#25454;&#28857;&#30340;coreset&#12290;&#25511;&#21046;&#36825;&#26679;&#19968;&#20010;coreset&#24517;&#39035;&#26377;&#22810;&#22823;&#30340;&#19968;&#20010;&#20851;&#38190;&#25968;&#37327;&#26159;&#21253;&#21547;&#22312;&#32463;&#39564;&#20984;&#38598;&#20013;&#30340;&#32463;&#39564;&#27979;&#37327;&#21608;&#22260;&#30340;&#26368;&#22823;&#29699;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23548;&#20986;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#29699;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#25216;&#26415;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#26680;&#23725;&#22238;&#24402;&#65292;&#26469;&#34917;&#20805;&#36825;&#31181;&#19979;&#38480;&#30340;&#27966;&#29983;&#12290;&#25105;&#20204;&#26368;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38480;&#32500;RKHS&#30340;&#26500;&#36896;&#65292;&#20854;&#20013;&#21387;&#32553;&#24456;&#24046;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods.Such an approximation gives under certain conditions rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions. We complement this derivation of the lower bound by developing techniques that allow us to apply the compression approach to concrete inference problems such as kernel ridge regression. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one face
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2002.09377</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#24418;&#19979;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Misspecification-robust likelihood-free inference in high dimensions. (arXiv:2002.09377v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.09377
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#32479;&#35745;&#27169;&#22411;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#23454;&#36341;&#20013;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#25512;&#26029;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#26469;&#27010;&#29575;&#21270;&#22320;&#36924;&#36817;&#24046;&#24322;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#20110;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#21442;&#25968;&#20351;&#29992;&#21333;&#29420;&#30340;&#37319;&#38598;&#20989;&#25968;&#21644;&#24046;&#24322;&#20989;&#25968;&#26469;&#23454;&#29616;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#12290;&#26377;&#25928;&#30340;&#21152;&#24615;&#37319;&#38598;&#32467;&#26500;&#19982;&#25351;&#25968;&#25439;&#22833;-&#20284;&#28982;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35823;&#24046;&#27169;&#22411;&#35828;&#26126;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#36793;&#38469;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The me
&lt;/p&gt;</description></item></channel></rss>