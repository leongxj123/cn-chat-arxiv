<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17943</link><description>&lt;p&gt;
&#20351;&#29992;SoS&#23494;&#24230;&#20272;&#35745;&#21644;&#945;-&#31163;&#25955;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Sequential transport maps using SoS density estimation and $\alpha$-divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20256;&#36755;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#36817;&#20284;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;&#25552;&#20986;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31995;&#21015;&#32452;&#25104;&#30340;Knothe-Rosenblatt&#65288;KR&#65289;&#26144;&#23556;&#20043;&#19978;&#12290;&#20854;&#20013;&#27599;&#20010;&#26144;&#23556;&#37117;&#26159;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#20013;&#31561;&#22797;&#26434;&#24230;&#30340;&#20013;&#38388;&#23494;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#20174;&#21442;&#32771;&#23494;&#24230;&#21040;&#39044;&#35745;&#31639;&#36817;&#20284;&#23494;&#24230;&#30340;&#31934;&#30830;KR&#26144;&#23556;&#32780;&#26500;&#24314;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23558;SoS&#23494;&#24230;&#19982;&#945;-&#31163;&#25955;&#24230;&#30456;&#32467;&#21512;&#20135;&#29983;&#20102;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#21322;&#23450;&#32534;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#945;-&#31163;&#25955;&#24230;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20351;&#24471;&#33021;&#22815;&#22788;&#29702;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17943v1 Announce Type: cross  Abstract: Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#23494;&#24230;&#27604;&#20272;&#35745;(DRE)&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22240;DRE&#30340;&#25439;&#22833;&#20989;&#25968;&#32780;&#20986;&#29616;&#20102;&#20248;&#21270;&#38382;&#39064;&#65306;KL&#25955;&#24230;&#38656;&#35201;&#22823;&#26679;&#26412;&#65292;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#65292;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#26377;&#20559;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25216;&#26415;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;DRE&#30340;&#26679;&#26412;&#35201;&#27714;&#65292;&#20197;$L_1$&#35823;&#24046;&#30340;&#19978;&#30028;&#32852;&#31995;&#36215;&#26469;&#65292;&#35813;&#19978;&#30028;&#23558;&#39640;&#32500;&#24230;DRE&#20219;&#21153;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#20316;&#20026;&#19968;&#20010;&#20849;&#21516;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#29699;&#24207;&#21015;&#65292;&#21487;&#20197;&#21516;&#26102;&#39640;&#27010;&#29575;&#22320;&#21253;&#21547;&#21508;&#31181;&#26679;&#26412;&#37327;&#19979;&#38543;&#26426;&#21521;&#37327;&#30340;&#22343;&#20540;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#20998;&#24067;&#20551;&#35774;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#32479;&#19968;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2311.08168</link><description>&lt;p&gt;
&#38543;&#26426;&#21521;&#37327;&#22343;&#20540;&#30340;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#29699;
&lt;/p&gt;
&lt;p&gt;
Time-Uniform Confidence Spheres for Means of Random Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#29699;&#24207;&#21015;&#65292;&#21487;&#20197;&#21516;&#26102;&#39640;&#27010;&#29575;&#22320;&#21253;&#21547;&#21508;&#31181;&#26679;&#26412;&#37327;&#19979;&#38543;&#26426;&#21521;&#37327;&#30340;&#22343;&#20540;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#20998;&#24067;&#20551;&#35774;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#32479;&#19968;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#24182;&#30740;&#31350;&#20102;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#29699;&#8212;&#8212;&#21253;&#21547;&#38543;&#26426;&#21521;&#37327;&#22343;&#20540;&#24182;&#19988;&#36328;&#36234;&#25152;&#26377;&#26679;&#26412;&#37327;&#20855;&#26377;&#24456;&#39640;&#27010;&#29575;&#30340;&#32622;&#20449;&#29699;&#24207;&#21015;&#65288;CSSs&#65289;&#12290;&#21463;Catoni&#21644;Giulini&#21407;&#22987;&#24037;&#20316;&#21551;&#21457;&#65292;&#25105;&#20204;&#32479;&#19968;&#24182;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#39034;&#24207;&#35774;&#32622;&#24182;&#22788;&#29702;&#21508;&#31181;&#20998;&#24067;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21253;&#25324;&#26377;&#30028;&#38543;&#26426;&#21521;&#37327;&#30340;&#32463;&#39564;&#20271;&#24681;&#26031;&#22374;CSS&#65288;&#23548;&#33268;&#26032;&#39062;&#30340;&#32463;&#39564;&#20271;&#24681;&#26031;&#22374;&#32622;&#20449;&#21306;&#38388;&#65292;&#28176;&#36817;&#23485;&#24230;&#25353;&#29031;&#30495;&#23454;&#26410;&#30693;&#26041;&#24046;&#25104;&#27604;&#20363;&#32553;&#25918;&#65289;&#12289;&#29992;&#20110;&#23376;-$\psi$&#38543;&#26426;&#21521;&#37327;&#30340;CSS&#65288;&#21253;&#25324;&#23376;&#20285;&#39532;&#12289;&#23376;&#27850;&#26494;&#21644;&#23376;&#25351;&#25968;&#20998;&#24067;&#65289;&#12289;&#21644;&#29992;&#20110;&#37325;&#23614;&#38543;&#26426;&#21521;&#37327;&#65288;&#20165;&#26377;&#20004;&#38454;&#30697;&#65289;&#30340;CSS&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#25269;&#25239;Huber&#22122;&#22768;&#27745;&#26579;&#30340;CSS&#12290;&#31532;&#19968;&#20010;&#26159;&#25105;&#20204;&#32463;&#39564;&#20271;&#24681;&#26031;&#22374;CSS&#30340;&#40065;&#26834;&#29256;&#26412;&#65292;&#31532;&#20108;&#20010;&#25193;&#23637;&#20102;&#21333;&#21464;&#37327;&#24207;&#21015;&#26368;&#36817;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08168v2 Announce Type: replace-cross  Abstract: We derive and study time-uniform confidence spheres -- confidence sphere sequences (CSSs) -- which contain the mean of random vectors with high probability simultaneously across all sample sizes. Inspired by the original work of Catoni and Giulini, we unify and extend their analysis to cover both the sequential setting and to handle a variety of distributional assumptions. Our results include an empirical-Bernstein CSS for bounded random vectors (resulting in a novel empirical-Bernstein confidence interval with asymptotic width scaling proportionally to the true unknown variance), CSSs for sub-$\psi$ random vectors (which includes sub-gamma, sub-Poisson, and sub-exponential), and CSSs for heavy-tailed random vectors (two moments only). Finally, we provide two CSSs that are robust to contamination by Huber noise. The first is a robust version of our empirical-Bernstein CSS, and the second extends recent work in the univariate se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38543;&#26426;&#30697;&#38453;&#35745;&#31639;&#30340;&#39640;&#25928;&#35823;&#24046;&#21644;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#24182;&#25351;&#23548;&#31639;&#27861;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2207.06342</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#30697;&#38453;&#35745;&#31639;&#30340;&#39640;&#25928;&#35823;&#24046;&#21644;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient error and variance estimation for randomized matrix computations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#38543;&#26426;&#30697;&#38453;&#35745;&#31639;&#30340;&#39640;&#25928;&#35823;&#24046;&#21644;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#24182;&#25351;&#23548;&#31639;&#27861;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#30697;&#38453;&#31639;&#27861;&#24050;&#25104;&#20026;&#31185;&#23398;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23433;&#20840;&#22320;&#22312;&#24212;&#29992;&#20013;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#65292;&#38656;&#35201;&#32467;&#21512;&#21518;&#39564;&#35823;&#24046;&#20272;&#35745;&#26469;&#35780;&#20272;&#36755;&#20986;&#30340;&#36136;&#37327;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35786;&#26029;&#26041;&#27861;&#65306;&#29992;&#20110;&#38543;&#26426;&#20302;&#31209;&#36924;&#36817;&#30340;&#30041;&#19968;&#27861;&#35823;&#24046;&#20272;&#35745;&#22120;&#21644;&#19968;&#31181;&#26480;&#22522;&#20992;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26426;&#30697;&#38453;&#35745;&#31639;&#30340;&#36755;&#20986;&#26041;&#24046;&#12290;&#36825;&#20004;&#31181;&#35786;&#26029;&#26041;&#27861;&#23545;&#20110;&#38543;&#26426;&#20302;&#31209;&#36924;&#36817;&#31639;&#27861;&#65288;&#22914;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#21644;&#38543;&#26426;Nystrom&#36924;&#36817;&#65289;&#35745;&#31639;&#36805;&#36895;&#65292;&#24182;&#25552;&#20379;&#21487;&#29992;&#20110;&#35780;&#20272;&#35745;&#31639;&#36755;&#20986;&#36136;&#37327;&#21644;&#25351;&#23548;&#31639;&#27861;&#21442;&#25968;&#36873;&#25321;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.06342v4 Announce Type: replace-cross  Abstract: Randomized matrix algorithms have become workhorse tools in scientific computing and machine learning. To use these algorithms safely in applications, they should be coupled with posterior error estimates to assess the quality of the output. To meet this need, this paper proposes two diagnostics: a leave-one-out error estimator for randomized low-rank approximations and a jackknife resampling method to estimate the variance of the output of a randomized matrix computation. Both of these diagnostics are rapid to compute for randomized low-rank approximation algorithms such as the randomized SVD and randomized Nystr\"om approximation, and they provide useful information that can be used to assess the quality of the computed output and guide algorithmic parameter choices.
&lt;/p&gt;</description></item><item><title>&#23558;$n$&#20010;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21521;&#37327;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#34892;&#24615;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01181</link><description>&lt;p&gt;
&#23558;&#22823;&#37327;&#38543;&#26426;&#28857;&#25311;&#21512;&#25104;&#26925;&#29699;&#20307;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fitting an ellipsoid to a quadratic number of random points. (arXiv:2307.01181v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01181
&lt;/p&gt;
&lt;p&gt;
&#23558;$n$&#20010;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21521;&#37327;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#34892;&#24615;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24403;$n, d \to \infty $&#26102;&#65292;&#23558;$n$&#20010;&#26631;&#20934;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#30340;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#29468;&#27979;&#20855;&#26377;&#23574;&#38160;&#30340;&#21487;&#34892;&#24615;&#36716;&#21464;&#65306;&#23545;&#20110;&#20219;&#24847;$\varepsilon &gt; 0$&#65292;&#22914;&#26524;$n \leq (1 - \varepsilon) d^2 / 4$&#65292;&#37027;&#20040;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26377;&#35299;&#65307;&#32780;&#22914;&#26524;$n \geq (1 + \varepsilon) d^2 /4$&#65292;&#37027;&#20040;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26080;&#35299;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#36127;&#38754;&#24773;&#20917;&#65292;&#21482;&#30693;&#36947;$n \geq d^2 / 2$&#26159;&#24179;&#20961;&#30340;&#19968;&#20010;&#19978;&#30028;&#65292;&#32780;&#23545;&#20110;&#27491;&#38754;&#24773;&#20917;&#65292;&#24050;&#30693;&#30340;&#26368;&#22909;&#32467;&#26524;&#26159;&#20551;&#35774;$n \leq d^2 / \mathrm{polylog}(d)$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;Bartl&#21644;Mendelson&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#30340;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26159;&#21487;&#34892;&#30340;&#65292;&#20854;&#20013;$C&gt; 0$&#26159;&#19968;&#20010;&#65288;&#21487;&#33021;&#24456;&#22823;&#30340;&#65289;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as $n, d \to \infty$. This problem is conjectured to have a sharp feasibility transition: for any $\varepsilon &gt; 0$, if $n \leq (1 - \varepsilon) d^2 / 4$ then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$ has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while the best results on the positive side assume $n \leq d^2 / \mathrm{polylog}(d)$. In this work, we improve over previous approaches using a key result of Bartl &amp; Mendelson on the concentration of Gram matrices of random vectors under mild assumptions on their tail behavior. This allows us to give a simple proof that $(\mathrm{P})$ is feasible with high probability when $n \leq d^2 / C$, for a (possibly large) constant $C &gt; 0$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#26816;&#27979;&#38382;&#39064;&#24314;&#31435;&#20102;&#26497;&#23567;&#26497;&#22823;&#20998;&#31163;&#36895;&#29575;&#65292;&#25581;&#31034;&#20102;&#31232;&#30095;&#24615;&#21644;&#20989;&#25968;&#31354;&#38388;&#36873;&#25321;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#23545;&#31232;&#30095;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#21644;&#20854;&#22312;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;Sobolev&#31354;&#38388;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23545;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09398</link><description>&lt;p&gt;
&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#26497;&#23567;&#26497;&#22823;&#20449;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Minimax Signal Detection in Sparse Additive Models. (arXiv:2304.09398v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#20013;&#30340;&#20449;&#21495;&#26816;&#27979;&#38382;&#39064;&#24314;&#31435;&#20102;&#26497;&#23567;&#26497;&#22823;&#20998;&#31163;&#36895;&#29575;&#65292;&#25581;&#31034;&#20102;&#31232;&#30095;&#24615;&#21644;&#20989;&#25968;&#31354;&#38388;&#36873;&#25321;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#23545;&#31232;&#30095;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#21644;&#20854;&#22312;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;Sobolev&#31354;&#38388;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23545;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#30340;&#24314;&#27169;&#38656;&#27714;&#20013;&#65292;&#31232;&#30095;&#21152;&#24615;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20449;&#21495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#31232;&#30095;&#21152;&#24615;&#20449;&#21495;&#26816;&#27979;&#30340;&#26497;&#23567;&#26497;&#22823;&#20998;&#31163;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#38750;&#28176;&#36817;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#21333;&#21464;&#37327;&#20998;&#37327;&#20989;&#25968;&#23646;&#20110;&#19968;&#33324;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#19982;&#20272;&#35745;&#29702;&#35770;&#19981;&#21516;&#65292;&#26497;&#23567;&#26497;&#22823;&#20998;&#31163;&#36895;&#29575;&#25581;&#31034;&#20102;&#31232;&#30095;&#24615;&#21644;&#20989;&#25968;&#31354;&#38388;&#36873;&#25321;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23545;&#31232;&#30095;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#33258;&#36866;&#24212;&#27979;&#35797;&#36895;&#29575;&#65307;&#22312;&#26576;&#20123;&#31354;&#38388;&#20013;&#65292;&#33258;&#36866;&#24212;&#24615;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#31354;&#38388;&#20013;&#21017;&#20250;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#20195;&#20215;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;Sobolev&#31354;&#38388;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#23545;&#31232;&#30095;&#24615;&#21644;&#24179;&#28369;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#26356;&#27491;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#35828;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse additive models are an attractive choice in circumstances calling for modelling flexibility in the face of high dimensionality. We study the signal detection problem and establish the minimax separation rate for the detection of a sparse additive signal. Our result is nonasymptotic and applicable to the general case where the univariate component functions belong to a generic reproducing kernel Hilbert space. Unlike the estimation theory, the minimax separation rate reveals a nontrivial interaction between sparsity and the choice of function space. We also investigate adaptation to sparsity and establish an adaptive testing rate for a generic function space; adaptation is possible in some spaces while others impose an unavoidable cost. Finally, adaptation to both sparsity and smoothness is studied in the setting of Sobolev space, and we correct some existing claims in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;MCMC&#31639;&#27861;IIT&#21450;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#35813;&#31639;&#27861;&#22987;&#32456;&#25509;&#21463;&#26377;&#20449;&#24687;&#30340;&#25552;&#35758;&#65292;&#21487;&#19982;&#20854;&#20182;MCMC&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#20248;&#21270;&#25277;&#26679;&#22120;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2304.06251</link><description>&lt;p&gt;
&#23454;&#29992;&#25351;&#21335;&#65306;&#20851;&#20110;&#30693;&#24773;&#37325;&#35201;&#24615;&#35843;&#33410;&#26041;&#27861;&#30340;&#35814;&#32454;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Importance is Important: A Guide to Informed Importance Tempering Methods. (arXiv:2304.06251v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;MCMC&#31639;&#27861;IIT&#21450;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#35813;&#31639;&#27861;&#22987;&#32456;&#25509;&#21463;&#26377;&#20449;&#24687;&#30340;&#25552;&#35758;&#65292;&#21487;&#19982;&#20854;&#20182;MCMC&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#20248;&#21270;&#25277;&#26679;&#22120;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#24773;&#37325;&#35201;&#24615;&#35843;&#33410; (IIT) &#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#30340;MCMC&#31639;&#27861;&#65292;&#21487;&#35270;&#20026;&#36890;&#24120;&#30340;Metropolis-Hastings&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#22987;&#32456;&#25509;&#21463;&#26377;&#20449;&#24687;&#30340;&#25552;&#35758;&#30340;&#29305;&#27530;&#21151;&#33021;&#65292;&#22312;Zhou&#21644;Smith&#65288;2022&#24180;&#65289;&#30340;&#30740;&#31350;&#20013;&#34920;&#26126;&#22312;&#19968;&#20123;&#24120;&#35265;&#24773;&#20917;&#19979;&#25910;&#25947;&#26356;&#24555;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#20840;&#38754;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;IIT&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;IIT&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#31163;&#25955;&#31354;&#38388;&#19978;&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#30693;&#24773;MCMC&#26041;&#27861;&#26356;&#24555;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#35745;&#31639;&#25152;&#26377;&#30456;&#37051;&#29366;&#24577;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;IIT&#19982;&#20854;&#20182;MCMC&#25216;&#26415;&#65288;&#21253;&#25324;&#27169;&#25311;&#22238;&#28779;&#12289;&#20266;&#36793;&#32536;&#21644;&#22810;&#37325;&#23581;&#35797;&#26041;&#27861;&#65292;&#22312;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;&#19978;&#23454;&#26045;&#20026;Metropolis-Hastings&#26041;&#26696;&#65292;&#21487;&#33021;&#36973;&#21463;&#20302;&#25509;&#21463;&#29575;&#30340;&#38382;&#39064;&#65289;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;&#20351;&#29992;IIT&#20351;&#25105;&#20204;&#33021;&#22815;&#22987;&#32456;&#25509;&#21463;&#25552;&#35758;&#65292;&#24182;&#24102;&#26469;&#20102;&#20248;&#21270;&#25277;&#26679;&#22120;&#30340;&#26032;&#26426;&#20250;&#65292;&#36825;&#26159;&#22312;Metropolis-Hastings&#31639;&#27861;&#19979;&#19981;&#21487;&#33021;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20197;&#36873;&#25321;IIT&#26041;&#26696;&#21644;&#35843;&#25972;&#31639;&#27861;&#21442;&#25968;&#12290;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informed importance tempering (IIT) is an easy-to-implement MCMC algorithm that can be seen as an extension of the familiar Metropolis-Hastings algorithm with the special feature that informed proposals are always accepted, and which was shown in Zhou and Smith (2022) to converge much more quickly in some common circumstances. This work develops a new, comprehensive guide to the use of IIT in many situations. First, we propose two IIT schemes that run faster than existing informed MCMC methods on discrete spaces by not requiring the posterior evaluation of all neighboring states. Second, we integrate IIT with other MCMC techniques, including simulated tempering, pseudo-marginal and multiple-try methods (on general state spaces), which have been conventionally implemented as Metropolis-Hastings schemes and can suffer from low acceptance rates. The use of IIT allows us to always accept proposals and brings about new opportunities for optimizing the sampler which are not possible under th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.06140</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#65306;&#26032;&#30340;&#38544;&#31169;&#20998;&#26512;&#19982;&#25512;&#26029;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies. (arXiv:2210.06140v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#65292;&#20294;&#22312;&#24212;&#29992;&#20013;&#65292;&#32479;&#35745;&#25512;&#26029;&#20173;&#28982;&#32570;&#20047;&#36890;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#24067;&#22810;&#20010;&#31169;&#26377;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#26469;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#38544;&#31169;&#20998;&#26512;&#25552;&#20379;&#20102;&#21333;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#38544;&#31169;&#25104;&#26412;&#26032;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#24341;&#23548;&#37319;&#26679;&#30340;&#19968;&#20123;&#35823;&#29992;&#12290;&#20351;&#29992;Gaussian-DP&#65288;GDP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20174;&#28385;&#36275; $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP &#30340;&#26426;&#21046;&#20013;&#37322;&#25918; $B$ &#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#65292;&#22312; $B$ &#36235;&#36817;&#26080;&#38480;&#22823;&#26102;&#28176;&#36817;&#22320;&#28385;&#36275; $\mu$-GDP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#21453;&#21367;&#31215;&#23545;&#26679;&#26412;&#20998;&#24067;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) mechanisms protect individual-level information by introducing randomness into the statistical analysis procedure. Despite the availability of numerous DP tools, there remains a lack of general techniques for conducting statistical inference under DP. We examine a DP bootstrap procedure that releases multiple private bootstrap estimates to infer the sampling distribution and construct confidence intervals (CIs). Our privacy analysis presents new results on the privacy cost of a single DP bootstrap estimate, applicable to any DP mechanisms, and identifies some misapplications of the bootstrap in the existing literature. Using the Gaussian-DP (GDP) framework (Dong et al.,2022), we show that the release of $B$ DP bootstrap estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Moreover, we use deconvolution with the DP bootstrap estimates to accurately infer the sampling distribution
&lt;/p&gt;</description></item></channel></rss>