<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.06672</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#20174;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#21487;&#35777;&#26126;&#30340;&#20114;&#24800;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#25968;&#25454;&#25152;&#26377;&#32773;&#36890;&#36807;&#20174;&#24444;&#27492;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#30410;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26381;&#21153;&#22120;&#21487;&#20197;&#35774;&#35745;&#19968;&#31181;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#21033;&#30340;FL&#21327;&#35758;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#20984;&#38543;&#26426;&#20248;&#21270;&#32972;&#26223;&#19979;&#23384;&#22312;&#30456;&#20114;&#26377;&#21033;&#21327;&#35758;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23545;&#31216;&#38544;&#31169;&#20559;&#22909;&#19979;&#65292;&#26368;&#22823;&#21270;&#24635;&#23458;&#25143;&#25928;&#29992;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26368;&#22823;&#21270;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21327;&#35758;&#65292;&#24182;&#22312;&#21512;&#25104;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06672v1 Announce Type: cross  Abstract: Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07248</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#65306;&#23558;&#32500;&#24230;&#19982;&#20934;&#30830;&#24230;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Depth Separations in Neural Networks: Separating the Dimension from the Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#19968;&#20010;$\mathcal{O}(1)$-Lipschitz&#30446;&#26631;&#20989;&#25968;&#33267;&#24120;&#25968;&#31934;&#24230;&#26102;&#30340;&#25351;&#25968;&#20998;&#31163;&#65292;&#23545;&#20110;&#25903;&#25345;&#22312;$[0,1]^{d}$&#19978;&#30340;&#20998;&#24067;&#65292;&#20551;&#35774;&#26435;&#37325;&#25351;&#25968;&#26377;&#30028;&#12290;&#36825;&#35299;&#20915;&#20102;&#22312;\citet{safran2019depth}&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#30340;&#23384;&#22312;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#65292;&#23558;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#20998;&#31163;&#30340;&#19979;&#30028;&#35201;&#27714;&#33267;&#23569;&#26377;&#19968;&#20010;Lipschitz&#21442;&#25968;&#12289;&#30446;&#26631;&#20934;&#30830;&#24230;&#25110;&#36924;&#36817;&#22495;&#30340;&#22823;&#23567;&#65288;&#26576;&#31181;&#24230;&#37327;&#65289;&#19982;&#36755;&#20837;&#32500;&#24230;&#22810;&#39033;&#24335;&#22320;&#32553;&#25918;&#65292;&#32780;&#25105;&#20204;&#20445;&#25345;&#21069;&#20004;&#32773;&#19981;&#21464;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22495;&#38480;&#21046;&#22312;&#21333;&#20301;&#36229;&#31435;&#26041;&#20307;&#19978;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#36866;&#29992;&#20110;&#21508;&#31181;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#24179;&#22343;&#24773;&#20917;&#21040;&#26368;&#22351;&#24773;&#20917;&#30340;&#38543;&#26426;&#33258;&#32422;&#21270;&#35770;&#35777;&#30340;&#24212;&#29992;&#65292;&#20197;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07193</link><description>&lt;p&gt;
&#26799;&#24230;&#22122;&#22768;&#30340;&#38544;&#24615;&#20559;&#35265;&#65306;&#20174;&#23545;&#31216;&#24615;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Gradient Noise: A Symmetry Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#26102;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35828;&#26126;&#20102;SGD&#21644;&#26799;&#24230;&#19979;&#38477;&#20043;&#38388;&#30340;&#20998;&#27495;&#26159;&#22810;&#20040;&#24040;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#23545;&#31216;&#24615;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#24433;&#21709;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19968;&#26063;&#23545;&#31216;&#24615;&#20998;&#20026;&#20004;&#31867;&#12290;&#23545;&#20110;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#33258;&#28982;&#22320;&#25910;&#25947;&#21040;&#20855;&#26377;&#24179;&#34913;&#21644;&#23545;&#40784;&#26799;&#24230;&#22122;&#22768;&#30340;&#35299;&#12290;&#23545;&#20110;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#28982;&#36866;&#29992;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#26222;&#36941;&#30340;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#32780;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#32454;&#33410;&#26080;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#23545;&#20110;&#36880;&#27493;&#21464;&#24418;&#21644;&#24179;&#22374;&#21270;&#25552;&#20379;&#20102;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#34920;&#31034;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;</title><link>http://arxiv.org/abs/2401.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#37327;&#23376;&#22330;&#35770;&#22810;&#26684;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#37319;&#26679;&#31163;&#25955;&#22330;&#37197;&#32622;$\phi$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$S$&#26159;&#26576;&#20010;&#37327;&#23376;&#22330;&#35770;&#36830;&#32493;&#27431;&#20960;&#37324;&#24471;&#20316;&#29992;$\mathcal S$&#30340;&#26684;&#28857;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#23558;&#35813;&#23494;&#24230;&#36817;&#20284;&#35270;&#20026;&#24213;&#23618;&#20989;&#25968;&#23494;&#24230;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#30340;&#23398;&#20064;&#31639;&#23376;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;$\mathcal V_t$&#30340;&#26041;&#27861;&#65292;&#20854;&#26102;&#38388;&#31215;&#20998;&#25552;&#20379;&#20102;&#33258;&#30001;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$&#30340;&#20989;&#25968;&#20998;&#24067;&#19982;&#30446;&#26631;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#24403;&#36873;&#25321;&#29305;&#23450;&#30340;&#26684;&#28857;&#26102;&#65292;&#31639;&#23376;$\mathcal V_t$&#21487;&#20197;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#32500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30690;&#37327;&#22330;$V_t$&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#26684;&#28857;&#19978;&#23454;&#29616;&#20102;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01968</link><description>&lt;p&gt;
&#26465;&#20214;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Model for Conditional Reservoir Facies Generation. (arXiv:2311.01968v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27833;&#27668;&#39046;&#22495;&#30340;&#30000;&#22320;&#24320;&#21457;&#21644;&#20648;&#23618;&#31649;&#29702;&#20013;&#65292;&#22522;&#20110;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#21019;&#24314;&#20934;&#30830;&#19988;&#22320;&#36136;&#30495;&#23454;&#30340;&#20648;&#23618;&#30456;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20004;&#28857;&#22320;&#36136;&#32479;&#35745;&#26041;&#27861;&#34429;&#28982;&#22522;&#30784;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#22320;&#36136;&#27169;&#24335;&#12290;&#22810;&#28857;&#32479;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#38543;&#30528;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#20852;&#36215;&#21644;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#20154;&#20204;&#24320;&#22987;&#20542;&#21521;&#20110;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30456;&#36739;&#20110;GANs&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#30340;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#65292;&#20005;&#26684;&#20445;&#30041;&#20102;&#26465;&#20214;&#25968;&#25454;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating accurate and geologically realistic reservoir facies based on limited measurements is crucial for field development and reservoir management, especially in the oil and gas sector. Traditional two-point geostatistics, while foundational, often struggle to capture complex geological patterns. Multi-point statistics offers more flexibility, but comes with its own challenges. With the rise of Generative Adversarial Networks (GANs) and their success in various fields, there has been a shift towards using them for facies generation. However, recent advances in the computer vision domain have shown the superiority of diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model is proposed, which is specifically designed for conditional generation of reservoir facies. The proposed model produces high-fidelity facies realizations that rigorously preserve conditioning data. It significantly outperforms a GAN-based alternative.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item></channel></rss>