<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.05288</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#19977;&#20803;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#21464;&#37327;&#20998;&#24067;&#26159;&#27169;&#22411;&#32858;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#28155;&#21152;&#65292;&#20174;&#32780;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#65288;&#22914;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#26368;&#36817;&#30340;&#20986;&#29616;&#65292;&#20851;&#20110;&#30697;&#38453;&#21464;&#37327;&#25968;&#25454;&#30340;&#25991;&#29486;&#26377;&#38480;&#65292;&#23545;&#20110;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#24322;&#24120;&#20540;&#30340;&#25991;&#29486;&#26356;&#23569;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23376;&#38598;&#23545;&#25968;&#20284;&#28982;&#30340;&#20998;&#24067;&#65292;&#23558;OCLUST&#31639;&#27861;&#25193;&#23637;&#21040;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#26041;&#27861;&#26816;&#27979;&#21644;&#21098;&#35009;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.10068</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#26680;&#23545;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29992;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#20989;&#25968;&#36817;&#20284;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32479;&#35745;&#25216;&#26415;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450;&#20854;&#22266;&#26377;&#30340;&#25552;&#20379;&#24378;&#20581;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;GP&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#26680;&#24515;&#26041;&#27861;&#30340;&#22797;&#26434;&#23450;&#21046;&#65292;&#36825;&#24448;&#24448;&#22312;&#20351;&#29992;&#26631;&#20934;&#35774;&#32622;&#21644;&#29616;&#25104;&#36719;&#20214;&#24037;&#20855;&#26102;&#20351;&#20174;&#19994;&#32773;&#19981;&#28385;&#24847;&#12290;&#21487;&#20197;&#35828;&#65292;GP&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#65292;&#23427;&#25198;&#28436;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#35282;&#33394;&#12290;Mat\'ern&#31867;&#30340;&#24179;&#31283;&#26680;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65307;&#20302;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#29616;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#26159;&#20854;&#32467;&#26524;&#12290;&#38750;&#24179;&#31283;&#26680;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#26356;&#21152;&#22797;&#26434;&#30340;&#23646;&#24615;&#65292;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12108</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31995;&#25968;&#37327;&#21270;&#22855;&#24322;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#20855;&#26377;&#22797;&#26434;&#36864;&#21270;&#30340;&#22855;&#24322;&#32479;&#35745;&#27169;&#22411;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#23427;&#22312;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#21270;&#31243;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;DNN&#20013;&#30340;&#36864;&#21270;&#19981;&#33021;&#20165;&#36890;&#36807;&#35745;&#31639;&#8220;&#24179;&#22374;&#8221;&#26041;&#21521;&#30340;&#25968;&#37327;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#29702;&#35770;&#20540;&#30340;&#20302;&#32500;&#27169;&#22411;&#19978;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#24863;&#20852;&#36259;&#21442;&#25968;&#21306;&#22495;&#20043;&#38388;&#36864;&#21270;&#30340;&#39034;&#24207;&#12290;&#23545;MNIST&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#21487;&#20197;&#25581;&#31034;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#26356;&#36864;&#21270;&#25110;&#19981;&#22826;&#36864;&#21270;&#30340;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
&lt;/p&gt;</description></item></channel></rss>