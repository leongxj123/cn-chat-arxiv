<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;</title><link>https://arxiv.org/abs/2403.04082</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#36827;&#34892;&#25512;&#26029;&#65306;&#23545;&#27604;&#34920;&#31034;&#21487;&#35777;&#26126;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#22914;&#20309;&#22238;&#31572;&#35832;&#22914;&#8220;&#26410;&#26469;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#25105;&#20204;&#26159;&#22914;&#20309;&#21040;&#36798;&#36825;&#37324;&#30340;&#65311;&#8221;&#36825;&#31867;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#22312;&#35266;&#27979;&#20540;&#20026;&#39640;&#32500;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#30340;&#32039;&#20945;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#27010;&#29575;&#27604;&#12290;&#36890;&#36807;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#20197;&#34920;&#26126;&#34920;&#31034;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#34920;&#31034;&#30340;&#32852;&#21512;&#20998;&#24067;&#20063;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#19968;&#31181;&#22270;&#24418;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#34920;&#31034;&#36827;&#34892;&#30340;&#25512;&#26029;&#65288;&#20363;&#22914;&#39044;&#27979;&#12289;&#35268;&#21010;&#65289;&#23545;&#24212;&#20110;&#21453;&#28436;&#20302;&#32500;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
&lt;/p&gt;</description></item><item><title>&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00540</link><description>&lt;p&gt;
Epsilon-Greedy Thompson Sampling&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Epsilon-Greedy Thompson Sampling to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00540
&lt;/p&gt;
&lt;p&gt;
&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#65288;TS&#65289;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24320;&#21457;-&#25506;&#32034;&#22256;&#22659;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#34429;&#28982;&#23427;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#21644;&#26368;&#22823;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21518;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#26469;&#20248;&#20808;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;TS&#22312;&#27599;&#27425;&#25191;&#34892;&#25506;&#32034;&#21518;&#36890;&#36807;&#25910;&#38598;&#20851;&#20110;&#30495;&#23454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#26469;&#24369;&#21270;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#26412;&#30740;&#31350;&#23558;&#22312;TS&#20013;&#24341;&#20837;$\varepsilon$-greedy&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;TS&#24212;&#29992;&#20110;BO&#30340;&#20004;&#20010;&#26497;&#31471;&#65292;&#21363;&#36890;&#29992;TS&#21644;&#26679;&#26412;&#24179;&#22343;TS&#12290;&#21069;&#32773;&#21644;&#21518;&#32773;&#20998;&#21035;&#25552;&#20513;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290; &#28982;&#21518;&#25105;&#20204;&#20351;&#29992;$\varepsilon$-greedy&#31574;&#30053;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#38543;&#26426;&#20999;&#25442;&#12290; $\varepsilon \in (0,1)$&#30340;&#23567;&#20540;&#20248;&#20808;&#32771;&#34385;&#24320;&#21457;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290; &#25105;&#20204;&#23454;&#35777;&#34920;&#26126;$\varepsilon$-greedy T
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12711</link><description>&lt;p&gt;
&#20855;&#26377;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;&#20445;&#35777;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#24615;&#33021;&#24230;&#37327;&#65292;&#22914;&#36951;&#25022;&#12289;PAC&#30028;&#38480;&#25110;&#32479;&#19968;PAC(Dann&#31561;&#20154;&#65292;2017)&#65292;&#36890;&#24120;&#35780;&#20272;&#32047;&#31215;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#20219;&#24847;&#26377;&#38480;&#26102;&#38388;t&#20869;&#29609;&#24369;&#21155;&#30340;&#33218;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#25439;&#22833;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#65292;&#25429;&#25417;&#36172;&#21338;&#31639;&#27861;&#30340;&#32047;&#31215;&#21644;&#30636;&#26102;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULI&#34920;&#24449;&#20102;&#30636;&#26102;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#25152;&#29609;&#24369;&#21155;&#33218;&#30340;&#27599;&#36718;&#36951;&#25022;&#21463;&#21040;&#19968;&#20010;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#65288;&#22823;&#65289;&#36718;&#27425;t&#21333;&#35843;&#36882;&#20943;&#65292;&#22312;&#26377;&#36275;&#22815;&#26679;&#26412;&#21487;&#29992;&#26102;&#38450;&#27490;&#37325;&#22797;&#35775;&#38382;&#21155;&#36136;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#24847;&#21619;&#30528;&#22312;&#19978;&#36848;&#24615;&#33021;&#24230;&#37327;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;ULI&#22312;&#26377;&#38480;&#33218;&#38598;&#19978;&#30340;&#21487;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12711v1 Announce Type: new  Abstract: Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm se
&lt;/p&gt;</description></item><item><title>&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07307</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07307
&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20855;&#26377;&#30456;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#37319;&#21462;&#30456;&#21516;&#30340;&#34892;&#21160;&#12290;&#31526;&#21512;&#39044;&#27979;&#24110;&#21161;&#20915;&#31574;&#32773;&#37327;&#21270;&#21160;&#20316;&#30340;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#21463;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#23427;&#20135;&#29983;&#20102;&#26082;&#31526;&#21512;Venn-Abers&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#24341;&#21457;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21518;&#39564;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#22120;&#65292;&#25552;&#20379;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#38388;&#30340;&#25928;&#29575;&#21644;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04952</link><description>&lt;p&gt;
&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#22270;&#65292;&#35813;&#22270;&#32534;&#30721;&#20102;&#29983;&#25104;&#25968;&#25454;&#36807;&#31243;&#30340;&#22240;&#26524;&#22270;&#30340;&#22270;&#24418;&#20998;&#31163;&#21644;&#36830;&#25509;&#38472;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#24212;&#35813;&#21253;&#25324;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#65292;&#20197;&#34913;&#37327;&#36825;&#19968;&#26126;&#30830;&#30446;&#26631;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#20004;&#20010;&#22240;&#26524;&#22270;&#30340;&#20998;&#31163;/&#36830;&#25509;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29609;&#20855;&#31034;&#20363;&#12289;&#23454;&#35777;&#23454;&#39564;&#21644;&#20266;&#20195;&#30721;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process. In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth. We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming. We complement our theoretical analysis with toy examples, empirical experiments and pseudocode.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03256</link><description>&lt;p&gt;
&#23398;&#20064;Predict-then-Optimize&#26694;&#26550;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Best-in-Class Policies for the Predict-then-Optimize Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03256
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#31216;&#20026;Perturbation Gradient&#65288;PG&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#12290;&#36825;&#20123;&#25439;&#22833;&#30452;&#25509;&#36817;&#20284;&#20102;&#19979;&#28216;&#20915;&#31574;&#25439;&#22833;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#25439;&#22833;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#30340;&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#20248;&#21270;&#25105;&#20204;&#30340;&#26367;&#20195;&#25439;&#22833;&#21487;&#20197;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#24471;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#35823;&#35774;&#32622;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#24403;&#22522;&#30784;&#27169;&#22411;&#35823;&#35774;&#32622;&#19988;&#22122;&#22768;&#19981;&#26159;&#20013;&#24515;&#23545;&#31216;&#26102;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#26696;&#12290;&#37492;&#20110;&#22312;&#23454;&#36341;&#20013;&#35823;&#35774;&#32622;&#24456;&#24120;&#35265;--&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#21487;&#33021;&#26356;&#21916;&#27426;&#19968;&#20010;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;--PG&#25439;&#22833;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#29702;&#35770;&#19978;&#26377;&#20381;&#25454;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#20915;&#31574;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2311.08376</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#33218;&#30340;&#38598;&#25104;&#25277;&#26679;&#65306;&#23567;&#38598;&#25104;&#36275;&#30691;
&lt;/p&gt;
&lt;p&gt;
Ensemble sampling for linear bandits: small ensembles suffice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#35774;&#23450;&#19979;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#26377;&#29992;&#19988;&#20005;&#35880;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#20132;&#20114;&#20316;&#29992;&#26102;&#38388;&#36328;&#24230;$T$&#30340;$d$&#32500;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#65292;&#37319;&#29992;&#38598;&#25104;&#22823;&#23567;&#20026;$\smash{d \log T}$&#30340;&#38598;&#25104;&#25277;&#26679;&#65292;&#36973;&#21463;&#30340;&#21518;&#24724;&#26368;&#22810;&#20026;$\smash{(d \log T)^{5/2} \sqrt{T}}$&#38454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#20219;&#20309;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#31532;&#19968;&#20010;&#19981;&#35201;&#27714;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#30340;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#38598;&#25104;&#25277;&#26679;&#22833;&#21435;&#24847;&#20041;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#25509;&#36817;$\smash{\sqrt{T}}$&#38454;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#26080;&#38480;&#21160;&#20316;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#20462;&#27491;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27491;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#24773;&#20917;&#19979;&#20248;&#20110;Bonferroni&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#65292;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2311.10900</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24378;&#22823;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A powerful rank-based correction to multiple testing under positive dependency. (arXiv:2311.10900v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10900
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31209;&#30340;&#22810;&#37325;&#26816;&#39564;&#20462;&#27491;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27491;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#24773;&#20917;&#19979;&#20248;&#20110;Bonferroni&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#65292;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#21487;&#33021;&#30456;&#20851;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#20043;&#38388;&#27491;&#30456;&#20851;&#24615;&#30340;&#23478;&#26063;&#35823;&#24046;&#29575;(FWER)&#25511;&#21046;&#30340;&#26032;&#22411;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#20462;&#27491;&#31639;&#27861;$\texttt{max-rank}$&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27010;&#24565;&#19978;&#24456;&#30452;&#35266;&#65292;&#20381;&#36182;&#20110;&#22312;&#35745;&#31639;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#31209;&#22495;&#20351;&#29992;$\max$&#31639;&#23376;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#32463;&#24120;&#20351;&#29992;&#30340;Bonferroni&#20462;&#27491;&#65292;&#32780;&#22312;&#19981;&#23384;&#22312;&#27491;&#30456;&#20851;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#31561;&#25928;&#12290;&#25105;&#20204;&#30340;&#20248;&#21183;&#38543;&#30528;&#27979;&#35797;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#21516;&#26102;&#22312;&#20445;&#35777;FWER&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#32479;&#35745;&#21151;&#25928;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24182;&#34892;&#32622;&#25442;&#26816;&#39564;&#30340;&#32972;&#26223;&#20013;&#65292;&#36825;&#26159;&#22312;&#25105;&#20204;&#20027;&#35201;&#24212;&#29992;&#30340;&#19968;&#31181;&#22797;&#26434;&#39044;&#27979;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel multiple hypothesis testing correction with family-wise error rate (FWER) control that efficiently exploits positive dependencies between potentially correlated statistical hypothesis tests. Our proposed algorithm $\texttt{max-rank}$ is conceptually straight-forward, relying on the use of a $\max$-operator in the rank domain of computed test statistics. We compare our approach to the frequently employed Bonferroni correction, theoretically and empirically demonstrating its superiority over Bonferroni in the case of existing positive dependency, and its equivalence otherwise. Our advantage over Bonferroni increases as the number of tests rises, and we maintain high statistical power whilst ensuring FWER control. We specifically frame our algorithm in the context of parallel permutation testing, a scenario that arises in our primary application of conformal prediction, a recently popularized approach for quantifying uncertainty in complex predictive settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17638</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Fractional Diffusion Models. (arXiv:2310.17638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;FBM&#65289;&#30340;&#36830;&#32493;&#26102;&#38388;&#26694;&#26550;&#25512;&#24191;&#21040;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#30340;&#36817;&#20284;&#24418;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;FBM&#34920;&#31034;&#20026;&#23478;&#26063;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#25512;&#23548;&#20986;&#36830;&#32493;&#20877;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#36870;&#26102;&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;&#20855;&#26377;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#65288;GFDM&#65289;&#12290;FBM&#30340;&#36203;&#26031;&#29305;&#25351;&#25968;$H \in (0,1)$ &#21487;&#20197;&#25511;&#21046;&#36335;&#24452;&#21464;&#25442;&#20998;&#24067;&#30340;&#31895;&#31961;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the continuous time framework for score-based generative models from an underlying Brownian motion (BM) to an approximation of fractional Brownian motion (FBM). We derive a continuous reparameterization trick and the reverse time model by representing FBM as a stochastic integral over a family of Ornstein-Uhlenbeck processes to define generative fractional diffusion models (GFDM) with driving noise converging to a non-Markovian process of infinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enables control of the roughness of the distribution transforming path. To the best of our knowledge, this is the first attempt to build a generative model upon a stochastic process with infinite quadratic variation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.16510</link><description>&lt;p&gt;
&#22312;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#19981;&#21487;&#34892;&#30830;&#23450;&#24615;&#12289;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#32422;&#26463;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#37117;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#20174;&#20027;&#25104;&#20998;&#20998;&#26512;&#21040;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#26469;&#27714;&#35299;&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26102;&#26368;&#32791;&#36153;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;Ablin&#65286;Peyr\'e&#65288;2022&#65289;&#25552;&#20986;&#20102;Landing&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24265;&#20215;&#36845;&#20195;&#26041;&#27861;&#65292;&#23427;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#65292;&#20294;&#20250;&#20197;&#24179;&#28369;&#30340;&#26041;&#24335;&#21560;&#24341;&#21040;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Landing&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#23454;&#29992;&#21644;&#29702;&#35770;&#21457;&#23637;&#12290;&#39318;&#20808;&#65292;&#35813;&#26041;&#27861;&#34987;&#25193;&#23637;&#21040;&#26031;&#25176;&#33778;&#23572;&#27969;&#24418;&#65292;&#21363;&#30697;&#24418;&#27491;&#20132;&#30697;&#38453;&#30340;&#38598;&#21512;&#12290;&#24403;&#25104;&#26412;&#20989;&#25968;&#26159;&#35768;&#22810;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#23427;&#20204;&#30340;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30456;&#21516;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin &amp; Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.14658</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Russo&#21644;Xu&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35777;&#26126;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#19978;&#30028;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#8220;&#24930;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#30340;&#26399;&#26395;&#25910;&#25947;&#36895;&#24230;&#30340;&#24418;&#24335;&#20026;$O(\sqrt{\lambda/n})$&#65292;&#20854;&#20013;$\lambda$&#26159;&#19968;&#20123;&#20449;&#24687;&#29702;&#35770;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#21495;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#20351;&#29992;&#36825;&#20010;&#30028;&#38480;&#26469;&#24471;&#21040;$O(\lambda/n)$&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36798;&#21040;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;$(\eta,c)$-&#20013;&#24515;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
&lt;/p&gt;</description></item></channel></rss>