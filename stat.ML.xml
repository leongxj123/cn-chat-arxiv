<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04867</link><description>&lt;p&gt;
&#32452;&#38544;&#31169;&#25918;&#22823;&#21644;&#23376;&#25277;&#26679;&#30340;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;(DP)&#20855;&#26377;&#22810;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#22914;&#23545;&#21518;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12289;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#65292;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#30456;&#20114;&#29420;&#31435;&#25512;&#23548;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#22810;&#20010;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#25552;&#20379;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20445;&#35777;&#65292;&#25105;&#20204;&#22312;R&#233;nyi-DP&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#27604;$(\epsilon,\delta)$-DP&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;&#20316;&#20026;&#36825;&#20010;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#19981;&#20165;&#35753;&#25105;&#20204;&#25913;&#36827;&#21644;&#27867;&#21270;&#29616;&#26377;&#30340;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18392</link><description>&lt;p&gt;
&#25581;&#31034;&#20581;&#22766;&#24615;&#22312;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Robustness in Evaluating Causal Inference Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18392
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#23545;&#20010;&#24615;&#21270;&#20915;&#31574;&#21046;&#23450;&#30340;&#38656;&#27714;&#23548;&#33268;&#20154;&#20204;&#23545;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#20132;&#21449;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#21508;&#31181;&#26377;&#25928;&#30340;CATE&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#22120;&#36890;&#24120;&#21463;&#21046;&#20110;&#32570;&#20047;&#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#29992;&#20256;&#32479;&#30340;&#20132;&#21449;&#39564;&#35777;&#31561;&#27169;&#22411;&#36873;&#25321;&#31243;&#24207;&#26469;&#36873;&#25321;&#29702;&#24819;&#30340;CATE&#20272;&#35745;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;CATE&#20272;&#35745;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#25554;&#20540;&#21644;&#20266;&#32467;&#26524;&#24230;&#37327;&#65292;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#30830;&#23450;&#24230;&#37327;&#24418;&#24335;&#21644;&#25311;&#21512;&#24178;&#25200;&#21442;&#25968;&#25110;&#25554;&#20214;&#23398;&#20064;&#32773;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#36873;&#25321;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#29305;&#23450;&#37325;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18392v1 Announce Type: cross  Abstract: The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for 
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#39640;&#32500;&#24773;&#20917;&#19979;&#37325;&#25277;&#26679;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20165;&#24403;$\alpha$&#36275;&#22815;&#22823;&#26102;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#20197;&#21450;&#22312;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#30340;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#39044;&#27979;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.13622</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#27491;&#21017;&#21270;&#22238;&#24402;&#20013;&#23545;&#33258;&#20030;&#21644;&#23376;&#25277;&#26679;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13622
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#39640;&#32500;&#24773;&#20917;&#19979;&#37325;&#25277;&#26679;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20165;&#24403;$\alpha$&#36275;&#22815;&#22823;&#26102;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#20197;&#21450;&#22312;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#30340;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#39044;&#27979;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#34892;&#37325;&#25277;&#26679;&#26041;&#27861;&#65292;&#22914;&#23376;&#25277;&#26679;&#12289;&#33258;&#20030;&#21644;jackknife&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#39640;&#32500;&#30417;&#30563;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#24773;&#22659;&#19979;&#65292;&#20363;&#22914;&#23725;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#26041;&#27861;&#20272;&#35745;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25552;&#20379;&#20102;&#32039;&#33268;&#30340;&#28176;&#36817;&#25551;&#36848;&#65292;&#32771;&#34385;&#21040;&#26679;&#26412;&#25968;&#37327;$n$&#21644;&#21327;&#21464;&#37327;&#32500;&#24230;$d$&#20197;&#21487;&#27604;&#22266;&#23450;&#36895;&#29575;$\alpha\!=\! n/d$&#22686;&#38271;&#30340;&#26497;&#38480;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#37325;&#25277;&#26679;&#26041;&#27861;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36825;&#20123;&#24773;&#20917;&#20856;&#22411;&#30340;&#21452;&#23792;&#34892;&#20026;&#65307;ii&#65289;&#21482;&#26377;&#22312;$\alpha$&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#20204;&#25165;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65288;&#25105;&#20204;&#32473;&#20986;&#25910;&#25947;&#29575;&#65289;&#65307;iii&#65289;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13622v1 Announce Type: cross  Abstract: We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\alpha\!&lt;\!1$ relevant to modern machine learning practice, their predictions are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.07419</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36275;&#20197;&#20174;&#20219;&#20309;&#22240;&#26524;&#25928;&#24212;&#27979;&#24230;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#23384;&#22312;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#38752;&#19988;&#23436;&#22791;&#30340;&#31639;&#27861;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#26174;&#24335;&#35775;&#38382;&#35266;&#27979;&#20998;&#24067;&#19978;&#30340;&#26465;&#20214;&#20284;&#28982;&#65292;&#32780;&#22312;&#39640;&#32500;&#22330;&#26223;&#20013;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#20272;&#35745;&#36825;&#20123;&#20284;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#27169;&#25311;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#22330;&#26223;&#65292;&#20363;&#22914;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#22240;&#26524;&#22270;&#65292;&#25110;&#32773;&#33719;&#24471;&#26465;&#20214;&#24178;&#39044;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20219;&#24847;&#22240;&#26524;&#22270;&#19979;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#37319;&#26679;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on ima
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06160</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Evidential Deep Learning via a Mixture of Dirichlet Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#23398;&#20064;&#39044;&#27979;&#20998;&#24067;&#19978;&#30340;&#20803;&#20998;&#24067;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;Bengs&#31561;&#20154;&#30340;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#26681;&#26412;&#32570;&#38519;&#65306;&#21363;&#20351;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#65292;&#23398;&#20064;&#21040;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#12290;&#36890;&#36807;&#25552;&#20379;&#25991;&#29486;&#20013;&#19968;&#31867;&#24191;&#27867;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36825;&#20010;&#35266;&#23519;&#30340;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;EDL&#26041;&#27861;&#26412;&#36136;&#19978;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#19982;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;&#24046;&#24322;&#24230;&#37327;&#26469;&#35757;&#32451;&#20803;&#20998;&#24067;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#29702;&#35770;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#20854;&#24314;&#27169;&#20026;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#28151;&#21512;&#29289;&#26469;&#23398;&#20064;&#19968;&#33268;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;EDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores a modern predictive uncertainty estimation approach, called evidential deep learning (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their strong empirical performance, recent studies by Bengs et al. identify a fundamental pitfall of the existing methods: the learned epistemic uncertainty may not vanish even in the infinite-sample limit. We corroborate the observation by providing a unifying view of a class of widely used objectives from the literature. Our analysis reveals that the EDL methods essentially train a meta distribution by minimizing a certain divergence measure between the distribution and a sample-size-independent target distribution, resulting in spurious epistemic uncertainty. Grounded in theoretical principles, we propose learning a consistent target distribution by modeling it with a mixture of Dirichlet distributions and lear
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16054</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
Metric Space Magnitude for Evaluating the Diversity of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16054
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#30340;&#22823;&#23567;&#26159;&#19968;&#31181;&#36817;&#26399;&#24314;&#31435;&#30340;&#19981;&#21464;&#24615;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#25552;&#20379;&#31354;&#38388;&#30340;&#8220;&#26377;&#25928;&#22823;&#23567;&#8221;&#30340;&#34913;&#37327;&#65292;&#24182;&#25429;&#25417;&#21040;&#35768;&#22810;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#20869;&#22312;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24418;&#24335;&#21270;&#20102;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#20989;&#25968;&#20043;&#38388;&#30340;&#26032;&#39062;&#19981;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#25968;&#25454;&#25200;&#21160;&#19979;&#20445;&#35777;&#31283;&#23450;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#20005;&#26684;&#30340;&#22810;&#23610;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#23454;&#39564;&#22871;&#20214;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#12289;&#27169;&#24335;&#23849;&#28291;&#26816;&#27979;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. We show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.12032</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#19979;&#20351;&#29992;&#31062;&#20808;GFlowNets&#36827;&#34892;&#28508;&#22312;&#28151;&#28102;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#20851;&#38190;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#31639;&#27861;&#24456;&#33030;&#24369;&#65292;&#21487;&#33021;&#25512;&#26029;&#20986;&#19982;&#19987;&#23478;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#19981;&#20934;&#30830;&#22240;&#26524;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#26102;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#21152;&#37325;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CD&#26041;&#27861;&#24182;&#19981;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#21644;&#25913;&#36827;&#25512;&#26029;&#36807;&#31243;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;CD&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20107;&#21153;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#19987;&#27880;&#20110;&#26500;&#24314;&#26082;&#33021;&#36755;&#20986;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21448;&#33021;&#19982;&#19987;&#23478;&#36827;&#34892;&#20132;&#20114;&#36845;&#20195;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#65292;&#26681;&#25454;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#65288;&#22914;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65289;&#30340;&#20449;&#24565;&#20998;&#24067;&#65292;&#25353;&#27604;&#20363;&#23545;&#65288;&#22240;&#26524;&#65289;&#31062;&#20808;&#22270;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20505;&#36873;&#22270;&#30340;&#22810;&#26679;&#24615;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#65292;&#20197;&#36845;&#20195;&#24615;&#22320;&#25506;&#32034;&#23454;&#39564;&#26469;&#19982;&#19987;&#23478;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expe
&lt;/p&gt;</description></item><item><title>AGNES&#26159;&#19968;&#31181;&#33021;&#22312;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#24456;&#23567;&#65292;&#23427;&#20063;&#33021;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05515</link><description>&lt;p&gt;
&#23454;&#29616;&#21152;&#36895;&#23613;&#31649;&#26799;&#24230;&#38750;&#24120;&#22024;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving acceleration despite very noisy gradients. (arXiv:2302.05515v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05515
&lt;/p&gt;
&lt;p&gt;
AGNES&#26159;&#19968;&#31181;&#33021;&#22312;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#24456;&#23567;&#65292;&#23427;&#20063;&#33021;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Nesterov&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#19968;&#33324;&#21270;&#12290;&#22914;&#26524;&#22122;&#22768;&#30340;&#24378;&#24230;&#19982;&#26799;&#24230;&#30340;&#22823;&#23567;&#25104;&#27604;&#20363;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;AGNES&#65289;&#21487;&#20197;&#35777;&#26126;&#22312;&#20855;&#26377;&#22024;&#26434;&#26799;&#24230;&#20272;&#35745;&#30340;&#24179;&#28369;&#20984;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#21152;&#36895;&#12290;&#22914;&#26524;&#24120;&#25968;&#27604;&#20363;&#36229;&#36807;&#19968;&#65292;Nesterov&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#22312;&#36825;&#31181;&#22122;&#22768;&#27169;&#22411;&#19979;&#19981;&#20250;&#25910;&#25947;&#12290;AGNES&#33021;&#20462;&#22797;&#36825;&#31181;&#19981;&#36275;&#65292;&#24182;&#19988;&#21487;&#20197;&#35777;&#26126;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#65292;&#26080;&#35770;&#26799;&#24230;&#20272;&#35745;&#30340;&#20449;&#22122;&#27604;&#26377;&#22810;&#23567;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26159;&#29992;&#20110;&#36229;&#21442;&#25968;&#36807;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#23567;&#25209;&#37327;&#26799;&#24230;&#30340;&#36866;&#24403;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;AGNES&#22312;CNN&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient. Nesterov's accelerated gradient descent does not converge under this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency and provably achieves an accelerated convergence rate no matter how small the signal to noise ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES outperforms stochastic gradient descent with momentum and Nesterov's method in the training of CNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.03861</link><description>&lt;p&gt;
&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Regularized Learning Over Random Time-Varying Graphs. (arXiv:2206.03861v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#37117;&#36816;&#34892;&#19968;&#20010;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#21019;&#26032;&#39033;&#65288;&#22788;&#29702;&#33258;&#36523;&#26032;&#27979;&#37327;&#20540;&#65289;&#12289;&#20849;&#35782;&#39033;&#65288;&#21152;&#26435;&#24179;&#22343;&#33258;&#36523;&#21450;&#20854;&#37051;&#23621;&#30340;&#20272;&#35745;&#65292;&#24102;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#36890;&#20449;&#22122;&#22768;&#65289;&#21644;&#27491;&#21017;&#21270;&#39033;&#65288;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65289;&#12290;&#19981;&#35201;&#27714;&#22238;&#24402;&#30697;&#38453;&#21644;&#22270;&#28385;&#36275;&#29305;&#27530;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#22914;&#30456;&#20114;&#29420;&#31435;&#12289;&#26102;&#31354;&#29420;&#31435;&#25110;&#24179;&#31283;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#22914;&#26524;&#31639;&#27861;&#22686;&#30410;&#12289;&#22270;&#21644;&#22238;&#24402;&#30697;&#38453;&#20849;&#21516;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#22320;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#22686;&#30410;&#65292;&#35813;&#26465;&#20214;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the decentralized online regularized linear regression algorithm over random time-varying graphs. At each time step, every node runs an online estimation algorithm consisting of an innovation term processing its own new measurement, a consensus term taking a weighted sum of estimations of its own and its neighbors with additive and multiplicative communication noises and a regularization term preventing over-fitting. It is not required that the regression matrices and graphs satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity. We develop the nonnegative supermartingale inequality of the estimation error, and prove that the estimations of all nodes converge to the unknown true parameter vector almost surely if the algorithm gains, graphs and regression matrices jointly satisfy the sample path spatio-temporal persistence of excitation condition. Especially, this condition holds by choosing appropriate algorithm gains 
&lt;/p&gt;</description></item></channel></rss>