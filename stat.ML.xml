<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2202.00769</link><description>&lt;p&gt;
&#20351;&#29992;Sinkhorn&#25955;&#24230;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning by Sinkhorn Divergence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2202.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25104;&#21151;&#39640;&#24230;&#20381;&#36182;&#20110;&#20998;&#24067;&#34920;&#31034;&#21644;&#20998;&#24067;&#25955;&#24230;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sinkhorn&#20998;&#24067;&#24378;&#21270;&#23398;&#20064; (SinkhornDRL)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22238;&#25253;&#20998;&#24067;&#20013;&#23398;&#20064;&#26080;&#38480;&#21046;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20174;&#29702;&#35770;&#19978;&#26469;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SinkhornDRL&#30340;&#25910;&#32553;&#24615;&#36136;&#65292;&#19982;Sinkhorn&#25955;&#24230;&#22312;Wasserstein&#36317;&#31163;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322; (MMD)&#20043;&#38388;&#30340;&#25554;&#20540;&#24615;&#36136;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;Sinkhorn&#25955;&#24230;&#19982;&#24102;&#26377;&#27491;&#21017;&#21270;Moment Matching&#34892;&#20026;&#30340;&#27491;&#21017;&#21270;MMD&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;SinkhornDRL&#30340;&#20248;&#36234;&#24615;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SinkhornDRL&#22312;Atari&#28216;&#25103;&#22871;&#20214;&#19978;&#22987;&#32456;&#34920;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#25110;&#21487;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical success of distributional reinforcement learning~(RL) highly depends on the distribution representation and the choice of distribution divergence. In this paper, we propose \textit{Sinkhorn distributional RL~(SinkhornDRL)} that learns unrestricted statistics from return distributions and leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish the equivalence between Sinkhorn divergence and a regularized MMD with a regularized Moment Matching behavior, contributing to explaining the superiority of SinkhornDRL. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the Atari games suite.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07356</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Gaussian Min-Max Theorem and its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gordon&#30340;&#19968;&#20010;&#33879;&#21517;&#32467;&#26524;&#20801;&#35768;&#27604;&#36739;&#20004;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#23567;&#26368;&#22823;&#34892;&#20026;&#65292;&#22914;&#26524;&#28385;&#36275;&#26576;&#20123;&#19981;&#31561;&#24335;&#26465;&#20214;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#32467;&#26524;&#21253;&#25324;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;GMT&#65289;&#21644;&#20984;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;CGMT&#65289;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#21457;&#29616;&#28385;&#36275;&#36825;&#20123;&#19981;&#31561;&#24335;&#30340;&#20854;&#20182;&#19968;&#23545;&#39640;&#26031;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#23545;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#23450;&#29702;&#23558;&#32463;&#20856;&#30340;GMT&#23450;&#29702;&#21644;CGMT&#23450;&#29702;&#20174;&#22522;&#26412;&#36807;&#31243;&#20013;&#30340;&#24213;&#23618;&#39640;&#26031;&#30697;&#38453;&#20855;&#26377;iid&#34892;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20855;&#26377;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#26032;&#30340;CGMT&#23450;&#29702;&#24212;&#29992;&#20110;&#22810;&#28304;&#39640;&#26031;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#23646;&#20110;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.15448</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems. (arXiv:2310.15448v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#65288;FORMDA&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$&#20197;&#36798;&#21040;$\varepsilon$-&#31283;&#23450;&#28857;&#65292;&#36825;&#22312;&#30446;&#26631;&#20989;&#25968;&#31283;&#23450;&#24615;&#19979;&#23454;&#29616;&#20102;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26368;&#20339;&#24050;&#30693;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$ to obtain an $\varepsilon$-stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#20915;&#20105;&#22842;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.00968</link><description>&lt;p&gt;
&#38543;&#26426;&#24773;&#22659;&#23545;&#20915;&#20105;&#22842;&#20915;&#31574;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits. (arXiv:2310.00968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#20915;&#20105;&#22842;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20915;&#20105;&#22842;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#28041;&#21450;&#21040;&#20559;&#22909;&#21453;&#39304;&#30340;&#20915;&#31574;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#26377;&#20215;&#20540;&#29305;&#24615;&#65292;&#20363;&#22914;&#25490;&#21517;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#34429;&#28982;&#22312;&#23545;&#20915;&#20105;&#22842;&#20013;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#26368;&#23567;&#21270;&#32047;&#35745;&#36951;&#25022;&#65292;&#20294;&#30446;&#21069;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#31354;&#30333;&#65292;&#21363;&#36951;&#25022;&#30028;&#38480;&#26410;&#32771;&#34385;&#21040;&#23545;&#20915;&#25163;&#34920;&#38388;&#25104;&#23545;&#27604;&#36739;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#38382;&#39064;&#30340;&#38590;&#24230;&#26356;&#39640;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#22659;&#23545;&#20915;&#20105;&#22842;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#25163;&#34920;&#30340;&#20108;&#20803;&#23545;&#27604;&#26159;&#30001;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SupLinUCB&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#36825;&#20010;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#19968;&#20010;&#24863;&#30693;&#26041;&#24046;&#36951;&#25022;&#30028;&#38480;$\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$&#65292;&#20854;&#20013;$\sigma_t$&#26159;&#27599;&#36718;&#25104;&#23545;&#27604;&#36739;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.01377</link><description>&lt;p&gt;
&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Function Descent. (arXiv:2305.01377v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21313;&#20998;&#24120;&#35265;&#65292;&#20294;&#26159;&#36873;&#25321;&#27491;&#30830;&#30340;&#27493;&#38271;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#8220;&#36229;&#21442;&#25968;&#35843;&#25972;&#8221;&#12290;&#36825;&#26159;&#22240;&#20026;&#22238;&#28335;&#31243;&#24207;&#22914;Armijo's&#20934;&#21017;&#20381;&#36182;&#20110;&#27599;&#20010;&#27493;&#39588;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20272;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#30001;&#20110;&#20248;&#21270;&#26041;&#26696;&#21487;&#20197;&#29992;Taylor&#36924;&#36817;&#26469;&#35299;&#37322;&#65292;&#25105;&#20204;&#23558;Taylor&#36924;&#36817;&#26367;&#25442;&#20026;&#26465;&#20214;&#26399;&#26395;&#65288;&#26368;&#20339;&#30340;$L^2$&#20272;&#35745;&#65289;&#65292;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#8221;&#65288;RFD&#65289;&#12290; &#22312;Bayesian&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#36731;&#24494;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RFD&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#26159;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25193;&#23637;&#65292;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
&lt;/p&gt;</description></item></channel></rss>