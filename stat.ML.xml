<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12000</link><description>&lt;p&gt;
Vecchia-Laplace&#36817;&#20284;&#27861;&#22312;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#26159;&#28789;&#27963;&#30340;&#27010;&#29575;&#38750;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#12290;Vecchia&#36817;&#20284;&#26159;&#29992;&#20110;&#20811;&#26381;&#22823;&#25968;&#25454;&#35745;&#31639;&#29942;&#39048;&#30340;&#20934;&#30830;&#36817;&#20284;&#26041;&#27861;&#65292;Laplace&#36817;&#20284;&#26159;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#38750;&#39640;&#26031;&#20284;&#28982;&#20989;&#25968;&#30340;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#30452;&#25509;&#27714;&#35299;&#26041;&#27861;&#65288;&#22914;Cholesky&#20998;&#35299;&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Vecchia-Laplace&#36817;&#20284;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#36229;&#32447;&#24615;&#22320;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#19982;Vecchia-Laplace&#36817;&#20284;&#35745;&#31639;&#30456;&#20851;&#30340;&#36816;&#31639;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#26159;&#26368;&#20934;&#30830;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20250;&#21464;&#24471;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;Vecchia-Laplace&#36817;&#20284;&#25512;&#26029;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Cholesky&#30340;&#35745;&#31639;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11375</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interpretable Distribution-Invariant Fairness Measures for Continuous Scores. (arXiv:2308.11375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11375
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24230;&#37327;&#36890;&#24120;&#22312;&#20108;&#20803;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#35780;&#20998;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22522;&#20110;ROC&#30340;&#24230;&#37327;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35780;&#20998;&#30340;&#20998;&#24067;&#65292;&#19981;&#36866;&#29992;&#20110;&#25490;&#21517;&#20219;&#21153;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#25928;&#26524;&#22823;&#23567;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#36830;&#32493;&#35780;&#20998;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#36866;&#29992;&#20110;&#37327;&#21270;&#21644;&#35299;&#37322;&#32676;&#20307;&#24046;&#24322;&#30340;&#24378;&#24230;&#65292;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#26377;&#35780;&#20998;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#30340;&#19981;&#21516;&#26063;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#26126;&#30830;&#65292;&#24182;&#19988;&#21487;&#20197;&#37327;&#21270;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#32780;ROC-based&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-ba
&lt;/p&gt;</description></item><item><title>&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#19988;&#23545;&#31639;&#27861;&#36755;&#20986;&#26377;&#22122;&#22768;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01282</link><description>&lt;p&gt;
&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#20998;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalized mutual information is a biased measure for classification and community detection. (arXiv:2307.01282v1 [cs.SI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01282
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#19988;&#23545;&#31639;&#27861;&#36755;&#20986;&#26377;&#22122;&#22768;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#24402;&#19968;&#20114;&#20449;&#24687;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;&#32858;&#31867;&#21644;&#20998;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#26412;&#25991;&#34920;&#26126;&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#30340;&#32467;&#26524;&#26377;&#20004;&#20010;&#20559;&#20506;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#65307;&#20854;&#27425;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#23545;&#31216;&#24402;&#19968;&#21270;&#24341;&#20837;&#20102;&#23545;&#31639;&#27861;&#36755;&#20986;&#30340;&#22122;&#22768;&#20381;&#36182;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#32570;&#38519;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#20013;&#19968;&#31726;&#23376;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#22823;&#37327;&#25968;&#20540;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#26174;&#31034;&#20256;&#32479;&#20114;&#20449;&#24687;&#20013;&#30340;&#20559;&#20506;&#23545;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#30340;&#32467;&#35770;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalized mutual information is widely used as a similarity measure for evaluating the performance of clustering and classification algorithms. In this paper, we show that results returned by the normalized mutual information are biased for two reasons: first, because they ignore the information content of the contingency table and, second, because their symmetric normalization introduces spurious dependence on algorithm output. We introduce a modified version of the mutual information that remedies both of these shortcomings. As a practical demonstration of the importance of using an unbiased measure, we perform extensive numerical tests on a basket of popular algorithms for network community detection and show that one's conclusions about which algorithm is best are significantly affected by the biases in the traditional mutual information.
&lt;/p&gt;</description></item></channel></rss>