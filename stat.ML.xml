<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#35774;&#32622;&#65292;&#20854;&#20013;$N$&#20010;&#26234;&#33021;&#20307;&#26088;&#22312;&#36890;&#36807;&#24182;&#34892;&#25805;&#20316;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#23450;&#19978;&#34892;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#20256;&#36755;&#21463;&#21040;&#24322;&#27493;&#21644;&#28508;&#22312;&#26080;&#30028;&#26102;&#21464;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#32773;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21448;&#33021;&#33719;&#24471;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DASA&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;DASA&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#20551;&#35774;&#26234;&#33021;&#20307;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#27604;&#65292;DASA&#26159;&#31532;&#19968;&#20010;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;$tmix$&#21644;&#24179;&#22343;&#24310;&#36831;$\tau_{avg}$&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v1 Announce Type: new  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.00736</link><description>&lt;p&gt;
&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#30340;&#24433;&#21709;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25216;&#24039;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#24067;&#36890;&#24120;&#19981;&#21487;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#24314;&#27169;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;Lipschitz&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#32447;&#24615;&#27169;&#22411;&#25110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#27839;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20018;&#34892;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#26032;&#22411;PAC&#36125;&#21494;&#26031;&#30028;&#38480;&#26469;&#30830;&#23450;&#19968;&#20010;&#38543;&#26426;&#20272;&#35745;&#20540;&#12290;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#27169;&#25311;STOU&#36807;&#31243;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
&lt;/p&gt;</description></item></channel></rss>