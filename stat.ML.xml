<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12668</link><description>&lt;p&gt;
&#38543;&#26426;&#21270;&#26082;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#21448;&#21487;&#20197;&#20943;&#23569;&#26041;&#24046;&#65306;&#38543;&#26426;&#26862;&#26519;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12668
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65292;&#39318;&#27425;&#22312;\cite{breiman2001random}&#20013;&#25351;&#20986;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#20284;&#20046;&#27604;&#35013;&#34955;&#27861;&#20943;&#23569;&#20102;&#20559;&#24046;&#12290;&#21463;\cite{mentch2020randomization}&#19968;&#31687;&#26377;&#36259;&#30340;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20316;&#32773;&#35748;&#20026;&#38543;&#26426;&#26862;&#26519;&#20943;&#23569;&#20102;&#26377;&#25928;&#33258;&#30001;&#24230;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#29615;&#22659;&#19979;&#25165;&#33021;&#32988;&#36807;&#35013;&#34955;&#38598;&#25104;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#22914;&#20309;&#33021;&#22815;&#25581;&#31034;&#34987;&#35013;&#34955;&#27861;&#24573;&#35270;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#65292;&#22312;&#23384;&#22312;&#36825;&#31181;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#20559;&#24046;&#36824;&#33021;&#20943;&#23567;&#26041;&#24046;&#65292;&#24182;&#19988;&#24403;&#20449;&#22122;&#27604;&#39640;&#26102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34920;&#29616;&#24840;&#21457;&#22909;&#20110;&#35013;&#34955;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#20026;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#22312;&#21508;&#31181;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;&#38543;&#26426;&#26862;&#26519;&#19982;&#35013;&#34955;&#38598;&#25104;&#22312;&#27599;&#27425;&#20998;&#21106;&#27880;&#20837;&#30340;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#24046;&#24322;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#36824;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12668v1 Announce Type: cross  Abstract: We study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
&#26071;&#24092;&#28216;&#25103;&#65306;&#36890;&#36807;&#26071;&#24092;&#27969;&#24418;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#20027;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21450;&#20854;&#23545;&#27969;&#24418;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#25193;&#23637;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PCA&#21450;&#20854;&#21464;&#31181;&#30340;&#32479;&#19968;&#24418;&#24335;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#30340;&#26694;&#26550;&#65292;&#21363;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#22871;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#20165;&#20801;&#35768;&#20849;&#21516;&#23454;&#29616;&#65292;&#36824;&#20135;&#29983;&#20102;&#26032;&#30340;&#26410;&#26366;&#25506;&#32034;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#21270;&#20256;&#32479;&#30340;PCA&#26041;&#27861;&#24320;&#22987;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26368;&#22823;&#21270;&#26041;&#24046;&#65292;&#35201;&#20040;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#25105;&#20204;&#25193;&#23637;&#36825;&#20123;&#35299;&#37322;&#65292;&#36890;&#36807;&#32771;&#34385;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#65292;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#21644;&#23545;&#20598;&#24418;&#24335;&#30340;PCA&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65288;&#20999;&#32447;PCA&#65289;&#25972;&#21512;&#21040;&#36825;&#20010;&#22522;&#20110;&#26071;&#24092;&#30340;&#26694;&#26550;&#20013;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20005;&#26684;&#20998;&#26512;&#20102;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#36924;&#36817;&#32467;&#26524;&#21644;&#29699;&#35856;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#39564;&#35777;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09605</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks. (arXiv:2308.09605v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#20998;&#26512;&#20102;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#36924;&#36817;&#32467;&#26524;&#21644;&#29699;&#35856;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#39564;&#35777;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#21508;&#31181;&#23454;&#39564;&#35282;&#24230;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#38750;&#24120;&#39640;&#25928;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#34920;&#38754;&#65292;&#21253;&#25324;&#29699;&#38754;&#19978;&#30340;PDEs&#30340;PINN&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;PINNs&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#34920;&#38754;&#25110;&#27969;&#24418;&#19978;&#30340;PINNs&#65292;&#20173;&#28982;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#21644;&#25913;&#36827;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#29699;&#35856;&#20998;&#26512;&#30340;&#26368;&#26032;&#36924;&#36817;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#19982;&#21019;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;PICNN&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20063;&#24471;&#21040;&#20102;&#23454;&#39564;&#30340;&#39564;&#35777;&#21644;&#34917;&#20805;&#12290;&#37492;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.01357</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#22238;&#24402;(PCR)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22266;&#23450;&#35774;&#35745;&#35823;&#24046;&#21464;&#37327;&#22238;&#24402;&#25216;&#26415;&#65292;&#23427;&#26159;&#32447;&#24615;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#35266;&#27979;&#30340;&#21327;&#21464;&#37327;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#25552;&#20379;&#20102;&#22312;&#32447;&#65288;&#27491;&#21017;&#21270;&#65289;PCR&#30340;&#31532;&#19968;&#27425;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#30001;&#20110;&#20998;&#26512;&#22266;&#23450;&#35774;&#35745;&#20013;PCR&#30340;&#35777;&#26126;&#25216;&#26415;&#26080;&#27861;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23558;&#29616;&#20195;&#38789;&#27987;&#24230;&#30340;&#24037;&#20855;&#36866;&#24212;&#21040;&#35823;&#24046;&#21464;&#37327;&#35774;&#32622;&#20013;&#12290;&#20316;&#20026;&#25105;&#20204;&#30028;&#38480;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#38754;&#26495;&#25968;&#25454;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#65292;&#24403;&#24178;&#39044;&#34987;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26694;&#26550;&#30340;&#27867;&#21270;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#24178;&#39044;&#20998;&#37197;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2210.15659</link><description>&lt;p&gt;
&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints. (arXiv:2210.15659v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#26368;&#36817;&#36890;&#36807;&#19968;&#31181;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#25552;&#20986;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#31216;&#20026;ACVI&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#35745;&#31639;&#20854;&#23376;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#19968;&#33324;&#24773;&#20917;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22312;&#20808;&#21069;&#36845;&#20195;&#20013;&#25214;&#21040;&#30340;&#36817;&#20284;&#35299;&#21021;&#22987;&#21270;&#21464;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#24403;&#31639;&#23376;&#20026;$L$-Lipschitz&#19988;&#21333;&#35843;&#26102;&#65292;&#36825;&#31181;&#19981;&#31934;&#30830;&#30340;ACVI&#26041;&#27861;&#30340;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#38388;&#38553;&#20989;&#25968;&#19979;&#38477;&#30340;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{\sqrt{K}})$&#65292;&#21069;&#25552;&#26159;&#38169;&#35823;&#20197;&#36866;&#24403;&#30340;&#36895;&#24230;&#19979;&#38477;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#36890;&#24120;&#36825;&#31181;&#25216;&#26415;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#25910;&#25947;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) recently addressed the open problem of solving Variational Inequalities (VIs) with equality and inequality constraints through a first-order gradient method. However, the proposed primal-dual method called ACVI is applicable when we can compute analytic solutions of its subproblems; thus, the general case remains an open problem. In this paper, we adopt a warm-starting technique where we solve the subproblems approximately at each iteration and initialize the variables with the approximate solution found at the previous iteration. We prove its convergence and show that the gap function of the last iterate of this inexact-ACVI method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone, provided that the errors decrease at appropriate rates. Interestingly, we show that often in numerical experiments, this technique converges faster than its exact counterpart. Furthermore, for the cases when the inequality constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12494</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#21644;&#19968;&#31867;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#20915;&#23450;&#35266;&#23519;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#30446;&#26631;&#31867;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;GLRT&#65289;&#30340;OCC&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24403;&#30446;&#26631;&#31867;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#26102;&#65292;GLRT&#35299;&#20915;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#12290;GLRT&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#12290;&#23427;&#20204;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#38598;&#35757;&#32451;&#20026;&#20004;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26367;&#20195;&#31867;&#20351;&#29992;&#22312;&#30446;&#26631;&#31867;&#25968;&#25454;&#38598;&#30340;&#23450;&#20041;&#22495;&#19978;&#22343;&#21248;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#21040;&#20102;GLRT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#24403;&#26680;&#20989;&#25968;&#30340;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#65288;OCLSSVM&#65289;&#22312;&#25910;&#25947;&#26102;&#34920;&#29616;&#20026;GLRT&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
&lt;/p&gt;</description></item></channel></rss>