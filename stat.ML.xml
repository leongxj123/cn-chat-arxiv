<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;</title><link>https://rss.arxiv.org/abs/2312.12616</link><description>&lt;p&gt;
&#22312;&#32447;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Variational Sequential Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.12616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26159;AI&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#32463;&#20856;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;&#21442;&#25968;&#23398;&#20064;&#25110;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#65292;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#30340;&#28508;&#22312;&#29366;&#24577;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#22312;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;VSMC&#65289;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#31890;&#23376;&#26041;&#27861;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#36125;&#21494;&#26031;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#12290;&#20256;&#32479;&#30340;VSMC&#26041;&#27861;&#22312;&#31163;&#32447;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#36890;&#36807;&#37325;&#22797;&#22788;&#29702;&#32473;&#23450;&#30340;&#25968;&#25454;&#25209;&#27425;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#23558;VSMC&#20195;&#29702;ELBO&#30340;&#26799;&#24230;&#36924;&#36817;&#20998;&#24067;&#21040;&#26102;&#38388;&#19978;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#27969;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#32447;VSMC&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#65292;&#32780;&#19988;&#23436;&#20840;&#23454;&#26102;&#22788;&#29702;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.00849</link><description>&lt;p&gt;
NeuraLUT: &#22312;Boolean&#21512;&#25104;&#20989;&#25968;&#20013;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00849
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21152;&#36895;&#22120;&#24050;&#32463;&#35777;&#26126;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#36164;&#28304;&#20851;&#38190;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#31070;&#32463;&#32593;&#32476;&#20013;&#35745;&#31639;&#23494;&#38598;&#24230;&#26368;&#39640;&#30340;&#25805;&#20316;&#20043;&#19968;&#26159;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#20043;&#38388;&#30340;&#28857;&#31215;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;FPGA&#21152;&#36895;&#24037;&#20316;&#25552;&#20986;&#23558;&#20855;&#26377;&#37327;&#21270;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31070;&#32463;&#20803;&#30452;&#25509;&#26144;&#23556;&#21040;&#26597;&#25214;&#34920;&#65288;LUTs&#65289;&#20197;&#36827;&#34892;&#30828;&#20214;&#23454;&#29616;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#31070;&#32463;&#20803;&#30340;&#36793;&#30028;&#19982;LUTs&#30340;&#36793;&#30028;&#37325;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#25918;&#23485;&#36825;&#20123;&#36793;&#30028;&#65292;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#12290;&#30001;&#20110;&#23376;&#32593;&#32476;&#34987;&#21560;&#25910;&#21040;LUT&#20013;&#65292;&#20998;&#21306;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#20351;&#29992;&#20855;&#26377;&#28014;&#28857;&#31934;&#24230;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#36825;&#20123;&#23618;&#21463;&#30410;&#20110;&#25104;&#20026;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10649</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;Wasserstein Lagrangian&#27969;&#30340;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#30340;&#22522;&#30784;&#20960;&#20309;&#65288;&#21160;&#33021;&#65289;&#21644;&#23494;&#24230;&#36335;&#24452;&#30340;&#27491;&#21017;&#21270;&#65288;&#21183;&#33021;&#65289;&#65292;&#21487;&#20197;&#23545;&#26368;&#20248;&#36755;&#36816;&#30340;&#21160;&#21147;&#23398;&#24418;&#24335;&#36827;&#34892;&#25512;&#24191;&#12290;&#36825;&#20123;&#32452;&#21512;&#20135;&#29983;&#19981;&#21516;&#30340;&#21464;&#20998;&#38382;&#39064;&#65288;Lagrangians&#65289;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22914;Schr&#246;dinger&#26725;&#12289;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#24102;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#26368;&#20248;&#36755;&#36816;&#31561;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#26368;&#20248;&#23494;&#24230;&#36335;&#24452;&#26159;&#26410;&#30693;&#30340;&#65292;&#35299;&#20915;&#36825;&#20123;&#21464;&#20998;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20511;&#21161;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#25311;&#25110;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#65292;&#20063;&#19981;&#38656;&#35201;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.14507</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#23454;&#29616;&#32467;&#26500;&#21270;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35889;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#35889;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65306;&#23427;&#36890;&#36807;&#23545;&#35266;&#27979;&#36827;&#34892;&#36866;&#24403;&#39044;&#22788;&#29702;&#24471;&#21040;&#30340;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;&#23613;&#31649;&#35889;&#20272;&#35745;&#22120;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#32467;&#26500;&#21270;&#65288;&#21363;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#21644;&#21704;&#23572;&#65289;&#35774;&#35745;&#65292;&#30446;&#21069;&#20165;&#26377;&#23545;&#35889;&#20272;&#35745;&#22120;&#30340;&#20005;&#26684;&#24615;&#33021;&#34920;&#24449;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#22522;&#26412;&#26041;&#27861;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#23454;&#38469;&#30340;&#35774;&#35745;&#30697;&#38453;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#24182;&#19988;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25429;&#25417;&#27979;&#37327;&#30340;&#38750;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#30340;&#30456;&#20851;&#39640;&#26031;&#35774;&#35745;&#65292;&#36890;&#36807;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#36827;&#34892;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#35889;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#36825;&#19968;&#32467;&#26524;&#26469;&#30830;&#23450;&#26368;&#20248;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25152;&#38656;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
&lt;/p&gt;</description></item><item><title>VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.10167</link><description>&lt;p&gt;
VITS: &#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10167
&lt;/p&gt;
&lt;p&gt;
VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20256;&#32479;&#30340;TS&#31639;&#27861;&#22312;&#27599;&#36718;&#38656;&#35201;&#20174;&#24403;&#21069;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#24182;&#25552;&#20379;&#25509;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36817;&#20284;&#25216;&#26415;&#35201;&#20040;&#20272;&#35745;&#19981;&#20934;&#30830;&#65288;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65288;MCMC&#26041;&#27861;&#65292;&#38598;&#25104;&#25277;&#26679;...&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#25512;&#29702;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;VITS&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#24182;&#19988;&#23481;&#26131;&#20174;&#20013;&#25277;&#26679;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#26159;TS&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#65292;VITS&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#19982;&#32500;&#24230;&#21644;&#22238;&#21512;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2307.03034</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#30001;&#20110;&#36164;&#28304;&#32422;&#26463;&#25110;&#29615;&#22659;&#25110;&#22266;&#26377;&#22122;&#22768;&#65292;&#29609;&#23478;&#25805;&#20316;&#38656;&#35201;&#22522;&#20110;&#26576;&#31181;&#26377;&#35823;&#24046;&#30340;&#21453;&#39304;&#26426;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#21453;&#39304;/&#35266;&#27979;&#21160;&#21147;&#23398;&#30340;&#19968;&#33324;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20174;&#20219;&#24847;&#21021;&#22987;&#20449;&#24565;&#65288;&#20808;&#39564;&#20449;&#24687;&#65289;&#24320;&#22987;&#30340;&#20855;&#26377;&#21487;&#25968;&#20449;&#24565;&#29366;&#24577;&#31354;&#38388;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#37096;&#20998;&#23432;&#24658;&#23450;&#24459;&#65288;PCL&#65289;&#30340;&#21487;&#23454;&#29616;&#21306;&#22495;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26080;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;&#21487;&#32034;&#24341;&#24615;&#21644;&#20248;&#20808;&#32423;&#32034;&#24341;&#65288;Whittle&#32034;&#24341;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#36807;&#31243;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#24212;&#29992;Ni&#241;o-Mora&#21644;Bertsimas&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;AG&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01237</link><description>&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#20013;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#31639;&#27861;&#24517;&#39035;&#20165;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#20248;&#21270;&#20915;&#31574;&#12290;&#31163;&#32447;&#36172;&#21338;&#20013;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#19988;&#36880;&#28176;&#27969;&#34892;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23454;&#29616;&#20302;&#36125;&#21494;&#26031;&#36951;&#25022;&#24182;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#21033;&#29992;&#39640;&#25928;&#30340;&#38181;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#33719;&#24471;&#20102;&#26356;&#20248;&#30340;&#31163;&#32447;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB&#65288;lower confidence bound&#65289;-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#21512;&#31163;&#32447;&#36172;&#21338;&#20013;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#35774;&#32622;&#65292;&#24182;&#21457;&#23637;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#20197;&#20943;&#23569;&#20195;&#29702;&#20043;&#38388;&#30340;&#38598;&#20307;&#36951;&#25022;&#65292;&#22312;&#25968;&#23398;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18784</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24322;&#26500;&#22810;&#33218;&#32769;&#34382;&#26426;&#32763;&#35793;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits. (arXiv:2305.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#35774;&#32622;&#65292;&#24182;&#21457;&#23637;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#20197;&#20943;&#23569;&#20195;&#29702;&#20043;&#38388;&#30340;&#38598;&#20307;&#36951;&#25022;&#65292;&#22312;&#25968;&#23398;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#30340;&#30740;&#31350;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#35774;&#32622;&#65292;&#20854;&#20013;$N$&#20010;&#26234;&#33021;&#20307;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#27491;&#22312;&#23398;&#20064;$M$&#20010;&#20855;&#26377;&#38543;&#26426;&#24615;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#20197;&#20943;&#23569;&#20182;&#20204;&#30340;&#38598;&#20307;&#32047;&#35745;&#36951;&#25022;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#65292;&#20419;&#36827;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#24182;&#38024;&#23545;&#20004;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#24615;&#33021;&#34920;&#24449;&#12290;&#36890;&#36807;&#25512;&#23548;&#27599;&#20010;&#20195;&#29702;&#30340;&#32047;&#31215;&#36951;&#25022;&#21644;&#38598;&#20307;&#36951;&#25022;&#30340;&#19978;&#38480;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#38598;&#20307;&#36951;&#25022;&#30340;&#19979;&#38480;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item></channel></rss>