<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;</title><link>https://arxiv.org/abs/2402.15478</link><description>&lt;p&gt;
Transformer&#26159;&#34920;&#29616;&#21147;&#24378;&#22823;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#26469;&#35828;&#34920;&#29616;&#21147;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformers are Expressive, But Are They Expressive Enough for Regression?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15478
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#25351;&#30340;&#26159;&#23427;&#33021;&#22815;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#12290;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#23436;&#20840;&#34920;&#29616;&#21147;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#20805;&#24403;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#22312;&#21487;&#38752;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20381;&#36182;&#20110;&#20855;&#26377;&#21487;&#35266;&#21306;&#38388;&#30340;&#20998;&#27573;&#24120;&#25968;&#36924;&#36817;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#8220;Transformer&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#36890;&#36807;&#23454;&#39564;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#8230;&#8230;&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.05442</link><description>&lt;p&gt;
Wasserstein&#22810;&#20803;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#21450;&#20854;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#21253;&#25324;&#19968;&#32452;&#22312;&#23454;&#32447;&#26377;&#30028;&#38388;&#38548;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#22810;&#20010;&#31995;&#21015;&#65292;&#24182;&#19988;&#34987;&#19981;&#21516;&#26102;&#38388;&#30636;&#38388;&#25152;&#32034;&#24341;&#12290;&#27010;&#29575;&#27979;&#24230;&#34987;&#24314;&#27169;&#20026;Wasserstein&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Lebesgue&#27979;&#24230;&#30340;&#20999;&#31354;&#38388;&#20013;&#24314;&#31435;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#39318;&#20808;&#23545;&#25152;&#26377;&#21407;&#22987;&#27979;&#24230;&#36827;&#34892;&#23621;&#20013;&#22788;&#29702;&#65292;&#20197;&#20415;&#23427;&#20204;&#30340;Fr&#233;chet&#24179;&#22343;&#20540;&#25104;&#20026;Lebesgue&#27979;&#24230;&#12290;&#21033;&#29992;&#36845;&#20195;&#38543;&#26426;&#20989;&#25968;&#31995;&#32479;&#30340;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#24179;&#31283;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#22411;&#31995;&#25968;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#38500;&#20102;&#23545;&#27169;&#25311;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27169;&#22411;&#28436;&#31034;&#65306;&#19968;&#20010;&#26159;&#19981;&#21516;&#22269;&#23478;&#24180;&#40836;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#24052;&#40654;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
&lt;/p&gt;</description></item></channel></rss>