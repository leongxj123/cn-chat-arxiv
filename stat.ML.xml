<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01089</link><description>&lt;p&gt;
&#26080;&#20813;&#36153;&#20462;&#21098;&#65306;&#21021;&#22987;&#21270;&#26102;&#21098;&#26525;&#30340;&#20449;&#24687;&#35770;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
No Free Prune: Information-Theoretic Barriers to Pruning at Initialization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24576;&#30097;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25277;&#22870;&#20013;&#22870;&#32773;&#8221;&#26159;&#21542;&#22312;&#21021;&#22987;&#21270;&#26102;&#23384;&#22312;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#20196;&#20154;&#30528;&#36855;&#30340;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#38656;&#35201;&#22823;&#22411;&#27169;&#22411;&#65292;&#25110;&#32773;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;&#21253;&#21547;&#23427;&#20204;&#30340;&#23494;&#38598;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#35782;&#21035;&#21644;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23581;&#35797;&#22312;&#21021;&#22987;&#21270;&#26102;&#25214;&#21040;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;&#8220;&#21021;&#22987;&#21270;&#26102;&#20462;&#21098;&#8221;&#65289;&#30340;&#21162;&#21147;&#22312;&#24191;&#27867;&#19978;&#37117;&#27809;&#26377;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;$p_\text{eff}$&#65292;&#30001;&#26368;&#32456;&#32593;&#32476;&#20013;&#38750;&#38646;&#26435;&#37325;&#30340;&#25968;&#37327;&#21644;&#31232;&#30095;&#25513;&#30721;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30340;&#24635;&#21644;&#32473;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#40065;&#26834;&#24615;&#23450;&#24459;&#8221;&#65288;arXiv:2105.12806&#65289;&#24310;&#20280;&#21040;&#31232;&#30095;&#32593;&#32476;&#65292;&#20854;&#20013;&#24120;&#35268;&#21442;&#25968;&#25968;&#37327;&#34987;$p_\text{eff}$&#25152;&#21462;&#20195;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#33021;&#22815;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#40065;&#26834;&#22320;&#25554;&#20540;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#35757;&#32451;&#21518;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of "lottery tickets" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ("pruning at initialization") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Combinatorial Optimization via Heat Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#28909;&#25193;&#25955;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#12290;&#38024;&#23545;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35775;&#38382;&#35299;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33324;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19968;&#31995;&#21015;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#24191;&#27867;&#36935;&#21040;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
&lt;/p&gt;</description></item><item><title>QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.02290</link><description>&lt;p&gt;
&#29699;&#24418;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#32858;&#31867;&#65306;R&#21644;Python&#20013;&#30340;QuadratiK&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02290
&lt;/p&gt;
&lt;p&gt;
QuadratiK&#36719;&#20214;&#21253;&#26159;&#19968;&#20010;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25311;&#21512;&#24230;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#29699;&#24418;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QuadratiK&#36719;&#20214;&#21253;&#65292;&#35813;&#36719;&#20214;&#21253;&#21253;&#21547;&#20102;&#21019;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#22312;R&#21644;Python&#20013;&#23454;&#29616;&#65292;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#21644;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#20108;&#27425;&#36317;&#31163;&#30340;&#32858;&#31867;&#25216;&#26415;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#23454;&#29616;&#20102;&#21333;&#26679;&#26412;&#12289;&#21452;&#26679;&#26412;&#21644;k&#26679;&#26412;&#36866;&#24212;&#24230;&#25311;&#21512;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#25968;&#23398;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27010;&#29575;&#20998;&#24067;&#30340;&#25311;&#21512;&#24230;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#25193;&#23637;&#20102;&#21151;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;$d$&#32500;&#29699;&#19978;&#22343;&#21248;&#24615;&#27979;&#35797;&#65292;&#20197;&#21450;&#20174;&#27850;&#26494;&#26680;&#23494;&#24230;&#20013;&#29983;&#25104;&#38543;&#26426;&#26679;&#26412;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#19968;&#31181;&#38024;&#23545;&#29699;&#24418;&#25968;&#25454;&#32780;&#29305;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#29420;&#29305;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#29699;&#38754;&#19978;&#22522;&#20110;&#27850;&#26494;&#26680;&#23494;&#24230;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#36824;&#21253;&#25324;&#20854;&#20182;&#22270;&#24418;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms for generating random samples from Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson-kernel-based densities on the sphere. Alongside this, our software includes additional graphical func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04280</link><description>&lt;p&gt;
&#39044;&#27979;&#21160;&#24577;&#22270;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Predicting the structure of dynamic graphs. (arXiv:2401.04280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#12289;&#24402;&#32435;&#21644;&#22686;&#37327;&#23398;&#20064;&#26377;&#21161;&#20110;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20174;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#22270;&#32467;&#26500;&#65292;&#20801;&#35768;&#26377;&#26032;&#33410;&#28857;&#65292;&#24182;&#27809;&#26377;&#21463;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#23558;&#20854;&#19982;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#65288;&#19968;&#31181;&#22312;&#29983;&#29289;&#21270;&#23398;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#39044;&#27979;&#22270;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00773</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#23376;&#25277;&#26679;&#38598;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#32479;&#35745;&#21407;&#29702;&#19978;&#26377;&#30452;&#35266;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#20256;&#32479;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#24191;&#27867;&#37319;&#29992;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#21463;&#21040;&#19982;&#26500;&#24314;&#26816;&#27979;&#22120;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#65292;&#19981;&#20165;&#30830;&#20445;&#20102;&#39640;&#25928;&#35745;&#31639;&#65292;&#36824;&#22686;&#24378;&#20102;&#32467;&#26524;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#19968;&#33324;&#36866;&#29992;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.20498</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#36830;&#32493;&#25968;&#25454;&#30340;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Learning of Continuous Data by Tensor Networks. (arXiv:2310.20498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20498
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#19968;&#33324;&#36866;&#29992;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#38500;&#20102;&#29992;&#20110;&#24314;&#27169;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#22806;&#65292;&#36824;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#30417;&#30563;&#29983;&#25104;&#23398;&#20064;&#20013;&#12290;&#28982;&#32780;&#65292;&#20197;&#37327;&#23376;&#21551;&#21457;&#24335;&#20026;&#29305;&#28857;&#30340;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#20043;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#20108;&#36827;&#21046;&#25110;&#31867;&#21035;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38382;&#39064;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#21253;&#21547;&#36830;&#32493;&#38543;&#26426;&#21464;&#37327;&#30340;&#20998;&#24067;&#30340;&#26032;&#22411;&#24352;&#37327;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#23616;&#38480;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#30697;&#38453;&#31215;&#24577;&#30340;&#35774;&#32622;&#19979;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#27169;&#22411;&#26063;&#33021;&#22815;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#20219;&#20309;&#30456;&#23545;&#24179;&#28369;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#19968;&#33324;&#34920;&#36798;&#24615;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.09149</link><description>&lt;p&gt;
&#24494;&#20998;&#27700;&#24179;&#31354;&#38388;&#20013;&#30340;&#26684;&#28857;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Lattice Approximations in Wasserstein Space. (arXiv:2310.09149v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;Wasserstein&#31354;&#38388;$W_p(\mathbb{R}^d)$&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#26469;&#23545;&#27979;&#24230;&#36827;&#34892;&#32467;&#26500;&#36924;&#36817;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#19968;&#20010;&#28385;&#31209;&#30340;&#26684;&#28857;$\Lambda$&#25353;&#29031;$h\in(0,1]$&#30340;&#27604;&#20363;&#36827;&#34892;&#32553;&#25918;&#65292;&#37027;&#20040;&#22522;&#20110;$h\Lambda$&#30340;Voronoi&#20998;&#21106;&#24471;&#21040;&#30340;&#27979;&#24230;&#36924;&#36817;&#26159;$O(h)$&#65292;&#19981;&#35770;$d$&#25110;$p$&#30340;&#21462;&#20540;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35206;&#30422;&#35770;&#35777;&#35777;&#26126;&#65292;&#23545;&#20110;&#32039;&#25903;&#25745;&#30340;&#27979;&#24230;&#30340;$N$&#39033;&#36924;&#36817;&#26159;$O(N^{-\frac1d})$&#65292;&#36825;&#19982;&#26368;&#20248;&#37327;&#21270;&#22120;&#21644;&#32463;&#39564;&#27979;&#24230;&#36924;&#36817;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24050;&#30693;&#30340;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#65292;&#35201;&#27714;&#20854;&#20855;&#26377;&#36275;&#22815;&#30340;&#34928;&#20943;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider structured approximation of measures in Wasserstein space $W_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ by discrete and piecewise constant measures based on a scaled Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice $\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Finally, we extend these results to noncompactly supported measures with sufficient decay.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17557</link><description>&lt;p&gt;
&#26368;&#21487;&#33021;&#30340;&#20849;&#21516;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
The most likely common cause. (arXiv:2306.17557v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17557
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;A&#21644;B&#30340;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#22312;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24403;&#23427;&#20204;&#30340;&#20849;&#21516;&#21407;&#22240;C&#34987;&#35748;&#20026;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#21482;&#35266;&#27979;&#21040;&#20102;A&#21644;B&#30340;&#32852;&#21512;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;C&#19981;&#33021;&#34987;&#21807;&#19968;&#30830;&#23450;&#65288;&#28508;&#22312;&#28151;&#26434;&#22240;&#23376;&#38382;&#39064;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#20801;&#35768;&#35782;&#21035;&#19982;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#19968;&#33268;&#30340;C&#12290;&#23427;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26465;&#20214;&#27010;&#29575;&#30340;&#38750;&#35299;&#26512;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#12290;&#36825;&#21457;&#29983;&#22312;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#20998;&#24067;&#20174;&#30456;&#20851;&#21040;&#21453;&#30456;&#20851;&#30340;&#36807;&#28193;&#26399;&#38388;&#12290;&#35752;&#35770;&#20102;&#24191;&#20041;&#20284;&#28982;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#39044;&#27979;&#20284;&#28982;&#21644;&#26368;&#23567;&#20849;&#21516;&#21407;&#22240;&#29109;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common cause principle for two random variables $A$ and $B$ is examined in the case of causal insufficiency, when their common cause $C$ is known to exist, but only the joint probability of $A$ and $B$ is observed. As a result, $C$ cannot be uniquely identified (the latent confounder problem). We show that the generalized maximum likelihood method can be applied to this situation and allows identification of $C$ that is consistent with the common cause principle. It closely relates to the maximum entropy principle. Investigation of the two binary symmetric variables reveals a non-analytic behavior of conditional probabilities reminiscent of a second-order phase transition. This occurs during the transition from correlation to anti-correlation in the observed probability distribution. The relation between the generalized likelihood approach and alternative methods, such as predictive likelihood and the minimum common cause entropy, is discussed. The consideration of the common cause
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#36827;&#34892;&#32858;&#31867;&#65306;&#33021;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Clustering with minimum spanning trees: How good can it be?. (arXiv:2303.05679v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#22312;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#20379;&#26041;&#20415;&#30340;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#24182;&#19988;&#35745;&#31639;&#30456;&#23545;&#36739;&#24555;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;MST&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#20013;&#30340;&#24847;&#20041;&#31243;&#24230;&#12290;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#65288;oracle&#65289;&#31639;&#27861;&#19982;&#22823;&#37327;&#22522;&#20934;&#25968;&#25454;&#30340;&#19987;&#23478;&#26631;&#31614;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#19981;&#26159;&#25552;&#20986;&#21478;&#19968;&#20010;&#21482;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#65292;&#32780;&#26159;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#26368;&#26032;MST-based&#21010;&#20998;&#26041;&#26696;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#38750;MST&#31639;&#27861;&#65292;&#22914;k-means&#65292;&#39640;&#26031;&#28151;&#21512;&#65292;&#35889;&#32858;&#31867;&#65292;Birch&#65292;&#22522;&#20110;&#23494;&#24230;&#21644;&#32463;&#20856;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#36824;&#26159;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum spanning trees (MSTs) provide a convenient representation of datasets in numerous pattern recognition activities. Moreover, they are relatively fast to compute. In this paper, we quantify the extent to which they can be meaningful in partitional data clustering tasks in low-dimensional spaces. By identifying the upper bounds for the agreement between the best (oracle) algorithm and the expert labels from a large battery of benchmark data, we discover that MST methods are overall very competitive. Next, instead of proposing yet another algorithm that performs well on a limited set of examples, we review, study, extend, and generalise existing, state-of-the-art MST-based partitioning schemes. This leads to a few new and noteworthy approaches. Overall, Genie and the information-theoretic methods often outperform the non-MST algorithms such as k-means, Gaussian mixtures, spectral clustering, Birch, density-based, and classical hierarchical agglomerative procedures. Nevertheless, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.08403</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#29983;&#25104;&#24207;&#21015;&#65306;&#29702;&#35770;&#21450;&#20854;&#22312;&#26080;&#20154;&#26426;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#21512;&#25104;&#24207;&#21015;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#29983;&#25104;&#26694;&#26550;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#65292;&#36890;&#36807;&#30456;&#20284;&#24615;&#29983;&#25104;&#23376;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#23545;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#27425;&#24615;&#29983;&#25104;&#27169;&#22411;&#26469;&#20174;&#21333;&#20010;&#24207;&#21015;&#30340;&#33539;&#22260;&#20869;&#21462;&#26679;&#65292;&#24182;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#22686;&#24378;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2209.02935</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#32858;&#31867;&#20934;&#30830;&#24230;&#65306;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#19968;&#20010;&#26368;&#22909;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#21306;&#20998;&#20986;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#20869;&#37096;&#25110;&#22806;&#37096;&#26377;&#25928;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#20869;&#37096;&#24230;&#37327;&#37327;&#21270;&#25152;&#24471;&#20998;&#21306;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#65292;&#31751;&#32039;&#23494;&#24230;&#30340;&#24179;&#22343;&#31243;&#24230;&#25110;&#28857;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#20351;&#30340;&#32858;&#31867;&#26377;&#26102;&#21487;&#33021;&#26159;&#26080;&#24847;&#20041;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22806;&#37096;&#24230;&#37327;&#23558;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24120;&#29992;&#30340;&#32463;&#20856;&#20998;&#21306;&#30456;&#20284;&#24615;&#35780;&#20998;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#20114;&#20449;&#24687;&#12289;Fowlkes-Mallows&#25110;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968;&#65292;&#32570;&#23569;&#19968;&#20123;&#21487;&#21462;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#19981;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#22351;&#24773;&#20917;&#65292;&#20063;&#19981;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#30340;&#25193;&#23637;&#65292;&#21487;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#35745;&#31639;&#38750;&#21442;&#25968;&#38750;&#28176;&#36827;&#32479;&#35745;&#25512;&#26029;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#29992;&#20110;&#29983;&#25104;&#25928;&#29575;&#39640;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#24207;&#21015;&#12290;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#24182;&#20135;&#29983;&#31169;&#26377;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2202.08728</link><description>&lt;p&gt;
&#38543;&#26426;&#21709;&#24212;&#31169;&#26377;&#32622;&#20449;&#38598;&#30340;&#38750;&#21442;&#25968;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nonparametric extensions of randomized response for private confidence sets. (arXiv:2202.08728v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#30340;&#25193;&#23637;&#65292;&#21487;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#35745;&#31639;&#38750;&#21442;&#25968;&#38750;&#28176;&#36827;&#32479;&#35745;&#25512;&#26029;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#29992;&#20110;&#29983;&#25104;&#25928;&#29575;&#39640;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#24207;&#21015;&#12290;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#24182;&#20135;&#29983;&#31169;&#26377;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#32422;&#26463;&#19979;&#25191;&#34892;&#38750;&#21442;&#25968;&#12289;&#38750;&#28176;&#36827;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#22343;&#20540;$\mu^\star$&#30340;&#26377;&#30028;&#35266;&#27979;$(X_1,\dots,X_n)$&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#21644;&#26102;&#38388;&#22343;&#21248;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#24403;&#21482;&#26377;&#35775;&#38382;&#31169;&#26377;&#25968;&#25454;$(Z_1,\dots,Z_n)$&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#12289;&#39034;&#24207;&#20132;&#20114;&#30340; Warner &#30340;&#33879;&#21517;&#8220;&#38543;&#26426;&#21709;&#24212;&#8221;&#26426;&#21046;&#30340;&#25512;&#24191;&#65292;&#20026;&#20219;&#24847;&#26377;&#30028;&#38543;&#26426;&#21464;&#37327;&#28385;&#36275; LDP&#65292;&#24182;&#25552;&#20379; CIs &#21644; CSs&#65292;&#29992;&#20110;&#35775;&#38382;&#25152;&#24471;&#31169;&#26377;&#21270;&#30340;&#35266;&#27979;&#20540;&#30340;&#22343;&#20540;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22266;&#23450;&#26102;&#38388;&#21644;&#26102;&#38388;&#22343;&#21248;&#21306;&#22495;&#37117;&#20135;&#29983;&#20102; Hoeffding &#19981;&#31561;&#24335;&#30340;&#31169;&#26377;&#27169;&#25311;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123; Hoeffding  &#31867;&#22411;&#30340; CSs &#25193;&#23637;&#21040;&#25429;&#33719;&#26102;&#38388;&#21464;&#21270;&#65288;&#38750;&#24179;&#31283;&#65289;&#30340;&#22343;&#20540;&#65292;&#26368;&#21518;&#35828;&#26126;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work derives methods for performing nonparametric, nonasymptotic statistical inference for population means under the constraint of local differential privacy (LDP). Given bounded observations $(X_1, \dots, X_n)$ with mean $\mu^\star$ that are privatized into $(Z_1, \dots, Z_n)$, we present confidence intervals (CI) and time-uniform confidence sequences (CS) for $\mu^\star$ when only given access to the privatized data. To achieve this, we introduce a nonparametric and sequentially interactive generalization of Warner's famous ``randomized response'' mechanism, satisfying LDP for arbitrary bounded random variables, and then provide CIs and CSs for their means given access to the resulting privatized observations. For example, our results yield private analogues of Hoeffding's inequality in both fixed-time and time-uniform regimes. We extend these Hoeffding-type CSs to capture time-varying (non-stationary) means, and conclude by illustrating how these methods can be used to conduct
&lt;/p&gt;</description></item></channel></rss>