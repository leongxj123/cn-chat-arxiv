<rss version="2.0"><channel><title>Chat Arxiv stat.AP</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.AP</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitative knowledge retrieval from large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#36741;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#29983;&#25104;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#30340;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#20316;&#20026;&#23450;&#37327;&#20449;&#24687;&#26816;&#32034;&#30340;&#23454;&#29992;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#20316;&#20026;&#23450;&#37327;&#30693;&#35782;&#26816;&#32034;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#24341;&#23548;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#34917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#23558;LLMs&#35270;&#20026;&#31185;&#23398;&#25991;&#29486;&#28508;&#22312;&#31354;&#38388;&#30340;&#25509;&#21475;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#39046;&#22495;&#20013;&#27604;&#36739;&#21709;&#24212;&#19982;&#26356;&#25104;&#29087;&#30340;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#30340;&#24433;&#21709;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using LLMs as 'experts' are discussed.
&lt;/p&gt;</description></item></channel></rss>