<rss version="2.0"><channel><title>Chat Arxiv stat.AP</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.AP</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.15454</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion Detection with Transformers: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#36825;&#20123;&#20803;&#32032;&#20173;&#28982;&#33021;&#22815;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#32780;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#25968;&#25454;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#21464;&#20307;&#30340;Transformer&#23545;Emotion&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#35770;&#25991;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#27604;&#22914;Transformer&#23618;&#30340;&#24494;&#35843;&#12289;&#23618;&#30340;&#21487;&#35757;&#32451;&#24615;&#20197;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#25216;&#26415;&#22914;&#21435;&#38500;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Transformer&#30340;&#20248;&#21183;&#22312;&#20110;&#29702;&#35299;&#25991;&#26412;&#20869;&#30340;&#35821;&#22659;&#20851;&#31995;&#12290;&#20687;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#29992;&#35789;&#36825;&#26679;&#30340;&#20803;&#32032;&#20173;&#28982;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#25110;&#24378;&#35843;&#65292;&#21435;&#38500;&#23427;&#20204;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15454v1 Announce Type: new  Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;</title><link>https://arxiv.org/abs/2403.00957</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#22240;&#21407;&#21017;&#35299;&#20915;&#36763;&#26222;&#26862;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Resolution of Simpson's paradox via the common cause principle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00957
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36763;&#26222;&#26862;&#24726;&#35770;&#26159;&#24314;&#31435;&#20004;&#20010;&#20107;&#20214;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#27010;&#29575;&#20851;&#32852;&#26102;&#30340;&#38556;&#30861;&#65292;&#32473;&#23450;&#31532;&#19977;&#20010;&#65288;&#28508;&#22312;&#30340;&#65289;&#38543;&#26426;&#21464;&#37327;$B$&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#26223;&#26159;&#38543;&#26426;&#21464;&#37327;$A$&#65288;&#27719;&#24635;&#20102;$a_1$&#12289;$a_2$&#21450;&#20854;&#34917;&#38598;&#65289;&#21644;$B$&#26377;&#19968;&#20010;&#21487;&#33021;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;$C$&#12290;&#25110;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#20551;&#35774;$C$&#23558;$A$&#20174;$B$&#20013;&#31579;&#36873;&#20986;&#21435;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#27491;&#30830;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#24212;&#35813;&#36890;&#36807;&#23545;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#26469;&#23450;&#20041;&#12290;&#36825;&#19968;&#35774;&#32622;&#23558;&#21407;&#22987;&#36763;&#26222;&#26862;&#24726;&#35770;&#25512;&#24191;&#20102;&#12290;&#29616;&#22312;&#23427;&#30340;&#20004;&#20010;&#30456;&#20114;&#30683;&#30462;&#30340;&#36873;&#39033;&#31616;&#21333;&#22320;&#25351;&#30340;&#26159;&#20004;&#20010;&#29305;&#23450;&#19988;&#19981;&#21516;&#30340;&#21407;&#22240;$C$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;$B$&#21644;$C$&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;$A$&#26159;&#22235;&#36827;&#21046;&#30340;&#65288;&#23545;&#20110;&#26377;&#25928;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#26469;&#35828;&#26159;&#26368;&#23567;&#19988;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#65292;&#22312;&#20219;&#20309;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#23558;&#24314;&#31435;&#19982;&#22312;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00957v1 Announce Type: cross  Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This set-up generalizes the original Simpson's paradox. Now its two contradicting options simply refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for valid Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of the association between $a_1$ and $a_2$ as the conditioning over $B$ in the original
&lt;/p&gt;</description></item></channel></rss>