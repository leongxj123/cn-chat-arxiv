<rss version="2.0"><channel><title>Chat Arxiv stat.AP</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.AP</description><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12108</link><description>&lt;p&gt;
AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65311;&#19968;&#31181;&#29992;&#20110;&#23454;&#39564;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Does AI help humans make better decisions? A methodological framework for experimental evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12108
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24403;&#20170;&#31038;&#20250;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#24403;&#21033;&#30410;&#39640;&#26114;&#26102;&#65292;&#20154;&#31867;&#20173;&#28982;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#27604;&#21333;&#29420;&#30340;&#20154;&#31867;&#25110;&#21333;&#29420;&#30340;AI&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#39564;&#24615;&#22320;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22522;&#20934;&#28508;&#22312;&#32467;&#26524;&#30340;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#27979;&#37327;&#20915;&#31574;&#32773;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20010;&#35774;&#35745;&#20013;&#65292;&#25552;&#20379;AI&#29983;&#25104;&#30340;&#24314;&#35758;&#22312;&#19981;&#21516;&#26696;&#20363;&#20013;&#34987;&#38543;&#26426;&#20998;&#37197;&#32473;&#26368;&#32456;&#20915;&#31574;&#30340;&#20154;&#31867;&#12290;&#22312;&#36825;&#31181;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27604;&#36739;&#19977;&#31181;&#26367;&#20195;&#20915;&#31574;&#31995;&#32479;&#30340;&#24615;&#33021;--&#20165;&#20154;&#31867;&#12289;&#20154;&#31867;&#19982;AI&#12289;&#20165;AI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12108v1 Announce Type: new  Abstract: The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#39044;&#27979;&#21306;&#38388;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#21512;&#25104;&#23545;&#29031;&#39044;&#27979;&#25110;&#20272;&#35745;&#22312;&#38169;&#20301;&#22788;&#29702;&#37319;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05026</link><description>&lt;p&gt;
&#24102;&#26377;&#38169;&#20301;&#22788;&#29702;&#37319;&#29992;&#30340;&#21512;&#25104;&#23545;&#29031;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption. (arXiv:2210.05026v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#39044;&#27979;&#21306;&#38388;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#21512;&#25104;&#23545;&#29031;&#39044;&#27979;&#25110;&#20272;&#35745;&#22312;&#38169;&#20301;&#22788;&#29702;&#37319;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#29992;&#20110;&#37327;&#21270;&#22312;&#38169;&#20301;&#22788;&#29702;&#37319;&#29992;&#30340;&#24773;&#20917;&#19979;&#22823;&#31867;&#21512;&#25104;&#23545;&#29031;&#39044;&#27979;&#25110;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#31934;&#30830;&#30340;&#38750;&#28176;&#36817;&#35206;&#30422;&#27010;&#29575;&#20445;&#35777;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#38656;&#35201;&#39044;&#27979;&#30340;&#19981;&#21516;&#22240;&#26524;&#37327;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;&#8220;&#22240;&#26524;&#39044;&#27979;&#37327;&#8221;&#65292;&#20801;&#35768;&#22312;&#21487;&#33021;&#19981;&#21516;&#26102;&#21051;&#36827;&#34892;&#22810;&#20010;&#22788;&#29702;&#21333;&#20803;&#30340;&#22788;&#29702;&#37319;&#29992;&#12290;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#20043;&#21069;&#25991;&#29486;&#30340;&#27700;&#24179;&#65292;&#20855;&#20307;&#34920;&#29616;&#22312;&#65306;&#65288;i&#65289;&#35206;&#30422;&#20102;&#38169;&#20301;&#37319;&#29992;&#35774;&#32622;&#20013;&#30340;&#22823;&#31867;&#22240;&#26524;&#39044;&#27979;&#37327;&#65292;&#65288;ii&#65289;&#20801;&#35768;&#20855;&#26377;&#21487;&#33021;&#38750;&#32447;&#24615;&#32422;&#26463;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#65288;iii&#65289;&#25552;&#20986;&#21487;&#25193;&#23637;&#30340;&#40065;&#26834;&#38181;&#20248;&#21270;&#26041;&#27861;&#21644;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#39537;&#21160;&#35843;&#21442;&#36873;&#25321;&#65292;&#65288;iv&#65289;&#25552;&#20379;&#20102;&#22312;&#21518;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#26377;&#25928;&#22343;&#21248;&#25512;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#24212;&#29992;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions or estimators in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call `causal predictands', allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical appl
&lt;/p&gt;</description></item></channel></rss>