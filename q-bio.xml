<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11436</link><description>&lt;p&gt;
&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#22823;&#33041;&#30382;&#23618;V2&#21306;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layerwise complexity-matched learning yields an improved model of cortical area V2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35782;&#21035;&#22797;&#26434;&#35270;&#35273;&#27169;&#24335;&#30340;&#33021;&#21147;&#26159;&#36890;&#36807;&#39034;&#27425;&#21306;&#22495;&#22312;&#33145;&#20391;&#35270;&#35273;&#30382;&#23618;&#20013;&#25191;&#34892;&#30340;&#21464;&#25442;&#25152;&#24418;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#23618;&#27425;&#32467;&#26500;&#30340;&#21518;&#26399;&#31070;&#32463;&#21453;&#24212;&#30340;&#26368;&#20339;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#25163;&#24037;&#35774;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#25110;&#32773;&#19982;&#20248;&#21270;&#32534;&#30721;&#25928;&#29575;&#25110;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#23545;&#21069;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#36739;&#24046;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#29983;&#29289;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#29420;&#31435;&#22320;&#20316;&#29992;&#20110;&#36830;&#32493;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#20102;&#23545;&#23616;&#37096;&#21464;&#24418;&#33258;&#28982;&#22270;&#20687;&#34917;&#19969;&#23545;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#37319;&#26679;&#33258;&#20854;&#20182;&#20301;&#32622;&#30340;&#34917;&#19969;&#26102;&#20351;&#29305;&#24449;&#21435;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11436v2 Announce Type: replace-cross  Abstract: Human ability to recognize complex visual patterns arises through transformations performed by successive areas in the ventral visual cortex. Deep neural networks trained end-to-end for object recognition approach human capabilities, and offer the best descriptions to date of neural responses in the late stages of the hierarchy. But these networks provide a poor account of the early stages, compared to traditional hand-engineered models, or models optimized for coding efficiency or prediction. Moreover, the gradient backpropagation used in end-to-end learning is generally considered to be biologically implausible. Here, we overcome both of these limitations by developing a bottom-up self-supervised training methodology that operates independently on successive layers. Specifically, we maximize feature similarity between pairs of locally-deformed natural image patches, while decorrelating features across patches sampled from oth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11447</link><description>&lt;p&gt;
&#39044;&#27979;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#20013;&#24739;&#32773;&#20381;&#20174;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;(SCIT)&#26159;&#36807;&#25935;&#24615;&#40763;&#28814;&#30340;&#38271;&#25928;&#22240;&#26524;&#27835;&#30103;&#12290;&#22914;&#20309;&#25552;&#39640;&#24739;&#32773;&#23545;&#21464;&#24212;&#21407;&#20813;&#30123;&#27835;&#30103;(AIT)&#30340;&#20381;&#20174;&#24615;&#20197;&#26368;&#22823;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#22312;AIT&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;AIT&#30340;&#31649;&#29702;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#24207;&#21015;&#28508;&#22312;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#27169;&#22411;(SLAC)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;(LSTM)&#65292;&#24182;&#22522;&#20110;&#35780;&#20998;&#21644;&#20381;&#20174;&#24615;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;&#22312;&#25490;&#38500;&#31532;&#19968;&#26102;&#38388;&#27493;&#30340;&#20559;&#20506;&#26679;&#26412;&#21518;&#65292;SLAC&#27169;&#22411;&#30340;&#39044;&#27979;&#20381;&#20174;&#20934;&#30830;&#29575;&#20026;60%-72%&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;66%-84%&#65292;&#26681;&#25454;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;SLAC&#27169;&#22411;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#33539;&#22260;&#22312;0.93&#21040;2.22&#20043;&#38388;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;RMSE&#33539;&#22260;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.04082</link><description>&lt;p&gt;
&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#25913;&#36827;&#20102;&#22270;&#26696;&#25903;&#26550;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved motif-scaffolding with SE(3) flow matching. (arXiv:2401.04082v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35774;&#35745;&#36890;&#24120;&#20174;&#19968;&#20010;&#22270;&#26696;&#30340;&#26399;&#26395;&#21151;&#33021;&#24320;&#22987;&#65292;&#22270;&#26696;&#25903;&#26550;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21151;&#33021;&#24615;&#34507;&#30333;&#36136;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#35774;&#35745;&#21508;&#31181;&#22270;&#26696;&#30340;&#25903;&#26550;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25903;&#26550;&#24448;&#24448;&#32570;&#20047;&#32467;&#26500;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#28287;&#23454;&#39564;&#39564;&#35777;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;FrameFlow&#65292;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#30340;SE(3)&#27969;&#21305;&#37197;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#20004;&#31181;&#20114;&#34917;&#30340;&#26041;&#27861;&#36827;&#34892;&#22270;&#26696;&#25903;&#26550;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#25674;&#38144;&#65292;&#21363;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;FrameFlow&#35757;&#32451;&#20026;&#20197;&#22270;&#26696;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;FrameFlow&#30340;&#26465;&#20214;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#25903;&#26550;&#26500;&#24314;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20135;&#29983;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;2.5&#20493;&#30340;&#25903;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein design often begins with knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a diverse range of motifs. However, the generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow, and requires no additional training. Both approaches achieve an equivalent or higher success rate than previous state-of-the-art methods, with 2.5 times more structurally diverse scaffolds. Code: https://github.com/ mi
&lt;/p&gt;</description></item></channel></rss>