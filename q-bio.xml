<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;</title><link>https://arxiv.org/abs/2403.09193</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#32441;&#29702;&#20559;&#35265;&#36824;&#26159;&#24418;&#29366;&#20559;&#35265;&#65292;&#25105;&#20204;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26684;&#23616;&#65292;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#24212;&#29992;&#65292;&#20174;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21040;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#20877;&#21040;&#35270;&#35273;&#38382;&#31572;&#12290;&#19982;&#32431;&#35270;&#35273;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35775;&#38382;&#35270;&#35273;&#20869;&#23481;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#24341;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#26159;&#21542;&#20063;&#19982;&#20154;&#31867;&#35270;&#35273;&#19968;&#33268; - &#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#26377;&#22810;&#22823;&#31243;&#24230;&#22320;&#37319;&#29992;&#20102;&#20154;&#31867;&#24341;&#23548;&#30340;&#35270;&#35273;&#20559;&#35265;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#21482;&#26159;&#20174;&#32431;&#35270;&#35273;&#27169;&#22411;&#20013;&#32487;&#25215;&#20102;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20559;&#35265;&#26159;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21363;&#23616;&#37096;&#20449;&#24687;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;VLMs&#20013;&#30340;&#36825;&#31181;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#20110;&#24418;&#29366;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
&lt;/p&gt;</description></item></channel></rss>