<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.00386</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#65292;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#29983;&#29289;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20195;&#29702;&#24207;&#21015;-&#21151;&#33021;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#30340;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21463;&#21040;&#21478;&#22806;&#19968;&#20010;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32467;&#26500;&#29305;&#24449;&#30340;&#22686;&#24378;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;MCEM&#65289;&#26469;&#23398;&#20064;&#36825;&#20010;&#27169;&#22411;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20854;MRF&#29305;&#24449;&#21017;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#22312;&#20843;&#39033;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#39640;55&#65285;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item></channel></rss>