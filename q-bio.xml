<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00791</link><description>&lt;p&gt;
$\textit{L+M-24}$&#65306;&#22312;ACL 2024&#24180;&#20026;&#35821;&#35328;+&#20998;&#23376;&#26500;&#24314;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#20998;&#23376;&#27169;&#22411;&#24050;&#25104;&#20026;&#20998;&#23376;&#21457;&#29616;&#21644;&#29702;&#35299;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;-&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26377;&#20197;&#19979;&#20960;&#31181;&#31867;&#22411;&#65306;1) &#23567;&#35268;&#27169;&#19988;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#25235;&#21462;&#65292;2) &#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#19988;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#29486;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#26469;&#26500;&#24314;&#65292;3) &#36890;&#36807;&#23558;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#27169;&#26495;&#32780;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#21019;&#24314;&#30340;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;$\textit{L+M-24}$&#26088;&#22312;&#38598;&#20013;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#39033;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00791v1 Announce Type: cross  Abstract: Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the $\textit{L+M-24}$ dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.09142</link><description>&lt;p&gt;
&#24403;&#34920;&#31034;&#23545;&#40784;&#26102;&#65306;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Representations Align: Universality in Representation Learning Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#26550;&#26500;&#30340;&#36873;&#25321;&#65292;&#32467;&#21512;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#26222;&#36941;&#35748;&#20026;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#21516;&#30340;&#26550;&#26500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24778;&#20154;&#30340;&#23450;&#24615;&#30456;&#20284;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#23558;&#36755;&#20837;&#21040;&#38544;&#34255;&#34920;&#31034;&#30340;&#32534;&#30721;&#26144;&#23556;&#21644;&#20174;&#34920;&#31034;&#21040;&#36755;&#20986;&#30340;&#35299;&#30721;&#26144;&#23556;&#37117;&#26159;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#30340;&#20551;&#35774;&#19979;&#65292;&#25512;&#23548;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;&#12290;&#22312;&#22797;&#26434;&#21644;&#22823;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#34920;&#31034;&#27809;&#26377;&#34987;&#21442;&#25968;&#21270;&#24378;&#32422;&#26463;&#65292;&#35813;&#29702;&#35770;&#27010;&#25324;&#20102;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#26377;&#25928;&#29702;&#35770;&#25551;&#36848;&#20102;&#20855;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.02011</link><description>&lt;p&gt;
FakET: &#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#27169;&#25311;&#20919;&#20923;&#30005;&#23376;&#26029;&#23618;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#35745;&#31639;&#26174;&#24494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#26159;&#19982;&#27169;&#25311;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#29289;&#29702;&#30340;&#22797;&#26434;&#25968;&#20540;&#27491;&#21521;&#27169;&#22411;&#20013;&#30340;&#31890;&#23376;&#27169;&#22411;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24050;&#32463;&#24314;&#31435;&#30340;&#29366;&#24577;&#20043;&#19968;&#23545;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#20934;&#27979;&#35797;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#36816;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
&lt;/p&gt;</description></item></channel></rss>