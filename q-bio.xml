<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06199</link><description>&lt;p&gt;
xTrimoPGLM: &#32479;&#19968;&#30340;&#30334;&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06199
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#29983;&#29289;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#23616;&#38480;&#20110;&#33258;&#32534;&#30721;&#25110;&#33258;&#22238;&#24402;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#26102;&#24456;&#38590;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;xTrimoPGLM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25506;&#32034;&#36825;&#20004;&#31867;&#30446;&#26631;&#30340;&#20860;&#23481;&#24615;&#21644;&#32852;&#21512;&#20248;&#21270;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#65292;&#20351;&#29992;1000&#20159;&#21442;&#25968;&#21644;1&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#26469;&#35757;&#32451;xTrimoPGLM&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;1&#65289;xTrimoPGLM&#22312;&#22235;&#20010;&#31867;&#21035;&#30340;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#22522;&#32447;&#12290;&#35813;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
&lt;/p&gt;</description></item></channel></rss>