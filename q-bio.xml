<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.06302</link><description>&lt;p&gt;
GCondNet: &#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06302
&lt;/p&gt;
&lt;p&gt;
GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22788;&#29702;&#39640;&#32500;&#20294;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#20551;&#23450;&#26435;&#37325;&#20043;&#38388;&#30456;&#20114;&#29420;&#31435;&#65292;&#24403;&#26679;&#26412;&#19981;&#36275;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#21033;&#29992;&#20854;&#20182;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCondNet&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#32467;&#26500;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#32500;&#24230;&#22312;&#26679;&#26412;&#20043;&#38388;&#21019;&#24314;&#19968;&#20010;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#25552;&#21462;&#36825;&#31181;&#38544;&#21547;&#32467;&#26500;&#65292;&#20197;&#21450;&#35843;&#25972;&#28508;&#22312;&#39044;&#27979; MLP &#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#21442;&#25968;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#12290;&#36890;&#36807;&#21019;&#24314;&#35768;&#22810;&#23567;&#22270;&#65292;GCondNet &#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's training stability and performance. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor MLP network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method 
&lt;/p&gt;</description></item></channel></rss>