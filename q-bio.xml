<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.20324</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#20174;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#20013;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306;
&lt;/p&gt;
&lt;p&gt;
Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#35768;&#22810;&#24739;&#32773;&#22312;&#33647;&#29289;&#26080;&#27861;&#25511;&#21046;&#30315;&#30187;&#21457;&#20316;&#26102;&#38656;&#35201;&#25163;&#26415;&#24178;&#39044;&#12290;&#20026;&#20102;&#21462;&#24471;&#26377;&#25928;&#30340;&#25163;&#26415;&#32467;&#26524;&#65292;&#20934;&#30830;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; - &#36890;&#24120;&#36817;&#20284;&#20026;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; (SOZ) - &#33267;&#20851;&#37325;&#35201;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#30005;&#21050;&#28608;&#36827;&#34892;&#20027;&#21160;&#25506;&#27979;&#24050;&#32463;&#25104;&#20026;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#21306;&#22495;&#30340;&#26631;&#20934;&#20020;&#24202;&#23454;&#36341;&#12290;&#26412;&#25991;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20351;&#29992;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608; (SPES) &#21709;&#24212;&#36827;&#34892; SOZ &#23450;&#20301;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20445;&#30041;&#30340;&#24739;&#32773;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20324v1 Announce Type: new  Abstract: Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.15176</link><description>&lt;p&gt;
&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#25913;&#21892;&#20102;&#31070;&#32463;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Brain-grounding of semantic vectors improves neural decoding of visual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20934;&#30830;&#20840;&#38754;&#30340;&#31639;&#27861;&#26469;&#35299;&#30721;&#22823;&#33041;&#20869;&#23481;&#26159;&#31070;&#32463;&#31185;&#23398;&#21644;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#22823;&#33041;&#27963;&#21160;&#27169;&#24335;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#30340;&#31070;&#32463;&#35299;&#30721;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#23427;&#23545;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15176v1 Announce Type: cross  Abstract: Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in t
&lt;/p&gt;</description></item></channel></rss>