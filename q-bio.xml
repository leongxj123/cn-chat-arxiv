<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#30340;&#31639;&#27861;&#65292;&#22312;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#19988;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#24212;&#29992;&#23454;&#20363;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2401.11562</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#20197;&#22686;&#24378;&#36873;&#25321;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing selectivity using Wasserstein distance based reweighing. (arXiv:2401.11562v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11562
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#30340;&#31639;&#27861;&#65292;&#22312;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#19988;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#24212;&#29992;&#23454;&#20363;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#119982;&#21644;&#119983;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#36138;&#23146;&#31639;&#27861;&#26469;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#21152;&#26435;&#65292;&#20351;&#24471;&#22312;&#119982;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#26497;&#38480;&#20998;&#24067;&#36924;&#36817;&#22312;&#119983;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24230;&#37327;&#29109;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#30340;&#36138;&#23146;&#31639;&#27861;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#21363;&#32593;&#32476;&#26435;&#37325;&#30340;&#20004;&#20010;&#19981;&#21464;&#20998;&#24067;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#21487;&#20197;&#35777;&#26126;&#25509;&#36817;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#20197;&#36827;&#34892;&#65288;&#36719;&#65289;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#19968;&#20010;&#21160;&#26426;&#24212;&#29992;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;MNK2&#65288;&#19968;&#31181;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#65289;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1
&lt;/p&gt;</description></item></channel></rss>