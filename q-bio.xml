<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04845</link><description>&lt;p&gt;
AlphaFold&#36935;&#21040;Flow Matching&#29983;&#25104;&#34507;&#30333;&#36136;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
AlphaFold Meets Flow Matching for Generating Protein Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#29983;&#29289;&#21151;&#33021;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#24577;&#32467;&#26500;&#38598;&#21512;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#39640;&#31934;&#24230;&#30340;&#21333;&#24577;&#39044;&#27979;&#22120;&#65292;&#22914;AlphaFold&#21644;ESMFold&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#27969;&#21305;&#37197;&#26694;&#26550;&#19979;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#24207;&#21015;&#26465;&#20214;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#12290;&#22312;PDB&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;AlphaFold&#21644;MSA&#23376;&#37319;&#26679;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#21512;&#12290;&#24403;&#36827;&#19968;&#27493;&#35757;&#32451;&#25152;&#26377;&#21407;&#23376;MD&#30340;&#32452;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#26410;&#35265;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#12289;&#20301;&#32622;&#20998;&#24067;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26356;&#24555;&#30340;&#26102;&#38047;&#25910;&#25947;&#36895;&#24230;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#27604;&#22797;&#21046;&#30340;MD&#36712;&#36857;&#26356;&#20855;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.03887</link><description>&lt;p&gt;
&#29992;&#19968;&#31181;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35270;&#39057;&#26174;&#24494;&#38236;&#35760;&#24405;&#20934;&#30830;&#36319;&#36394;&#27963;&#32454;&#32990;&#20173;&#28982;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26041;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#20960;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#24212;&#29992;&#23581;&#35797;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#25972;&#21512;&#21040;&#35813;&#20219;&#21153;&#20013;&#65292;&#20294;&#22823;&#37096;&#20998;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#23884;&#20837;&#20854;&#26550;&#26500;&#25110;&#20854;&#20182;&#21069;&#25552;&#26465;&#20214;&#20013;&#30340;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24191;&#20041;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#32454;&#32990;&#21487;&#20197;&#26681;&#25454;&#20854;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#30340;&#20551;&#35774;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#36830;&#32493;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39069;&#22806;&#20248;&#28857;&#26159;&#32454;&#32990;&#30340;&#36816;&#21160;&#27169;&#24335;&#21487;&#20197;&#23436;&#20840;&#30001;&#39044;&#27979;&#22120;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#35270;&#39057;&#24103;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.10230</link><description>&lt;p&gt;
&#20174;&#38745;&#24577;&#21040;&#21160;&#24577;&#30340;&#32467;&#26500;&#65306;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#32467;&#21512;&#20146;&#21644;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Static to Dynamic Structures: Improving Binding Affinity Prediction with a Graph-Based Deep Learning Model. (arXiv:2208.10230v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026; Dynaformer &#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;CAS-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#26159;&#32467;&#26500;&#22522;&#30784;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#20013;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#20173;&#28982;&#21463;&#38480;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#21482;&#21033;&#29992;&#38745;&#24577;&#26230;&#20307;&#32467;&#26500;&#65292;&#32780;&#23454;&#38469;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#36890;&#24120;&#30001;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#25551;&#36848;&#12290;&#36924;&#36817;&#36825;&#26679;&#30340;&#28909;&#21147;&#23398;&#38598;&#21512;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#12290;&#26412;&#25991;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;3,218&#20010;&#19981;&#21516;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;MD&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Dynaformer&#30340;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290; Dynaformer&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#20174;MD&#36712;&#36857;&#20013;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#20934;&#30830;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#20307;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CASF-2016&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35780;&#20998;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of the protein-ligand binding affinities is an essential challenge in the structure-based drug design. Despite recent advance in data-driven methods in affinity prediction, their accuracy is still limited, partially because they only take advantage of static crystal structures while the actual binding affinities are generally depicted by the thermodynamic ensembles between proteins and ligands. One effective way to approximate such a thermodynamic ensemble is to use molecular dynamics (MD) simulation. Here, we curated an MD dataset containing 3,218 different protein-ligand complexes, and further developed Dynaformer, which is a graph-based deep learning model. Dynaformer was able to accurately predict the binding affinities by learning the geometric characteristics of the protein-ligand interactions from the MD trajectories. In silico experiments demonstrated that our model exhibits state-of-the-art scoring and ranking power on the CASF-2016 benchmark dataset, outpe
&lt;/p&gt;</description></item></channel></rss>