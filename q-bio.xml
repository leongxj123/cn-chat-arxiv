<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08029</link><description>&lt;p&gt;
&#35268;&#21010;&#23398;&#20064;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning. (arXiv:2308.08029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#26159;&#19968;&#31181;&#36817;&#26399;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#24773;&#22659;&#19979;&#35268;&#21010;&#24314;&#27169;&#30340;&#26694;&#26550;&#12290;&#29616;&#22312;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#23427;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#25299;&#23637;-&#22797;&#26434;&#27169;&#22411;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#36882;&#24402;&#20915;&#31574;&#26641;&#25628;&#32034;&#22312;&#22810;&#27493;&#35268;&#21010;&#38382;&#39064;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#24456;&#23569;&#26377;&#24037;&#20316;&#23545;&#27604;SI&#19982;&#20854;&#20182;&#24050;&#24314;&#31435;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;SI&#31639;&#27861;&#20063;&#20027;&#35201;&#20851;&#27880;&#25512;&#29702;&#32780;&#19981;&#26159;&#23398;&#20064;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;SI&#19982;&#26088;&#22312;&#35299;&#20915;&#30456;&#20284;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SI&#22797;&#26434;&#23398;&#20064;&#65288;SL&#65289;&#30340;&#25299;&#23637;&#65292;&#35813;&#25299;&#23637;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#26356;&#21152;&#20805;&#20998;&#22320;&#24341;&#20837;&#20102;&#20027;&#21160;&#23398;&#20064;&#12290;SL&#32500;&#25345;&#23545;&#26410;&#26469;&#35266;&#27979;&#19979;&#27599;&#20010;&#31574;&#30053;&#19979;&#27169;&#22411;&#21442;&#25968;&#22914;&#20309;&#21464;&#21270;&#30340;&#20449;&#24565;&#12290;&#36825;&#20801;&#35768;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective in
&lt;/p&gt;</description></item></channel></rss>