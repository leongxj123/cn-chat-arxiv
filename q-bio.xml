<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18567</link><description>&lt;p&gt;
&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Are Versatile Protein Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18567
&lt;/p&gt;
&lt;p&gt;
DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#25955;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;DPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#20855;&#26377;&#24378;&#22823;&#30340;&#29983;&#25104;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31181;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#26694;&#26550;&#20013;&#20174;&#36827;&#21270;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#39044;&#35757;&#32451;&#21487;&#25193;&#23637;&#30340;DPLM&#65292;&#36825;&#20026;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;&#24314;&#27169;&#25552;&#20379;&#20102;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;DPLM&#23637;&#31034;&#20102;&#29983;&#25104;&#20986;&#31526;&#21512;&#32467;&#26500;&#30340;&#12289;&#26032;&#39062;&#30340;&#12289;&#22810;&#26679;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#24471;DPLM&#23545;&#34507;&#30333;&#36136;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20248;&#31168;&#30340;&#34920;&#31034;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#19982;ESM2&#65288;Lin et al., 2022&#65289;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;DPLM&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#38656;&#27714;&#36827;&#34892;&#23450;&#21046;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18567v1 Announce Type: new  Abstract: This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03136</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#30340;&#24191;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03136
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#20989;&#25968;&#23558;&#29983;&#29289;&#24207;&#21015;&#30340;&#22823;&#32452;&#21512;&#31354;&#38388;&#26144;&#23556;&#21040;&#25152;&#20851;&#27880;&#30340;&#29305;&#24615;&#19978;&#12290;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22810;&#27169;&#24577;&#20989;&#25968;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#25928;&#19988;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#31232;&#30095;&#30340;&#28508;&#22312;&#20989;&#25968;&#36890;&#36807;&#21333;&#35843;&#38750;&#32447;&#24615;&#21464;&#25442;&#20197;&#21457;&#23556;&#21487;&#27979;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914; Bradley-Terry &#25439;&#22833;&#65289;&#26159;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#25152;&#38544;&#31034;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#30340;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#36866;&#24212;&#24615;-&#19978;&#20301;&#32852;&#31995;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20105;&#36777;&#65292;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#21487;&#20197;&#20135;&#29983;&#19981;&#20855;&#22791;&#31232;&#30095;&#34920;&#31034;&#30340;&#35266;&#23519;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#36866;&#21512;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#65289;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#25439;&#22833;&#21487;&#29992;&#20110;&#25512;&#26029;&#19981;&#36866;&#21512; MSE &#25439;&#22833;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
&lt;/p&gt;</description></item></channel></rss>