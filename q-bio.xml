<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.04155</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24212;&#29992;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large language models in bioinformatics: applications and perspectives. (arXiv:2401.04155v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30001;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#29978;&#33267;&#36229;&#36807;&#20102;&#22312;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#30340;&#20960;&#20010;&#37325;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;GPT&#65292;&#24182;&#37325;&#28857;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#19981;&#21516;&#32452;&#23398;&#27700;&#24179;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#21253;&#25324;&#22522;&#22240;&#32452;&#23398;&#12289;&#36716;&#24405;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#21333;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will present a summary of the prominent large language models used in natural language processing, such as BERT and GPT, and focus on exploring the applications of large language models at different omics levels in bioinformatics, mainly including applications of large language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects of large language models in solving bioinfo
&lt;/p&gt;</description></item></channel></rss>