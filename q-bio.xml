<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.</title><link>https://arxiv.org/abs/2402.05982</link><description>&lt;p&gt;
Anfinsen Goes Neural: &#19968;&#31181;&#29992;&#20110;&#26465;&#20214;&#25239;&#20307;&#35774;&#35745;&#30340;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05982
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#22312;&#25512;&#21160;&#27835;&#30103;&#23398;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23545;&#19968;&#33324;&#34507;&#30333;&#36136;&#30693;&#35782;&#30340;&#21033;&#29992;&#26377;&#38480;&#65292;&#24182;&#20551;&#35774;&#22270;&#27169;&#22411;&#36829;&#21453;&#34507;&#30333;&#36136;&#30340;&#32463;&#39564;&#21457;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Anfinsen Goes Neural (AGN)&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;(pLM)&#24182;&#32534;&#30721;&#20102;&#19968;&#31181;&#20851;&#20110;&#34507;&#30333;&#36136;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#21363;Anfinsen's dogma&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#24207;&#21015;&#29983;&#25104;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#32467;&#26500;&#39044;&#27979;&#30340;&#20004;&#27493;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#36807;&#22810;&#37325;&#22797;&#26631;&#35760;&#30340;&#19981;&#29616;&#23454;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#32452;&#21512;&#30340;&#27491;&#21017;&#21270;&#39033;&#21040;&#20132;&#21449;&#29109;&#30446;&#26631;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design plays a pivotal role in advancing therapeutics. Although deep learning has made rapid progress in this field, existing methods make limited use of general protein knowledge and assume a graphical model (GM) that violates empirical findings on proteins. To address these limitations, we present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained protein language model (pLM) and encodes a seminal finding on proteins called Anfinsen's dogma. Our framework follows a two-step process of sequence generation with pLM and structure prediction with graph neural network (GNN). Experiments show that our approach outperforms state-of-the-art results on benchmark experiments. We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens. To resolve this, we introduce a composition-based regularization term to the cross-entropy objective that allows an efficient trade-off be
&lt;/p&gt;</description></item></channel></rss>