<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2311.13870</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-intention Inverse Q-learning for Interpretable Behavior Representation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#26041;&#38754;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#22312;&#37325;&#26500;&#21160;&#29289;&#22797;&#26434;&#34892;&#20026;&#20013;&#30340;&#22810;&#20010;&#24847;&#22270;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#37492;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#24847;&#22270;IRL&#26694;&#26550;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;IRL&#25512;&#26029;&#31163;&#25955;&#30340;&#26102;&#21464;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#65288;&#39532;&#23572;&#31185;&#22827;&#65289;&#21464;&#37327;&#36870;Q&#23398;&#20064;&#65288;L(M)V-IQL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#36866;&#24212;&#31163;&#25955;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#30340;IRL&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#24182;&#20026;&#27599;&#20010;&#24847;&#22270;&#29420;&#31435;&#35299;&#20915;IRL&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23545;&#19981;&#21516;&#30495;&#23454;&#40736;&#31867;&#34892;&#20026;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#26395;&#25171;&#24320;&#25512;&#21160;&#31185;&#23398;&#19982;&#24037;&#31243;&#24212;&#29992;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advancing the understanding of decision-making processes, Inverse Reinforcement Learning (IRL) have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To tackle the challenge, we introduce Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a novel class of IRL algorthms tailored for accommodating discrete intrinsic reward functions. Leveraging an Expectation-Maximization approach, we cluster observed expert trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise f
&lt;/p&gt;</description></item><item><title>Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11729</link><description>&lt;p&gt;
Prospector Heads:&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24191;&#20041;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Prospector Heads: Generalized Feature Attribution for Large Models &amp; Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11729
&lt;/p&gt;
&lt;p&gt;
Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#23450;&#20301;&#36755;&#20837;&#25968;&#25454;&#20013;&#19982;&#20998;&#31867;&#30456;&#20851;&#30340;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#8220;&#35299;&#37322;&#8221;&#31471;&#21040;&#31471;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#65292;&#23384;&#22312;&#29305;&#24449;&#23450;&#20301;&#19981;&#31934;&#30830;&#20197;&#21450;&#30001;&#20110;&#35745;&#31639;&#25361;&#25112;&#32780;&#26080;&#27861;&#22312;&#23567;&#26679;&#26412;&#23610;&#23544;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#65288;prospector heads&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#12290;&#36890;&#36807;&#23545;&#24207;&#21015;&#65288;&#25991;&#26412;&#65289;&#12289;&#22270;&#20687;&#65288;&#30149;&#29702;&#23398;&#65289;&#21644;&#22270;&#65288;&#34507;&#30333;&#36136;&#32467;&#26500;&#65289;&#30340;&#23454;&#39564;&#65292;&#25506;&#23547;&#32773;&#22836;&#37096;&#22312;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#24179;&#22343;&#23616;&#37096;&#21270;AUPRC&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;49&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#22914;&#20309;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16113</link><description>&lt;p&gt;
&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#30452;&#35266;&#22320;&#36827;&#34892;&#35266;&#23519;&#65292;&#38656;&#35201;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#23558;&#20854;&#21464;&#21270;&#25237;&#24433;&#21040;&#32039;&#20945;&#12289;&#21487;&#23548;&#33322;&#30340;&#31354;&#38388;&#20013;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22522;&#22240;&#34920;&#36798;&#65289;&#20013;&#65292;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35299;&#21078;&#21644;&#36716;&#24405;&#27169;&#24335;&#30340;&#32852;&#21512;&#22797;&#26434;&#24615;&#35201;&#27714;&#26368;&#22823;&#21387;&#32553;&#12290;&#30446;&#21069;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#20854;&#35745;&#31639;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#22823;&#21387;&#32553;&#27604;&#19979;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20840;&#33041;&#20307;&#32032;&#32423;Allen&#22823;&#33041;&#22270;&#35889;&#36716;&#24405;&#25968;&#25454;&#65292;&#31995;&#32479;&#27604;&#36739;&#20102;&#22522;&#20110;&#26368;&#24191;&#27867;&#25903;&#25345;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65288;PCA&#65292;&#26680;PCA&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#65292;t-&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;t-SNE&#65289;&#65292;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#65292;&#28145;&#24230;&#33258;&#32534;&#30721;&#65289;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#37327;&#21270;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#35299;&#21078;&#36830;&#36143;&#24615;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item></channel></rss>