<rss version="2.0"><channel><title>Chat Arxiv q-bio</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-bio</description><item><title>&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05645</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Network based on Phase Space for BCI decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05645
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL)&#31639;&#27861;&#19982;&#33041;&#20449;&#21495;&#20998;&#26512;&#30340;&#25972;&#21512;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#30456;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;(BCI)&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;BCI&#36890;&#36807;&#35299;&#30721;&#22823;&#33041;&#27963;&#21160;&#25511;&#21046;&#22806;&#37096;&#35774;&#22791;&#32780;&#26080;&#38656;&#32908;&#32905;&#25511;&#21046;&#12290;&#33041;&#30005;&#22270;(EEG)&#26159;&#35774;&#35745;BCI&#31995;&#32479;&#30340;&#24191;&#27867;&#36873;&#25321;&#65292;&#22240;&#20854;&#26080;&#21019;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20986;&#33394;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#32570;&#23569;&#35757;&#32451;&#25968;&#25454;&#12289;&#20449;&#22122;&#27604;&#20302;&#12289;&#20197;&#21450;&#22312;&#20010;&#20307;&#38388;&#21644;&#20869;&#37096;&#30340;&#22823;&#37327;&#21464;&#21270;&#12290; &#26368;&#21518;&#65292;&#20351;&#29992;&#22810;&#20010;&#30005;&#26497;&#35774;&#32622;BCI&#31995;&#32479;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#65292;&#38459;&#30861;&#21487;&#38752;DL&#26550;&#26500;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#20043;&#22806;&#30340;BCI&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290; &#20026;&#20102;&#25552;&#39640;&#37319;&#32435;&#29575;&#65292;&#25105;&#20204;&#38656;&#35201;&#25913;&#21892;&#29992;&#25143;&#33298;&#36866;&#24230;&#65292;&#20363;&#22914;&#20351;&#29992;&#23569;&#37327;&#30005;&#26497;&#25805;&#20316;&#30340;&#21487;&#38752;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.03230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Large language models surpass human experts in predicting neuroscience results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03230
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21457;&#29616;&#24120;&#24120;&#21462;&#20915;&#20110;&#32508;&#21512;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#36825;&#19968;&#20219;&#21153;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24191;&#27867;&#30340;&#31185;&#23398;&#25991;&#29486;&#19978;&#35757;&#32451;&#30340;LLMs&#21487;&#33021;&#33021;&#22815;&#25972;&#21512;&#22024;&#26434;&#20294;&#30456;&#20851;&#30340;&#21457;&#29616;&#65292;&#20197;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#39044;&#27979;&#26032;&#39062;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BrainBench&#65292;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20102;&#19987;&#23478;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#19978;&#35843;&#25972;&#30340;&#19968;&#20010;LLM&#65292;BrainGPT&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#19982;&#20154;&#31867;&#19987;&#23478;&#19968;&#26679;&#65292;&#24403;LLMs&#23545;&#20182;&#20204;&#30340;&#39044;&#27979;&#26377;&#20449;&#24515;&#26102;&#65292;&#20182;&#20204;&#26356;&#26377;&#21487;&#33021;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#39044;&#31034;&#30528;&#26410;&#26469;&#20154;&#31867;&#21644;LLMs&#23558;&#21512;&#20316;&#36827;&#34892;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#38750;&#29305;&#23450;&#20110;&#31070;&#32463;&#31185;&#23398;&#65292;&#24182;&#19988;&#21487;&#36716;&#31227;&#21040;&#20854;&#20182;&#30693;&#35782;&#23494;&#38598;&#22411;&#20107;&#19994;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03230v1 Announce Type: cross  Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.
&lt;/p&gt;</description></item></channel></rss>