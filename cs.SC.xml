<rss version="2.0"><channel><title>Chat Arxiv cs.SC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SC</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.15458</link><description>&lt;p&gt;
LogicMP: &#19968;&#31181;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints. (arXiv:2309.15458v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#24314;&#27169;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#20197;&#28385;&#36275;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;LogicMP&#65292;&#20854;&#23618;&#23545;MLN&#36827;&#34892;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#12290;&#23427;&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#32534;&#30721;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22359;&#21270;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MLN&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#33391;&#22909;&#12289;&#39640;&#25928;&#30340;&#22343;&#22330;&#36845;&#20195;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;MLN&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#23558;&#25512;&#26029;&#20174;&#39034;&#24207;&#35745;&#31639;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#24352;&#37327;&#25805;&#20316;&#12290;&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#19977;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LogicMP&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#37117;&#20248;&#20110;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
&lt;/p&gt;</description></item></channel></rss>