<rss version="2.0"><channel><title>Chat Arxiv cs.SC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SC</description><item><title>SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05821</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05821
&lt;/p&gt;
&lt;p&gt;
SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#38590;&#20197;&#24402;&#22240;&#30340;&#38382;&#39064;&#20197;&#21450;&#19981;&#23545;&#40784;&#31561;&#31561;&#37117;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#36825;&#20123;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24182;&#32416;&#27491;&#38169;&#35823;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65288;SCoBots&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65292;&#20351;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#36879;&#26126;&#21270;&#12290;SCoBots&#19981;&#20165;&#21033;&#29992;&#30456;&#20851;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#36824;&#21033;&#29992;&#20851;&#31995;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;SCoBots&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#21644;&#35268;&#33539;&#20182;&#20204;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SCoBots&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#26368;&#31616;&#21333;&#19988;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;Pong&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#21152;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
&lt;/p&gt;</description></item></channel></rss>