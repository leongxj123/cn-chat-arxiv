<rss version="2.0"><channel><title>Chat Arxiv cs.SC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SC</description><item><title>Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.12243</link><description>&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): &#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12243
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Constraint-Generation Policy Optimization (CGPO)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;Markov Decision Processes (DC-MDPs)&#20013;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;CGPO&#19981;&#20165;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#35206;&#30422;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26080;&#25968;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#30340;DC-MDPs&#65292;&#32780;&#19988;&#22312;&#32467;&#26463;&#26102;&#21487;&#20197;&#26126;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;CGPO&#36824;&#33021;&#22815;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#26368;&#20248;&#34892;&#21160;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;CGPO&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#30340;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23450;&#20041;&#30340;&#34920;&#36798;&#33021;&#21147;&#31867;&#21035;&#65288;&#21363;&#20998;&#27573;(&#38750;)&#32447;&#24615;&#65289;&#20869;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#30340;&#32422;&#26463;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;&#29616;&#20195;&#38750;&#32447;&#24615;&#20248;&#21270;&#22120;&#65292;CGPO&#21487;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain soluti
&lt;/p&gt;</description></item></channel></rss>