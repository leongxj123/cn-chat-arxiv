<rss version="2.0"><channel><title>Chat Arxiv cs.SC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SC</description><item><title>&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2403.06294</link><description>&lt;p&gt;
ArgMed-Agents: &#20351;&#29992;&#20105;&#35758;&#26041;&#26696;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#34429;&#28982;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#34920;&#29616;&#21364;&#19981;&#23613;&#20154;&#24847;&#12290;&#20854;&#27425;&#65292;LLMs&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#35748;&#30693;&#36807;&#31243;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#23548;&#33268;&#29992;&#25143;&#19981;&#20449;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ArgMed-Agents&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20132;&#20114;&#20351;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#33021;&#22815;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#12290;ArgMed-Agents&#36890;&#36807;&#20020;&#24202;&#20915;&#31574;&#35770;&#25454;&#65288;&#19968;&#31181;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#35748;&#30693;&#36807;&#31243;&#30340;&#25512;&#29702;&#26426;&#21046;&#65289;&#25191;&#34892;&#33258;&#35770;&#35777;&#36845;&#20195;&#65292;&#28982;&#21518;&#23558;&#20105;&#35770;&#36807;&#31243;&#26500;&#24314;&#20026;&#34920;&#31034;&#20914;&#31361;&#20851;&#31995;&#30340;&#26377;&#21521;&#22270;&#12290;&#26368;&#32456;&#65292;Reasoner&#65288;&#19968;&#31181;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#35782;&#21035;&#20986;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 Announce Type: new  Abstract: There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a
&lt;/p&gt;</description></item></channel></rss>