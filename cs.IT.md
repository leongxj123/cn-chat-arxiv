# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [An Information-Theoretic Framework for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.19895) | 提出了一个信息论框架用于机器学习中的超出分布泛化，可以自由插值并产生新的泛化界限，同时具有最优输运解释。 |

# 详细

[^1]: 一种信息论框架用于超出分布泛化

    An Information-Theoretic Framework for Out-of-Distribution Generalization

    [https://arxiv.org/abs/2403.19895](https://arxiv.org/abs/2403.19895)

    提出了一个信息论框架用于机器学习中的超出分布泛化，可以自由插值并产生新的泛化界限，同时具有最优输运解释。

    

    我们研究了机器学习中的超出分布（OOD）泛化，并提出了一个通用框架，提供了信息论泛化界限。我们的框架在Integral Probability Metric（IPM）和$f$-divergence之间自由插值，自然地恢复了一些已知结果（包括Wasserstein和KL-bound），并产生了新的泛化界限。此外，我们展示了我们的框架具有最优输运解释。在两个具体示例中评估时，所提出的界限在某些情况下严格改进了现有界限，或者恢复了现有OOD泛化界限中的最佳者。

    arXiv:2403.19895v1 Announce Type: cross  Abstract: We study the Out-of-Distribution (OOD) generalization in machine learning and propose a general framework that provides information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Moreover, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or recover the best among existing OOD generalization bounds.
    

