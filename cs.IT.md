# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Minimising the Expected Posterior Entropy Yields Optimal Summary Statistics.](http://arxiv.org/abs/2206.02340) | 该论文介绍了从大型数据集中提取低维摘要统计量的重要性，提出了通过最小化后验熵来获取最优摘要统计量的方法，并提供了实践建议和示例验证。 |

# 详细

[^1]: 最小化后验熵产生了最优摘要统计量

    Minimising the Expected Posterior Entropy Yields Optimal Summary Statistics. (arXiv:2206.02340v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.02340](http://arxiv.org/abs/2206.02340)

    该论文介绍了从大型数据集中提取低维摘要统计量的重要性，提出了通过最小化后验熵来获取最优摘要统计量的方法，并提供了实践建议和示例验证。

    

    从大型数据集中提取低维摘要统计量对于高效（无似然）推断非常重要。我们对不同类别的摘要进行了表征，并证明它们对于正确分析降维算法至关重要。我们建议通过在模型的先验预测分布下最小化期望后验熵（EPE）来获取摘要。许多现有方法等效于或是最小化EPE的特殊或极限情况。我们开发了一种方法来获取最小化EPE的高保真摘要；我们将其应用于基准和真实世界的示例。我们既提供了获取有效摘要的统一视角，又为实践者提供了具体建议。

    Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterise different classes of summaries and demonstrate their importance for correctly analysing dimensionality reduction algorithms. We propose obtaining summaries by minimising the expected posterior entropy (EPE) under the prior predictive distribution of the model. Many existing methods are equivalent to or are special or limiting cases of minimising the EPE. We develop a method to obtain high-fidelity summaries that minimise the EPE; we apply it to benchmark and real-world examples. We both offer a unifying perspective for obtaining informative summaries and provide concrete recommendations for practitioners.
    

