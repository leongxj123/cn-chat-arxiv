# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Random Vector Functional Link Networks for Function Approximation on Manifolds](https://arxiv.org/abs/2007.15776) | 本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口 |
| [^2] | [Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach.](http://arxiv.org/abs/2308.12747) | 本文提出了一种使用更高的批评力量将人类触摸与AI生成的文本分离的方法，通过多个困惑度测试和统计模型的结合，能够有效识别出可能被编辑的部分，并探讨了方法的有效性及相关挑战。 |

# 详细

[^1]: 用于流形上函数逼近的随机向量功能链接网络

    Random Vector Functional Link Networks for Function Approximation on Manifolds

    [https://arxiv.org/abs/2007.15776](https://arxiv.org/abs/2007.15776)

    本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口

    

    feed-forward神经网络的学习速度因慢而著名，在深度学习应用中已经成为瓶颈数十年。为了应对这一问题，研究人员和实践者尝试引入随机性来减少学习需求。基于Igelnik和Pao的原始构造，具有随机输入到隐藏层权重和偏置的单层神经网络在实践中取得成功，但缺乏必要的理论证明。本文填补了这一理论空白。我们提供了一个（更正的）严格证明，证明Igelnik和Pao的构造是一个连续函数在紧致域上的通用逼近器，逼近误差像渐近衰减。

    arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
    
[^2]: 使用更高的批评力量将人类触摸与AI生成的文本分离：一种信息论方法

    Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])

    [http://arxiv.org/abs/2308.12747](http://arxiv.org/abs/2308.12747)

    本文提出了一种使用更高的批评力量将人类触摸与AI生成的文本分离的方法，通过多个困惑度测试和统计模型的结合，能够有效识别出可能被编辑的部分，并探讨了方法的有效性及相关挑战。

    

    我们提出了一种方法，用于确定一篇给定文章是完全由生成性语言模型编写的，还是包含了一些由其他作者（可能是人类）进行了重要编辑的替代情况。我们的过程涉及对个别句子或其他文本单位起源进行的多个困惑度测试，将这些多个测试结合起来使用更高的批评力量（HC）。作为副产品，该方法还能够识别出可能被编辑的部分。该方法受到了对数困惑度收敛到交叉熵率的启发，以及一个统计模型来描述被编辑的文本，即句子主要由语言模型生成，但可能有少数句子是通过其他机制产生的。我们使用真实数据证明了我们方法的有效性，并分析了影响其成功的因素。这个分析提出了一些有趣的开放挑战，解决这些挑战可能会改善该方法的有效性。

    We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
    

