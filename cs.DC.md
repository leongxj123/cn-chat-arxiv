# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Unified Framework for Gradient-based Clustering of Distributed Data](https://rss.arxiv.org/abs/2402.01302) | 这是一篇关于梯度聚类分布式数据的统一框架的论文，作者提出了一族分布式聚类算法，可以在用户网络中工作。通过控制用户中心估计的接近程度和定义聚类损失函数，这些算法适用于不同的聚类任务。在提供了统一分析和几个强结果的基础上，这些算法都表现出了良好的收敛性和可行性。 |
| [^2] | [PartIR: Composing SPMD Partitioning Strategies for Machine Learning.](http://arxiv.org/abs/2401.11202) | PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。 |

# 详细

[^1]: 梯度聚类分布式数据的统一框架

    A Unified Framework for Gradient-based Clustering of Distributed Data

    [https://rss.arxiv.org/abs/2402.01302](https://rss.arxiv.org/abs/2402.01302)

    这是一篇关于梯度聚类分布式数据的统一框架的论文，作者提出了一族分布式聚类算法，可以在用户网络中工作。通过控制用户中心估计的接近程度和定义聚类损失函数，这些算法适用于不同的聚类任务。在提供了统一分析和几个强结果的基础上，这些算法都表现出了良好的收敛性和可行性。

    

    我们开发了一族分布式聚类算法，可以在用户网络中工作。在提出的场景中，用户包含一个本地数据集，并且只与其直接邻居进行通信，目标是寻找完整数据的聚类。所提出的家族称为分布式梯度聚类（DGC-$\mathcal{F}_\rho$），由参数化的$\rho\geq1$确定，控制用户中心估计的接近程度，而$\mathcal{F}$确定聚类损失。针对流行的聚类损失如$K$均值和Huber损失，DGC-$\mathcal{F}_\rho$产生了新的分布式聚类算法DGC-KM$_\rho$和DGC-HL$_\rho$，而基于逻辑函数的新型聚类损失导致了DGC-LL$_\rho$。我们提供了统一的分析并建立了几个强结果，在温和的假设下。首先，方法生成的中心序列在任何中心初始化和$...

    We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$, controling the proximity of users' center estimates, with $\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $
    
[^2]: PartIR: 为机器学习组合SPMD分区策略

    PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])

    [http://arxiv.org/abs/2401.11202](http://arxiv.org/abs/2401.11202)

    PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。

    

    现代大规模神经网络（NN）的训练需要结合数据、模型或优化器分片的并行化策略。当策略变得复杂时，分区工具需要具备以下特点：1）表达力强，允许组合简单策略；2）可预测性强，可以通过分析估算性能。我们提出了PartIR，一种用于NN分区的设计。PartIR采用增量重写方法，与硬件和运行时无关。我们提供了一个简单而强大的API用于组合分片策略，并提供了一个模拟器进行验证。整个过程由高级程序员发出的分区策略驱动，既可以手动也可以自动。重要的是，这些策略与模型代码分开指定，易于更改。我们通过对几种不同模型的评估来展示PartIR的可预测性、表达能力和达到峰值性能的能力。

    Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
    

