# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency.](http://arxiv.org/abs/2309.15659) | FeDEQ是一个联邦学习框架，采用深度平衡学习和共识优化，通过紧凑的共享数据表示在边缘节点之间共享模型，解决了深度联邦学习在边缘环境中的通信瓶颈和数据异构性问题。 |

# 详细

[^1]: 联邦深度平衡学习：边缘通信效率的紧凑共享表示

    Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])

    [http://arxiv.org/abs/2309.15659](http://arxiv.org/abs/2309.15659)

    FeDEQ是一个联邦学习框架，采用深度平衡学习和共识优化，通过紧凑的共享数据表示在边缘节点之间共享模型，解决了深度联邦学习在边缘环境中的通信瓶颈和数据异构性问题。

    

    联邦学习是一种卓越的分布式学习范式，促进了边缘网络节点之间的协作，以在不集中数据的情况下共同训练全局模型。通过将计算转移到网络边缘，联邦学习提供了鲁棒和响应迅速的边缘人工智能解决方案，并增强了隐私保护。然而，在边缘环境中部署深度联邦学习模型通常受到通信瓶颈、数据异构性和内存限制的阻碍。为了共同解决这些挑战，我们引入了FeDEQ，这是一个开创性的联邦学习框架，它有效地采用深度平衡学习和共识优化，在边缘节点之间利用紧凑的共享数据表示，允许派生出针对每个节点特定的个性化模型。我们深入探讨了一个独特的模型结构，由一个平衡层和传统神经网络层组成。在这里，平衡层充当全局特征表示，边缘节点可以根据自己的需求进行个性化调整。

    Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei
    

