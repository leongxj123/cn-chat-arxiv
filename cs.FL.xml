<rss version="2.0"><channel><title>Chat Arxiv cs.FL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.FL</description><item><title>&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.18212</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#22522;&#20110;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#30340;&#39318;&#36873;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18212
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20559;&#24207;&#26102;&#24207;&#30446;&#26631;&#65292;&#23558;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26144;&#23556;&#21040;MDP&#31574;&#30053;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#24207;&#29702;&#35770;&#23454;&#29616;&#26368;&#20248;&#31574;&#30053;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#24635;&#26159;&#36890;&#36807;&#23436;&#20840;&#30340;&#32447;&#24615;&#39034;&#24207;&#26469;&#34920;&#31034;&#65306;&#20351;&#29992;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#26469;&#34920;&#36798;&#19981;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#26159;&#33258;&#28982;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#38543;&#26426;&#31995;&#32479;&#20013;&#20570;&#20915;&#31574;&#21644;&#27010;&#29575;&#35268;&#21010;&#65292;&#36825;&#20123;&#31995;&#32479;&#34987;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#32473;&#23450;&#19968;&#32452;&#26377;&#24207;&#20559;&#22909;&#30340;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26102;&#38388;&#24310;&#20280;&#30446;&#26631;&#37117;&#26159;&#20351;&#29992;&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#26377;&#38480;&#36712;&#36857;&#65288;LTL$_f$&#65289;&#20013;&#30340;&#20844;&#24335;&#26469;&#34920;&#31034;&#30340;&#12290;&#20026;&#20102;&#26681;&#25454;&#37096;&#20998;&#26377;&#24207;&#20559;&#22909;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24207;&#29702;&#35770;&#26469;&#23558;&#23545;&#26102;&#38388;&#30446;&#26631;&#30340;&#20559;&#22909;&#26144;&#23556;&#21040;&#23545;MDP&#31574;&#30053;&#30340;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#38543;&#26426;&#39034;&#24207;&#19979;&#30340;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#23558;&#23548;&#33268;MDP&#20013;&#26377;&#38480;&#36335;&#24452;&#19978;&#30340;&#19968;&#20010;&#38543;&#26426;&#38750;&#25903;&#37197;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#21512;&#25104;&#19968;&#20010;&#26368;&#20248;&#36873;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18212v1 Announce Type: cross  Abstract: Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to
&lt;/p&gt;</description></item></channel></rss>