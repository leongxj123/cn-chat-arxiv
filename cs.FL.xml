<rss version="2.0"><channel><title>Chat Arxiv cs.FL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.FL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02019</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Mealy Machines with Timers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#19968;&#33324;&#31867;&#21035;&#30340;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#65288;MMTs&#65289;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;Vaandrager&#31561;&#20154;&#30340;L&#65283;&#31639;&#27861;&#23545;&#23450;&#26102;&#35774;&#32622;&#30340;&#25193;&#23637;&#12290;&#31867;&#20284;&#20110;Waga&#25552;&#20986;&#30340;&#29992;&#20110;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;Maler&#65286;Pnueli&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;Waga&#30340;&#31639;&#27861;&#37117;&#20351;&#29992;&#31526;&#21495;&#26597;&#35810;&#36827;&#34892;&#22522;&#30784;&#35821;&#35328;&#23398;&#20064;&#65292;&#28982;&#21518;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#20855;&#20307;&#26597;&#35810;&#36827;&#34892;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;Waga&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#20855;&#20307;&#26597;&#35810;&#26469;&#23454;&#29616;&#21333;&#20010;&#31526;&#21495;&#26597;&#35810;&#65292;&#32780;&#25105;&#20204;&#21482;&#38656;&#35201;&#22810;&#39033;&#24335;&#25968;&#37327;&#12290;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#65292;&#23398;&#20064;&#32773;&#38656;&#35201;&#30830;&#23450;&#27599;&#20010;&#36716;&#25442;&#30340;&#30830;&#20999;&#21355;&#20853;&#21644;&#37325;&#32622;&#65288;&#26377;&#25351;&#25968;&#22810;&#31181;&#21487;&#33021;&#24615;&#65289;&#65292;&#32780;&#35201;&#23398;&#20064;MMT&#65292;&#23398;&#20064;&#32773;&#21482;&#38656;&#35201;&#24324;&#28165;&#26970;&#21738;&#20123;&#20808;&#21069;&#30340;&#36716;&#25442;&#23548;&#33268;&#36229;&#26102;&#12290;&#27491;&#22914;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02019v1 Announce Type: cross  Abstract: We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a black-box context. Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler &amp; Pnueli. Based on the elementary languages of, both Waga's and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, 
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.13897</link><description>&lt;p&gt;
&#25513;&#30721;&#30828;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#24067;&#23572;RASP&#20934;&#30830;&#35782;&#21035;&#26080;&#26143;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13897
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#65288;&#21363;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65289;&#21644;&#20005;&#26684;&#30340;&#26410;&#26469;&#25513;&#30721;&#65288;&#21363;&#27599;&#20010;&#20301;&#32622;&#21482;&#19982;&#20005;&#26684;&#24038;&#20391;&#30340;&#20301;&#32622;&#36827;&#34892;&#27880;&#24847;&#21147;&#20132;&#20114;&#65289;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#23558;&#34987;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#38480;&#20110;&#24067;&#23572;&#20540;&#30340;RASP&#21464;&#31181;&#12290;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#65292;&#25105;&#20204;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
&lt;/p&gt;</description></item></channel></rss>