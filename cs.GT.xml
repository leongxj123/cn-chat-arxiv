<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;N&#20154;&#21338;&#24328;&#24212;&#29992;&#30340;&#28216;&#25103;&#21464;&#25442;&#20013;&#65292;&#21738;&#20123;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#25110;&#32435;&#20160;&#22343;&#34913;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27491;&#20223;&#23556;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#12290;&#36825;&#20010;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#25551;&#36848;&#65292;&#35828;&#26126;&#21738;&#20123;&#28216;&#25103;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#25110;&#32435;&#20160;&#22343;&#34913;&#38598;&#12290;</title><link>http://arxiv.org/abs/2111.00076</link><description>&lt;p&gt;
&#20445;&#25345;&#32435;&#20160;&#22343;&#34913;&#25110;&#26368;&#20339;&#21453;&#24212;&#38598;&#30340;&#28216;&#25103;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Game Transformations That Preserve Nash Equilibria or Best Response Sets. (arXiv:2111.00076v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;N&#20154;&#21338;&#24328;&#24212;&#29992;&#30340;&#28216;&#25103;&#21464;&#25442;&#20013;&#65292;&#21738;&#20123;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#25110;&#32435;&#20160;&#22343;&#34913;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27491;&#20223;&#23556;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#12290;&#36825;&#20010;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#25551;&#36848;&#65292;&#35828;&#26126;&#21738;&#20123;&#28216;&#25103;&#21464;&#25442;&#21487;&#20197;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#38598;&#25110;&#32435;&#20160;&#22343;&#34913;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#26102;&#38750;&#21512;&#20316;&#21338;&#24328;&#30340;&#25991;&#29486;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#20107;&#23454;&#26159;&#65292;&#25928;&#29992;&#25910;&#30410;&#30340;&#27491;&#20223;&#23556;&#65288;&#32447;&#24615;&#65289;&#21464;&#25442;&#26082;&#19981;&#25913;&#21464;&#26368;&#20339;&#21453;&#24212;&#38598;&#65292;&#20063;&#19981;&#25913;&#21464;&#32435;&#20160;&#22343;&#34913;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21738;&#20123;&#20854;&#20182;&#28216;&#25103;&#21464;&#25442;&#22312;&#24212;&#29992;&#20110;&#20219;&#24847;N&#20154;&#28216;&#25103;&#65288;N&#8805;2&#65289;&#26102;&#20063;&#20855;&#26377;&#36825;&#20004;&#31181;&#23646;&#24615;&#20043;&#19968;&#65306;&#65288;i&#65289;&#32435;&#20160;&#22343;&#34913;&#38598;&#20445;&#25345;&#19981;&#21464;&#65307;&#65288;ii&#65289;&#26368;&#20339;&#21453;&#24212;&#38598;&#20445;&#25345;&#19981;&#21464;&#12290;&#23545;&#20110;&#20197;&#29609;&#23478;&#21644;&#31574;&#30053;&#20026;&#22522;&#30784;&#30340;&#28216;&#25103;&#21464;&#25442;&#65292;&#25105;&#20204;&#35777;&#26126;&#65288;i&#65289;&#24847;&#21619;&#30528;&#65288;ii&#65289;&#65292;&#20855;&#26377;&#23646;&#24615;&#65288;ii&#65289;&#30340;&#21464;&#25442;&#24517;&#39035;&#26159;&#27491;&#20223;&#23556;&#30340;&#12290;&#24471;&#21040;&#30340;&#31561;&#20215;&#38142;&#26126;&#30830;&#25551;&#36848;&#20102;&#37027;&#20123;&#24635;&#26159;&#20445;&#25345;&#32435;&#20160;&#22343;&#34913;&#38598;&#65288;&#25110;&#26368;&#20339;&#21453;&#24212;&#38598;&#65289;&#30340;&#28216;&#25103;&#21464;&#25442;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27491;&#20223;&#23556;&#21464;&#25442;&#31867;&#30340;&#20004;&#20010;&#26032;&#29305;&#24449;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the literature on simultaneous non-cooperative games, it is a widely used fact that a positive affine (linear) transformation of the utility payoffs neither changes the best response sets nor the Nash equilibrium set. We investigate which other game transformations also possess one of these two properties when being applied to an arbitrary N-player game (N &gt;= 2):  (i) The Nash equilibrium set stays the same.  (ii) The best response sets stay the same.  For game transformations that operate player-wise and strategy-wise, we prove that (i) implies (ii) and that transformations with property (ii) must be positive affine. The resulting equivalence chain gives an explicit description of all those game transformations that always preserve the Nash equilibrium set (or, respectively, the best response sets). Simultaneously, we obtain two new characterizations of the class of positive affine transformations.
&lt;/p&gt;</description></item></channel></rss>