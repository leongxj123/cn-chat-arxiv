<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#65292;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16617</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#28216;&#25103;&#20013;&#26080;&#20851;&#26354;&#29575;&#30340;&#26368;&#21518;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds. (arXiv:2306.16617v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#65292;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#35768;&#22810;&#24212;&#29992;&#21487;&#20197;&#20197;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#22343;&#34913;&#35745;&#31639;&#24418;&#24335;&#21270;&#12290;&#23613;&#31649;&#23545;&#23427;&#20204;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#30340;&#21407;&#22987;&#26041;&#26696;&#65292;&#24182;&#22312;&#23545;&#27979;&#22320;&#32447;&#21333;&#35843;&#24615;&#20551;&#35774;&#36827;&#34892;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#30740;&#31350;&#20805;&#20998;&#30340;&#27979;&#22320;&#32447;&#20984;&#20985;&#26497;&#20540;&#20248;&#21270;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#36317;&#31163;&#22833;&#30495;&#29616;&#35937;&#65292;&#20294;&#20855;&#26377;&#23545;&#26354;&#29575;&#19981;&#25935;&#24863;&#30340;&#22266;&#23450;&#27493;&#38271;&#30340;RGD&#26041;&#26696;&#22312;&#27979;&#22320;&#32447;&#24378;&#21333;&#35843;&#35774;&#32622;&#19979;&#21487;&#20197;&#23454;&#29616;&#26354;&#29575;&#26080;&#20851;&#21644;&#32447;&#24615;&#30340;&#26368;&#21518;&#25910;&#25947;&#36895;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#21069;&#20174;&#26410;&#32771;&#34385;&#36807;&#22312;&#40654;&#26364;&#35774;&#32622;&#20013;&#23384;&#22312;&#26354;&#29575;&#26080;&#20851;&#36895;&#29575;&#21644;/&#25110;&#26368;&#21518;&#25910;&#25947;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous applications in machine learning and data analytics can be formulated as equilibrium computation over Riemannian manifolds. Despite the extensive investigation of their Euclidean counterparts, the performance of Riemannian gradient-based algorithms remain opaque and poorly understood. We revisit the original scheme of Riemannian gradient descent (RGD) and analyze it under a geodesic monotonicity assumption, which includes the well-studied geodesically convex-concave min-max optimization problem as a special case. Our main contribution is to show that, despite the phenomenon of distance distortion, the RGD scheme, with a step size that is agnostic to the manifold's curvature, achieves a curvature-independent and linear last-iterate convergence rate in the geodesically strongly monotone setting. To the best of our knowledge, the possibility of curvature-independent rates and/or last-iterate convergence in the Riemannian setting has not been considered before.
&lt;/p&gt;</description></item></channel></rss>