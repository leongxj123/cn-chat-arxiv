<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2403.15524</link><description>&lt;p&gt;
PPA-Game&#65306;&#34920;&#24449;&#21644;&#23398;&#20064;&#22312;&#32447;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#31454;&#20105;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15524
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27604;&#20363;&#24615;&#25910;&#30410;&#20998;&#37197;&#28216;&#25103;&#65288;PPA-Game&#65289;&#26469;&#27169;&#25311;&#20195;&#29702;&#32773;&#22914;&#20309;&#31454;&#20105;&#21487;&#20998;&#37197;&#36164;&#28304;&#21644;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#31867;&#20284;&#20110;YouTube&#21644;TikTok&#31561;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#26681;&#25454;&#24322;&#36136;&#26435;&#37325;&#20026;&#20195;&#29702;&#32773;&#20998;&#37197;&#25910;&#30410;&#65292;&#21453;&#26144;&#20102;&#21019;&#20316;&#32773;&#20043;&#38388;&#20869;&#23481;&#36136;&#37327;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32431;&#32435;&#20160;&#22343;&#34913;&#65288;PNE&#65289;&#24182;&#19981;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#37117;&#26377;&#20445;&#35777;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#20250;&#35266;&#23519;&#21040;&#65292;&#20854;&#32570;&#20047;&#24773;&#20917;&#26159;&#32597;&#35265;&#30340;&#12290;&#38500;&#20102;&#20998;&#26512;&#38745;&#24577;&#25910;&#30410;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20195;&#29702;&#32773;&#20851;&#20110;&#36164;&#28304;&#25910;&#30410;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#23558;&#22810;&#29609;&#23478;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;$T$&#36718;&#20013;&#20419;&#36827;&#27599;&#20010;&#20195;&#29702;&#32773;&#32047;&#31215;&#25910;&#30410;&#30340;&#26368;&#22823;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20219;&#20309;&#20195;&#29702;&#32773;&#30340;&#36951;&#25022;&#22312;&#20219;&#20309;$\eta &gt; 0$&#19979;&#37117;&#21463;&#21040;$O(\log^{1 + \eta} T)$&#30340;&#38480;&#21046;&#12290;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15524v1 Announce Type: cross  Abstract: We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers' attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our simulations. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta &gt; 0$. Empirical results further validate the effectiveness of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.00841</link><description>&lt;p&gt;
&#31454;&#20105;&#28216;&#25103;&#30340;&#31163;&#32447;&#34394;&#26500;&#33258;&#25105;&#23545;&#24328;
&lt;/p&gt;
&lt;p&gt;
Offline Fictitious Self-Play for Competitive Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#25913;&#36827;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#23613;&#31649;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#31454;&#20105;&#28216;&#25103;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
&lt;/p&gt;</description></item></channel></rss>