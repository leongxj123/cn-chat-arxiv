<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#21457;&#29616;&#22810;&#20010;&#23454;&#38469;&#21464;&#20307;&#22312;&#31616;&#21333;&#30340;&#28216;&#25103;&#20013;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#30340;&#26368;&#36817;&#21464;&#20307;&#21017;&#20855;&#26377;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00676</link><description>&lt;p&gt;
Regret-Matching&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games. (arXiv:2311.00676v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#21457;&#29616;&#22810;&#20010;&#23454;&#38469;&#21464;&#20307;&#22312;&#31616;&#21333;&#30340;&#28216;&#25103;&#20013;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#30340;&#26368;&#36817;&#21464;&#20307;&#21017;&#20855;&#26377;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#36951;&#25022;&#21305;&#37197;+ (RM+)&#21450;&#20854;&#21464;&#31181;&#65292;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#19982;&#20855;&#26377;&#38646;&#21644;&#28216;&#25103;&#30340;&#24378;&#26368;&#32456;&#36845;&#20195;&#21644;&#36941;&#21382;&#25910;&#25947;&#24615;&#36136;&#30340;&#31639;&#27861;&#65288;&#22914;&#20048;&#35266;&#26799;&#24230;&#19978;&#21319;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#23545;&#20110;&#36951;&#25022;&#21305;&#37197;&#31639;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#24615;&#36136;&#20960;&#20046;&#19968;&#26080;&#25152;&#30693;&#12290;&#37492;&#20110;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#23545;&#20110;&#25968;&#20540;&#20248;&#21270;&#21644;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28216;&#25103;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;RM+&#21464;&#20307;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#21253;&#25324;&#21516;&#26102;RM+&#12289;&#20132;&#26367;RM+&#21644;&#21516;&#26102;&#39044;&#27979;RM+&#22312;&#20869;&#30340;&#20960;&#20010;&#23454;&#38469;&#21464;&#20307;&#65292;&#29978;&#33267;&#22312;&#31616;&#21333;&#30340;3x3&#28216;&#25103;&#20013;&#20063;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26368;&#36817;&#21464;&#20307;&#65292;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#24471;&#21040;&#20102;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#20026;&#32447;&#24615;&#21512;&#21516;&#22312;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#24182;&#34920;&#26126;&#22312;&#32447;&#24615;&#21512;&#21516;&#20013;&#65292;&#24403;&#22996;&#25176;-&#20195;&#29702;&#29615;&#22659;&#20013;&#23384;&#22312;&#36275;&#22815;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#32447;&#24615;&#21512;&#21516;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.06850</link><description>&lt;p&gt;
&#32447;&#24615;&#21512;&#21516;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Analysis of Linear Contracts. (arXiv:2211.06850v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#20026;&#32447;&#24615;&#21512;&#21516;&#22312;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#35299;&#37322;&#21644;&#35777;&#26126;&#65292;&#24182;&#34920;&#26126;&#22312;&#32447;&#24615;&#21512;&#21516;&#20013;&#65292;&#24403;&#22996;&#25176;-&#20195;&#29702;&#29615;&#22659;&#20013;&#23384;&#22312;&#36275;&#22815;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#32447;&#24615;&#21512;&#21516;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#20026;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#32447;&#24615;&#65288;&#20323;&#37329;&#21046;&#65289;&#21512;&#21516;&#25552;&#20379;&#20102;&#21512;&#29702;&#24615;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38544;&#34255;&#34892;&#21160;&#30340;&#22996;&#25176;-&#20195;&#29702;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#19981;&#21516;&#34892;&#21160;&#38656;&#35201;&#19981;&#21516;&#30340;&#21162;&#21147;&#37327;&#65292;&#24182;&#19988;&#20195;&#29702;&#20154;&#30340;&#21162;&#21147;&#25104;&#26412;&#26159;&#31169;&#26377;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#22312;&#22996;&#25176;-&#20195;&#29702;&#29615;&#22659;&#20013;&#23384;&#22312;&#36275;&#22815;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#32447;&#24615;&#21512;&#21516;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a justification for the prevalence of linear (commission-based) contracts in practice under the Bayesian framework. We consider a hidden-action principal-agent model, in which actions require different amounts of effort, and the agent's cost per-unit-of-effort is private. We show that linear contracts are near-optimal whenever there is sufficient uncertainty in the principal-agent setting.
&lt;/p&gt;</description></item></channel></rss>