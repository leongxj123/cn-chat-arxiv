<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07425</link><description>&lt;p&gt;
&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#25353;&#29031;&#31616;&#21333;&#30340;&#22810;&#33218;&#21163;&#21290;&#21327;&#35758;&#20849;&#21516;&#34892;&#21160;&#12290;&#20195;&#29702;&#20197;&#39034;&#24207;&#26041;&#24335;&#21040;&#36798;&#65292;&#36873;&#25321;&#27494;&#22120;&#24182;&#25509;&#25910;&#30456;&#20851;&#22870;&#21169;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#20808;&#21069;&#20195;&#29702;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65288;&#27494;&#22120;&#21644;&#22870;&#21169;&#65289;&#65292;&#19981;&#23384;&#22312;&#31169;&#26377;&#20449;&#21495;&#12290;&#23613;&#31649;&#20195;&#29702;&#20849;&#21516;&#38754;&#20020;&#24320;&#21457;&#21644;&#21033;&#29992;&#30340;&#25506;&#32034;&#25240;&#34935;&#65292;&#20294;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26159;&#19968;&#35265;&#38047;&#24773;&#30340;&#65292;&#26080;&#38656;&#32771;&#34385;&#25506;&#32034;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#31995;&#21015;&#19982;&#65288;&#21442;&#25968;&#21270;&#65289;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#21253;&#25324;&#8220;&#26080;&#20559;&#8221;&#34892;&#20026;&#21644;&#21508;&#31181;&#34892;&#20026;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#34892;&#20026;&#30340;&#26497;&#31471;&#29256;&#26412;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#21163;&#21290;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#28201;&#21644;&#30340;&#29256;&#26412;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#25506;&#32034;&#22833;&#36133;&#65292;&#22240;&#27492;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#8220;&#28201;&#21644;&#20048;&#35266;&#8221;&#30340;&#20195;&#29702;&#25552;&#20379;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#28608;&#21169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65306;&#27494;&#22120;&#25506;&#32034;&#26159;&#22266;&#26377;&#20110;&#21163;&#21290;&#38382;&#39064;&#30340;&#65292;&#21482;&#21463;&#24403;&#21069;&#20195;&#29702;&#30340;&#34892;&#21160;&#24433;&#21709;&#65292;&#32780;&#31038;&#20132;&#25506;&#32034;&#26159;&#30001;&#20808;&#21069;&#20195;&#29702;&#34892;&#20026;&#39537;&#21160;&#30340;&#65292;&#22240;&#27492;&#26377;&#21033;&#20110;&#26410;&#26469;&#20195;&#29702;&#12290;&#30001;&#20110;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#38480;&#21046;&#20102;&#31038;&#20132;&#25506;&#32034;&#65292;&#36825;&#31181;&#26435;&#34913;&#34987;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
&lt;/p&gt;</description></item></channel></rss>