<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2306.01990</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#19978;&#19979;&#25991;&#21644;&#32452;&#21512;&#21160;&#20316;&#28608;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Exploration with Linear Contexts and Combinatorial Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01990
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#36827;&#20102;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#25163;&#33218;&#36873;&#25321;&#34987;&#35270;&#20026;&#25512;&#33616;&#65292;&#24182;&#19988;&#35201;&#27714;&#26159;&#36125;&#21494;&#26031;&#28608;&#21169;&#20860;&#23481;&#30340;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#29420;&#31435;&#24615;&#20551;&#35774;&#21518;&#65292;&#32463;&#36807;&#36275;&#22815;&#30340;&#21021;&#22987;&#26679;&#26412;&#25910;&#38598;&#65292;&#27969;&#34892;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#21464;&#24471;&#28608;&#21169;&#20860;&#23481;&#12290;&#25105;&#20204;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#25552;&#20379;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#20808;&#39564;&#30340;&#29420;&#31435;&#24615;&#34987;&#33258;&#28982;&#30340;&#20984;&#24615;&#26465;&#20214;&#21462;&#20195;&#12290;&#36825;&#25171;&#24320;&#20102;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#21644;&#36951;&#25022;&#26368;&#20248;&#30340;&#28608;&#21169;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29992;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#21069;&#27748;&#26222;&#26862;&#25277;&#26679;&#38454;&#27573;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01990v2 Announce Type: replace-cross  Abstract: We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.
&lt;/p&gt;</description></item></channel></rss>