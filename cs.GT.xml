<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12562</link><description>&lt;p&gt;
&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing and Learning with Long-term Reference Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#23545;&#24403;&#21069;&#20215;&#26684;&#30340;&#21453;&#24212;&#21463;&#21040;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#65292;&#21363;&#21442;&#32771;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#20854;&#20013;&#21442;&#32771;&#20215;&#26684;&#26159;&#21334;&#23478;&#36807;&#21435;&#25552;&#20379;&#30340;&#20215;&#26684;&#30340;&#24179;&#22343;&#20540;&#12290;&#19982;&#26356;&#24120;&#35265;&#30340;&#25351;&#25968;&#24179;&#28369;&#26426;&#21046;&#30456;&#21453;&#65292;&#22312;&#25105;&#20204;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#20013;&#65292;&#21334;&#23478;&#25552;&#20379;&#30340;&#20215;&#26684;&#23545;&#26410;&#26469;&#39038;&#23458;&#26399;&#26395;&#26377;&#26356;&#38271;&#26399;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36825;&#31181;&#26426;&#21046;&#19979;&#65292;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#19981;&#21463;&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#36825;&#31526;&#21512;&#19968;&#20010;&#24120;&#35265;&#30340;&#30452;&#35273;&#65292;&#21363;&#21334;&#23478;&#21487;&#20197;&#36890;&#36807;&#20197;&#36739;&#39640;&#30340;&#20215;&#26684;&#20986;&#21457;&#65292;&#28982;&#21518;&#36880;&#28176;&#38477;&#20302;&#20215;&#26684;&#65292;&#22240;&#20026;&#39038;&#23458;&#20250;&#35273;&#24471;&#20182;&#20204;&#27491;&#22312;&#36141;&#20080;&#36890;&#24120;&#26356;&#26114;&#36149;&#30340;&#29289;&#21697;&#19978;&#30340;&#20415;&#23452;&#36135;&#12290;&#23545;&#20110;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#30340;&#35814;&#32454;&#29305;&#24449;&#24615;&#25551;&#36848;&#20197;&#21450;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12562v1 Announce Type: new  Abstract: We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11904</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#35770;&#22363; &#25688;&#35201;: &#33258;&#21160;&#21270;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#39640;&#25910;&#20837;&#21644;&#28608;&#21169;&#20860;&#23481;&#30340;&#26426;&#21046;&#12290;&#30830;&#20445;&#20027;&#23548;&#25112;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23558;&#26426;&#21046;&#38480;&#21046;&#22312;&#20223;&#23556;&#26368;&#22823;&#21270;&#25293;&#21334;&#65288;AMAs&#65289;&#33539;&#22260;&#20869;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AMA&#30340;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65288;&#30001;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#23548;&#33268;&#65289;&#21644;&#25910;&#20837;&#30340;&#19981;&#21487;&#24494;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;AMA&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25293;&#21334;&#26426;&#21046;&#38480;&#21046;&#22312;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65288;VVCAs&#65289;&#33539;&#22260;&#20869;&#65292;&#36825;&#26159;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;AMAs&#23376;&#38598;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24182;&#34892;&#21270;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#35745;&#31639;VVCA&#30340;&#33719;&#32988;&#20998;&#37197;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#38646;&#38454;&#21644;&#19968;&#38454;&#25216;&#26415;&#30340;&#26032;&#22411;&#20248;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;VVCA&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 Announce Type: cross  Abstract: Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning. Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs). Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue. In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters. Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA. Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters. Extens
&lt;/p&gt;</description></item><item><title>&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.12226</link><description>&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65306;&#31639;&#27861;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Smooth Nash Equilibria: Algorithms and Complexity. (arXiv:2309.12226v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12226
&lt;/p&gt;
&lt;p&gt;
&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#26494;&#24347;&#21464;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#19982;&#26368;&#20339;&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#26469;&#36798;&#21040;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24378;&#21644;&#24369;&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#24615;&#36136;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#20160;&#22343;&#34913;&#30340;&#19968;&#20010;&#22522;&#26412;&#32570;&#28857;&#26159;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65306;&#22312;&#27491;&#21017;&#24418;&#24335;&#30340;&#21338;&#24328;&#20013;&#65292;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#26159;PPAD&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#24179;&#28369;&#20998;&#26512;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#26494;&#24347;&#21464;&#31181;&#65292;&#20854;&#20013;$\sigma$&#26159;&#20809;&#28369;&#24615;&#21442;&#25968;&#12290;&#22312;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#29609;&#23478;&#20204;&#21482;&#38656;&#35201;&#23454;&#29616;&#33267;&#23569;&#19982;&#20182;&#20204;&#26368;&#20339;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#30340;&#20559;&#31163;&#30456;&#21516;&#30340;&#25928;&#29992;&#65292;&#32780;&#36825;&#20010;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#26159;&#19981;&#20250;&#23545;&#20219;&#20309;&#22266;&#23450;&#21160;&#20316;&#20135;&#29983;&#36807;&#22810;&#36136;&#37327;&#65288;&#26681;&#25454;$\sigma$&#21442;&#25968;&#21270;&#65289;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#31181;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#30340;&#21464;&#31181;&#65306;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29609;&#23478;&#20204;&#38656;&#35201;&#22312;&#22343;&#34913;&#20013;&#37319;&#29992;$\sigma$-&#20809;&#28369;&#31574;&#30053;&#36827;&#34892;&#28216;&#25103;&#65307;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#20013;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#26159;&#24369;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#36824;&#26159;&#24378;$\sigma$-&#20809;&#28369;&#32435;&#20160;&#22343;&#34913;&#65292;&#37117;&#27604;&#32435;&#20160;&#22343;&#34913;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.  We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibri
&lt;/p&gt;</description></item></channel></rss>