<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08576</link><description>&lt;p&gt;
&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regret Minimization in Stackelberg Games with Side Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#22522;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;Stackelberg&#21338;&#24328;&#26159;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;Stackelberg&#21338;&#24328;&#31639;&#27861;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#30340;&#26368;&#22823;&#25104;&#21151;&#20043;&#19968;&#65292;&#22240;&#20026;Stackelberg&#21338;&#24328;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22330;&#23433;&#20840;&#12289;&#21453;&#30423;&#29454;&#21644;&#32593;&#32476;&#29359;&#32618;&#39044;&#38450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#26410;&#33021;&#32771;&#34385;&#21040;&#27599;&#20010;&#29609;&#23478;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#20132;&#36890;&#27169;&#24335;&#65292;&#22825;&#27668;&#26465;&#20214;&#65292;&#32593;&#32476;&#25317;&#22622;&#65289;&#65292;&#36825;&#26159;&#29616;&#23454;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#21040;&#20004;&#20010;&#29609;&#23478;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#20391;&#20449;&#24687;&#30340;Stackelberg&#21338;&#24328;&#65292;&#20854;&#20013;&#20004;&#20010;&#29609;&#23478;&#22312;&#36827;&#34892;&#28216;&#25103;&#20043;&#21069;&#37117;&#35266;&#23519;&#21040;&#19968;&#20010;&#22806;&#37096;&#29615;&#22659;&#12290;&#28982;&#21518;&#65292;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#21487;&#33021;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#23545;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#19978;&#19979;&#25991;&#37117;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08017</link><description>&lt;p&gt;
Stackelberg&#36712;&#36857;&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#30340;&#36870;&#21521;&#23398;&#20064;&#26159;&#20174;&#29609;&#23478;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Stackelberg&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#27599;&#20010;&#29609;&#23478;&#30340;&#21160;&#24577;&#31995;&#32479;&#36712;&#36857;&#26469;&#23450;&#20041;&#19968;&#20010;&#36870;&#21521;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39046;&#23548;&#32773;&#21644;&#19968;&#20010;&#36319;&#38543;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35753;&#39046;&#23548;&#32773;&#25512;&#26029;&#20986;&#19968;&#20010;&#26377;&#38480;&#20505;&#36873;&#38598;&#20013;&#25551;&#36848;&#36319;&#38543;&#32773;&#30446;&#26631;&#20989;&#25968;&#30340;&#20551;&#35774;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#34987;&#21160;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#21160;&#22320;&#26368;&#22823;&#21270;&#19981;&#21516;&#20551;&#35774;&#19979;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#24046;&#24322;&#65292;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36882;&#36827;&#30340;&#37325;&#22797;&#36712;&#36857;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#19982;&#22343;&#21248;&#38543;&#26426;&#36755;&#20837;&#30456;&#27604;&#65292;&#25152;&#25552;&#20379;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#27010;&#29575;&#25910;&#25947;&#21040;&#26465;&#20214;&#20110;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#19981;&#21516;&#20551;&#35774;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#26469;&#26368;&#22823;&#21270;&#22312;&#20195;&#29702;&#20154;&#20449;&#21495;&#20998;&#24067;&#30340;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#23545;&#20110;&#26377;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65307;&#23545;&#20110;&#26080;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2107.07420</link><description>&lt;p&gt;
&#37096;&#20998;&#30693;&#35782;&#19979;&#30340;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Scoring Rule Design under Partial Knowledge. (arXiv:2107.07420v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#26469;&#26368;&#22823;&#21270;&#22312;&#20195;&#29702;&#20154;&#20449;&#21495;&#20998;&#24067;&#30340;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#23545;&#20110;&#26377;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65307;&#23545;&#20110;&#26080;&#38480;&#38598;&#21512;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#22996;&#25176;&#20154;&#23545;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#37096;&#20998;&#20102;&#35299;&#26102;&#65292;&#26368;&#20248;&#36866;&#24403;&#25171;&#20998;&#35268;&#21017;&#30340;&#35774;&#35745;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#22996;&#25176;&#20154;&#23436;&#20840;&#20102;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#30830;&#23450;&#22686;&#21152;&#20195;&#29702;&#20154;&#22238;&#25253;&#30340;&#26368;&#22823;&#36866;&#24403;&#25171;&#20998;&#35268;&#21017;&#65292;&#24403;&#20195;&#29702;&#20154;&#36873;&#25321;&#35775;&#38382;&#26114;&#36149;&#20449;&#21495;&#20197;&#23436;&#21892;&#20854;&#20808;&#39564;&#39044;&#27979;&#30340;&#21518;&#39564;&#20449;&#24565;&#26102;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#22996;&#25176;&#20154;&#21482;&#30693;&#36947;&#20195;&#29702;&#20154;&#30340;&#20449;&#21495;&#20998;&#24067;&#23646;&#20110;&#19968;&#32452;&#20998;&#24067;&#20013;&#30340;&#26576;&#20010;&#12290;&#25105;&#20204;&#23558;&#25171;&#20998;&#35268;&#21017;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#20998;&#24067;&#38598;&#21512;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#22238;&#25253;&#30340;&#22686;&#21152;&#12290;&#24403;&#20998;&#24067;&#38598;&#21512;&#26377;&#38480;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20248;&#25171;&#20998;&#35268;&#21017;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26080;&#38480;&#38598;&#21512;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#25171;&#20998;&#35268;&#21017;&#65292;&#22914;&#20108;&#27425;&#26041;&#25171;&#20998;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the design of optimal proper scoring rules when the principal has partial knowledge of an agent's signal distribution. Recent work characterizes the proper scoring rules that maximize the increase of an agent's payoff when the agent chooses to access a costly signal to refine a posterior belief from her prior prediction, under the assumption that the agent's signal distribution is fully known to the principal. In our setting, the principal only knows about a set of distributions where the agent's signal distribution belongs. We formulate the scoring rule design problem as a max-min optimization that maximizes the worst-case increase in payoff across the set of distributions.  We propose an efficient algorithm to compute an optimal scoring rule when the set of distributions is finite, and devise a fully polynomial-time approximation scheme that accommodates various infinite sets of distributions. We further remark that widely used scoring rules, such as the quadratic 
&lt;/p&gt;</description></item></channel></rss>