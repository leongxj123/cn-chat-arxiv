<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.12797</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;&#26426;&#21046;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#30740;&#31350;&#20102;&#26426;&#21046;&#35774;&#35745;&#32773;&#22312;&#26102;&#21464;&#29615;&#22659;&#20013;&#24212;&#35813;&#22914;&#20309;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#38382;&#39064;&#65292;&#21363;&#20195;&#29702;&#26681;&#25454;&#26410;&#30693;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#19982;&#26426;&#21046;&#35774;&#35745;&#32773;&#20114;&#21160;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20195;&#29702;&#30340;&#22870;&#21169;&#21644;&#26426;&#21046;&#35774;&#35745;&#32773;&#30340;&#29366;&#24577;&#26681;&#25454;&#19968;&#20010;&#24102;&#26377;&#26410;&#30693;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#26680;&#30340;&#24773;&#33410;MDP&#28436;&#21270;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22810;&#36718;&#20114;&#21160;&#20013;&#24674;&#22797;&#21160;&#24577;Vickrey-Clarke-Grove(VCG)&#26426;&#21046;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;(RL)&#32467;&#21512;&#36827;&#26469;&#65292;&#20197;&#24110;&#21161;&#22312;&#20016;&#23500;&#30340;&#31574;&#30053;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#20272;&#35745;&#21160;&#24577;VCG&#26426;&#21046;&#20013;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.12797v2 Announce Type: replace  Abstract: Dynamic mechanism design studies how mechanism designers should allocate resources among agents in a time-varying environment. We consider the problem where the agents interact with the mechanism designer according to an unknown Markov Decision Process (MDP), where agent rewards and the mechanism designer's state evolve according to an episodic MDP with unknown reward functions and transition kernels. We focus on the online setting with linear function approximation and propose novel learning algorithms to recover the dynamic Vickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key contribution of our approach is incorporating reward-free online Reinforcement Learning (RL) to aid exploration over a rich policy space to estimate prices in the dynamic VCG mechanism. We show that the regret of our proposed method is upper bounded by $\tilde{\mathcal{O}}(T^{2/3})$ and further devise a lower bound to show that our a
&lt;/p&gt;</description></item></channel></rss>