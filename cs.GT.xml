<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08705</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#19982;&#65288;&#20934;&#65289;&#25928;&#29575;&#65306;&#20449;&#24687;&#20849;&#20139;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing. (arXiv:2308.08705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;POSGs&#65289;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#20026;&#20102;&#35268;&#36991;&#24050;&#30693;&#30340;&#38590;&#24230;&#38382;&#39064;&#21644;&#20351;&#29992;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#20513;&#23548;&#21033;&#29992;Agent&#20043;&#38388;&#30340;&#28508;&#22312;&#8220;&#20449;&#24687;&#20849;&#20139;&#8221;&#65292;&#36825;&#26159;&#23454;&#35777;MARL&#20013;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20063;&#26159;&#20855;&#22791;&#36890;&#20449;&#21151;&#33021;&#30340;&#22810;Agent&#25511;&#21046;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33509;&#24178;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#26469;&#35777;&#26126;&#20449;&#24687;&#20849;&#20139;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#21450;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#20026;&#20102;&#27714;&#35299;POSGs&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#20351;&#24471;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20934;&#25928;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#36827;&#19968;&#27493;&#8220;&#36817;&#20284;&#8221;&#20849;&#20139;&#30340;&#20844;&#20849;&#20449;&#24687;&#26500;&#24314;POSG&#30340;&#8220;&#36817;&#20284;&#27169;&#22411;&#8221;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#35745;&#21010;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65288;&#20174;&#35299;&#20915;&#21407;&#22987;POSG&#30340;&#35282;&#24230;&#65289;&#21487;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#65292;&#21363;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21069;&#25552;&#26159;&#19978;&#36848;&#20551;&#35774;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermo
&lt;/p&gt;</description></item></channel></rss>