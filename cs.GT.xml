<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#37197;&#21338;&#24328;&#30340;&#26356;&#21152;&#20844;&#24179;&#26680;&#20998;&#37197;&#30340;&#32452;&#21512;&#22810;&#39033;&#24335;&#26102;&#38388;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11437</link><description>&lt;p&gt;
&#12298;&#20998;&#37197;&#21338;&#24328;&#65306;&#20844;&#24179;&#26680;&#20998;&#37197;&#30340;&#26032;&#26426;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Assignment Game: New Mechanisms for Equitable Core Imputations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#37197;&#21338;&#24328;&#30340;&#26356;&#21152;&#20844;&#24179;&#26680;&#20998;&#37197;&#30340;&#32452;&#21512;&#22810;&#39033;&#24335;&#26102;&#38388;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#20998;&#37197;&#21338;&#24328;&#12299;&#30340;&#26680;&#20998;&#37197;&#38598;&#24418;&#25104;&#19968;&#20010;&#65288;&#38750;&#26377;&#38480;&#65289;&#20998;&#37197;&#26684;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20165;&#24050;&#30693;&#26377;&#25928;&#31639;&#27861;&#29992;&#20110;&#35745;&#31639;&#20854;&#20004;&#20010;&#26497;&#31471;&#20998;&#37197;&#65307;&#20294;&#26159;&#65292;&#20854;&#20013;&#27599;&#19968;&#20010;&#37117;&#26368;&#22823;&#31243;&#24230;&#22320;&#20559;&#34962;&#19968;&#20010;&#26041;&#65292;&#19981;&#21033;&#20110;&#21452;&#26041;&#30340;&#20998;&#37197;&#65292;&#23548;&#33268;&#30408;&#21033;&#19981;&#22343;&#34913;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#30001;&#19968;&#20010;&#29609;&#23478;&#32452;&#25104;&#30340;&#23376;&#32852;&#30431;&#65288;&#25110;&#32773;&#26469;&#33258;&#20998;&#37197;&#20004;&#20391;&#30340;&#19968;&#31995;&#29609;&#23478;&#65289;&#21487;&#20197;&#33719;&#24471;&#38646;&#21033;&#28070;&#65292;&#22240;&#27492;&#26680;&#20998;&#37197;&#19981;&#24517;&#32473;&#20104;&#20182;&#20204;&#20219;&#20309;&#21033;&#28070;&#12290;&#22240;&#27492;&#65292;&#26680;&#20998;&#37197;&#22312;&#20010;&#20307;&#20195;&#29702;&#20154;&#23618;&#38754;&#19978;&#19981;&#25552;&#20379;&#20219;&#20309;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#36825;&#24341;&#20986;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35745;&#31639;&#26356;&#20844;&#24179;&#30340;&#26680;&#20998;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#20998;&#37197;&#21338;&#24328;&#30340;Leximin&#21644;Leximax&#26680;&#20998;&#37197;&#30340;&#32452;&#21512;&#65288;&#21363;&#65292;&#35813;&#26426;&#21046;&#19981;&#28041;&#21450;LP&#27714;&#35299;&#22120;&#65289;&#22810;&#39033;&#24335;&#26102;&#38388;&#26426;&#21046;&#12290;&#36825;&#20123;&#20998;&#37197;&#20197;&#19981;&#21516;&#26041;&#24335;&#23454;&#29616;&#20102;&#8220;&#20844;&#24179;&#24615;&#8221;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11437v1 Announce Type: cross  Abstract: The set of core imputations of the assignment game forms a (non-finite) distributive lattice. So far, efficient algorithms were known for computing only its two extreme imputations; however, each of them maximally favors one side and disfavors the other side of the bipartition, leading to inequitable profit sharing. Another issue is that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, therefore a core imputation is not obliged to give them any profit. Hence core imputations make no fairness guarantee at the level of individual agents. This raises the question of computing {\em more equitable core imputations}.   In this paper, we give combinatorial (i.e., the mechanism does not invoke an LP-solver) polynomial time mechanisms for computing the leximin and leximax core imputations for the assignment game. These imputations achieve ``fairness'' in different ways: w
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07588</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scaling Laws for Learning in Strategic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21453;&#26144;&#20986;&#19968;&#20010;&#20849;&#35782;&#65306;&#27169;&#22411;&#36234;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#36234;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#65292;&#23601;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#25112;&#30053;&#29615;&#22659;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27169;&#22411;&#19982;&#25112;&#30053;&#20114;&#21160;&#23545;&#27604;&#20363;&#23450;&#24459;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36825;&#20010;&#33258;&#28982;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#27604;&#20363;&#23450;&#24459;&#35266;&#28857;&#65292;&#21363;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#38543;&#30528;&#27169;&#22411;&#30340;&#25193;&#22823;&#21644;/&#25110;&#34920;&#36798;&#33021;&#21147;&#30340;&#22686;&#24378;&#65288;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65289;&#32780;&#21333;&#35843;&#25552;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#25112;&#30053;&#22238;&#24402;&#12289;&#25112;&#30053;&#20998;&#31867;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#20363;&#23376;&#23637;&#31034;&#20102;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#27169;&#22411;&#25110;&#31574;&#30053;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2301.13306</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#30340;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#65306;&#25928;&#29575;&#12289;&#21518;&#24724;&#21644;&#33410;&#22863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#22312;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#19978;&#36827;&#34892;&#21338;&#24328;&#30340;&#24773;&#20917;&#12290;&#27599;&#20010;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#34987;&#36171;&#20104;&#20219;&#21153;&#65292;&#22312;&#22810;&#36718;&#37325;&#22797;&#25293;&#21334;&#20013;&#65292;&#26368;&#22823;&#21270;&#20854;&#24191;&#21578;&#20027;&#30340;&#24635;&#20215;&#20540;&#65292;&#21516;&#26102;&#21463;&#21040;&#39044;&#31639;&#21644;/&#25110;&#25237;&#36164;&#22238;&#25253;&#29575;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#36798;&#21040;&#36880;&#28176;&#20943;&#23567;&#30340;&#20010;&#20307;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#33258;&#21161;&#21453;&#39304;&#65292;&#24182;&#21487;&#19982;&#31532;&#19968;&#25110;&#31532;&#20108;&#20215;&#26684;&#25293;&#21334;&#20197;&#21450;&#20219;&#20309;&#8220;&#20013;&#38388;&#8221;&#25293;&#21334;&#26684;&#24335;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24403;&#36825;&#20123;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#30456;&#20114;&#31454;&#20105;&#26102;&#65292;&#25152;&#26377;&#36718;&#27425;&#30340;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160; welfare &#37117;&#33267;&#23569;&#36798;&#21040;&#20102;&#20219;&#20309;&#20998;&#37197;&#25152;&#23454;&#29616;&#30340;&#26399;&#26395;&#26368;&#20248;&#27969;&#21160; welfare &#30340;&#19968;&#21322;&#12290;&#36825;&#22312;&#20986;&#20215;&#21160;&#24577;&#26159;&#21542;&#25910;&#25947;&#21040;&#22343;&#34913;&#20197;&#21450;&#24191;&#21578;&#20027;&#20272;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#22914;&#20309;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
&lt;/p&gt;</description></item></channel></rss>