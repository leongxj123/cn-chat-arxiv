<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#36890;&#36807;&#26368;&#20248;&#36816;&#36755;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33268;&#20559;&#22909;&#30340;&#21305;&#37197;&#24066;&#22330;&#30340;&#31283;&#23450;&#24615;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#31561;&#35774;&#35745;&#30446;&#26631;&#65292;&#25581;&#31034;&#20102;&#21305;&#37197;&#32467;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13378</link><description>&lt;p&gt;
&#31283;&#23450;&#21305;&#37197;&#20316;&#20026;&#36816;&#36755;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stable matching as transportation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36816;&#36755;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33268;&#20559;&#22909;&#30340;&#21305;&#37197;&#24066;&#22330;&#30340;&#31283;&#23450;&#24615;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#31561;&#35774;&#35745;&#30446;&#26631;&#65292;&#25581;&#31034;&#20102;&#21305;&#37197;&#32467;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33268;&#20559;&#22909;&#30340;&#21305;&#37197;&#24066;&#22330;&#65292;&#24182;&#24314;&#31435;&#20102;&#31283;&#23450;&#24615;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#31561;&#20849;&#21516;&#35774;&#35745;&#30446;&#26631;&#19982;&#26368;&#20248;&#36816;&#36755;&#29702;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26368;&#20248;&#36816;&#36755;&#20026;&#36861;&#27714;&#36825;&#20123;&#30446;&#26631;&#33719;&#24471;&#30340;&#21305;&#37197;&#30340;&#32467;&#26500;&#29305;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#26377;&#19968;&#33268;&#20559;&#22909;&#30340;&#21305;&#37197;&#24066;&#22330;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#30340;&#20379;&#38656;&#19981;&#24179;&#34913;&#65292;&#27604;&#22914;&#20249;&#20276;&#20851;&#31995;&#24418;&#25104;&#12289;&#23398;&#26657;&#36873;&#25321;&#12289;&#22120;&#23448;&#25424;&#36192;&#20132;&#25442;&#65292;&#20197;&#21450;&#22312;&#21305;&#37197;&#24418;&#25104;&#21518;&#36827;&#34892;&#36716;&#31227;&#35848;&#21028;&#30340;&#21487;&#36716;&#35753;&#25928;&#29992;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13378v1 Announce Type: new  Abstract: We study matching markets with aligned preferences and establish a connection between common design objectives -- stability, efficiency, and fairness -- and the theory of optimal transport. Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives. Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.09806</link><description>&lt;p&gt;
Logit-Q&#21160;&#21147;&#23398;&#23545;&#20110;&#38543;&#26426;&#22242;&#38431;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Logit-Q Dynamics for Efficient Learning in Stochastic Teams. (arXiv:2302.09806v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#19968;&#20010;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;Logit-Q&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Logit-Q&#21160;&#21147;&#23398;&#23545;&#32431;&#23450;&#24577;&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21160;&#21147;&#23398;&#22312;&#22870;&#21169;&#20989;&#25968;&#23548;&#33268;&#28508;&#22312;&#21338;&#24328;&#30340;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#28982;&#32780;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#29366;&#24577;&#36716;&#25442;&#36229;&#20986;&#38543;&#26426;&#22242;&#38431;&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#21160;&#21147;&#23398;&#19982;&#19968;&#20010;&#34394;&#26500;&#30340;&#22330;&#26223;&#36817;&#20284;&#65292;&#20854;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#20165;&#22312;&#26377;&#38480;&#38271;&#24230;&#30340;&#32426;&#20803;&#20013;&#26159;&#23450;&#24577;&#30340;&#65292;&#20165;&#29992;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#35201;&#22330;&#26223;&#21644;&#34394;&#26500;&#22330;&#26223;&#20013;&#30340;&#21160;&#21147;&#23398;&#32806;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#36825;&#20004;&#20010;&#22330;&#26223;&#30001;&#20110;&#36880;&#27493;&#20943;&#23567;&#30340;&#27493;&#38271;&#32780;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two logit-Q learning dynamics combining the classical and independent log-linear learning updates with an on-policy value iteration update for efficient learning in stochastic games. We show that the logit-Q dynamics presented reach (near) efficient equilibrium in stochastic teams. We quantify a bound on the approximation error. We also show the rationality of the logit-Q dynamics against agents following pure stationary strategies and the convergence of the dynamics in stochastic games where the reward functions induce potential games, yet only a single agent controls the state transitions beyond stochastic teams. The key idea is to approximate the dynamics with a fictional scenario where the Q-function estimates are stationary over finite-length epochs only for analysis. We then couple the dynamics in the main and fictional scenarios to show that these two scenarios become more and more similar across epochs due to the vanishing step size.
&lt;/p&gt;</description></item></channel></rss>