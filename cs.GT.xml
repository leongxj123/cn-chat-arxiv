<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03515</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28608;&#21169;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#35757;&#32451;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#22402;&#30452;&#21010;&#20998;&#30340;&#31169;&#26377;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;VFL&#35774;&#32622;&#20013;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#26041;&#65288;&#25317;&#26377;&#24102;&#26631;&#31614;&#26679;&#26412;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#36890;&#36807;&#19982;&#26576;&#20123;&#34987;&#21160;&#26041;&#65288;&#25317;&#26377;&#30456;&#21516;&#26679;&#26412;&#20294;&#27809;&#26377;&#26631;&#31614;&#30340;&#39069;&#22806;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#21512;&#20316;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28608;&#21169;&#34987;&#21160;&#26041;&#21442;&#19982;VFL&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#34987;&#21160;&#26041;&#22312;VFL&#36807;&#31243;&#20013;&#30340;&#36129;&#29486;&#26469;&#20026;&#20182;&#20204;&#20998;&#37197;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#26680;&#24515;&#28216;&#25103;&#35770;&#27010;&#24565;&#30340;&#19968;&#31181;&#21464;&#20307;&#8212;&#8212;&#30772;&#20135;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22612;&#26408;&#24503;&#21010;&#20998;&#35268;&#21017;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#30830;&#20445;&#20102;&#28608;&#21169;&#30340;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.16272</link><description>&lt;p&gt;
&#22312;&#21327;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#28608;&#21169;&#31454;&#20105;&#23545;&#25163;&#35802;&#23454;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#27604;&#20165;&#21033;&#29992;&#21333;&#19968;&#25968;&#25454;&#28304;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#30340;&#21442;&#19982;&#32773;&#26159;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#22914;&#27599;&#20010;&#37117;&#24076;&#26395;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#25512;&#33616;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20844;&#21496;&#12290;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#19981;&#35802;&#23454;&#30340;&#26356;&#26032;&#65292;&#25439;&#23475;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#33021;&#30772;&#22351;&#21327;&#20316;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#31181;&#20132;&#20114;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20869;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#65306;&#21333;&#36718;&#22343;&#20540;&#20272;&#35745;&#21644;&#24378;&#20984;&#30446;&#26631;&#30340;&#22810;&#36718; SGD&#12290;&#23545;&#20110;&#19968;&#31867;&#33258;&#28982;&#30340;&#21442;&#19982;&#32773;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#29702;&#24615;&#30340;&#23458;&#25143;&#20250;&#34987;&#28608;&#21169;&#24378;&#28872;&#22320;&#25805;&#32437;&#20182;&#20204;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
&lt;/p&gt;</description></item></channel></rss>