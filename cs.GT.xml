<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.02347</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Federated Learning Algorithms without Data Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#20256;&#32479;&#19978;&#34987;&#24191;&#27867;&#20381;&#36182;&#20110;&#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#26681;&#25454;&#25968;&#25454;&#30456;&#20284;&#24615;&#31243;&#24230;&#24494;&#35843;&#27493;&#38271;&#12290;&#24403;&#25968;&#25454;&#30456;&#20284;&#24615;&#36739;&#20302;&#26102;&#65292;&#36825;&#20123;&#23567;&#27493;&#38271;&#20250;&#23548;&#33268;&#32852;&#37030;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#21487;&#25509;&#21463;&#22320;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#19968;&#20010;&#19981;&#31561;&#24335;&#19978;&#65292;&#36825;&#20010;&#19981;&#31561;&#24335;&#25429;&#25417;&#20102;&#27493;&#38271;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#23450;&#29702;&#24212;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65306;&#22266;&#23450;&#27493;&#38271;&#12289;&#36882;&#20943;&#27493;&#38271;&#21644;&#27493;&#34928;&#20943;&#27493;&#38271;&#65292;&#36825;&#20123;&#34920;&#36798;&#24335;&#29420;&#31435;&#20110;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20132;&#26131;&#25163;&#32493;&#36153;&#26426;&#21046;&#35774;&#35745;&#20013;&#30340;&#38450;&#21246;&#32467;&#24615;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#22810;&#20010;&#35201;&#27714;&#21644;&#23646;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#23384;&#22312;&#20132;&#26131;&#31454;&#20105;&#26102;&#65292;&#20219;&#20309;TFM&#37117;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#35201;&#27714;&#21644;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09321</link><description>&lt;p&gt;
&#20132;&#26131;&#25163;&#32493;&#36153;&#26426;&#21046;&#35774;&#35745;&#20013;&#30340;&#38450;&#21246;&#32467;&#24615;
&lt;/p&gt;
&lt;p&gt;
Collusion-Resilience in Transaction Fee Mechanism Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20132;&#26131;&#25163;&#32493;&#36153;&#26426;&#21046;&#35774;&#35745;&#20013;&#30340;&#38450;&#21246;&#32467;&#24615;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#22810;&#20010;&#35201;&#27714;&#21644;&#23646;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#23384;&#22312;&#20132;&#26131;&#31454;&#20105;&#26102;&#65292;&#20219;&#20309;TFM&#37117;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#35201;&#27714;&#21644;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21306;&#22359;&#38142;&#21327;&#35758;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#20132;&#26131;&#25163;&#32493;&#36153;&#26426;&#21046;&#65288;TFM&#65289;&#36827;&#34892;&#31454;&#26631;&#65292;&#20197;&#20415;&#23558;&#20854;&#20132;&#26131;&#21253;&#21547;&#24182;&#33719;&#24471;&#30830;&#35748;&#12290;Roughgarden&#65288;EC'21&#65289;&#23545;TFM&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#22788;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#35201;&#27714;&#65306;&#29992;&#25143;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;UIC&#65289;&#65292;&#30719;&#24037;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;MIC&#65289;&#20197;&#21450;&#19968;&#31181;&#31216;&#20026;OCA-proofness&#30340;&#38450;&#21246;&#32467;&#24615;&#24418;&#24335;&#12290;&#24403;&#27809;&#26377;&#20132;&#26131;&#20043;&#38388;&#30340;&#31454;&#20105;&#26102;&#65292;Ethereum&#30340;EIP-1559&#26426;&#21046;&#21516;&#26102;&#28385;&#36275;&#36825;&#19977;&#20010;&#23646;&#24615;&#65292;&#20294;&#24403;&#26377;&#36807;&#22810;&#30340;&#31526;&#21512;&#26465;&#20214;&#30340;&#20132;&#26131;&#26080;&#27861;&#25918;&#20837;&#21333;&#20010;&#21306;&#22359;&#26102;&#65292;&#22833;&#21435;&#20102;UIC&#23646;&#24615;&#12290;Chung&#21644;Shi&#65288;SODA'23&#65289;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#38450;&#21246;&#32467;&#24615;&#27010;&#24565;&#65292;&#31216;&#20026;c-side-construct-proofness(c-SCP)&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#20132;&#26131;&#20043;&#38388;&#23384;&#22312;&#31454;&#20105;&#26102;&#65292;&#20219;&#20309;TFM&#37117;&#19981;&#33021;&#28385;&#36275;UIC&#12289;MIC&#21644;&#33267;&#23569;&#20026;1&#30340;&#20219;&#20309;c&#30340;c-SCP&#12290;OCA-proofness&#26029;&#35328;&#29992;&#25143;&#21644;&#30719;&#24037;&#19981;&#24212;&#35813;&#33021;&#22815;&#20174;&#21327;&#35758;&#20013;&#8220;&#20599;&#21462;&#8221;&#65292;&#24182;&#19988;&#22312;&#30452;&#35273;&#19978;&#27604;UIC&#12289;MIC&#26356;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09321v1 Announce Type: cross Abstract: Users bid in a transaction fee mechanism (TFM) to get their transactions included and confirmed by a blockchain protocol. Roughgarden (EC'21) initiated the formal treatment of TFMs and proposed three requirements: user incentive compatibility (UIC), miner incentive compatibility (MIC), and a form of collusion-resilience called OCA-proofness. Ethereum's EIP-1559 mechanism satisfies all three properties simultaneously when there is no contention between transactions, but loses the UIC property when there are too many eligible transactions to fit in a single block. Chung and Shi (SODA'23) considered an alternative notion of collusion-resilience, called c-side-constract-proofness (c-SCP), and showed that, when there is contention between transactions, no TFM can satisfy UIC, MIC, and c-SCP for any c at least 1. OCA-proofness asserts that the users and a miner should not be able to "steal from the protocol" and is intuitively weaker than the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04971</link><description>&lt;p&gt;
&#22810;&#21457;&#20449;&#32773;&#35828;&#26381; - &#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-Sender Persuasion -- A Computational Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21040;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#22810;&#20010;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#20351;&#20854;&#37319;&#21462;&#26576;&#20123;&#34892;&#21160;&#12290;&#36825;&#20123;&#35774;&#32622;&#26159;&#35745;&#31639;&#32463;&#27982;&#23398;&#65292;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#26159;&#21457;&#20449;&#32773;&#20449;&#21495;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;&#22343;&#34913;&#26159;PPAD-Hard&#30340;;&#23454;&#38469;&#19978;&#65292;&#35745;&#31639;&#19968;&#20010;&#21457;&#20449;&#32773;&#30340;&#26368;&#20339;&#21709;&#24212;&#29978;&#33267;&#26159;NP-Hard&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36716;&#32780;&#23547;&#25214;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35813;&#28216;&#25103;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#25928;&#29992;&#12290;&#32467;&#21512;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#23637;&#31034;&#22343;&#34913;&#21644;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#30340;&#23616;&#37096;&#22343;&#34913;&#12290;&#24191;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#36129;&#29486;&#23545;&#24191;&#27867;&#30340;&#31867;&#21035;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
&lt;/p&gt;</description></item></channel></rss>