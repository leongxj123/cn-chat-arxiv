<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#26368;&#20248;&#39044;&#31639;&#32858;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#32500;&#27867;&#21270;&#30340;&#26143;&#24418;&#25928;&#29992;&#20989;&#25968;&#31867;&#21035;&#20013;&#25506;&#35752;&#19981;&#21516;&#27169;&#22411;&#12290;&#23545;&#20110;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#32479;&#19968;&#24187;&#24433;&#26426;&#21046;&#26159;&#21807;&#19968;&#28385;&#36275;&#27604;&#20363;&#24615;&#30340;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#36229;&#36807;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#30340;&#24773;&#20917;&#65292;&#35770;&#25991;&#34920;&#26126;&#19981;&#23384;&#22312;&#21516;&#26102;&#28385;&#36275;&#25928;&#29575;&#12289;&#31574;&#30053;&#24615;&#21644;&#27604;&#20363;&#24615;&#30340;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15904</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#26368;&#20248;&#39044;&#31639;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Budget Aggregation with Single-Peaked Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15904
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#26368;&#20248;&#39044;&#31639;&#32858;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#32500;&#27867;&#21270;&#30340;&#26143;&#24418;&#25928;&#29992;&#20989;&#25968;&#31867;&#21035;&#20013;&#25506;&#35752;&#19981;&#21516;&#27169;&#22411;&#12290;&#23545;&#20110;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#32479;&#19968;&#24187;&#24433;&#26426;&#21046;&#26159;&#21807;&#19968;&#28385;&#36275;&#27604;&#20363;&#24615;&#30340;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#36229;&#36807;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#30340;&#24773;&#20917;&#65292;&#35770;&#25991;&#34920;&#26126;&#19981;&#23384;&#22312;&#21516;&#26102;&#28385;&#36275;&#25928;&#29575;&#12289;&#31574;&#30053;&#24615;&#21644;&#27604;&#20363;&#24615;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#20998;&#24067;&#65288;&#22914;&#39044;&#31639;&#25552;&#26696;&#65289;&#32858;&#21512;&#25104;&#38598;&#20307;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#29702;&#24819;&#30340;&#32858;&#21512;&#26426;&#21046;&#24212;&#35813;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#12289;&#31574;&#30053;&#35777;&#26126;&#21644;&#20844;&#24179;&#30340;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#20195;&#29702;&#26681;&#25454;&#20854;&#29702;&#24819;&#39044;&#31639;&#19982;$\ell_1$&#36317;&#31163;&#26469;&#35780;&#20272;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#26469;&#33258;&#26143;&#24418;&#25928;&#29992;&#20989;&#25968;&#26356;&#22823;&#31867;&#21035;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#36825;&#26159;&#21333;&#23792;&#20559;&#22909;&#30340;&#22810;&#32500;&#27867;&#21270;&#12290;&#23545;&#20110;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#22312;&#38750;&#24120;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#32479;&#19968;&#24187;&#24433;&#26426;&#21046;&#26159;&#21807;&#19968;&#28385;&#36275;&#27604;&#20363;&#24615;&#30340;&#31574;&#30053;&#35777;&#26126;&#26426;&#21046;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#23545;&#20110;&#36229;&#36807;&#20004;&#31181;&#22791;&#36873;&#26041;&#26696;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23545;$\ell_1$&#21644;$\ell_\infty$&#30340;&#19981;&#28385;&#24847;&#24615;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#19981;&#21487;&#33021;&#24615;&#65306;&#27809;&#26377;&#26426;&#21046;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#25928;&#29575;&#12289;&#31574;&#30053;&#24615;&#21644;&#27604;&#20363;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15904v1 Announce Type: new  Abstract: We study the problem of aggregating distributions, such as budget proposals, into a collective distribution. An ideal aggregation mechanism would be Pareto efficient, strategyproof, and fair. Most previous work assumes that agents evaluate budgets according to the $\ell_1$ distance to their ideal budget. We investigate and compare different models from the larger class of star-shaped utility functions - a multi-dimensional generalization of single-peaked preferences. For the case of two alternatives, we extend existing results by proving that under very general assumptions, the uniform phantom mechanism is the only strategyproof mechanism that satisfies proportionality - a minimal notion of fairness introduced by Freeman et al. (2021). Moving to the case of more than two alternatives, we establish sweeping impossibilities for $\ell_1$ and $\ell_\infty$ disutilities: no mechanism satisfies efficiency, strategyproofness, and proportionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.09806</link><description>&lt;p&gt;
Logit-Q&#21160;&#21147;&#23398;&#23545;&#20110;&#38543;&#26426;&#22242;&#38431;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Logit-Q Dynamics for Efficient Learning in Stochastic Teams. (arXiv:2302.09806v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#19968;&#20010;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;Logit-Q&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Logit-Q&#21160;&#21147;&#23398;&#23545;&#32431;&#23450;&#24577;&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21160;&#21147;&#23398;&#22312;&#22870;&#21169;&#20989;&#25968;&#23548;&#33268;&#28508;&#22312;&#21338;&#24328;&#30340;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#28982;&#32780;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#29366;&#24577;&#36716;&#25442;&#36229;&#20986;&#38543;&#26426;&#22242;&#38431;&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#21160;&#21147;&#23398;&#19982;&#19968;&#20010;&#34394;&#26500;&#30340;&#22330;&#26223;&#36817;&#20284;&#65292;&#20854;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#20165;&#22312;&#26377;&#38480;&#38271;&#24230;&#30340;&#32426;&#20803;&#20013;&#26159;&#23450;&#24577;&#30340;&#65292;&#20165;&#29992;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#35201;&#22330;&#26223;&#21644;&#34394;&#26500;&#22330;&#26223;&#20013;&#30340;&#21160;&#21147;&#23398;&#32806;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#36825;&#20004;&#20010;&#22330;&#26223;&#30001;&#20110;&#36880;&#27493;&#20943;&#23567;&#30340;&#27493;&#38271;&#32780;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two logit-Q learning dynamics combining the classical and independent log-linear learning updates with an on-policy value iteration update for efficient learning in stochastic games. We show that the logit-Q dynamics presented reach (near) efficient equilibrium in stochastic teams. We quantify a bound on the approximation error. We also show the rationality of the logit-Q dynamics against agents following pure stationary strategies and the convergence of the dynamics in stochastic games where the reward functions induce potential games, yet only a single agent controls the state transitions beyond stochastic teams. The key idea is to approximate the dynamics with a fictional scenario where the Q-function estimates are stationary over finite-length epochs only for analysis. We then couple the dynamics in the main and fictional scenarios to show that these two scenarios become more and more similar across epochs due to the vanishing step size.
&lt;/p&gt;</description></item></channel></rss>