<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00188</link><description>&lt;p&gt;
&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Decentralized Learning on Player Utilities in Stackelberg Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00188
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26102;&#65292;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#21453;&#22797;&#19982;&#21478;&#19968;&#20010;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#29992;&#25143;&#65289;&#20132;&#20114;&#12290;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#21452;&#20195;&#29702;&#31995;&#32479;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#21333;&#29420;&#23398;&#20064;&#65292;&#32780;&#20004;&#20010;&#20195;&#29702;&#30340;&#22870;&#21169;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31867;&#24773;&#20917;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20195;&#29702;&#31995;&#32479;&#30340;&#23398;&#20064;&#21160;&#24577;&#20197;&#21450;&#23545;&#27599;&#20010;&#20195;&#29702;&#30446;&#26631;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31995;&#32479;&#24314;&#27169;&#20026;&#20855;&#26377;&#20998;&#25955;&#23398;&#20064;&#30340;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#65292;&#24182;&#23637;&#31034;&#26631;&#20934;&#36951;&#25022;&#22522;&#20934;&#65288;&#22914;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#34913;&#22238;&#25253;&#65289;&#23548;&#33268;&#33267;&#23569;&#26377;&#19968;&#21517;&#29609;&#23478;&#20986;&#29616;&#26368;&#22351;&#24773;&#20917;&#30340;&#32447;&#24615;&#36951;&#25022;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#23545;&#20195;&#29702;&#30340;&#23398;&#20064;&#35823;&#24046;&#23481;&#24525;&#30340;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#23398;&#20064;&#31639;&#27861;&#26410;&#33021;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#24320;&#21457;&#20102;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#21452;&#26041;&#29609;&#23478;&#32780;&#35328;&#19982;&#29702;&#24819;$O(T^{2/3})$&#36951;&#25022;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02761</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#22312;&#26410;&#30693;&#25104;&#26412;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
One-Shot Strategic Classification Under Unknown Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#20998;&#31867;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#23545;&#31574;&#30053;&#36755;&#20837;&#25805;&#32437;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20915;&#31574;&#35268;&#21017;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#36825;&#20123;&#21709;&#24212;&#26159;&#24050;&#30693;&#30340;&#65307;&#32780;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22788;&#29702;&#26410;&#30693;&#21709;&#24212;&#65292;&#20294;&#23427;&#20204;&#19987;&#38376;&#30740;&#31350;&#37325;&#22797;&#27169;&#22411;&#37096;&#32626;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#25919;&#31574;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#28608;&#21169;&#29992;&#20363;&#20013;&#65292;&#22810;&#27425;&#37096;&#32626;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29978;&#33267;&#19968;&#20010;&#31967;&#31957;&#30340;&#36718;&#27425;&#37117;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#30340;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#27491;&#24335;&#30740;&#31350;&#65292;&#36825;&#38656;&#35201;&#22312;&#19968;&#27425;&#24615;&#36873;&#25321;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#30528;&#37325;&#20851;&#27880;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#23545;&#30495;&#23454;&#25104;&#26412;&#30340;&#23567;&#35823;&#24046;&#20063;&#21487;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#33267;&#26497;&#20302;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#26694;&#23450;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02761v2 Announce Type: replace  Abstract: The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use case$\unicode{x2014}$where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of one-shot strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, with the goal of identifying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.07479</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;TikTok&#21644;YouTube&#36825;&#26679;&#30340;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#24179;&#21488;&#30340;&#20915;&#31574;&#31639;&#27861;&#22609;&#36896;&#20102;&#20869;&#23481;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#65292;&#21253;&#25324;&#29983;&#20135;&#32773;&#22312;&#20869;&#23481;&#36136;&#37327;&#19978;&#25237;&#20837;&#22810;&#23569;&#21162;&#21147;&#12290;&#35768;&#22810;&#24179;&#21488;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#20250;&#20135;&#29983;&#36328;&#26102;&#38388;&#30340;&#28608;&#21169;&#65292;&#22240;&#20026;&#20170;&#22825;&#29983;&#20135;&#30340;&#20869;&#23481;&#20250;&#24433;&#21709;&#26410;&#26469;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20135;&#29983;&#30340;&#28608;&#21169;&#65292;&#20998;&#26512;&#20102;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#29983;&#20135;&#30340;&#20869;&#23481;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20687;Hedge&#21644;EXP3&#36825;&#26679;&#30340;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#22320;&#65292;&#20869;&#23481;&#36136;&#37327;&#22312;&#23398;&#20064;&#29575;&#26041;&#38754;&#26377;&#19978;&#38480;&#65292;&#24182;&#19988;&#38543;&#30528;&#20856;&#22411;&#23398;&#20064;&#29575;&#36827;&#23637;&#32780;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#24809;&#32602;&#21019;&#24314;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#29983;&#20135;&#32773;&#8212;&#8212;&#27491;&#30830;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#31574;&#30053;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
&lt;/p&gt;</description></item></channel></rss>