<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.12944</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning in Mean Field Games: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12944
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21512;&#20316;&#21644;&#21512;&#20316;&#28216;&#25103;&#22312;&#25317;&#26377;&#22823;&#37327;&#29609;&#23478;&#26102;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#22343;&#22330;&#21338;&#24328;(Mean Field Games, MFGs)&#30001;Lasry&#21644;Lions&#20197;&#21450;Huang&#65292;Caines&#21644;Malham\'e&#24341;&#20837;&#65292;&#20381;&#38752;&#22343;&#22330;&#36817;&#20284;&#20801;&#35768;&#29609;&#23478;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#20123;&#28216;&#25103;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35299;&#20915;&#24102;&#26377;&#23545;&#27169;&#22411;&#30340;&#23436;&#20840;&#20102;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#20986;&#29616;&#22312;&#35299;&#20915;&#35268;&#27169;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;RL&#21644;MFGs&#30340;&#32467;&#21512;&#26377;&#26395;&#35299;&#20915;&#22312;&#20154;&#21475;&#35268;&#27169;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#24222;&#22823;&#30340;&#28216;&#25103;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#36805;&#36895;&#22686;&#38271;&#30340;&#20851;&#20110;RL&#26041;&#27861;&#22312;MFGs&#20013;&#23398;&#20064;&#22343;&#34913;&#21644;&#31038;&#20132;&#26368;&#20248;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;M&#20013;&#26368;&#24120;&#35265;&#30340;&#35774;&#32622;(&#38745;&#24577;&#12289;&#31283;&#24577;&#21644;&#36827;&#21270;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.00322</link><description>&lt;p&gt;
&#32418;&#38431;&#28216;&#25103;&#65306;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37096;&#32626;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24517;&#39035;&#31526;&#21512;&#26377;&#30410;&#21644;&#26080;&#23475;&#24615;&#30340;&#26631;&#20934;&#65292;&#20174;&#32780;&#23454;&#29616;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#32418;&#38431;&#25216;&#26415;&#26159;&#23454;&#29616;&#36825;&#19968;&#26631;&#20934;&#30340;&#20851;&#38190;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#25163;&#21160;&#32418;&#38431;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#23545;&#25239;&#25552;&#31034;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#65292;&#38480;&#21046;&#20102;&#22312;&#21487;&#37327;&#21270;&#24230;&#37327;&#21644;&#25910;&#25947;&#20445;&#35777;&#19979;&#23545;LLM&#36827;&#34892;&#22810;&#26679;&#25915;&#20987;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#12290;RTG&#26088;&#22312;&#20998;&#26512;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#22312;RTG&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#12290;GRTS&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#32418;&#38431;&#28216;&#25103;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.06027</link><description>&lt;p&gt;
&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20195;&#29702;&#21830;&#19982;&#22810;&#20010;&#23545;&#31435;&#20195;&#29702;&#21830;&#36827;&#34892;&#25112;&#30053;&#20114;&#21160;&#65292;&#23545;&#25163;&#21487;&#33021;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#24773;&#22659;&#65292;&#35774;&#35745;&#20195;&#29702;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#35745;&#31639;&#25110;&#36924;&#36817;&#30456;&#20851;&#30340;&#21338;&#24328;&#29702;&#35770;&#35299;&#65292;&#22914;&#32435;&#20160;&#22343;&#34913;&#65292;&#28982;&#21518;&#36981;&#24490;&#35268;&#23450;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#24573;&#30053;&#20102;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#20219;&#20309;&#35266;&#23519;&#65292;&#36825;&#20123;&#35266;&#23519;&#21487;&#33021;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#25910;&#38598;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#23545;&#19977;&#20154; Kuhn &#25169;&#20811;&#23637;&#24320;&#20102;&#23545;&#35768;&#22810;&#30495;&#23454;&#23545;&#25163;&#21644;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
&lt;/p&gt;</description></item></channel></rss>