<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11256</link><description>&lt;p&gt;
&#22312;&#27714;&#35299;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24724;&#31639;&#27861;&#22312;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#26631;&#20934;&#22411;&#28216;&#25103;&#21644;&#25193;&#23637;&#22411;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#32771;&#34385;&#20102;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#25910;&#25947;&#30340;&#26080;&#24724;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#26368;&#26377;&#21517;&#30340;&#20004;&#20010;&#31639;&#27861;&#26159;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#12290;&#28982;&#32780;&#65292;OGDA&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;OMWU&#20855;&#26377;&#36739;&#20302;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20294;&#23454;&#39564;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#23427;&#30340;&#25910;&#25947;&#20165;&#22312;&#32435;&#20160;&#22343;&#34913;&#21807;&#19968;&#26102;&#25104;&#31435;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#29992;&#20110;MWU&#65292;&#23427;&#28040;&#38500;&#20102;&#21807;&#19968;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#19982;OMWU&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;RT&#30340;&#31639;&#27861;&#22312;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#19979;&#34920;&#29616;&#19981;&#22914;OGDA&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21453;&#39304;&#20551;&#35774;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;RT&#26694;&#26550;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
&lt;/p&gt;</description></item></channel></rss>