<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08906</link><description>&lt;p&gt;
&#38024;&#23545;Q-&#23398;&#20064;&#32773;&#30340;&#31574;&#30053;&#21270;&#23545;&#25239;&#65306;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strategizing against Q-learners: A Control-theoretical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08906
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;(&#19968;&#31181;&#32463;&#20856;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;)&#22312;&#28216;&#25103;&#20013;&#23545;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#22914;&#26524;&#31574;&#30053;&#24615;&#23545;&#25163;&#20102;&#35299;&#23545;&#25163;&#30340;Q-learning&#31639;&#27861;&#65292;&#22905;&#21487;&#20197;&#21033;&#29992;&#19968;&#20010;&#22825;&#30495;&#30340;Q-&#23398;&#20064;&#32773;&#22810;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#34892;&#20026;&#32773;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(&#20855;&#26377;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;Q&#20540;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;)&#65292;&#23601;&#22909;&#20687;Q-&#23398;&#20064;&#31639;&#27861;&#26159;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#21270;&#30340;&#36817;&#20284;&#26041;&#26696;&#26469;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08906v1 Announce Type: cross  Abstract: In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11700</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs. (arXiv:2302.11700v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#23398;&#20064;&#29702;&#35770;&#21644;&#35745;&#31639;&#32463;&#27982;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#36817;&#24180;&#26469;&#34028;&#21187;&#21457;&#23637;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#25512;&#36827;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31867;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;&#65292;&#20998;&#21035;&#26159;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#21069;&#32773;&#26159;&#19968;&#31867;&#26088;&#22312;&#38144;&#21806;&#22810;&#20010;&#29289;&#21697;&#30340;&#38543;&#26426;&#26426;&#21046;&#65292;&#24050;&#30693;&#33021;&#22815;&#23454;&#29616;&#36229;&#20986;&#30830;&#23450;&#24615;&#26426;&#21046;&#30340;&#25910;&#30410;&#65292;&#32780;&#21518;&#32773;&#21017;&#26159;&#38024;&#23545;&#38144;&#21806;&#21333;&#20010;&#29289;&#21697;&#22810;&#20010;&#21333;&#20301;&#65288;&#21103;&#26412;&#65289;&#30340;&#35774;&#35745;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65292;&#22914;&#27773;&#36710;&#25110;&#33258;&#34892;&#36710;&#20849;&#20139;&#26381;&#21153;&#31561;&#12290;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#20174;&#20080;&#23478;&#20272;&#20540;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25910;&#30410;&#30340;&#36825;&#31867;&#26426;&#21046;&#65292;&#28085;&#30422;&#22810;&#31181;&#20998;&#24067;&#35774;&#32622;&#65292;&#26082;&#26377;&#30452;&#25509;&#33719;&#24471;&#20080;&#23478;&#20272;&#20540;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20063;&#26377;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#30740;&#31350;&#36739;&#23569;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#20080;&#23478;&#19968;&#20010;&#25509;&#19968;&#20010;&#21040;&#26469;&#65292;&#24182;&#19988;&#23545;&#20182;&#20204;&#30340;&#20272;&#20540;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers' valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values.  Our main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs wit
&lt;/p&gt;</description></item></channel></rss>