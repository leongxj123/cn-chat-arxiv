<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item></channel></rss>