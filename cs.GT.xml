<rss version="2.0"><channel><title>Chat Arxiv cs.GT</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.GT</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.06879</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19979;&#30340;&#25191;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Neural Networks. (arXiv:2304.06879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#39044;&#27979;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#24182;&#24433;&#21709;&#20854;&#39044;&#27979;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#65292;&#21363;&#36866;&#29992;&#20110;&#20854;&#20135;&#29983;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#12290;&#22312;&#20351;&#29992;&#37325;&#22797;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#25910;&#25947;&#32467;&#26524;&#20013;&#65292;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#21487;Lipschitz&#36830;&#32493;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25439;&#22833;&#24517;&#39035;&#23545;&#36825;&#20123;&#21442;&#25968;&#24378;&#20984;&#21644;&#24179;&#28369;&#65307;&#21542;&#21017;&#65292;&#35813;&#26041;&#27861;&#23558;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#21457;&#25955;&#12290;&#28982;&#32780;&#26412;&#25991;&#21017;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#30340;&#65292;&#36825;&#26159;&#25191;&#34892;&#31995;&#32479;&#30340;&#26356;&#21152;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24314;&#27169;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#20854;&#26469;&#23454;&#35777;&#25191;&#34892;&#31283;&#23450;&#24615;&#30456;&#23545;&#20110;&#20854;&#20182;&#30446;&#26631;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction is a framework for learning models that influence the data they intend to predict. We focus on finding classifiers that are performatively stable, i.e. optimal for the data distribution they induce. Standard convergence results for finding a performatively stable classifier with the method of repeated risk minimization assume that the data distribution is Lipschitz continuous to the model's parameters. Under this assumption, the loss must be strongly convex and smooth in these parameters; otherwise, the method will diverge for some problems. In this work, we instead assume that the data distribution is Lipschitz continuous with respect to the model's predictions, a more natural assumption for performative systems. As a result, we are able to significantly relax the assumptions on the loss function. In particular, we do not need to assume convexity with respect to the model's parameters. As an illustration, we introduce a resampling procedure that models realisti
&lt;/p&gt;</description></item></channel></rss>