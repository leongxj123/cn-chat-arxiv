# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models.](http://arxiv.org/abs/2310.07937) | Co-NavGPT是一个创新的框架，使用大型语言模型作为全局规划器，实现多机器人合作的视觉目标导航。在实验中表现出了超越现有模型的成功率和效率，展示了大型语言模型的巨大潜力。 |

# 详细

[^1]: Co-NavGPT: 使用大型语言模型的多机器人合作视觉语义导航

    Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])

    [http://arxiv.org/abs/2310.07937](http://arxiv.org/abs/2310.07937)

    Co-NavGPT是一个创新的框架，使用大型语言模型作为全局规划器，实现多机器人合作的视觉目标导航。在实验中表现出了超越现有模型的成功率和效率，展示了大型语言模型的巨大潜力。

    

    在高级人机交互任务中，对于自主机器人在未知环境中进行视觉目标导航至关重要。尽管过去已经开发了许多方法，但大多数都是设计用于单一机器人操作，这往往由于环境复杂性而导致效率和鲁棒性降低。此外，学习多机器人协作的策略需要资源密集型。为了解决这些挑战，我们提出了Co-NavGPT，这是一个创新的框架，将大型语言模型(LLMs)作为多机器人合作视觉目标导航的全局规划器。Co-NavGPT将探索的环境数据编码为提示，增强LLMs对场景的理解。然后，它为每个机器人分配探索前沿以实现高效的目标搜索。在Habitat-Matterport 3D (HM3D)上的实验结果表明，Co-NavGPT在成功率和效率方面超过了现有模型，而无需任何学习过程，展示了LLMs的巨大潜力。

    In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
    

