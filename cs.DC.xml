<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01468</link><description>&lt;p&gt;
DLRover&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#21160;&#20316;&#19994;&#36164;&#28304;&#25512;&#33616;&#30340;&#24377;&#24615;&#28145;&#24230;&#35757;&#32451;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DLRover: An Elastic Deep Training Extension with Auto Job Resource Recommendation. (arXiv:2304.01468v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01468
&lt;/p&gt;
&lt;p&gt;
DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#20113;&#24179;&#21488;&#19978;&#36827;&#34892;&#36164;&#28304;&#20849;&#20139;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#65292;&#22240;&#27492;&#20113;&#20173;&#28982;&#26159;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#35757;&#32451;&#20316;&#19994;&#30340;&#27969;&#34892;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#27492;&#31867;&#20849;&#20139;&#20063;&#20026;DL&#35757;&#32451;&#20316;&#19994;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#65292;&#20363;&#22914;&#39640;&#20248;&#20808;&#32423;&#20316;&#19994;&#21487;&#33021;&#20250;&#24433;&#21709;&#12289;&#29978;&#33267;&#20013;&#26029;&#20302;&#20248;&#20808;&#32423;&#20316;&#19994;&#12290;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#31995;&#32479;&#35201;&#27714;&#29992;&#25143;&#22312;&#20316;&#19994;&#25552;&#20132;&#20043;&#21069;&#25163;&#21160;&#37197;&#32622;&#20316;&#19994;&#30340;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#30340;&#33410;&#28857;&#25968;&#21644;CPU&#12289;&#20869;&#23384;&#31561;&#36164;&#28304;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#22312;&#36816;&#34892;&#26102;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;&#20316;&#19994;&#30340;&#36164;&#28304;&#37197;&#32622;&#20250;&#28145;&#21051;&#24433;&#21709;&#35813;&#20316;&#19994;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#35757;&#32451;&#21534;&#21520;&#37327;&#12289;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#23436;&#25104;&#29575;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#20316;&#19994;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26080;&#27861;&#25552;&#20379;&#26368;&#20339;&#30340;&#36164;&#28304;&#37197;&#32622;&#12290;DLRover&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;DL&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#37197;&#32622;DL&#20316;&#19994;&#30340;&#21021;&#22987;&#36164;&#28304;&#24182;&#21160;&#24577;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cloud is still a popular platform for distributed deep learning (DL) training jobs since resource sharing in the cloud can improve resource utilization and reduce overall costs. However, such sharing also brings multiple challenges for DL training jobs, e.g., high-priority jobs could impact, even interrupt, low-priority jobs. Meanwhile, most existing distributed DL training systems require users to configure the resources (i.e., the number of nodes and resources like CPU and memory allocated to each node) of jobs manually before job submission and can not adjust the job's resources during the runtime. The resource configuration of a job deeply affect this job's performance (e.g., training throughput, resource utilization, and completion rate). However, this usually leads to poor performance of jobs since users fail to provide optimal resource configuration in most cases. \system~is a distributed DL framework can auto-configure a DL job's initial resources and dynamically tune the j
&lt;/p&gt;</description></item></channel></rss>