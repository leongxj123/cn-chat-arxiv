<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01302</link><description>&lt;p&gt;
&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Gradient-based Clustering of Distributed Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#22312;&#25552;&#20986;&#30340;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#21253;&#21547;&#19968;&#20010;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21482;&#19982;&#20854;&#30452;&#25509;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#23436;&#25972;&#25968;&#25454;&#30340;&#32858;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#23478;&#26063;&#31216;&#20026;&#20998;&#24067;&#24335;&#26799;&#24230;&#32858;&#31867;&#65288;DGC-$\mathcal{F}_\rho$&#65289;&#65292;&#30001;&#21442;&#25968;&#21270;&#30340;$\rho\geq1$&#30830;&#23450;&#65292;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#32780;$\mathcal{F}$&#30830;&#23450;&#32858;&#31867;&#25439;&#22833;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;&#32858;&#31867;&#25439;&#22833;&#22914;$K$&#22343;&#20540;&#21644;Huber&#25439;&#22833;&#65292;DGC-$\mathcal{F}_\rho$&#20135;&#29983;&#20102;&#26032;&#30340;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;DGC-KM$_\rho$&#21644;DGC-HL$_\rho$&#65292;&#32780;&#22522;&#20110;&#36923;&#36753;&#20989;&#25968;&#30340;&#26032;&#22411;&#32858;&#31867;&#25439;&#22833;&#23548;&#33268;&#20102;DGC-LL$_\rho$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#20998;&#26512;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#24378;&#32467;&#26524;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#12290;&#39318;&#20808;&#65292;&#26041;&#27861;&#29983;&#25104;&#30340;&#20013;&#24515;&#24207;&#21015;&#22312;&#20219;&#20309;&#20013;&#24515;&#21021;&#22987;&#21270;&#21644;$...
&lt;/p&gt;
&lt;p&gt;
We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$, controling the proximity of users' center estimates, with $\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $
&lt;/p&gt;</description></item><item><title>PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.11202</link><description>&lt;p&gt;
PartIR: &#20026;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;SPMD&#20998;&#21306;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11202
&lt;/p&gt;
&lt;p&gt;
PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#32467;&#21512;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20248;&#21270;&#22120;&#20998;&#29255;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#24403;&#31574;&#30053;&#21464;&#24471;&#22797;&#26434;&#26102;&#65292;&#20998;&#21306;&#24037;&#20855;&#38656;&#35201;&#20855;&#22791;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#34920;&#36798;&#21147;&#24378;&#65292;&#20801;&#35768;&#32452;&#21512;&#31616;&#21333;&#31574;&#30053;&#65307;2&#65289;&#21487;&#39044;&#27979;&#24615;&#24378;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PartIR&#65292;&#19968;&#31181;&#29992;&#20110;NN&#20998;&#21306;&#30340;&#35774;&#35745;&#12290;PartIR&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#19982;&#30828;&#20214;&#21644;&#36816;&#34892;&#26102;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;API&#29992;&#20110;&#32452;&#21512;&#20998;&#29255;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#25972;&#20010;&#36807;&#31243;&#30001;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#26082;&#21487;&#20197;&#25163;&#21160;&#20063;&#21487;&#20197;&#33258;&#21160;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#19982;&#27169;&#22411;&#20195;&#30721;&#20998;&#24320;&#25351;&#23450;&#65292;&#26131;&#20110;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#23637;&#31034;PartIR&#30340;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
&lt;/p&gt;</description></item></channel></rss>