<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: &#29992;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#35299;&#20915;&#21355;&#26143;&#35745;&#31639;&#20013;&#30340;&#19979;&#34892;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20256;&#24863;&#22120;&#30340;&#32435;&#21355;&#26143;&#26143;&#24231;&#25429;&#33719;&#22823;&#33539;&#22260;&#22320;&#29702;&#21306;&#22495;&#65292;&#20026;&#22320;&#29699;&#35266;&#27979;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#26143;&#24231;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20105;&#29992;&#24418;&#25104;&#20102;&#19979;&#34892;&#29942;&#39048;&#12290;&#36712;&#36947;&#36793;&#32536;&#35745;&#31639;&#65288;OEC&#65289;&#21033;&#29992;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#36890;&#36807;&#22312;&#28304;&#22836;&#22788;&#29702;&#21407;&#22987;&#25429;&#33719;&#26469;&#20943;&#23569;&#20256;&#36755;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#31895;&#31961;&#30340;&#36807;&#28388;&#26041;&#27861;&#25110;&#36807;&#20998;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FOOL&#65292;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20445;&#30041;&#39044;&#27979;&#24615;&#33021;&#12290;FOOL&#23558;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#21306;&#65292;&#20197;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#23884;&#20837;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36739;&#20302;&#30340;&#24320;&#38144;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#12290;&#34429;&#28982;FOOL&#26159;&#19968;&#31181;&#29305;&#24449;&#21387;&#32553;&#22120;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item></channel></rss>