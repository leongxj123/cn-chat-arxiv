<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06442</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#21327;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06442
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32771;&#34385;&#36164;&#28304;&#32422;&#26463;&#21644;&#19981;&#21516;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25910;&#38598;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#20256;&#24863;&#22120;/&#20195;&#29702;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;/&#20195;&#29702;&#26679;&#26412;&#26469;&#33258;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#38416;&#36848;&#20026;&#36153;&#33293;&#23572;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;&#25110;Cramer-Rao&#30028;&#26368;&#23567;&#21270;&#65289;&#38382;&#39064;&#12290;&#24403;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#22320;&#35782;&#21035;&#20986;&#20004;&#20010;&#29305;&#23450;&#24773;&#20917;&#65306;&#65288;1&#65289;&#19981;&#33021;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#36827;&#34892;&#21327;&#20316;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#65288;2&#65289;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#28041;&#21450;&#25237;&#36164;&#26377;&#38480;&#36164;&#28304;&#20197;&#21327;&#20316;&#37319;&#26679;&#21644;&#36716;&#31227;&#24050;&#30693;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
&lt;/p&gt;</description></item><item><title>FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;</title><link>http://arxiv.org/abs/2305.05920</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#20998;&#24067;&#24335;&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05920
&lt;/p&gt;
&lt;p&gt;
FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#21160;&#20102;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;&#26032;&#19968;&#20195;&#20114;&#21160;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#20132;&#20114;&#24615;&#35201;&#27714;&#27169;&#22411;&#25512;&#26029;&#30340;&#20302;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;&#29616;&#26377;&#30340;LLM&#26381;&#21153;&#31995;&#32479;&#20351;&#29992;&#30340;&#26159;&#36816;&#34892;&#21040;&#23436;&#25104;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#23384;&#22312;&#22836;&#37096;&#38459;&#22622;&#21644;&#38271;JCT&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastServe&#65292;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#12290;FastServe&#21033;&#29992;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#27169;&#24335;&#65292;&#20197;&#27599;&#20010;&#36755;&#20986;&#26631;&#35760;&#30340;&#31890;&#24230;&#23454;&#29616;&#25250;&#21344;&#24335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#35843;&#24230;&#22120;&#26368;&#23567;&#21270;JCT&#12290;&#22522;&#20110;LLM&#25512;&#29702;&#30340;&#26032;&#21322;&#20449;&#24687;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#36755;&#20837;&#38271;&#24230;&#20449;&#24687;&#26469;&#20026;&#27599;&#20010;&#21040;&#36798;&#20316;&#19994;&#20998;&#37197;&#36866;&#24403;&#30340;&#21021;&#22987;&#38431;&#21015;&#26469;&#36830;&#25509;&#12290;&#39640;&#20110;&#25152;&#36830;&#25509;&#38431;&#21015;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#34987;&#36339;&#36807;&#20197;&#20943;&#23569;&#38477;&#32423;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GPU&#20869;&#23384;&#31649;&#29702;&#26426;&#21046;&#65292;&#20197;&#25552;&#21069;&#28165;&#38500;&#19981;&#20877;&#20351;&#29992;&#30340;GPU&#32531;&#23384;&#65292;&#24182;&#23545;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#32531;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
&lt;/p&gt;</description></item></channel></rss>