<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.07128</link><description>&lt;p&gt;
FAX: JAX&#20013;&#21487;&#25193;&#23637;&#19988;&#21487;&#24494;&#20998;&#30340;&#32852;&#37030;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
FAX: Scalable and Differentiable Federated Primitives in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07128
&lt;/p&gt;
&lt;p&gt;
FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FAX&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#35774;&#35745;&#30340;&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#25968;&#25454;&#20013;&#24515;&#21644;&#36328;&#35774;&#22791;&#24212;&#29992;&#20013;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#35745;&#31639;&#12290;FAX&#21033;&#29992;JAX&#30340;&#20998;&#29255;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#21407;&#29983;&#38024;&#23545;TPU&#21644;&#26368;&#20808;&#36827;&#30340;JAX&#36816;&#34892;&#26102;&#65288;&#21253;&#25324;Pathways&#65289;&#30340;&#23450;&#20301;&#12290;FAX&#23558;&#32852;&#37030;&#35745;&#31639;&#30340;&#22522;&#26412;&#26500;&#20214;&#23884;&#20837;JAX&#20013;&#65292;&#24102;&#26469;&#20102;&#19977;&#20010;&#20851;&#38190;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#36716;&#25442;&#20026;XLA HLO&#12290;&#20854;&#27425;&#65292;FAX&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23436;&#25972;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#32852;&#37030;&#35745;&#31639;&#30340;&#34920;&#36798;&#12290;&#26368;&#21518;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#35299;&#37322;&#25104;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FAX&#20026;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#32852;&#37030;&#35745;&#31639;&#25552;&#20379;&#20102;&#26131;&#32534;&#31243;&#12289;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#12290;FAX&#21487;&#22312;https://github.com/google-research/google-research/tree/master/fax &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07128v1 Announce Type: cross  Abstract: We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15113</link><description>&lt;p&gt;
MSPipe: &#36890;&#36807;&#24847;&#35782;&#21040;&#38472;&#26087;&#24615;&#30340;&#31649;&#36947;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24615;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#22411;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MTGNNs&#65289;&#26159;&#19968;&#31867;&#21033;&#29992;&#33410;&#28857;&#35760;&#24518;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#23545;&#20110;&#26080;&#35760;&#24518;&#30340;&#23545;&#24212;&#32593;&#32476;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;MTGNNs&#20013;&#65292;&#20026;&#20102;&#33719;&#21462;&#26368;&#26032;&#30340;&#20449;&#24687;&#65292;&#35760;&#24518;&#27169;&#22359;&#30340;&#36845;&#20195;&#35835;&#21462;&#21644;&#26356;&#26032;&#36807;&#31243;&#38656;&#35201;&#36981;&#24490;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#24182;&#38480;&#21046;&#20102;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#38745;&#24577;GNNs&#30340;&#20248;&#21270;&#19981;&#36866;&#29992;&#20110;MTGNNs&#65292;&#22240;&#20026;&#20004;&#32773;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32570;&#20047;&#35760;&#24518;&#27169;&#22359;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24182;&#26410;&#26377;&#25928;&#22320;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20351;&#20854;&#23545;MTGNN&#35757;&#32451;&#26080;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSPipe&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14088</link><description>&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#38598;&#32676;&#19979;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31169;&#26377;&#25968;&#25454;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters. (arXiv:2306.14088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#36890;&#36807;&#22810;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#31169;&#26377;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#19968;&#31181;&#33879;&#21517;&#24182;&#24191;&#27867;&#20351;&#29992;&#30340;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#35745;&#31639;&#23616;&#37096;&#26799;&#24230;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#32852;&#21512;&#22120;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38544;&#31169;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#35266;&#23519;&#21040;&#23616;&#37096;&#26799;&#24230;&#23601;&#36275;&#20197;&#27844;&#38706;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;&#24050;&#30740;&#31350;&#20102;&#29992;&#20110;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#29992;&#25143;&#37117;&#24444;&#27492;&#36830;&#25509;&#24182;&#19982;&#32852;&#21512;&#22120;&#36830;&#25509;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#26550;&#26500;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20165;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#12290;&#24403;&#38656;&#35201;&#20449;&#24687;&#35770;&#38544;&#31169;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36890;&#20449;&#25104;&#26412;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#24182;&#24341;&#20837;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#37327;&#36523;&#23450;&#21046;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning collaboratively trains a neural network on privately owned data held by several participating clients. The gradient descent algorithm, a well-known and popular iterative optimization procedure, is run to train the neural network. Every client uses its local data to compute partial gradients and sends it to the federator which aggregates the results. Privacy of the clients' data is a major concern. In fact, observing the partial gradients can be enough to reveal the clients' data. Private aggregation schemes have been investigated to tackle the privacy problem in federated learning where all the users are connected to each other and to the federator. In this paper, we consider a wireless system architecture where clients are only connected to the federator via base stations. We derive fundamental limits on the communication cost when information-theoretic privacy is required, and introduce and analyze a private aggregation scheme tailored for this setting.
&lt;/p&gt;</description></item></channel></rss>