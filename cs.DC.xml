<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15760</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23558;&#26381;&#21153;&#22120;&#31471;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39640;&#25928;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#65288;HtFL&#65289;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#31216;&#20026;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#24490;&#29615;&#65288;FedKTL&#65289;&#65292;&#20197;&#22788;&#29702;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;FedKTL&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#25512;&#29702;&#20135;&#29983;&#19982;&#23458;&#25143;&#31471;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22411;&#22270;&#20687;-&#21521;&#37327;&#23545;&#12290;&#20511;&#21161;&#36825;&#20123;&#23545;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#30340;&#30417;&#30563;&#26412;&#22320;&#20219;&#21153;&#23558;&#26469;&#33258;&#29983;&#25104;&#22120;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CNN&#21644;ViT&#22312;&#20869;&#30340;14&#31181;&#27169;&#22411;&#19979;&#65292;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;FedKTL&#36229;&#36234;&#20102;&#19971;&#31181;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15760v1 Announce Type: new  Abstract: Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12778</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#21644;&#22870;&#21169;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#22870;&#21169;&#21152;&#26435;&#65288;R-Weighted&#65289;&#21644;&#25439;&#22833;&#21152;&#26435;&#65288;L-Weighted&#65289;&#26799;&#24230;&#21512;&#24182;&#12290; R / L &#21152;&#26435;&#26041;&#27861;&#26367;&#25442;&#20102;&#35757;&#32451;&#22810;&#20010;&#20195;&#29702;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#20363;&#22914;&#23545;&#26799;&#24230;&#27714;&#21644;&#25110;&#24179;&#22343;&#12290;&#27599;&#20010;&#20195;&#29702;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#29256;&#26412;&#30340;&#30456;&#21516;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#20250;&#20174;&#19981;&#21516;&#30340;actor&#33719;&#24471;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
&lt;/p&gt;</description></item></channel></rss>