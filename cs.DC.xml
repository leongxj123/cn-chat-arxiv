<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11795</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Cost Privacy-Aware Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11795
&lt;/p&gt;
&lt;p&gt;
ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;ZIP-DL&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#12290;&#36825;&#31181;&#25216;&#26415;&#30830;&#20445;&#20102;&#30001;&#20110;&#20854;&#30456;&#20851;&#24615;&#65292;&#22312;&#32858;&#21512;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#20960;&#20046;&#30456;&#20114;&#25269;&#28040;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;ZIP-DL&#19981;&#38656;&#35201;&#22810;&#27425;&#36890;&#20449;&#36718;&#36827;&#34892;&#22122;&#22768;&#25269;&#28040;&#65292;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#20449;&#24320;&#38144;&#20043;&#38388;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#20351;ZIP-DL&#21487;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;ZIP-DL&#22312;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#26435;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#32447;DL&#30456;&#27604;&#65292;ZIP-DL&#65288;i&#65289;&#23558;&#21487;&#36861;&#36394;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#38477;&#20302;&#20102;&#22810;&#36798;52&#20010;&#28857;&#65292;&#65288;ii&#65289;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;37&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item></channel></rss>