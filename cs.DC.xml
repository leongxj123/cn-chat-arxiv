<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17460</link><description>&lt;p&gt;
&#20351;&#26080;&#32447;&#29615;&#22659;&#23545;&#26799;&#24230;&#20272;&#35745;&#22120;&#26377;&#29992;&#65306;&#19968;&#31181;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#36890;&#20449;&#26102;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#28857;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#30693;&#36947;&#36890;&#36947;&#29366;&#24577;&#31995;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#26080;&#32447;&#36890;&#36947;&#21253;&#21547;&#22312;&#23398;&#20064;&#31639;&#27861;&#26412;&#36523;&#20013;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#28010;&#36153;&#36164;&#28304;&#26469;&#20998;&#26512;&#21644;&#28040;&#38500;&#20854;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#26159;&#65292;&#22312;FL&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#19981;&#26159;&#20984;&#30340;&#65292;&#36825;&#20351;&#24471;&#23558;FL&#25193;&#23637;&#21040;ZO&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20197;&#21450;&#21253;&#25324;&#24433;&#21709;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a novel approach to machine learning that allows multiple edge devices to collaboratively train a model without disclosing their raw data. However, several challenges hinder the practical implementation of this approach, especially when devices and the server communicate over wireless channels, as it suffers from communication and computation bottlenecks in this case. By utilizing a communication-efficient framework, we propose a novel zero-order (ZO) method with a one-point gradient estimator that harnesses the nature of the wireless communication channel without requiring the knowledge of the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. The two main difficulties of this work are that in FL, the objective function is usually not convex, which makes the extension of FL to ZO methods challenging, and that including the impa
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item></channel></rss>