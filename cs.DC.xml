<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11204</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20998;&#21306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Partitioned Neural Network Training via Synthetic Intermediate Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26222;&#21450;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290; GPU &#20869;&#23384;&#32422;&#26463;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#36825;&#20123;&#24222;&#22823;&#27169;&#22411;&#30340;&#19968;&#20010;&#26126;&#26174;&#29942;&#39048;&#12290;&#29616;&#26377;&#31574;&#30053;&#65292;&#21253;&#25324;&#25968;&#25454;&#24182;&#34892;&#12289;&#27169;&#22411;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#21644;&#23436;&#20840;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#12290; &#29305;&#21035;&#26159;&#27169;&#22411;&#24182;&#34892;&#20801;&#35768;&#23558;&#25972;&#20010;&#27169;&#22411;&#20998;&#24067;&#22312;&#22810;&#20010; GPU &#19978;&#65292;&#20294;&#38543;&#21518;&#30340;&#36825;&#20123;&#20998;&#21306;&#20043;&#38388;&#30340;&#25968;&#25454;&#36890;&#20449;&#20943;&#24930;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#22312;&#27599;&#20010; GPU &#19978;&#23384;&#20648;&#36741;&#21161;&#21442;&#25968;&#25152;&#38656;&#30340;&#22823;&#37327;&#20869;&#23384;&#24320;&#38144;&#22686;&#21152;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290; &#26412;&#30740;&#31350;&#20027;&#24352;&#19981;&#20351;&#29992;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26159;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040; GPU &#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#12290; &#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#36825;&#20123;&#26631;&#31614;&#20943;&#32531;&#20102;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
&lt;/p&gt;</description></item></channel></rss>