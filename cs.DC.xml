<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item></channel></rss>