<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;</title><link>https://arxiv.org/abs/2402.18789</link><description>&lt;p&gt;
FlexLLM&#65306;&#19968;&#31181;&#29992;&#20110;&#20849;&#21516;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18789
&lt;/p&gt;
&lt;p&gt;
FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter-efficient finetuning&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#20250;&#20026;&#29992;&#25143;&#21019;&#24314;&#21333;&#29420;&#30340;&#31995;&#32479;&#65292;&#20197;&#25191;&#34892;PEFT&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#21253;&#21547;&#25512;&#29702;&#21644;PEFT&#24494;&#35843;&#35831;&#27714;&#28151;&#21512;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20026;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#26469;&#20849;&#21516;&#36816;&#34892;&#23427;&#20204;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#25552;&#20379;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;FlexLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#65292;&#23558;&#24207;&#21015;&#30340;&#24494;&#35843;&#35745;&#31639;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#26631;&#35760;&#32423;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
&lt;/p&gt;</description></item></channel></rss>