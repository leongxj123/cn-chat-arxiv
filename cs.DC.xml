<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03448</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#24191;&#20041;&#25910;&#25947;&#20445;&#35777;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36817;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#26356;&#26032;&#21644;&#27169;&#22411;&#32858;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#37117;&#30001;&#23458;&#25143;&#31471;&#36827;&#34892;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;DFL&#26041;&#27861;&#65292;&#23427;&#22312;&#36825;&#20004;&#20010;&#36807;&#31243;&#20013;&#24191;&#20041;&#21270;&#20102;&#38388;&#27463;&#24615;&#30340;&#27010;&#24565;&#65292;&#24314;&#27169;&#20102;&#22312;&#23454;&#38469;DFL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;DSpodFL&#23558;&#35768;&#22810;&#30528;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#65292;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#65292;&#32479;&#19968;&#21040;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#23545;DSpodFL&#30340;&#25910;&#25947;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#21487;&#20197;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#23558;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26377;&#38480;&#30340;&#26368;&#20339;&#24615;&#24046;&#36317;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
&lt;/p&gt;</description></item></channel></rss>