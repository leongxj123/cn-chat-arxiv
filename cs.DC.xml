<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#26412;&#65292;&#25351;&#20986;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#29306;&#29298;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.14712</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#25928;&#29575;&#25110;&#38544;&#31169;&#65306;&#21482;&#33021;&#36873;&#20004;&#26679;
&lt;/p&gt;
&lt;p&gt;
Robustness, Efficiency, or Privacy: Pick Two in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#26412;&#65292;&#25351;&#20986;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#29306;&#29298;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#22686;&#38271;&#65292;&#36825;&#20123;&#26550;&#26500;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#25968;&#25454;&#27745;&#26579;&#21644;&#30828;&#20214;&#25925;&#38556;&#31561;&#38382;&#39064;&#24456;&#24120;&#35265;&#12290;&#30830;&#20445;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#23545;&#20110;ML&#22312;&#20844;&#20849;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;ML&#26550;&#26500;&#20013;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20998;&#24067;&#24335;ML&#20013;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#21547;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#21333;&#29420;&#39640;&#25928;&#23454;&#29616;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26377;&#26174;&#33879;&#30340;&#25240;&#34935;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20256;&#32479;&#30340;&#22122;&#22768;&#27880;&#20837;&#36890;&#36807;&#38544;&#34255;&#27602;&#23475;&#36755;&#20837;&#26469;&#25439;&#23475;&#20934;&#30830;&#24615;&#65292;&#32780;&#21152;&#23494;&#26041;&#27861;&#19982;&#38450;&#27602;&#38450;&#24481;&#30456;&#20914;&#31361;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14712v2 Announce Type: replace  Abstract: The success of machine learning (ML) applications relies on vast datasets and distributed architectures which, as they grow, present major challenges. In real-world scenarios, where data often contains sensitive information, issues like data poisoning and hardware failures are common. Ensuring privacy and robustness is vital for the broad adoption of ML in public life. This paper examines the costs associated with achieving these objectives in distributed ML architectures, from both theoretical and empirical perspectives. We overview the meanings of privacy and robustness in distributed ML, and clarify how they can be achieved efficiently in isolation. However, we contend that the integration of these two objectives entails a notable compromise in computational efficiency. In short, traditional noise injection hurts accuracy by concealing poisoned inputs, while cryptographic methods clash with poisoning defenses due to their non-line
&lt;/p&gt;</description></item></channel></rss>