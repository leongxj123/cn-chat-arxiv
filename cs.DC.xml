<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15106</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Distributed Training with Message Passing Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Nystrom-&#36817;&#20284;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DS-MPNN&#65288;&#20854;&#20013;D&#21644;S&#20998;&#21035;&#20195;&#34920;&#20998;&#24067;&#24335;&#21644;&#37319;&#26679;&#65289;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;$O(10^5)$&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65306;&#65288;a&#65289;Darcy&#27969;&#25968;&#25454;&#38598;&#21644;&#65288;b&#65289;2-D&#26426;&#32764;&#30340;&#31283;&#24577;RANS&#27169;&#25311;&#65292;&#25552;&#20379;&#20102;&#19982;&#21333;GPU&#23454;&#29616;&#21644;&#22522;&#20110;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#27604;&#36739;&#12290;DS-MPNN&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#21333;GPU&#23454;&#29616;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23481;&#32435;&#27604;&#21333;&#20010;GPU&#23454;&#29616;&#26356;&#22810;&#25968;&#37327;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12812</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#20010;&#24615;&#21270;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Decentralized Algorithms for Online Personalized Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#24403;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#20250;&#24341;&#20837;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#65292;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#20195;&#29702;&#38543;&#26102;&#38388;&#20174;&#23454;&#20540;&#20998;&#24067;&#20013;&#25910;&#38598;&#26679;&#26412;&#26469;&#20272;&#35745;&#20854;&#22343;&#20540;&#12290;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#20195;&#29702;&#25968;&#37327;A&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20195;&#29702;&#33258;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#20351;&#24471;&#27599;&#20010;&#20195;&#29702;&#21482;&#33021;&#19982;&#36873;&#23450;&#25968;&#37327;&#30340;&#23545;&#31561;&#20307;r&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#21327;&#20316;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#20449;&#24565;&#20256;&#25773;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
&lt;/p&gt;</description></item></channel></rss>