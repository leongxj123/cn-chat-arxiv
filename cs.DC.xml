<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item></channel></rss>