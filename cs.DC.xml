<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;</title><link>https://arxiv.org/abs/2402.15166</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Split Federated Learning on Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;SFL&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#36890;&#24120;&#34987;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#32852;&#37030;&#26041;&#24335;&#35757;&#32451;&#19968;&#37096;&#20998;&#65292;&#20027;&#26381;&#21153;&#22120;&#35757;&#32451;&#21478;&#19968;&#37096;&#20998;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;SFL&#31639;&#27861;&#21457;&#23637;&#30340;&#30740;&#31350;&#24456;&#22810;&#65292;&#20294;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#22312;&#25991;&#29486;&#20013;&#36824;&#26410;&#26377;&#25552;&#21450;&#65292;&#26412;&#25991;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23545;SFL&#36827;&#34892;&#20998;&#26512;&#21487;&#33021;&#27604;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20998;&#26512;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#23458;&#25143;&#31471;&#21644;&#20027;&#26381;&#21153;&#22120;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#21452;&#36895;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#19978;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#12290;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})$&#65292;&#20854;&#20013;$T$&#34920;&#31034;SFL&#35757;&#32451;&#30340;&#24635;&#36718;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#38750;&#20984;&#30446;&#26631;&#21644;&#19968;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15166v1 Announce Type: cross  Abstract: Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during trai
&lt;/p&gt;</description></item></channel></rss>