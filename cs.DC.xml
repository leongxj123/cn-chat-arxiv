<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>CATGNN&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#21463;&#36793;&#27969;&#20316;&#20026;&#36755;&#20837;&#24182;&#25552;&#20986;&#21517;&#20026;SPRING&#30340;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#23454;&#29616;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#20197;&#19978;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;</title><link>https://arxiv.org/abs/2404.02300</link><description>&lt;p&gt;
CATGNN&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#26412;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02300
&lt;/p&gt;
&lt;p&gt;
CATGNN&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#21463;&#36793;&#27969;&#20316;&#20026;&#36755;&#20837;&#24182;&#25552;&#20986;&#21517;&#20026;SPRING&#30340;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#23454;&#29616;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#20197;&#19978;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;GNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31995;&#32479;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#22270;&#19978;&#36827;&#34892;GNN&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#38656;&#35201;&#23558;&#25972;&#20010;&#22270;&#21152;&#36733;&#21040;&#20869;&#23384;&#20013;&#20197;&#36827;&#34892;&#22270;&#20998;&#21306;&#65292;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#31354;&#38388;&#26469;&#22788;&#29702;&#22823;&#22270;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20351;&#29992;&#26222;&#36890;&#24037;&#20316;&#31449;&#22312;&#36825;&#20123;&#22823;&#22270;&#19978;&#36827;&#34892;GNN&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;CATGNN&#65292;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#19987;&#27880;&#20110;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#26356;&#22823;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;&#22312;&#20854;&#20182;&#21151;&#33021;&#20013;&#65292;&#23427;&#25509;&#21463;&#19968;&#31995;&#21015;&#36793;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#19981;&#26159;&#23558;&#25972;&#20010;&#22270;&#21152;&#36733;&#21040;&#20869;&#23384;&#20013;&#36827;&#34892;&#20998;&#21306;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPRING&#30340;&#26032;&#22411;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;16&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;CATGNN&#19982;SPRING&#30340;&#27491;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02300v1 Announce Type: new  Abstract: Graph neural networks have been shown successful in recent years. While different GNN architectures and training systems have been developed, GNN training on large-scale real-world graphs still remains challenging. Existing distributed systems load the entire graph in memory for graph partitioning, requiring a huge memory space to process large graphs and thus hindering GNN training on such large graphs using commodity workstations. In this paper, we propose CATGNN, a cost-efficient and scalable distributed GNN training system which focuses on scaling GNN training to billion-scale or larger graphs under limited computational resources. Among other features, it takes a stream of edges as input, instead of loading the entire graph in memory, for partitioning. We also propose a novel streaming partitioning algorithm named SPRING for distributed GNN training. We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datase
&lt;/p&gt;</description></item></channel></rss>