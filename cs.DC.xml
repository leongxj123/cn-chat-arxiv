<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>FLASH&#26159;&#19968;&#20010;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#31561;&#22240;&#32032;&#65292;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08769</link><description>&lt;p&gt;
FLASH: &#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLASH: Federated Learning Across Simultaneous Heterogeneities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08769
&lt;/p&gt;
&lt;p&gt;
FLASH&#26159;&#19968;&#20010;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#31561;&#22240;&#32032;&#65292;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20851;&#38190;&#21069;&#25552;&#26159;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#23458;&#25143;&#31471;&#65289;&#20043;&#38388;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#21487;&#33021;&#19981;&#20165;&#26469;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36824;&#26469;&#33258;&#25968;&#25454;&#36136;&#37327;&#20197;&#21450;&#35745;&#31639;/&#36890;&#20449;&#24310;&#36831;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23545;&#36825;&#20123;&#19981;&#21516;&#19988;&#21516;&#26102;&#23384;&#22312;&#30340;&#24322;&#36136;&#24615;&#30340;&#32508;&#21512;&#35270;&#22270;&#33267;&#20851;&#37325;&#35201;&#65307;&#20363;&#22914;&#65292;&#24310;&#36831;&#36739;&#20302;&#30340;&#23458;&#25143;&#31471;&#21487;&#33021;&#20855;&#26377;&#36739;&#24046;&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLASH&#65288;&#36328;&#21516;&#26102;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#19968;&#20010;&#36731;&#37327;&#19988;&#28789;&#27963;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#34913;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#36136;&#37327;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#24310;&#36831;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FLASH&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#32479;&#19968;&#30340;&#26041;&#27861;&#20013;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#24322;&#36136;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08769v1 Announce Type: new Abstract: The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified m
&lt;/p&gt;</description></item></channel></rss>