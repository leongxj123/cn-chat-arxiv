<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;</title><link>http://arxiv.org/abs/2401.16445</link><description>&lt;p&gt;
OMPGPT: &#19968;&#31181;&#29992;&#20110;OpenMP&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16445
&lt;/p&gt;
&lt;p&gt;
OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#38543;&#30528;&#36825;&#19968;&#36235;&#21183;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;StarCoder&#12289;WizardCoder&#21644;CodeLlama&#31561;&#65292;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#65292;&#22312;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#22266;&#26377;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#27880;&#37322;&#29983;&#25104;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#21450;&#23545;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#19968;&#33324;&#25903;&#25345;&#12290;&#34429;&#28982;&#20195;&#30721;LLMs&#30340;&#36890;&#29992;&#33021;&#21147;&#23545;&#35768;&#22810;&#31243;&#24207;&#21592;&#26469;&#35828;&#24456;&#26377;&#29992;&#65292;&#20294;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20855;&#26377;&#26356;&#31364;&#30340;&#38656;&#27714;&#38598;&#65292;&#20351;&#24471;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#30340;LM&#25104;&#20026;&#19968;&#20010;&#26356;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OMPGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;OpenMP pragma&#29983;&#25104;&#26041;&#38754;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24182;&#25913;&#36827;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#38142;&#24335;OMP&#65288;chain-of-OMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
&lt;/p&gt;</description></item></channel></rss>