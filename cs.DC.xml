<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06787</link><description>&lt;p&gt;
ForestColl: &#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#19978;&#39640;&#25928;&#30340;&#38598;&#21512;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06787
&lt;/p&gt;
&lt;p&gt;
ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#20449;&#65288;&#22914;allreduce&#31561;&#65289;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#22312;&#24403;&#20170;&#39640;&#24230;&#22810;&#26679;&#21270;&#21644;&#24322;&#26500;&#30340;&#32593;&#32476;&#32467;&#26500;&#19979;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#35843;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForestColl&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#30340;&#35843;&#24230;&#12290;ForestColl&#20351;&#29992;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#20316;&#20026;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#12290;&#20854;&#35843;&#24230;&#29983;&#25104;&#36816;&#34892;&#22312;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#65292;&#19988;&#20855;&#26377;&#39640;&#25193;&#23637;&#24615;&#12290;ForestColl&#25903;&#25345;&#21253;&#25324;&#20132;&#25442;&#32593;&#32476;&#21644;&#30452;&#25509;&#36830;&#25509;&#22312;&#20869;&#30340;&#20219;&#20309;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#20219;&#20309;&#32593;&#32476;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#22810;&#38598;&#32676;&#30340;AMD MI250&#21644;NVIDIA A100&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;ForestColl&#12290;&#19982;&#20379;&#24212;&#21830;&#33258;&#24049;&#20248;&#21270;&#30340;&#36890;&#20449;&#24211;RCCL&#21644;NCCL&#30456;&#27604;&#65292;ForestColl&#30340;&#35843;&#24230;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;52&#65285;&#12290;ForestColl&#36824;&#20248;&#20110;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging given today's highly diverse and heterogeneous network fabrics. In this paper, we present ForestColl, a tool that generates efficient schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretically minimum network congestion. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct connections, as well as any network graph structure. We evaluated ForestColl on multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules achieved up to 52\% higher performance compared to the vendors' own optimized communication libraries, RCCL and NCCL. ForestColl also outperforms other s
&lt;/p&gt;</description></item><item><title>FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;</title><link>http://arxiv.org/abs/2305.05920</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#20998;&#24067;&#24335;&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05920
&lt;/p&gt;
&lt;p&gt;
FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#21160;&#20102;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;&#26032;&#19968;&#20195;&#20114;&#21160;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#20132;&#20114;&#24615;&#35201;&#27714;&#27169;&#22411;&#25512;&#26029;&#30340;&#20302;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;&#29616;&#26377;&#30340;LLM&#26381;&#21153;&#31995;&#32479;&#20351;&#29992;&#30340;&#26159;&#36816;&#34892;&#21040;&#23436;&#25104;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#23384;&#22312;&#22836;&#37096;&#38459;&#22622;&#21644;&#38271;JCT&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastServe&#65292;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#12290;FastServe&#21033;&#29992;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#27169;&#24335;&#65292;&#20197;&#27599;&#20010;&#36755;&#20986;&#26631;&#35760;&#30340;&#31890;&#24230;&#23454;&#29616;&#25250;&#21344;&#24335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#35843;&#24230;&#22120;&#26368;&#23567;&#21270;JCT&#12290;&#22522;&#20110;LLM&#25512;&#29702;&#30340;&#26032;&#21322;&#20449;&#24687;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#36755;&#20837;&#38271;&#24230;&#20449;&#24687;&#26469;&#20026;&#27599;&#20010;&#21040;&#36798;&#20316;&#19994;&#20998;&#37197;&#36866;&#24403;&#30340;&#21021;&#22987;&#38431;&#21015;&#26469;&#36830;&#25509;&#12290;&#39640;&#20110;&#25152;&#36830;&#25509;&#38431;&#21015;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#34987;&#36339;&#36807;&#20197;&#20943;&#23569;&#38477;&#32423;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GPU&#20869;&#23384;&#31649;&#29702;&#26426;&#21046;&#65292;&#20197;&#25552;&#21069;&#28165;&#38500;&#19981;&#20877;&#20351;&#29992;&#30340;GPU&#32531;&#23384;&#65292;&#24182;&#23545;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#32531;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
&lt;/p&gt;</description></item></channel></rss>