<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12403</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23567;&#25209;&#27425;
&lt;/p&gt;
&lt;p&gt;
Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26102;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#36807;&#31243;&#38750;&#24120;&#23494;&#38598;&#12290;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#19982;&#22270;&#37319;&#26679;&#30456;&#32467;&#21512;&#12290;GNN&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#23567;&#25209;&#37327;&#20013;&#30340;&#39033;&#20855;&#26377;&#37325;&#21472;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29420;&#31435;&#23567;&#25209;&#37327;&#26041;&#27861;&#23558;&#27599;&#20010;&#22788;&#29702;&#21333;&#20803;&#65288;PE&#65289;&#20998;&#37197;&#32473;&#33258;&#24049;&#30340;&#23567;&#25209;&#37327;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#37325;&#22797;&#35745;&#31639;&#21644;&#36328;PE&#30340;&#36755;&#20837;&#25968;&#25454;&#35775;&#38382;&#12290;&#36825;&#25918;&#22823;&#20102;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#36825;&#26159;&#38480;&#21046;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#22810;PE&#29615;&#22659;&#20013;NEP&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#26159;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20985;&#20989;&#25968;&#36825;&#19968;&#29305;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable 
&lt;/p&gt;</description></item></channel></rss>