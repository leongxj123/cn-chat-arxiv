<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item></channel></rss>