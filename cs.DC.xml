<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2301.05872</link><description>&lt;p&gt;
CEDAS&#65306;&#19968;&#31181;&#20855;&#26377;&#25913;&#36827;&#25910;&#25947;&#24615;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.05872
&lt;/p&gt;
&lt;p&gt;
CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#36890;&#20449;&#21463;&#38480;&#29615;&#22659;&#19979;&#35299;&#20915;&#22810;&#20195;&#29702;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#21387;&#32553;&#31934;&#30830;&#25193;&#25955;&#65288;CEDAS&#65289;&#8221;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#28176;&#36817;&#22320;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CEDAS&#36804;&#20170;&#20026;&#27490;&#20197;&#20854;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65288;&#20851;&#20110;&#22270;&#30340;&#29305;&#24615;&#65289;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;SGD&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#22312;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$&#65292;&#22312;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$&#65292;&#20854;&#20013;$(1-\lambda_2)$&#34920;&#31034;&#35889;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\lambda_2)$ denotes the spectr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.10640</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#36127;&#36131;&#20154;&#25171;&#31639;&#22312;&#19968;&#32452;&#20855;&#26377;&#19968;&#20123;&#26041;&#31243;&#32452;&#23376;&#38598;&#30340;&#26426;&#22120;&#30340;&#20998;&#24067;&#24335;/&#32852;&#21512;&#24110;&#21161;&#19979;&#35299;&#20915;&#35813;&#31995;&#32479;&#30340;&#35774;&#32622;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#23545;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20005;&#26684;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#36825;&#20004;&#31867;&#31639;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#26368;&#36817;&#25552;&#20986;&#30340;&#21152;&#36895;&#25237;&#24433;&#19968;&#33268;&#24615;(APC)&#21644;&#20998;&#24067;&#24335;&#37325;&#29699;&#26041;&#27861;(D-HBM)&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31216;&#20026;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#26222;&#36941;&#24615;&#12290;&#20351;&#29992;&#35813;&#27010;&#24565;&#65292;&#25105;&#20204;&#32422;&#26463;&#24182;&#27604;&#36739;&#25152;&#30740;&#31350;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25429;&#25417;&#20004;&#31181;&#26041;&#27861;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
&lt;/p&gt;</description></item></channel></rss>