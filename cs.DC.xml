<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>ServerlessLLM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#26412;&#22320;&#21270;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#26597;&#28857;&#21152;&#36733;&#12289;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#26381;&#21153;&#22120;&#20998;&#37197;&#26469;&#23454;&#29616;&#39640;&#25928;&#19988;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.14351</link><description>&lt;p&gt;
ServerlessLLM&#65306;&#22686;&#24378;&#26412;&#22320;&#21270;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models. (arXiv:2401.14351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14351
&lt;/p&gt;
&lt;p&gt;
ServerlessLLM&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#26412;&#22320;&#21270;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#26597;&#28857;&#21152;&#36733;&#12289;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#26381;&#21153;&#22120;&#20998;&#37197;&#26469;&#23454;&#29616;&#39640;&#25928;&#19988;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ServerlessLLM&#65292;&#19968;&#31181;&#22686;&#24378;&#26412;&#22320;&#21270;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26080;&#26381;&#21153;&#22120;&#25512;&#29702;&#31995;&#32479;&#12290;ServerlessLLM&#21033;&#29992;GPU&#26381;&#21153;&#22120;&#19978;&#21487;&#29992;&#30340;&#23384;&#20648;&#21644;&#20869;&#23384;&#35774;&#22791;&#30340;&#22823;&#23481;&#37327;&#21644;&#24102;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#36828;&#31243;&#26816;&#26597;&#28857;&#19979;&#36733;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#26816;&#26597;&#28857;&#21152;&#36733;&#12290;ServerlessLLM&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65306;(i)&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#36733;&#20248;&#21270;&#26816;&#26597;&#28857;&#26684;&#24335;&#35774;&#35745;&#21644;&#39640;&#25928;&#30340;&#22810;&#32423;&#26816;&#26597;&#28857;&#21152;&#36733;&#31995;&#32479;&#23454;&#29616;&#24555;&#36895;LLM&#26816;&#26597;&#28857;&#21152;&#36733;&#65307;(ii)&#21033;&#29992;&#26412;&#22320;&#21270;&#25512;&#29702;&#21644;&#23454;&#26102;&#36801;&#31227;&#65292;&#20351;ServerlessLLM&#33021;&#22815;&#22312;&#20445;&#25345;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#23454;&#29616;&#26412;&#22320;&#21270;&#39537;&#21160;&#30340;&#26381;&#21153;&#22120;&#20998;&#37197;&#65307;(iii)&#26412;&#22320;&#21270;&#24863;&#30693;&#30340;&#26381;&#21153;&#22120;&#20998;&#37197;&#65292;&#20351;ServerlessLLM&#33021;&#22815;&#35780;&#20272;&#38598;&#32676;&#20013;&#27599;&#20010;&#26381;&#21153;&#22120;&#30340;&#29366;&#24577;&#65292;&#24182;&#26377;&#25928;&#22320;&#23433;&#25490;&#27169;&#22411;&#21551;&#21160;&#26102;&#38388;&#20197;&#20805;&#20998;&#21033;&#29992;&#26412;&#22320;&#26816;&#26597;&#28857;&#20301;&#32622;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#24494;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;ServerlessLLM&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include micr
&lt;/p&gt;</description></item></channel></rss>