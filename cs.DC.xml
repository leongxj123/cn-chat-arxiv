<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.11522</link><description>&lt;p&gt;
LOOPer: &#19968;&#20010;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11522
&lt;/p&gt;
&lt;p&gt;
LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#22312;&#23454;&#29616;&#39640;&#32423;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#36873;&#25321;&#33021;&#22815;&#24102;&#26469;&#26368;&#20339;&#21152;&#36895;&#30340;&#26368;&#26377;&#21033;&#36716;&#25442;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20419;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25104;&#26412;&#27169;&#22411;&#26469;&#24341;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#30340;&#25628;&#32034;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#27010;&#24565;&#39564;&#35777;&#12290;&#34429;&#28982;&#36825;&#31181;&#27010;&#24565;&#39564;&#35777;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#21482;&#25903;&#25345;&#23569;&#37327;&#20223;&#23556;&#21464;&#25442;&#30340;&#23376;&#38598;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#29992;&#22797;&#26434;&#20195;&#30721;&#21464;&#25442;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#36824;&#21482;&#25903;&#25345;&#20855;&#26377;&#21333;&#20010;&#24490;&#29615;&#23884;&#22871;&#21644;&#30697;&#24418;&#36845;&#20195;&#22495;&#30340;&#31616;&#21333;&#31243;&#24207;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#35768;&#22810;&#31243;&#24207;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#36825;&#26679;&#30340;&#32534;&#35793;&#22120;&#21644;&#33258;&#21160;&#35843;&#24230;&#22120;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11522v1 Announce Type: cross  Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put in
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item></channel></rss>