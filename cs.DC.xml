<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07612</link><description>&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#19979;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#19979;&#38480;&#21644;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression. (arXiv:2305.07612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#20943;&#36731;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#35745;&#31639;&#33410;&#28857;&#38388;&#20449;&#24687;&#20132;&#25442;&#37327;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#65292;&#24182;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#21387;&#32553;&#22120;&#65306;&#26080;&#20559;&#21644;&#21387;&#32553;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#21387;&#32553;&#22120;&#33719;&#24471;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35774;&#32622;&#65292;&#32467;&#21512;&#24378;&#20984;&#12289;&#19968;&#33324;&#20984;&#25110;&#38750;&#20984;&#20989;&#25968;&#65292;&#24182;&#29992;&#26080;&#20559;&#25110;&#21387;&#32553;&#22411;&#21387;&#32553;&#22120;&#24314;&#31435;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30456;&#27604;&#32463;&#20856;&#26041;&#27861;&#65292;&#32553;&#23567;&#20102;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#33021;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication compression is an essential strategy for alleviating communication overhead by reducing the volume of information exchanged between computing nodes in large-scale distributed stochastic optimization. Although numerous algorithms with convergence guarantees have been obtained, the optimal performance limit under communication compression remains unclear.  In this paper, we investigate the performance limit of distributed stochastic optimization algorithms employing communication compression. We focus on two main types of compressors, unbiased and contractive, and address the best-possible convergence rates one can obtain with these compressors. We establish the lower bounds for the convergence rates of distributed stochastic optimization in six different settings, combining strongly-convex, generally-convex, or non-convex functions with unbiased or contractive compressor types. To bridge the gap between lower bounds and existing algorithms' rates, we propose NEOLITHIC, a n
&lt;/p&gt;</description></item></channel></rss>