<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13421</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated learning with distributed fixed design quantum chips and quantum channels. (arXiv:2401.13421v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#23458;&#25143;&#31471;&#30340;&#31934;&#24515;&#35774;&#35745;&#26597;&#35810;&#65292;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21487;&#20197;&#34987;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#27979;&#37327;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#37327;&#23376;&#36890;&#20449;&#20449;&#36947;&#34987;&#35748;&#20026;&#26356;&#21152;&#23433;&#20840;&#65292;&#22240;&#20026;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#29256;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37327;&#23376;&#20449;&#36947;&#21457;&#36865;N&#32500;&#25968;&#25454;&#21521;&#37327;&#38656;&#35201;&#21457;&#36865;log N&#20010;&#32416;&#32544;&#24577;&#37327;&#23376;&#27604;&#29305;&#65292;&#22914;&#26524;&#25968;&#25454;&#21521;&#37327;&#20316;&#20026;&#37327;&#23376;&#24577;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#25552;&#20379;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#22522;&#20110;&#30001;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#21457;&#36865;&#30340;&#37327;&#23376;&#24577;&#65292;&#25805;&#20316;&#22266;&#23450;&#35774;&#35745;&#30340;&#37327;&#23376;&#33455;&#29255;&#12290;&#22522;&#20110;&#25509;&#25910;&#21040;&#30340;&#21472;&#21152;&#24577;&#65292;&#23458;&#25143;&#31471;&#35745;&#31639;&#24182;&#23558;&#20854;&#26412;&#22320;&#26799;&#24230;&#20316;&#20026;&#37327;&#23376;&#24577;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#26799;&#24230;&#32858;&#21512;&#20197;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#19981;&#21457;&#36865;&#27169;&#22411;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.  In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model
&lt;/p&gt;</description></item></channel></rss>