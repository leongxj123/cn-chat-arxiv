<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.16402</link><description>&lt;p&gt;
GradSkip&#65306;&#20855;&#26377;&#26356;&#22909;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#36890;&#20449;&#21152;&#36895;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. (arXiv:2210.16402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#36890;&#20449;&#20043;&#21069;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26799;&#24230;&#31867;&#22411;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#20102;&#32422;&#21313;&#24180;&#65292;&#20294;&#26412;&#22320;&#35757;&#32451;&#30340;&#21152;&#36895;&#24615;&#36136;&#22312;&#29702;&#35770;&#19978;&#36824;&#26410;&#24471;&#21040;&#23436;&#20840;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;Mishchenko&#31561;&#20154;(2022 International Conference on Machine Learning)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#35777;&#26126;&#20102;&#24403;&#26412;&#22320;&#35757;&#32451;&#24471;&#21040;&#27491;&#30830;&#25191;&#34892;&#26102;&#65292;&#20250;&#23548;&#33268;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#65292;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#36825;&#19968;&#28857;&#25104;&#31435;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;ProxSkip&#35201;&#27714;&#25152;&#26377;&#23458;&#25143;&#31471;&#22312;&#27599;&#27425;&#36890;&#20449;&#36718;&#20013;&#25191;&#34892;&#30456;&#21516;&#25968;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#12290;&#28789;&#24863;&#26469;&#33258;&#24120;&#35782;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#36890;&#36807;&#29468;&#27979;&#35748;&#20026;&#25317;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#24212;&#35813;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#23601;&#33021;&#23436;&#25104;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing the clients to perform multiple local gradient-type training steps prior to communication. While methods of this type have been studied for about a decade, the empirically observed acceleration properties of local training eluded all attempts at theoretical understanding. In a recent breakthrough, Mishchenko et al. (ICML 2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their method ProxSkip requires all clients to take the same number of local training steps in each communication round. Inspired by a common sense intuition, we start our investigation by conjecturing that clients with ``less important'' data should be able to get away with fewer local training steps without this impacting the overall communication c
&lt;/p&gt;</description></item></channel></rss>