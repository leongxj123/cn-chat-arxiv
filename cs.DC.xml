<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.13108</link><description>&lt;p&gt;
&#20998;&#26512;&#37096;&#20998;&#20849;&#20139;&#23545;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13108
&lt;/p&gt;
&lt;p&gt;
PSO-Fed&#31639;&#27861;&#30340;&#37096;&#20998;&#20849;&#20139;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#36127;&#36733;&#65292;&#36824;&#33021;&#22686;&#24378;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#20445;&#25345;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23457;&#26597;&#20102;&#37096;&#20998;&#20849;&#20139;&#30340;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65288;PSO-Fed&#65289;&#31639;&#27861;&#23545;&#25269;&#25239;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290; PSO-Fed&#36890;&#36807;&#20351;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#26356;&#26032;&#36718;&#27425;&#20165;&#19982;&#26381;&#21153;&#22120;&#20132;&#25442;&#37096;&#20998;&#27169;&#22411;&#20272;&#35745;&#26469;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#12290;&#27169;&#22411;&#20272;&#35745;&#30340;&#37096;&#20998;&#20849;&#20139;&#36824;&#22686;&#24378;&#20102;&#31639;&#27861;&#23545;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#24378;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PSO-Fed&#31639;&#27861;&#22312;&#23384;&#22312;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#22312;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20043;&#21069;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#36731;&#24494;&#31713;&#25913;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PSO-Fed&#22312;&#22343;&#20540;&#21644;&#22343;&#26041;&#24847;&#20041;&#19978;&#37117;&#33021;&#20445;&#25345;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#27169;&#22411;&#25237;&#27602;&#25915;&#20987;&#30340;&#21387;&#21147;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;PSO-Fed&#30340;&#29702;&#35770;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#23558;&#20854;&#19982;&#27493;&#38271;&#12289;&#25915;&#20987;&#27010;&#29575;&#12289;&#25968;&#23383;&#31561;&#21508;&#31181;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13108v1 Announce Type: new  Abstract: We scrutinize the resilience of the partial-sharing online federated learning (PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the communication load by enabling clients to exchange only a fraction of their model estimates with the server at each update round. Partial sharing of model estimates also enhances the robustness of the algorithm against model-poisoning attacks. To gain better insights into this phenomenon, we analyze the performance of the PSO-Fed algorithm in the presence of Byzantine clients, malicious actors who may subtly tamper with their local models by adding noise before sharing them with the server. Through our analysis, we demonstrate that PSO-Fed maintains convergence in both mean and mean-square senses, even under the strain of model-poisoning attacks. We further derive the theoretical mean square error (MSE) of PSO-Fed, linking it to various parameters such as stepsize, attack probability, numb
&lt;/p&gt;</description></item></channel></rss>