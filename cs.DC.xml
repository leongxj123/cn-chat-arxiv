<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01863</link><description>&lt;p&gt;
DFML&#65306;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFML: Decentralized Federated Mutual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01863
&lt;/p&gt;
&lt;p&gt;
DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#35774;&#22791;&#39046;&#22495;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#21644;&#23481;&#26131;&#21463;&#21040;&#21333;&#28857;&#25925;&#38556;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35774;&#22791;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#27492;&#24322;&#36136;&#24615;&#19988;&#19981;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#25110;&#20551;&#23450;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#25955;&#24335;FL&#65288;DFL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#65288;DFML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#26080;&#26381;&#21153;&#22120;&#30340;&#65292;&#25903;&#25345;&#38750;&#38480;&#21046;&#24615;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20381;&#36182;&#20844;&#20849;&#25968;&#25454;&#12290;DFML&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#24182;&#24490;&#29615;&#25913;&#21464;&#30417;&#30563;&#21644;&#25552;&#21462;&#20449;&#21495;&#30340;&#25968;&#37327;&#26469;&#26377;&#25928;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFML&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#20840;&#23616;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
&lt;/p&gt;</description></item></channel></rss>