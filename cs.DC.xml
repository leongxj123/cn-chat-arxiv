<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03333</link><description>&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#39640;&#24230;&#24322;&#26500;&#30340;&#23458;&#25143;&#20998;&#24067;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22256;&#38590;&#37096;&#20998;&#28304;&#20110;&#20004;&#20010;&#30475;&#20284;&#30683;&#30462;&#30340;&#30446;&#26631;&#65306;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#23398;&#20064;&#24212;&#36866;&#24212;&#27599;&#20010;&#26412;&#22320;&#20998;&#24067;&#30340;&#26412;&#22320;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#26469;&#28040;&#38500;&#36825;&#31181;&#30683;&#30462;&#12290;&#22522;&#20110;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;SosicFL&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21333;&#32431;&#24418;&#20013;&#30340;&#23376;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;FL&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#33258;&#30001;&#24230;&#33539;&#22260;&#20869;&#20855;&#26377;&#20854;&#29305;&#24449;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#36890;&#29992;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SosicFL&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#24182;&#21152;&#36895;&#20102;&#20840;&#23616;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item></channel></rss>