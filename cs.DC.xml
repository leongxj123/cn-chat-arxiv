<rss version="2.0"><channel><title>Chat Arxiv cs.DC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DC</description><item><title>AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.10652</link><description>&lt;p&gt;
AutoChunk: &#33258;&#21160;&#28608;&#27963;&#22359;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10652
&lt;/p&gt;
&lt;p&gt;
AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20869;&#23384;&#30340;&#22823;&#37327;&#38656;&#27714;&#65292;&#21253;&#25324;&#21442;&#25968;&#20869;&#23384;&#21644;&#28608;&#27963;&#20869;&#23384;&#65292;&#24050;&#32463;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#21442;&#25968;&#20869;&#23384;&#65292;&#23545;&#28608;&#27963;&#20869;&#23384;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#28608;&#27963;&#20869;&#23384;&#39044;&#35745;&#20250;&#32463;&#21382;&#26174;&#33879;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoChunk&#65292;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#30340;&#20248;&#21270;&#29983;&#25104;&#22359;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#22359;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#22359;&#20505;&#36873;&#39033;&#65292;&#22359;&#36873;&#25321;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#22359;&#36827;&#34892;&#12290;&#36816;&#34892;&#26102;&#65292;AutoChunk&#37319;&#29992;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#24212;&#29992;&#22359;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
&lt;/p&gt;</description></item><item><title>CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.15136</link><description>&lt;p&gt;
CAGRA&#65306;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs. (arXiv:2308.15136v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15136
&lt;/p&gt;
&lt;p&gt;
CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28085;&#30422;&#20102;&#20449;&#24687;&#26816;&#32034;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#20010;&#23398;&#31185;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#31351;&#20030;&#31934;&#30830;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#35745;&#31639;&#25104;&#26412;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#24517;&#39035;&#37319;&#29992;&#36817;&#20284;&#25216;&#26415;&#12290;&#23613;&#31649;&#22270;&#24418;&#21270;&#26041;&#27861;&#30340;&#24179;&#34913;&#24615;&#33021;&#21644;&#21484;&#22238;&#29575;&#22312;ANNS&#31639;&#27861;&#20013;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;GPU&#21644;&#22810;&#26680;&#22788;&#29702;&#22120;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#65292;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21644;&#36890;&#29992;&#35745;&#31639;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#31639;&#30828;&#20214;&#30340;&#26032;&#39062;&#25509;&#36817;&#22270;&#21644;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#30828;&#20214;&#30340;&#39640;&#24615;&#33021;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbor Search (ANNS) plays a critical role in various disciplines spanning data mining and artificial intelligence, from information retrieval and computer vision to natural language processing and recommender systems. Data volumes have soared in recent years and the computational cost of an exhaustive exact nearest neighbor search is often prohibitive, necessitating the adoption of approximate techniques. The balanced performance and recall of graph-based approaches have more recently garnered significant attention in ANNS algorithms, however, only a few studies have explored harnessing the power of GPUs and multi-core processors despite the widespread use of massively parallel and general-purpose computing. To bridge this gap, we introduce a novel parallel computing hardware-based proximity graph and search algorithm. By leveraging the high-performance capabilities of modern hardware, our approach achieves remarkable efficiency gains. In particular, our method s
&lt;/p&gt;</description></item></channel></rss>