<rss version="2.0"><channel><title>Chat Arxiv cs.PF</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.PF</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#26681;&#25454;&#20219;&#21153;&#23545;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16492</link><description>&lt;p&gt;
GPU&#38598;&#32676;&#35843;&#24230;&#23545;&#32593;&#32476;&#25935;&#24863;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPU Cluster Scheduling for Network-Sensitive Deep Learning. (arXiv:2401.16492v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16492
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#26681;&#25454;&#20219;&#21153;&#23545;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;DL&#65288;DDL&#65289;&#24037;&#20316;&#36127;&#36733;&#65292;&#20197;&#22522;&#20110;DDL&#20316;&#19994;&#23545;&#39044;&#26399;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#19968;&#20010;&#32463;&#20856;&#30340;&#24310;&#36831;&#35843;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#20419;&#36827;&#20316;&#19994;&#25918;&#32622;&#21644;&#19968;&#33268;&#24615;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#23545;&#32593;&#32476;&#25935;&#24863;&#30340;&#20316;&#19994;&#25250;&#21344;&#31574;&#30053;&#65307;&#21644;&#65288;iii&#65289;&#19968;&#31181;&#8220;&#33258;&#21160;&#35843;&#25972;&#22120;&#8221;&#26426;&#21046;&#65292;&#29992;&#20110;&#20248;&#21270;&#24310;&#36831;&#35745;&#26102;&#22120;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24310;&#36831;&#35843;&#24230;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;DDL&#38598;&#32676;&#20223;&#30495;&#24179;&#21488;&#12290;&#36890;&#36807;&#20351;&#29992;&#20223;&#30495;&#24179;&#21488;&#65292;&#25105;&#20204;&#22312;&#23454;&#38469;&#24037;&#20316;&#36127;&#36733;&#36319;&#36394;&#20013;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#35774;&#35745;&#30340;&#20248;&#21183;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#35843;&#24230;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#24179;&#22343;j
&lt;/p&gt;
&lt;p&gt;
We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average j
&lt;/p&gt;</description></item></channel></rss>