<rss version="2.0"><channel><title>Chat Arxiv cs.PF</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.PF</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05981</link><description>&lt;p&gt;
&#25506;&#32034;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of In-Browser Deep Learning Inference on Quality of User Experience and Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#36890;&#36807;&#8220;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#8221;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20854;&#20013;DL&#22788;&#29702;&#30452;&#25509;&#22312;Web&#27983;&#35272;&#22120;&#20013;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21450;&#20854;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#36825;&#31181;&#30693;&#35782;&#30340;&#31354;&#30333;&#38656;&#35201;&#26032;&#24418;&#24335;&#30340;QoE&#27979;&#37327;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#25351;&#26631;&#65292;&#22914;&#39029;&#38754;&#21152;&#36733;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#30340;&#39318;&#27425;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#30740;&#31350;&#21253;&#25324;9&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#22312;50&#20010;&#24120;&#29992;&#30340;PC Web&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65306;&#22312;CPU&#19978;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#65292;&#22312;GPU&#19978;&#24930;4.9&#20493;&#12290;&#36825;&#31181;&#24310;&#36831;&#26377;&#20960;&#20010;&#22240;&#32032;&#23548;&#33268;&#65292;&#21253;&#25324;&#26410;&#20805;&#20998;&#20351;&#29992;&#30340;&#30828;&#20214;&#25351;&#20196;&#38598;&#65292;&#22266;&#26377;&#30340;&#24310;&#36831;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is increasingly being integrated into Web applications through a method known as "in-browser inference", where the DL processes occur directly within Web browsers. However, the actual performance of this method and its effect on user experience quality (QoE) is not well-understood. This gap in knowledge necessitates new forms of QoE measurement, going beyond traditional metrics such as page load time. To address this, we conducted the first extensive performance evaluation of in-browser inference. We introduced new metrics for this purpose: responsiveness, smoothness, and inference accuracy.   Our thorough study included 9 widely-used DL models and tested them across 50 popular PC Web browsers. The findings show a significant latency issue with in-browser inference: it's on average 16.9 times slower on CPU and 4.9 times slower on GPU than native inference methods. Several factors contribute to this latency, including underused hardware instruction sets, inherent dela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02750</link><description>&lt;p&gt;
KIVI&#65306;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#30340;&#38750;&#23545;&#31216;2&#20301;&#37327;&#21270;KV&#32531;&#23384;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#30340;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#21644;&#25512;&#26029;&#36895;&#24230;&#21463;&#38480;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#26381;&#21153;&#38656;&#35201;&#23558;&#35768;&#22810;&#35831;&#27714;&#25209;&#37327;&#22788;&#29702;&#20197;&#20943;&#23569;&#27599;&#20010;&#35831;&#27714;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#27880;&#24847;&#21147;&#38190;&#21644;&#20540;&#20197;&#36991;&#20813;&#37325;&#26032;&#35745;&#31639;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#26174;&#33879;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25104;&#20026;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#29942;&#39048;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;GPU&#30340;SRAM&#24517;&#39035;&#20174;&#20027;GPU&#20869;&#23384;&#20013;&#21152;&#36733;&#25972;&#20010;KV&#32531;&#23384;&#20197;&#29983;&#25104;&#27599;&#20010;&#26631;&#35760;&#65292;&#23548;&#33268;&#35745;&#31639;&#26680;&#24515;&#22312;&#27492;&#36807;&#31243;&#20013;&#22788;&#20110;&#31354;&#38386;&#29366;&#24577;&#12290;&#20943;&#23567;KV&#32531;&#23384;&#22823;&#23567;&#30340;&#19968;&#20010;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37327;&#21270;&#65292;&#36890;&#36807;&#20943;&#23569;KV&#32531;&#23384;&#25152;&#38656;&#30340;&#24635;&#23383;&#33410;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;KV&#32531;&#23384;&#20803;&#32032;&#20998;&#24067;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#20102;&#35299;KV&#32531;&#23384;&#37327;&#21270;&#30340;&#38590;&#24230;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#20803;&#32032;&#20998;&#24067;&#30740;&#31350;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
&lt;/p&gt;</description></item></channel></rss>