<rss version="2.0"><channel><title>Chat Arxiv cs.PF</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.PF</description><item><title>MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14361</link><description>&lt;p&gt;
MoE-Infinity&#65306;&#29992;&#20110;&#39640;&#25928;MoE&#26381;&#21153;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#21368;&#36733;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving. (arXiv:2401.14361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14361
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MoE-Infinity&#65292;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26381;&#21153;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#12290;MoE-Infinity&#20855;&#26377;&#24207;&#21015;&#32423;&#19987;&#23478;&#28608;&#27963;&#36861;&#36394;&#30340;&#29305;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#25797;&#38271;&#35782;&#21035;&#31232;&#30095;&#28608;&#27963;&#24182;&#25429;&#25417;MoE&#25512;&#29702;&#30340;&#26102;&#38388;&#23616;&#37096;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#36861;&#36394;&#65292;MoE-Infinity&#25191;&#34892;&#20102;&#26032;&#39062;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#39044;&#21462;&#21644;&#32531;&#23384;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#36890;&#24120;&#19982;&#21368;&#36733;&#19987;&#23478;&#30456;&#20851;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#38598;&#32676;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;MoE-Infinity&#20248;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31995;&#32479;&#21644;&#26041;&#27861;&#65292;&#23545;&#20110;&#21508;&#31181;MoEs&#65292;&#23558;&#24310;&#36831;&#38477;&#20302;&#20102;420&#20493;&#65292;&#23558;&#37096;&#32626;&#25104;&#26412;&#38477;&#20302;&#20102;8&#20493;&#20197;&#19978;&#12290;MoE-Infinity&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/TorchMoE/MoE-Infinity&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
&lt;/p&gt;</description></item></channel></rss>