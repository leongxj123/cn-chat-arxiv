# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning](https://arxiv.org/abs/2403.15048) | 该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。 |
| [^2] | [Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training.](http://arxiv.org/abs/2306.07346) | 本论文提出了一种新的自监督预训练方法MaPeT，不同于现有的使用掩码图像模型的方法，该方法使用自回归和置换预测来捕获图像块内的依赖关系并减少数据噪声的影响，从而提高了下游任务的一致性。 |

# 详细

[^1]: 卡通幻觉检测: 姿势感知上下文视觉学习

    Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning

    [https://arxiv.org/abs/2403.15048](https://arxiv.org/abs/2403.15048)

    该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。

    

    大规模文本到图像（TTI）模型已经成为各种生成领域中生成训练数据的常见方法。然而，视觉幻觉，尤其是在非照片真实风格如卡通人物中包含了感知上关键的缺陷，依然是一个令人担忧的问题。我们提出了一种新颖的用于检测TTI模型生成的卡通角色图像的视觉幻觉检测系统。我们的方法利用了姿势感知上下文视觉学习（PA-ICVL）与视觉语言模型（VLMs），同时利用RGB图像和姿势信息。通过从一个经过微调的姿势估计器中获得姿势指导，我们使VLM能够做出更准确的决策。实验结果表明，在识别视觉幻觉方面，与仅依赖于RGB图像的基线方法相比，取得了显著的改进。这项研究通过减轻视觉幻觉，推动了TTI模型在非照片真实领域的潜力。

    arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
    
[^2]: 学习用于视觉Transformer预训练的掩码和置换视觉令牌。

    Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training. (arXiv:2306.07346v1 [cs.CV])

    [http://arxiv.org/abs/2306.07346](http://arxiv.org/abs/2306.07346)

    本论文提出了一种新的自监督预训练方法MaPeT，不同于现有的使用掩码图像模型的方法，该方法使用自回归和置换预测来捕获图像块内的依赖关系并减少数据噪声的影响，从而提高了下游任务的一致性。

    

    使用自监督预训练技术已成为提高图像分类等视觉任务性能的有前途的方法。最近的方法使用掩码图像模型范式，通过重构与随机掩码图像块相关联的视觉令牌来预训练骨干网络。然而，这种掩蔽方法会在预训练过程中引入噪声进入输入数据，导致性能下降。此外，输入掩蔽忽略了受损块之间的依赖关系，增加了下游微调任务中观察到的不一致性。为了解决这些问题，我们提出了一种新的自监督预训练方法，名为掩蔽和置换视觉变压器（MaPeT），它使用自回归和置换预测来捕获块内依赖性。此外，MaPeT使用辅助位置信息来减少预训练和微调阶段中的差异性。

    The use of self-supervised pre-training has emerged as a promising approach to enhance the performance of visual tasks such as image classification. In this context, recent approaches have employed the Masked Image Modeling paradigm, which pre-trains a backbone by reconstructing visual tokens associated with randomly masked image patches. This masking approach, however, introduces noise into the input data during pre-training, leading to discrepancies that can impair performance during the fine-tuning phase. Furthermore, input masking neglects the dependencies between corrupted patches, increasing the inconsistencies observed in downstream fine-tuning tasks. To overcome these issues, we propose a new self-supervised pre-training approach, named Masked and Permuted Vision Transformer (MaPeT), that employs autoregressive and permuted predictions to capture intra-patch dependencies. In addition, MaPeT employs auxiliary positional information to reduce the disparity between the pre-trainin
    

